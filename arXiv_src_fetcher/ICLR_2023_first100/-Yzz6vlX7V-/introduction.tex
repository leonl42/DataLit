\section{Introduction}

Although the successes of deep neural networks in \textit{natural language processing} (NLP) are astounding and undeniable, they are still regularly criticised for lacking the powerful generalisation capacities that characterise human intelligence.
A frequently mentioned concept in such critiques is \emph{compositionality}: the ability to build up the meaning of a complex expression by combining the meanings of its parts \citep[e.g.][]{partee1984compositionality}.
Compositionality is assumed to play an essential role in how humans understand language, but whether neural networks also exhibit this property has since long been a topic of vivid debate \citep[e.g.][]{fodor1988connectionism,smolensky1990tensor,marcus2003algebraic,nefdt2020puzzle}.

Studies about the compositional abilities of neural networks consider almost exclusively models trained on synthetic datasets, in which compositionality can be ensured and isolated \citep[e.g.][]{lake2018generalization,hupkes2020compositionality}.\footnote{
Apart from \citet{raunak2019compositionality}, work on compositionality and `natural' language considers highly structured subsets of language \citep[e.g.][]{kim2020cogs,keysers2019measuring}.}
In such tests, the interpretation of expressions is computed completely \emph{locally}: every subpart is evaluated independently -- without taking into account any external context -- and the meaning of the whole expression is then formed by combining the meanings of its parts in a bottom-up fashion.
This protocol matches the type of compositionality observed in arithmetic: the meaning of $(3 + 5)$ is always $8$, independent of the context it occurs in.

However, as exemplified by the sub-par performance of symbolic models that allow only strict, local protocols, compositionality in natural domains is far more intricate than this rigid, arithmetic-like variant of compositionality.
Natural language seems very compositional, but at the same time, it is riddled with cases that are difficult to interpret with a strictly local interpretation of compositionality.
Sometimes, the meaning of an expression does not derive from its parts (e.g. for idioms), but the parts themselves are used compositionally in other contexts.
In other cases, the meaning of an expression does depend on its parts in a compositional way, but arriving at this meaning requires a more \textit{global} approach because the meanings of the parts need to be disambiguated by information from elsewhere.
For instance, consider the meaning of homonyms (``these dates are perfect for our dish/wedding''), potentially idiomatic expressions (``the child kicked the bucket off the pavement''), or scope ambiguities (``every human likes a cat'').
This paradoxical tension between local and global forms of compositionality inspired many debates on the compositionality of natural language.
Likewise, it impacts the evaluation of compositionality in NLP models.
On the one hand, local compositionality seems necessary for robust and reliable generalisation.
Yet, at the same time, global compositionality is needed to appropriately address the full complexity of language, which makes evaluating compositionality of state-of-the-art models `in the wild' a complicated endeavour.

In this work, we face this challenge head-on.
We concentrate on the domain of \textit{neural machine translation} (NMT), which is paradigmatically close to the tasks typically considered for compositionality tests, where the target represents the `meaning' of the input.\footnote{E.g. SCAN's inputs are instructions (\exa{walk twice}) with executions as outputs (\exa{walk walk}) \citep{lake2018generalization}.}
Furthermore, MT is an important domain of NLP, for which compositional generalisation is important to produce more robust translations and train adequate models for low-resource languages \citep[see, e.g.][]{chaabouni2021can}.
As an added advantage, compositionality is traditionally well studied and motivated for MT \citep{rosetta1994rosetta,janssen1997compositionality,janssen1998algebraic}.

We reformulate three theoretically grounded tests from \citet{hupkes2020compositionality}: \emph{systematicity}, \emph{substitutivity} and \emph{overgeneralisation}.
Since accuracy -- commonly used in artificial compositionality tests -- is not a suitable evaluation metric for MT, we base our evaluations on the extent to which models behave \emph{consistently}, rather than correctly.
In our tests for systematicity and substitutivity, we consider whether processing is \emph{local}; in our overgeneralisation test, we consider how models treat idioms that are assumed to require \emph{global} processing.

Our results indicate that models often do not behave compositionally under the local interpretation, but exhibit behaviour that is \emph{too local} in other cases.
In other words, models have the ability to process phrases both locally and globally but do not always correctly modulate between them.
We further show that some inconsistencies reflect variation in natural language, whereas others are true \emph{compositional mistakes}, exemplifying the need for both local and global compositionality as well as illustrating the need for tests that encompass them both.

With our study, we contribute to ongoing questions about the compositional abilities of neural networks, and we provide nuance to the nature of this question when natural language is concerned: how local should the compositionality of models for natural language actually be?
Aside from an empirical study, our work is also a call to action: we should rethink the evaluation of compositionality in neural networks and develop benchmarks using \emph{real} data to evaluate compositionality on natural language, where composing meaning is not as straightforward as doing the math.
