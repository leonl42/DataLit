\section{Local and global compositionality}
\label{sec:LoC}

Tests for compositional generalisation in neural networks typically assume an arithmetic-like version of compositionality, in which meaning can be computed bottom up.
The compositions require only local information -- they are context independent and unambiguous:  \exa{walk twice after jump thrice} \citep[a fragment from SCAN by][]{lake2018generalization} is evaluated similarly to $(2 + 1) \times (4 - 5)$.
In MT, this type of compositionality would imply that a change in a word or phrase should affect only the translation of that word or phrase, or at most the smallest constituent it is a part of.
For instance, the translation of \exa{the girl} should not change depending on the verb phrase that follows it, and in the translation of a conjunction of two sentences, making a change in the first conjunct should not change the translation of the second.
While translating in such a local way seems robust and productive, it is not always realistic -- e.g.\ consider the translation of 
\exa{dates} in \exa{She hated bananas and she liked dates}.\looseness=-1

In linguistics and philosophy of language, the \textit{level} of compositionality has been widely discussed, which led to a variety of definitions.
One of the most well-known ones is from \citet[][]{partee1984compositionality}:

\vspace{-.5mm}
\begin{quote}
``The meaning of a compound expression is a function of the meanings of its parts and of the way they are syntactically combined.''\footnote{This straightforwardly extends to translation, by replacing \emph{meaning} with \emph{translation} \citep{rosetta1994rosetta}.}
\end{quote}

\vspace{-.5mm}
\noindent This definition hardly places restrictions on the relationship between expressions and their parts.
The type of function that relates them is unspecified and could take into account the global syntactic structure or external arguments, and the parts' meanings can depend on global information.
\citeauthor{partee1984compositionality}'s definition is therefore called \textit{weak}, \textit{global}, or \textit{open} compositionality \citep{szabo2012case, garcia2019open}.
When, instead, the meaning of a compound depends only on the meanings of its largest parts, regardless of their internal structure (similar to arithmetic),
that is \emph{strong}, \emph{local} or \textit{closed} compositionality \citep{jacobson2002dis, szabo2012case}.
Under the local interpretation, natural language can hardly be considered compositional -- many frequent phenomena such as homonyms, idioms and scope ambiguities cannot be resolved locally \citep{pagin2010compositionality, pavlick2016most}.
The global interpretation handles such cases straightforwardly but does not match up with many a person's intuitions about the compositionality of language.
After all, how useful is compositionality if composing the meanings of parts requires the entire rest of the sentence?
This paradox inspired debates on the compositionality of natural language and is also highly relevant in the context of evaluating compositionality in neural models.

Previous compositionality tests (\S\ref{subsec:related_work}) considered only the local interpretation of compositionality, but to what extent is that relevant given the type of compositionality actually required to model natural language?
Here, we aim to open up the discussion about what it means for computational models of language to be compositional by considering properties that require composing meaning locally as well as globally and evaluating them in models trained on unadapted natural language corpora.
