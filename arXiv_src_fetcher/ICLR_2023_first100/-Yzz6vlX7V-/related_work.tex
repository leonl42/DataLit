\section{Related work}
\label{subsec:related_work}

In previous work, a variety of artificial tasks have been proposed to evaluate compositional generalisation using non-i.i.d.\ test sets that are designed to assess a specific characteristic of compositional behaviour.
Examples are \emph{systematicity} \citep{lake2018generalization, bastings2018jump, hupkes2020compositionality}, \emph{substitutivity} \citep{mul2019siamese,hupkes2020compositionality}, \emph{localism} \citep{hupkes2020compositionality,saphra-lopez-2020-lstms}, \emph{productivity} \citep{lake2018generalization} or \emph{overgeneralisation} \citep{korrel2019transcoding,hupkes2020compositionality,dankers-etal-2021-generalising}. 
Generally, neural models struggle to generalise in such evaluation setups.

There are also studies that consider compositional generalisation on more natural data.
Such studies typically focus on either MT \citep{lake2018generalization,raunak2019compositionality,li2021compositional} or semantic parsing \citep{finegan2018improving,keysers2019measuring,kim2020cogs,shaw-etal-2021-compositional}.
Most of these studies consider small and highly controlled subsets of natural language.

Instead, we focus on models trained on fully natural MT datasets, which we believe to be the setup for compositionality evaluation that does most justice to the complexity of natural language: contrary to semantic parsing, where the outputs are structures created by expert annotators, in translation both inputs and outputs are fully-fledged natural language sentences.
To the best of our knowledge, the only attempt to explicitly measure compositional generalisation of NMT models trained on large natural MT corpora is the study presented by \citet{raunak2019compositionality}.
They measure productivity -- generalisation to longer sentence lengths -- of an LSTM-based NMT model trained on a full-size, natural MT dataset.
Other studies using NMT, instead, consider toy datasets generated via templating \citep{lake2018generalization} or focus on short sentences excluding more complex constructions that contribute to the complexity of natural language for compositional generalisation, such as polysemous words or metaphors \citep{li2021compositional}.
