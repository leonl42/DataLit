\onecolumn
\section{Semi-natural templates}
\label{ap:ddop}

The semi-natural data that we use in our test sets is generated with the library \texttt{DiscoDOP},\footnote{\url{https://github.com/andreasvc/disco-dop}} developed for data-oriented parsing \citep{vancranenburgh2016disc}. 
We generate the data with the following seven step process:

\begin{enumerate}[wide, labelwidth=!, labelindent=0pt, itemsep=0pt, parsep=3pt]
\item[\textbf{Step 1.}] Sample 100k English OPUS sentences. 
\item[\textbf{Step 2.}] Generate a treebank using the disco-dop library and the \texttt{discodop parser en\_ptb} command. The library was developed for discontinuous data-oriented parsing. Use the library's \texttt{--fmt bracket} to turn off discontinuous parsing.
\item[\textbf{Step 3.}] Compute tree fragments from the resulting treebank (\texttt{discodop fragments}). These tree fragments are the building blocks of a Tree-Substitution Grammar.
\item[\textbf{Step 4.}] We assume the most frequent fragments to be common syntactic structures in English. To construct complex test sentences, we collect the 100 most frequent fragments containing at least 15 non-terminal nodes for NPs and VPs.
\item[\textbf{Step 5.}] Selection of three VP and five NP fragments to be used in our final semi-natural templates. These structures are selected through qualitative analysis for their diversity.
\item[\textbf{Step 6.}] Extract sentences matching the eight fragments (\texttt{discodop treesearch}).
\item[\textbf{Step 7.}] Create semi-natural sentences by varying one lexical item and varying the matching NPs and VPs retrieved in Step 6.
\end{enumerate}

In Table~\ref{tab:semi_natural_full}, we provide examples for each of the ten templates used, along with the internal structure of the complex NP or VP that is varied in the template.
In Table~\ref{tab:synthetic_data_appendix}, we provide some additional examples for our ten synthetic templates.

\begin{table}[!hb]
\centering\small
\begin{tabular}{lll}
\toprule
$n$& \textbf{Template} \\ \midrule\midrule
1  & The \bl{N}\SPSB{}{people} \textcolor{purple}{(VP (TO ) (VP (VB ) (NP (NP ) (PP (IN ) (NP (NP ) (PP (IN ) (NP )))))))} \\
   & \textit{E.g. The woman wants to use the Internet as a means of communication .}\\
2  & The \bl{N}\SPSB{}{people} \textcolor{purple}{(VP (VBP ) (VP (VBG ) (S (VP (TO ) (VP (VB ) (S (VP (TO ) (VP )))))))))} \\
   & \textit{E.g. The men are gon na have to move off-camera .}\\
3  & The \bl{N}\SPSB{}{people} \textcolor{purple}{(VP (VB ) (NP (NP ) (PP (IN ) (NP ))) (PP (IN ) (NP (NP ) (PP (IN ) (NP )))))} \\
   & \textit{E.g. The doctors retain 10 \% of these amounts by way of collection costs .} \\
4  & The \bl{N}\SPSB{}{people} reads an article about \textcolor{purple}{(NP (NP ) (PP (IN ) (NP (NP ) (PP (IN ) (NP (NP ) (PP (IN ) (NP )))))))} \\
   & \textit{E.g. The friend reads an article about the development of ascites in rats with liver cirrhosis .} \\
5  & The \bl{N}\SPSB{}{people} reads an article about \textcolor{purple}{(NP (NP (DT ) (NN )) (PP (IN ) (NP (NP ) (SBAR (S (WHNP (WDT )) (VP ))))))} . \\ 
   & \textit{E.g. The teachers read an article about the degree of progress that can be achieved by the industry .} \\
6  & An article about \textcolor{purple}{(NP (NP ) (PP (IN ) (NP (NP ) (PP (IN ) (NP (NP ) (PP (IN ) (NP )))))))} is read by the \bl{N}\SPSB{}{people} . \\
   & \textit{E.g. An article about the inland transport of dangerous goods from a variety of Member States is read by the lawyer .} \\
7  & An article about \textcolor{purple}{(NP (NP ) (PP (IN ) (NP (NP ) (, ,) (SBAR (S (WHNP (WDT )) (VP ))))))} , is read by the \bl{N}\SPSB{}{people} . \\
   & \textit{E.g. An article about the criterion on price stability , which was 27 \% , is read by the child .} \\
8 & Did the \bl{N}\SPSB{}{people} hear about \textcolor{purple}{(NP (NP ) (PP (IN ) (NP (NP ) (PP (IN ) (NP (NP ) (PP (IN ) (NP )))))))} . \\
   & \textit{E.g. Did the friend hear about an inhospitable fringe of land on the shores of the Dead Sea ?} \\
9 & Did the \bl{N}\SPSB{}{people} hear about \textcolor{purple}{(NP (NP (DT ) (NN )) (PP (IN ) (NP (NP ) (SBAR (S (WHNP (WDT )) (VP ))))))} ? \\
   & \textit{E.g. Did the teacher hear about the march on Employment which happened here on Sunday ?} \\
10 & Did the \bl{N}\SPSB{}{people} hear about \textcolor{purple}{(NP (NP ) (SBAR (S (VP (TO ) (VP (VB ) (NP (NP ) (PP (IN ) (NP ))))))))} ? \\
   & \textit{E.g. Did the lawyers hear about a qualification procedure to examine the suitability of the applicants ?}  \\
\bottomrule
\end{tabular}
\caption{Semi-natural data templates along with their identifiers ($n$). The syntactic structures for noun and verb phrases in purple are instantiated with data from the OPUS collection. 
Generated data from every template contains varying sentence structures and varying tokens but the predefined tokens in black remain the same.}
\label{tab:semi_natural_full}
\end{table}


\clearpage
\begin{table}[!h]
\small
\centering
\begin{tabular}{lll}
\toprule
$n$ & \textbf{Template} \\ \midrule\midrule
1   & The \bl{N}\SPSB{}{people} \bl{V}\SPSB{}{transitive} the \bl{N}\SPSB{sl}{people} . \\
    & \textit{E.g. The poet criticises the king .} \\
2   & The \bl{N}\SPSB{}{people} \bl{Adv} \bl{V}\SPSB{}{transitive} the \bl{N}\SPSB{sl}{people} . \\ 
    & \textit{E.g. The victim carefully observes the queen .}\\
3   & The \bl{N}\SPSB{}{people} \bl{P} the \bl{N}\SPSB{sl}{vehicle} \bl{V}\SPSB{}{transitive} the \bl{N}\SPSB{sl}{people} . \\ 
    & \textit{E.g. The athlete near the bike observes the leader .} \\
4   & The \bl{N}\SPSB{}{people} and the \bl{N}\SPSB{}{people} \bl{V}\SPSB{pl}{transitive} the \bl{N}\SPSB{sl}{people} . \\
    & \textit{E.g. The poet and the child understand the mayor .} \\
5   & The \bl{N}\SPSB{sl}{quantity} of \bl{N}\SPSB{pl}{people} \bl{P} the \bl{N}\SPSB{sl}{vehicle} \bl{V}\SPSB{sl}{transitive} the \bl{N}\SPSB{sl}{people} . \\
    & \textit{E.g. The group of friends beside the bike forgets the queen .} \\
6   & The \bl{N}\SPSB{}{people} \bl{V}\SPSB{}{transitive} that the \bl{N}\SPSB{pl}{people} \bl{V}\SPSB{pl}{intransitive}. \\
    & \textit{E.g. The farmer sees that the lawyers cry .} \\
7   & The \bl{N}\SPSB{}{people} \bl{Adv} \bl{V}\SPSB{}{transitive} that the \bl{N}\SPSB{pl}{people} \bl{V}\SPSB{pl}{intransitive} . \\
    & \textit{E.g. The mother probably thinks that the fathers scream .} \\
8   & The \bl{N}\SPSB{}{people} \bl{V}\SPSB{}{transitive} that the \bl{N}\SPSB{pl}{people} \bl{V}\SPSB{pl}{intransitive} \bl{Adv} . \\
    & \textit{E.g. The mother thinks that the fathers scream carefully .} \\
9   & The \bl{N}\SPSB{}{people} that \bl{V}\SPSB{}{intransitive} \bl{V}\SPSB{}{transitive} the \bl{N}\SPSB{sl}{people} . \\
    & \textit{E.g. The poets that sleep understand the queen .} \\
10  & The \bl{N}\SPSB{}{people} that \bl{V}\SPSB{}{transitive} \bl{Pro} \bl{V}\SPSB{sl}{transitive} the \bl{N}\SPSB{sl}{people} . \\
    & \textit{E.g. The mother that criticises him recognises the queen .} \\
    \bottomrule
    \end{tabular}
    \captionof{table}{Synthetic sentence templates similar to \citet{lakretz2019emergence}, along with their identifiers ($n$). }
    \label{tab:synthetic_data_appendix}
\vspace{-0.3cm}
\end{table}


\section{Systematicity}
\label{ap:systematicity}

Table~\ref{tab:systematicity_appendix} provides the numerical counterparts of the results visualised in Figure~\ref{fig:systematicity}.

\begin{table*}[!h]
    \centering\small\setlength{\tabcolsep}{4pt}
    \begin{subtable}[b]{0.49\textwidth}
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Data} &     \textbf{Condition} & \multicolumn{3}{c}{\textbf{Model}} \\
    & & small & medium & full \\
    \midrule\midrule
    \texttt{S\;$\rightarrow$\;NP\;VP} \\
    synthetic & NP & .73 & .84 & .84 \\
    synthetic & VP & .76 & .87 & .88 \\
    semi-natural & NP & .63 & .66 & .64 \\ \midrule
    \texttt{S\;$\rightarrow$\;S\;CONJ\;S} \\
    synthetic & $\text{S}^\prime_1$ & .81 & .90 & .92 \\
    synthetic & $\text{S}_3$ & .53 & .76 & .82 \\
    semi-natural & $\text{S}^\prime_1$ & .65 & .73 & .76 \\
    semi-natural & $\text{S}_3$ & .29 & .49 & .49 \\
    natural & $\text{S}^\prime_1$ & .58 & .67 & .72 \\
    natural & $\text{S}_3$ & .25 & .39 & .47 \\
    \bottomrule
    \end{tabular}
    \caption{Per models' training set size}
    \end{subtable}
    \begin{subtable}[b]{0.49\textwidth}
    \centering\small\setlength{\tabcolsep}{4pt}
    \begin{tabular}{cccccccccc}
    \toprule
    \multicolumn{10}{c}{\textbf{Template}} \\
    1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    \midrule\midrule
    \\
    .86 & .74 & .85 & .87 & .75 & .89 & .85 & .85 & .70 & .68 \\
    .92 & .73 & .90 & .91 & .84 & .88 & .85 & .82 & .77 & .74 \\
    .66 & .63 & .65 & .70 & .64 & .69 & .63 & .63 & .60 & .58 \\ \midrule \\
    .91 & .82 & .88 & .88 & .86 & .95 & .90 & .91 & .84 & .79 \\
    .75 & .54 & .72 & .66 & .73 & .88 & .74 & .81 & .66 & .55 \\
    .73 & .75 & .75 & .80 & .75 & .73 & .66 & .68 & .64 & .64 \\
    .50 & .50 & .51 & .58 & .52 & .43 & .35 & .31 & .28 & .29 \\
    .67 & .74 & .65 & .64 & .63 & .64 & .62 & .66 & .63 & .66 \\
    .39 & .49 & .35 & .35 & .34 & .37 & .33 & .38 & .34 & .38 \\
    \bottomrule
    \end{tabular}
    \caption{Per template}
    \end{subtable}
    \caption{Consistency scores for the systematicity experiments, detailed per experimental setup and evaluation data type. We provide scores (a) per models' training set size, and (b) per template of our generated evaluation data. For natural data, the template number is meaningless, apart from the fact that it determines sentence length and word frequency.}
    \label{tab:systematicity_appendix}
\end{table*}


\section{Substitutivity}
\label{ap:substitutivity}

\paragraph{Synonyms employed} In Table~\ref{tab:freqs_substitutivity_appendix}, we provide some information about the synonymous word pairs used in the substitutivity test, including their frequency in OPUS and their most common Dutch translation. 
The last column of the table contains the subordinate clauses that we used to include the synonyms in the synthetic and semi-natural data. 
We include them as a relative clause behind nouns representing a human, such as ``The poet criticises the king that eats the doughnut''.

\paragraph{Detecting synonym translations}
To find the span of text in the translation which is the translation of the synonym, we apply a relatively simple heuristic.
We generate a number of short sentences such as ``This is the NOUN'', feed those to all our trained models, and extract the top-5 answers in the beam.
We then use the list of all words resulting from this protocol -- which we manually checked -- to find synonym translations in the model output.

\paragraph{Results} In the main paper, Figures~\ref{fig:substitutivity} and~\ref{fig:per_synonym} provided the consistency scores for the substitutivity tests.
Here, Table~\ref{tab:substitutivity_appendix} further details the results from the figure, by presenting the average consistency per evaluation data type and training set size, and per evaluation data type and synonym pair.

\noindent\begin{minipage}[b]{0.99\textwidth}
\vspace{1cm}\small\centering
\begin{tabular}{llllll}
    \toprule
    \multicolumn{4}{l}{\textbf{Synonym pair}} & \textbf{Dutch translation} & \textbf{Subordinate clause} \\
    \textit{British} & \textit{Freq.} & \textit{American} & \textit{Freq.} \\\midrule \midrule
    aeroplane & 6728 & airplane & 5403 & vliegtuig & that travels by \dots \\
    aluminium & 17982 & aluminum & 5700 & aluminium & that sells \dots \\
    doughnut & 2014 & donut & 1889 & donut & that eats the \dots \\
    foetus & 1943 & fetus & 1878 & foetus & that researches the \dots \\
    flautist & 112 & flutist & 101 & fluitist & that knows the \dots  \\
    moustache & 1132 & mustache & 1639 & snor & that has a \dots \\
    tumour & 7338 & tumor & 6348 & tumor & that has a \dots \\
    pyjamas & 808 & pajamas & 1106 & pyjama & that wears \dots \\
    sulphate & 3776 & sulfate & 1143 & zwavel & that sells \dots  \\
    yoghurt & 1467 & yogurt & 2070 & yoghurt & that eats the \dots \\
    aubergine & 765 & eggplant & 762 & aubergine & that eats the \dots \\
    shopping trolley & 217 & shopping cart & 13366 & winkelwagen & that uses a \dots \\
    veterinary surgeon & 941 & veterinarian & 6995 & dierenarts & that knows the \dots \\
    sailing boat & 5097 & sailboat & 1977 & zeilboot & that owns a \dots \\
    football & 33125 & soccer & 6841 & voetbal & that plays \dots \\
    holiday & 125430 & vacation & 23532 & vakantie & that enjoys the \dots \\
    ladybird & 235 & ladybug & 303 & lieveheersbeestje & that caught a \dots \\
    theatre & 19451 & theater & 13508 & theater & that loves \dots \\
    postcode & 479 & zip code & 1392 & postcode & with the same \dots \\
    whisky & 3604 & whiskey & 4313 & whisky & that drinks \dots \\
    \bottomrule
\end{tabular}
\captionof{table}{Synonyms for the substitutivity test, along with their OPUS frequency, Dutch translation, and the subordinate clause used to insert them in the data.}
\label{tab:freqs_substitutivity_appendix}
\end{minipage}

\begin{table*}[!h]
    \centering\small\setlength{\tabcolsep}{4pt}
    \begin{subtable}[b]{\textwidth}\centering
    \begin{tabular}{llccc}
    \toprule
    \textbf{Data} &     \textbf{Metric} & \multicolumn{3}{c}{\textbf{Model}} \\
    & & small & medium & full \\
    \midrule\midrule
    synthetic    & con.       & .49  & .67  & .76  \\
                 & syn. con.  & .67  & .82  & .93  \\
    semi-natural & con.       & .34  & .55  & .62  \\
                 & syn. con.  & .62  & .84  & .93  \\
    natural      & con.       & .37  & .52  & .63  \\
                 & syn. con.  & .61  & .75  & .85  \\    \bottomrule
    \end{tabular}
    \caption{Per models' training set size}
    \end{subtable}
    \begin{subtable}[b]{\textwidth}\setlength{\tabcolsep}{3pt}\centering
    \begin{tabular}{llcccccccccccccccccccc}
    \toprule
    \textbf{Data} &     \textbf{Metric} & \multicolumn{20}{c}{\textbf{Synonym}} \\
& & \rotatebox{90}{aeroplane} & \rotatebox{90}{aluminium} & \rotatebox{90}{doughnut} & \rotatebox{90}{foetus} & \rotatebox{90}{flautist} & \rotatebox{90}{moustache} & \rotatebox{90}{tumour} & \rotatebox{90}{pyjamas} & \rotatebox{90}{sulphate} & \rotatebox{90}{yoghurt} & \rotatebox{90}{aubergine} & \rotatebox{90}{shopping trolley} & \rotatebox{90}{veterinary surgeon} & \rotatebox{90}{sailing boat} & \rotatebox{90}{football} & \rotatebox{90}{holiday} & \rotatebox{90}{ladybird} & \rotatebox{90}{theatre} & \rotatebox{90}{postcode} & \rotatebox{90}{whisky} \\
    \midrule\midrule
synthetic & con.    & .54  & .87  & .74  & .82  & .10  & .92  & .78  & .64  & .79  & .55  & .25  & .40  & .64  & .73  & .68  & .81  & .27  & .85  & .48  & .88  \\
 & syn. con.        & 1.0  & 1.0  & .87  & 1.0  & .10  & 1.0  & 1.0  & .80  & .95  & 1.0  & .38  & .48  & .90  & 1.0  & .75  & 1.0  & .40  & .99  & .53  & 1.0  \\
semi-natural & con. & .43  & .59  & .58  & .54  & .08  & .85  & .52  & .55  & .56  & .42  & .24  & .31  & .33  & .73  & .66  & .71  & .20  & .62  & .43  & .75  \\
 & syn. con.        & .99  & .99  & .83  & 1.0  & .09  & 1.0  & .98  & .72  & .90  & .98  & .40  & .50  & .77  & 1.0  & .90  & 1.0  & .38  & .95  & .58  & .99  \\
natural & con.      & .50  & .52  & .53  & .56  & .09  & .75  & .50  & .60  & .47  & .57  & .23  & .70  & .29  & .64  & .55  & .62  & .17  & .59  & .61  & .58  \\
 & syn. con.        & .89  & .85  & .73  & .91  & .11  & .87  & .87  & .82  & .88  & .86  & .32  & .92  & .75  & .71  & .79  & .81  & .27  & .82  & .81  & .80  \\    \bottomrule
    \end{tabular}
    \caption{Per synonym}
    \end{subtable}
    \caption{Consistency scores for the substitutivity experiments, detailed per evaluation data type. We present scores (a) per models' training set size and (b) per synonym.}
    \label{tab:substitutivity_appendix}
\end{table*}

\clearpage

\section{Global compositionality}
\label{ap:global_compositionality}

\paragraph{Idioms employed}
Table~\ref{tab:overgeneralisation_appendix} provides more information on the idioms used in our global compositionality test. 
In the first column, we list all idioms we used, along with the \emph{keywords} that we used to determine if their translation is local or not.
To extract the natural data, we retrieved exact matches with OPUS source sentences. 
The idioms' keywords are mostly nouns that either translate into a different word in an accurate paraphrased translation in Dutch (e.g. ``across the \textbf{board}'' would be ``over de hele linie''), or should disappear in the translation (e.g. ``do the right \textbf{thing}'' typically translates into ``het juiste doen'' in the corpus). 

In the second column of Table~\ref{tab:overgeneralisation_appendix}, we list the subordinate clauses that we used to include idioms in the synthetic and semi-natural data.
The clauses themselves are drawn from source sentences in OPUS.
To incorporate them in synthetic and semi-natural sentences, we include them as a relative clause behind nouns representing a human, by attaching \emph{``that said `[idiom]'''}. 
For instance: ``The poet criticises the king that said `Have you gone out of your mind'.''

In the third column of Table~\ref{tab:overgeneralisation_appendix}, we show local translations of the idioms, elicited from the model by embedding the idiom in a string of ten random nouns. 
Even ``out of the blue'', which is rarely overgeneralised when presented in synthetic, semi-natural or natural contexts, is locally translated. 
This indicates that the idiom is not stored as one lexical unit per se but that it is only translated globally in specific contexts.

\paragraph{Results}
In the main paper, in Figure~\ref{fig:global_compositionality}, we visualised how overgeneralisation changes over the course of training, averaged over idioms.
In Table~\ref{tab:global_compositionality_appendix}, we detail the maximum overgeneralisation observed per idiom.

\vspace{1cm}
\noindent\begin{minipage}[t]{\textwidth}
\small
\centering
\resizebox{0.8\textwidth}{!}{\begin{tabular}{lll}
\toprule
\textbf{Idiom} & \textbf{Subordinate clause} & \textbf{Local translation} \\ \midrule\midrule
\underline{once} in a \underline{while} & that said `` I will play it once in a while " & eens in een tijdje \\
do the right \underline{thing} & that said `` Just do the right thing " & doen het juiste ding \\
out of your \underline{mind} & that said `` Have you gone out of your mind " & uit je hoofd \\
\underline{state} of the \underline{art} & that said `` This is a state of the art, official facility " & stand van de kunst \\
from \underline{scratch} & that said `` We are cooking from scratch every day " & van kras\\
take \underline{stock} & that said `` Take stock of the lessons to be drawn " & nemen voorraad  \\
across the \underline{board} & that said `` I got red lights all across the board " & aan boord\\
in the final \underline{analysis} & that said `` In the final analysis, this is what matters " & in de laatste analyse\\
out of the \underline{blue} & that said `` It just came out of the blue " & uit het blauwe \\
in \underline{tandem} & that said `` We will work with them in tandem " & in tandem \\
by \underline{heart} & that said `` I knew the formula by heart " & door hart \\
come to \underline{terms} with & that said `` I have come to terms with my evil past " & komen overeen met \\
by the same \underline{token} & that said `` By the same token I will oppose what is evil " & bij dezelfde token \\
at your \underline{fingertips} & that said `` The answer is right at your fingertips " & binnen handbereik \\
look the other \underline{way} & that said `` We cannot look the other way either " & kijken de andere manier \\
follow \underline{suit} & that said `` And many others follow suit " & volgen pak \\
keep \underline{tabs} on & that said `` I keep tabs on you " & houden tabs \\
in the short \underline{run} & that said `` In the short run it clearly must be " & in de korte lopen \\
by \underline{dint} of & that said `` We are part of it by dint of our commitment " & door de int \\
set \underline{eyes} on & that said `` I wish I had never set eyes on him " & set ogen op \\

\bottomrule
\end{tabular}}
\captionof{table}{Idioms used in the overgeneralisation test. 
    The words that are indicative of a local translation are underlined, we check for their presence to label a translation as an overgeneralisation.
    The listed subordinate clauses are used to insert the idioms into synthetic and semi-natural templates.
    The local translation indicated is the translation given by the model when the idiom is embedded in a string of ten random words.
    }
\label{tab:overgeneralisation_appendix}
\end{minipage}


\begin{table*}[!ht]
    \centering\small\setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{llcccccccccccccccccccc}
    \toprule
    \textbf{Data} & \textbf{Model} & \multicolumn{20}{c}{\textbf{Idiom}} \\
    & & \rotatebox{90}{once in a while} &
        \rotatebox{90}{do the right thing} & 
        \rotatebox{90}{out of your mind} & 
        \rotatebox{90}{state of the art} &
        \rotatebox{90}{from scratch} & 
        \rotatebox{90}{take stock} & 
        \rotatebox{90}{across the board} & 
        \rotatebox{90}{in the final analysis} &
        \rotatebox{90}{out of the blue} &
        \rotatebox{90}{in tandem} &
        \rotatebox{90}{by heart} & 
        \rotatebox{90}{come to terms with} & 
        \rotatebox{90}{by the same token} & 
        \rotatebox{90}{look the other way} & 
        \rotatebox{90}{at your fingertips} & 
        \rotatebox{90}{follow suit} & 
        \rotatebox{90}{keep tabs on} & 
        \rotatebox{90}{in the short run} & 
        \rotatebox{90}{by dint of} & 
        \rotatebox{90}{set eyes on} \\
    \midrule \midrule
    synthetic & small   & .98 & .92 & .98 & 1.0 & .40 & .75 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & .01 & 1.0 & 1.0 & 1.0 & .99 & 1.0 & .72 & .20 & .74 \\
     & medium           & .99 & .96 & .98 & 1.0 & .76 & .73 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & .22 & 1.0 & 1.0 & 1.0 & 1.0 & .57 & .55 & .38 & .57 \\
     & full             & .97 & .86 & .97 & 1.0 & .50 & .56 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & .24 & 1.0 & .91 & 1.0 & 1.0 & .74 & .38 & .24 & .44 \\
    semi-natural & small& .95 & .66 & .98 & 1.0 & .49 & .73 & 1.0 & 1.0 & 1.0 & .97 & 1.0 & .08 & 1.0 & .98 & 1.0 & .88 & .99 & .56 & .15 & .81 \\
     & medium           & .91 & .60 & .95 & 1.0 & .78 & .63 & .96 & 1.0 & 1.0 & .97 & 1.0 & .31 & .99 & .99 & 1.0 & .97 & .74 & .45 & .30 & .59 \\
     & full             & .97 & .55 & .95 & 1.0 & .40 & .68 & .99 & 1.0 & 1.0 & .99 & 1.0 & .31 & 1.0 & .90 & 1.0 & .97 & .90 & .25 & .23 & .47 \\
    natural & small     & .80 & .51 & .80 & .97 & .84 & .31 & .75 & .96 & .92 & .82 & .88 & .14 & .74 & .60 & 1.0 & .40 & .96 & .29 & .23 & .87 \\
     & medium           & .80 & .50 & .82 & .96 & .84 & .32 & .71 & .94 & .92 & .68 & .90 & .22 & .74 & .63 & .99 & .39 & .61 & .33 & .29 & .84 \\
     & full             & .79 & .39 & .83 & .95 & .90 & .36 & .83 & .98 & .95 & .89 & .90 & .11 & .65 & .55 & 1.0 & .65 & .56 & .19 & .27 & .76 \\
 \bottomrule
    \end{tabular}
    \caption{Maximum overgeneralisation observed over the course of training, per evaluation data type, training set size and idiom.}
    \label{tab:global_compositionality_appendix}
\end{table*}



\section{Reproducibility details}\label{app:reproducibility}

\subsection{Data}

\paragraph{Training data} Our training data consists of the English-Dutch subset of the MT corpus \textsc{OPUS} \citep{tiedemann2020opus}, provided by \citet{tiedemann-2020-tatoeba}.
This data contains in total 69M source-target pairs.
The data can be found on
\url{https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/data/README-v2020-07-28.md}.

\paragraph{Preprocessing}
We tokenise the data using the tokenisation script\footnote{\url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl}} from the SMT library Moses.\footnote{\url{https://github.com/moses-smt/mosesdecoder}}
Following the number of subwords suggested by \citet{tiedemann-2020-tatoeba}, we generate a subword vocabulary applying 60k BPE merge-operations.
To do so, we use the \texttt{learn\_bpe.py} script provided in the \textsc{subword\_nmt}\footnote{\url{https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/learn_bpe.py}} repository hosted by Rico Sennrich.

\paragraph{Different corpora}
We train models on three different sizes of corpora: \textsc{small}, \textsc{medium} and \textsc{full}.
To generate these corpora, we first shuffle the OPUS training data using the bash function \texttt{shuffle}.
To generate the \textsc{small} and \textsc{medium} corpora, we take the first 8582811 and 1072851 sentences of this shuffled corpus, which corresponds to $\frac{1}{8}$th and $\frac{1}{64}$th of the full training corpus, respectively.
For each setting, we train models with seeds \{1, 2, 3, 4, 5\}.

\paragraph{Test and validation data}
Initially, we aimed to evaluate our models using the commonly used MT test sets OPUS-100\footnote{\url{http://data.statmt.org/opus-100-corpus/v1.0/supervised/en-nl/}} and the test partition of the TED talk corpus.\footnote{\url{https://github.com/neulab/word-embeddings-for-nmt}}
However, it turned out that both these test sets were almost fully contained in our training corpus.
We, therefore, adopted the newer \textsc{Flores-101} corpus \citep{goyal2021flores}, of which we used both the `dev' and the `devtest' set.
The data can be downloaded from \url{https://dl.fbaipublicfiles.com/flores101/dataset/flores101_dataset.tar.gz}.
To compute BLEU scores, we tokenised the data with the Moses tokenisation script mentioned above, and then used the commandline script \texttt{fairseq-generate} to compute scores.

We furthermore use several evaluation sets to assess the compositional abilities of our trained models.
The data for these tests, as well as scripts to run them and plot their results, can be found in the following repository: \url{https://github.com/i-machine-think/compositionality_paradox_mt}.

\subsection{Architecture and training} 
As reported in the main text, we focus on English-Dutch translation, and all our models are Transformer-base models, as implemented in Fairseq \citep{ott2019fairseq}.\footnote{We used the implementation as it was on May 12, 2021: \url{https://github.com/pytorch/fairseq/blob/d151f2787240cca4e3c7e47640e647f8ae028c37/fairseq/models/transformer.py}}
Both the encoder and the decoder of this model have an embedding dimension of 512, 6 layers, 8 attention heads and a feed-forward layer dimension of 2048.
With our vocabulary, the models have a total of around 80M trainable parameters.

To train our models, we follow the training procedure suggested by \citet{ott2018scaling}, which can be found at \url{https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt}.
To summarise, we share all embeddings between the encoder and the decoder, use Adam as optimiser with $\beta$-values (0.9, 0.98), starting from an initial warmup learning rate of 1e-07 for 4000 warmup updates and a learning rate of 0.0005 afterwards, using inverse square root as the learning rate scheduler.
We use a clip-norm of 0.0, dropout of 0.3, weight-decay of 0.0001, label-smoothing of 0.1.
The maximum number of tokens in a batch is 3584, we simulate larger batches by increasing the update frequency to 8.
To determine early stopping, we use a patience of 10 (i.e.\ we stop training if a model does not improve on the dev set anymore for 10 epochs, and take the best checkpoint at that point).
Any other hyperparameters involved follow the Fairseq default.
We provide the BLEU scores per model seed in Table~\ref{tab:ap_bleu}.

\subsection{Compute}
All experiments were ran using Tesla V100 GPUs on an internal SLURM-based cluster. 
Training a transformer-base model on our small, medium and full dataset takes on average 3.5, 17 and 113 minutes per epoch, respectively (numbers are rounded) on 32 GPUs.
This makes the total training time for these models, which are trained for around 160, 60 and 30 epochs, 10,  17 and 56 hours, respectively (again, spread over 32 GPUs).


\begin{table}\centering\small
\begin{tabular}{lccc}
\toprule
\textbf{Training set size} & \textbf{Seed} & \textbf{BLEU dev} & \textbf{BLEU devtest} \\ \midrule \midrule
small & 1 & 20.92 & 21.14 \\
      & 2 & 20.77 & 20.37 \\
      & 3 & 20.42 & 20.11 \\
      & 4 & 20.95 & 20.23 \\
      & 5 & 20.88 & 20.84 \\\midrule
medium& 1 & 24.09 & 24.18 \\
      & 2 & 25.05 & 24.71 \\
      & 3 & 24.55 & 24.42 \\
      & 4 & 24.09 & 23.93 \\
      & 5 & 24.55 & 24.10 \\\midrule
full  & 1 & 26.17 & 25.63 \\
      & 2 & 25.71 & 25.63 \\
      & 3 & 25.82 & 25.72 \\
      & 4 & 26.19 & 25.84 \\
      & 5 & 25.86 & 25.76 \\\bottomrule
\end{tabular}
\caption{BLEU scores for the `dev' and `devtest' subsets of the \textsc{Flores} datasets, for models trained on corpora of three sizes, for five seeds per training set size.}
\label{tab:ap_bleu}
\end{table}
