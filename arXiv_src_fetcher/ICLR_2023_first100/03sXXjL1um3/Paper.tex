% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.
% Now being used for CMU's 10725 Fall 2012 Optimization course
% taught by Geoff Gordon and Ryan Tibshirani.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi. "pdflatex template.tex" should also work.
%
%\documentclass{article}
\documentclass[a4paper,12pt]{article}

\usepackage{xr}
%\externaldocument{lecture0}


\usepackage{color}
\usepackage{authblk}
\usepackage{hyperref}
%\setlength{\oddsidemargin}{0.25 in}
%\setlength{\evensidemargin}{-0.25 in}
%\setlength{\oddsidemargin}{0 in}
%\setlength{\evensidemargin}{0 in}
%\setlength{\topmargin}{-0.6 in}
%\setlength{\textwidth}{6.5 in}
%\setlength{\textheight}{8.5 in}
%\setlength{\headsep}{0.75 in}
%\setlength{\parindent}{0 in}
%\setlength{\parskip}{0.1 in}
%\setlength{\jot}{13pt}
%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,amssymb,graphicx, algorithmic}
\usepackage{natbib,dsfont,bbm}
\usepackage{amsthm}

% Box for algorithms
\usepackage[boxruled]{algorithm2e}
\setlength{\algomargin}{.2cm}
\SetAlFnt{\normalsize}
\SetAlCapFnt{\normalsize}
\SetAlCapNameFnt{\normalsize}

\makeatletter
\renewcommand{\algocf@caption@boxruled}{%
  \hrule
  \hbox to \hsize{%
    \vrule\hskip-0.4pt
    \vbox{   
       \vskip\interspacetitleboxruled%
       \vskip.07cm
       \hspace{.2cm}\unhbox\algocf@capbox\hfill
       \vskip.05cm
       \vskip\interspacetitleboxruled
       }%
     \hskip-0.4pt\vrule%
   }\nointerlineskip%
}%
\makeatother
% End Box for algorithms


%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%


\usepackage{enumerate}


%   {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept.}
%
%   {\bf Disclaimer}: {\it These notes have not been subjected to the
%   usual scrutiny reserved for formal publications.}
%   \vspace*{4mm}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
%\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\newtheorem{solution}{Solution}
%\newenvironment{proof}{\hfill\rule{2mm}{2mm}}
\numberwithin{theorem}{section}


% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}
\newcommand\J{\mathcal{J}}
\newcommand\I{\mathbb{I}}
\renewcommand\P{\mathbb{P}}
\newcommand\PP{\mathcal{P}}
\newcommand\Q{\mathbf{Q}}
\newcommand\X{\mathcal{X}}
\newcommand\W{\mathcal{W}}
\newcommand\Y{\mathcal{Y}}
\newcommand\Z{\mathcal{Z}}
\newcommand\A{\mathcal{A}}
\newcommand\M{\mathcal{M}}
\renewcommand\H{\mathcal{H}}
\newcommand\C{\mathcal{C}}
\newcommand\N{\mathbb{N}}
\newcommand\NN{\mathcal{N}}
\newcommand\R{\mathbb{R}}
\newcommand\T{\mathcal{T}}
\renewcommand\L{\mathcal{L}}
\renewcommand\S{\mathcal{S}}
\newcommand\B{\mathcal{B}}
\newcommand\V{\mathcal{V}}
\newcommand\F{\mathcal{F}}
\newcommand\G{\mathcal{G}}
\newcommand\D{\mathcal{D}}
\renewcommand\d{\mathrm{d}}
\newcommand\1{\mathbf{1}}
\newcommand\lb{\langle}
\newcommand\rb{\rangle}
\newcommand{\K}{\mathcal{K}}

\newcommand{\argmin}{\mathop{\mathrm{argmin}}}  
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\arcsinh}{\mathop{\mathrm{arcsinh}}}
\newcommand{\arctanh}{\mathop{\mathrm{arctanh}}}  
\newcommand{\sign}{\mathop{\mathrm{sign}}}
\newcommand{\rint}{\mathop{\mathrm{rint}}}
\newcommand{\dom}{\mathop{\mathrm{dom}}}
\newcommand\rad{\mathtt{Rad}}
\newcommand\gauss{\mathtt{Gauss}}
\newcommand\cov{\mathtt{Cov}}
\newcommand\pack{\mathtt{Pack}}
\newcommand\vc{\mathtt{VC}}
\newcommand\rank{\mathtt{Rank}}
\newcommand\prox{\mathtt{Prox}}
\newcommand\KL{\mathtt{KL}}
\newcommand\TV{\mathtt{TV}}
\newcommand\trace{\mathtt{Trace}}
\newcommand\row{\mathtt{Row}}
\newcommand\col{\mathtt{Col}}
\newcommand\spann{\mathtt{Span}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\linespread{1}\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\title{Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization}


\author[*]{Carlo Alfano}
\author[*]{Patrick Rebeschini}
\affil[*]{Department of Statistics, University of Oxford}
\date{\vspace{-5ex}}

%\newcommand{\fix}{\marginpar{FIX}}
%\newcommand{\new}{\marginpar{NEW}}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle

\begin{abstract}
	We analyze the convergence rate of the \emph{unregularized} natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy.
	We then consider the sample-based case, when the best representation of the Q-value function among linear combinations of a known feature function is known up to an estimation error.
	In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametrization to the log-linear policy class.
\end{abstract}
\section{Introduction}
Sequential decision-making represents a framework of paramount importance in modern statistics and machine learning. In this framework, an agent sequentially interacts with an environment to maximize notions of reward. In these interactions, an agent observes its current state $s\in\S$, takes an action $a\in\A$ according to a policy that associates to each state a probability distribution over actions, receives a reward, and transitions to a new state. Reinforcement Learning (RL) focuses on the case where the agent does not have complete knowledge of the environment dynamics.

One of the most widely-used classes of algorithms for RL is represented by policy optimization. In policy optimization algorithms, an agent iteratively updates a policy that belongs to a given parametrized class with the aim of maximizing the expected sum of discounted rewards, where the expectation is taken over the trajectories induced by the policy. Many types of policy optimization techniques have been explored in the literature, such as policy gradient methods \citep{RN158}, natural policy gradient methods \citep{RN159}, trust region policy optimization \citep{RN214}, and proximal policy optimization \citep{RN182}.
Thanks to the versatility of the policy parametrization framework, in particular the possibility of incorporating flexible approximation schemes such as neural networks, these methods have been successfully applied in many settings.
However, a complete theoretical justification for the success of these methods is still lacking.

The simplest and most understood setting for policy optimization is the tabular case, where both the state space $\S$ and the action space $\A$ are finite and the policy has a direct parametrization,
i.e.\ it assigns a probability to each state-action pair. This setting has received a lot of attention in recent years and has seen several developments \citep{RN265, RN266}.
Its analysis is particularly convenient due to the decoupled nature of the parametrization, where the probability distribution over the action space that the policy assigns to each state can be updated and analyzed separately for each state.
This leads to a simplified analysis, where it is often possible to drop discounted visitation distribution terms in the policy update and take advantage of the contractivity property typical of value and policy iteration methods.
Recent results involve, in particular, natural policy gradient (NPG) and, more generally, policy mirror descent, showing how specific choices of learning rates yield linear convergence to the optimal policy for several formulations and variations of these algorithms \citep{RN150,RN242,RN268,RN266, RN269, RN270, RN273, RN272}.

Two of the main shortfalls of these methods are their computational and sample complexities, which depend polynomially on the cardinality of the state and action spaces, even in the case of linear convergence. Indeed, by design, these algorithms need to update at each iteration a parameter or a probability for all state-action pairs, which has an operation cost proportional to $|\S||\A|$. Furthermore, in order to preserve linear convergence in the sample-based case, the aforementioned works assume that the worst estimate ($\ell_\infty$ norm) of $Q^\pi(s,a)$---which is the expected sum of discounted rewards starting from the state-action pair $(s,a)$ and following a policy $\pi$---is exact up to a given error threshold. Without further assumptions, meeting this threshold requires a number of samples that depends polynomially on $|\S||\A|$.

%\cite{RN183}
A promising approach to deal with large and high-dimensional spaces that is recently being explored is that of assuming that the environment has a low-rank structure and that, as a consequence, it can be described or approximated by a lower dimensional representation. 
In particular, a popular framework is that of linear function approximation, which consists in assuming that quantities of interest in the problem formulation, such as the transition probability (Linear MDPs) or the action-value function $Q^\pi$ of a policy $\pi$, can be approximated by a linear combination of a certain $d$-dimentional feature function $\phi:\S\times\A\rightarrow\R^d$ up to a bias error $\varepsilon_\text{bias}$. This linear assumption reduces the dimensionality of the problem to that of the feature function.
In this setting, many researchers have proposed methods to learn the best representation $\phi$ \citep{RN220, RN236, RN240, RN276} and to exploit it to design efficient vairations of the upper confidence bound (UCB) algorithm, for instance~\citep{RN162, RN237, RN275}.

When applying the framework of linear approximation to policy optimization, researchers typically adopt the log-linear policy class, where a policy $\pi_\theta$ parametrized by $\theta\in\R^d$ is defined as proportional to $\exp(\theta^\top\phi)$. For this policy class, several works have obtained improvements in terms of computational and sample complexity, as the policy update requires a number of operations that scales only with the feature dimension $d$ and the estimation assumption to retain convergence rates in the sample-based setting is weaker than the tabular counterpart. In fact, theoretical guarantees for these algorithms only assume the expectation of $Q^\pi$ over a known distribution on the state and action spaces to be exact up to a statistical error $\varepsilon_\text{stat}$. In the linear function approximation setting meeting this assumption typically requires a number of samples that is only a function of $d$ and does not depend on $|\S|$ and $|\A|$ \citep{bhandari2018finite}. However, a complete understanding of the convergence rate of policy optimization methods in this setting is still missing. Recent results include \emph{sublinear} convergence rates for unregularized NPG \citep{RN265, RN274, RN277, RN279} and linear convergence rates for \emph{entropy-regularized} NPG with bounded updates \citep{RN280}. 

%This framework can then be generalized to nonlinear parametrized differentiable functions $f_\theta$ by assuming that the same quantities of interest in the problem can be approximated up to a bias error $\varepsilon_\text{bias}$ by a linear combination of the gradient of $f_\theta$, i.e. $\nabla_\theta f_\theta\in\R^d$. \cite{RN265} apply this framework to policy optimization by defining the policy as proportional to $\exp(f_\theta)$ and show that the computational and sample complexity of NPG for this policy class has no polinomial dependence on the cardinality of the state and action spaces, while achieving a sublinear convergence rate.

Our work fills the gap between the aforementioned findings and it extends the analysis and results of the tabular setting to the linear function approximation setting. In particular, we show that, under the standard assumptions on the $(\varepsilon_\text{stat},\varepsilon_\text{bias})$-approximation of $Q^\pi$ mentioned above, a choice of geometrically-increasing step-sizes leads to linear convergence of NPG for the log-linear policy class in both deterministic and sample-based settings. Our result directly improves upon the sub-linear iteration complexity of NPG previously established for the log-linear policy class by \cite{RN265} and \cite{RN279} and it removes the need for entropy regularization and bounded step-sizes used by \cite{RN280}, under the same assumptions on the linear approximation of $Q^\pi$. %Moreover, we have that the number of operations needed for the policy update and the number of samples needed to preserve the convergence rate in the sample-based setting depend on the dimension $d$ of $\phi$, as opposed to the tabular setting where the same quantities depend on $|\S||\A|$.
By extending the linear convergence rate of NPG from the tabular softmax parametrization to the setting of log-linear policy parametrizations, our result directly addresses the research direction outlined in the conclusion of \cite{RN266}, and it overcomes the aforementioned limitations of the tabular settings.

%\begin{itemize}    \item \textbf{Exact setting.} Assume knowledge of the true action-value function $Q^\pi$ for each policy $\pi$ in the algorithm path and that the feature func. In this setting, \emph{unregularized} NPG with a geometrically increasing step-size, where $\eta_{t+1} = \eta_t/\gamma$ and $\gamma$ is the discount factor, has linear convergence to an optimal policy.   \item \textbf{Inexact setting.}We then consider the inexact setting, where we assume that there exists some $w_t$ such that $\norm{\langle w_t, \nabla_\theta f_\theta\rangle-Q^\pi}_2^2\leq \varepsilon_\text{bias}$ and that we have access to a $\widehat{w}_t$ such that $\norm{w_t-\widehat{w}_t}_2^2\leq \varepsilon_\text{stat}$.	With these assumptions in place, we show that {unregularized} NPG with a geometrically increasing step-size retains linear convergence up to an asymptotic error floor, which depends on $\varepsilon_\text{stat}$, on $\varepsilon_\text{bias}$ and on the condition number of the Fisher information matrix of the policy.\end{itemize}


Our analysis is based on the equivalence of NPG and policy mirror descent with $\KL$ divergence~\citep{RN184}, which has been exploited for applying mirror-descent-type analysis to NPG by several works, such as \cite{RN265,RN279, RN280}. The advantages of this equivalence are twofold.
Firstly, NPG crucially ensures a simple update rule, i.e.\ $\log\pi_{t+1}(a|s) = \log\pi_t(a|s) + \eta_tQ^{\pi_t}(s,a)$ , which in the particular case of the log-linear policy class translates into $\theta_{t+1}^\top\phi(s,a) = \theta_t^\top\phi(s,a) + \eta_tQ^{\pi_t}(s,a)$.
Secondly, the mirror descent setup is useful to iteratively control the updates and the approximation errors, e.g.\ through tools like the three-point descent lemma (see \eqref{eq:breg} below), and to induce telescopic sums or recursions that are often used to analyze the converge rate of the last iterate.

In our work, we show how to exploit these advantages to use the linear approximation of $Q^\pi$ in the analysis and, consequently, make weaker assumptions on the accuracy of the estimation of $Q^\pi$ w.r.t.\ the tabular setting. While previous results for the tabular setting \citep{RN150,RN242,RN266} require an $\ell_\infty$ norm bound on the estimation error, i.e.\ $\lVert \widehat{Q}^\pi-Q^\pi \rVert_\infty\leq\tau$,
our convergence guarantee depends on the expected error of the estimate, i.e.\ $\E (\widehat{Q}^\pi(s,a)-Q^\pi(s,a) )^2\leq\varepsilon_\text{stat}$, where the expectation is taken w.r.t.\ the discounted state visitation distribution induced by the policy $\pi$ and the uniform distribution over the action space. As shown by previous works \citep{bhandari2018finite}, this assumption can be satisfied in the linear approximation setting with a number of samples that does not depend on the cardinality of the state and action spaces.%This allows us to employ sample-efficient policy evaluation algorithms, such as temporal difference learning , and to remove  $|\S||\A|$ from the sample complexity of the algorithm.

We acknowledge the concurrent work by \cite{yuan2023linear}, who also obtained linear convergence guarantees for log-linear policies, through an analysis similar to ours. 




The paper is organized as follows. Section~\ref{sec:setting} introduces the main setting of RL, and Section~\ref{sec:algo} introduces the algorithm framework we consider. Section~\ref{sec:main} contains the linear approximation set-up and our main result. Section~\ref{sec:analysis} presents the analysis of our main result, with the conclusions outlined in Section~\ref{conc}.




\section{Setting}
\label{sec:setting}
Consider an agent that acts in a discounted Markov Decision Process (MDP) $\M=(\S,\A,P,r,\gamma,\mu)$, where: $\S$ is the possibly infinite state space and $\A$ is the finite action space; $P(s'| s,a)$ is the transition probability; $r(s,a)\in [0,1]$ is the reward function; $\gamma$ is the discount factor; and $\mu$ is the starting state distribution.
A policy $\pi:\S\times\A\rightarrow\R$ is a probability distribution over $\A$ that represents the probability that an agent takes action $a$ when in state $s$. At time $t$ denote the current state and action by $s_t$ and $a_t$.

For a policy $\pi$, let $V^\pi:\S\rightarrow\R$ be the respective value function, which is defined as the expected discounted cumulative reward with starting state $s_0=s$, namely,
\[V^\pi_s= \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t) \bigg| \pi, s_0 = s\right],\]
where $a_t\sim\pi(\cdot|s_t)$ and $s_{t+1}\sim P(\cdot|s_t,a_t)$. Let $V^\pi(\mu) = \E_{s\sim\mu}V^\pi_s$. The agent aims to find an optimal policy $\pi^\star\in \argmax_\pi V^\pi(\mu)$. 

For a policy $\pi$, let $Q^\pi:\S\times\A\rightarrow\R$ be the respective action-value function, or Q-function, which is defined as the expected discounted cumulative reward with starting state $s_0=s$ and starting action $a_0=a$, namely,
\[Q^\pi(s, a) =\E\left[\sum_{t=0}^\infty \gamma^t r(s_t, a_t)\bigg|\pi,  s_0=s,a_0=a\right],\]
where $a_t\sim\pi(\cdot|s_t)$ and $s_{t+1}\sim P(\cdot|s_t,a_t)$. %Let the advantage function for policy $\pi$ be defined as $A^\pi(s,a)=Q^\pi(s,a)-V^\pi_s$.

Define the discounted state visitation distribution \citep{RN158}
\[d_{\mu}^\pi(s)= (1-\gamma)\E_{s_0 \sim \mu}\sum_{t=0}^{\infty}\gamma^t P(s_t = s|\pi, s_0),\]
and the discounted state-action visitation distribution
\[d_{\rho}^\pi(s,a)= (1-\gamma)\E_{s_0,a_0 \sim \rho}\sum_{t=0}^{\infty}\gamma^t P(s_t = s|\pi, s_0, a_0),\]
where the trajectory ${(s_t,a_t)}_{t\geq0}$ is generated by the MDP following policy $\pi$ and $\rho$ is a distribution over $\S\times\A$. Then we can formulate the performance difference lemma \citep{RN281}, a tool that will prove useful in our analysis,
\begin{equation}
	\label{lemma:pdiff}
	V^\pi(\mu)-V^{\bar{\pi}}(\mu) = \frac{1}{1-\gamma}\E_{s\sim d_\mu^{\bar{\pi}}}\sum_{a\in\A}Q^\pi(s,a)(\pi(a|s)-\bar{\pi}(a|s)).
\end{equation}

%\begin{assumption}(Linear Q-function)
%    Let $\phi:\S\times\A\rightarrow\R^d$ be a known feature map. Then, for any policy $\pi$, we assume there exists $w\in\R^d$ such that $Q^\pi(s,a) = \langle w,\phi(s,a)\rangle\,$ for all $(s,a)\in\S\times\A$.
%\end{assumption}

%In section~\ref{approx} we show that our results hold even if this assumption is approximately satisfied, that is if $\norm{Q^\pi(s,a) - \langle w,\phi(s,a)\rangle}_\infty\leq \varepsilon_\text{approx}$.

\subsection{Notation}
We make the following definitions for ease of exposition. As to the policy, let $\pi_s := \pi(s,\cdot)$ and $\pi^t := \pi_{\theta_t}$. For two functions $f$ and $g$, denote $(f\circ g)(x,y) =f(x)g(y) $. As to the action-value function, let $Q^\pi_s := Q^\pi(s,\cdot)$ and $Q^t(s,a) := Q^{\pi_t}(s,a)$. As to the discounted visitation distributions, let $d^t_\mu := d^{\pi^t}_\mu$, $d^t = d^t_\mu \circ \text{Unif}_\A$, and $d^\star := d^\star_\mu \circ \text{Unif}_\A$. Lastly, denote $\KL^\star_t = \E_{s\sim d^\star_\mu}\KL(\pi^\star_s,\pi^t_s)$.


\section{Natural policy gradient and mirror descent}
\label{sec:algo}
\textit{Policy class} --- In this work, we consider the log-linear policy parametrization \citep{RN265}. Let $\theta\in\R^d$ be a parameter vector and $\phi:\S\times\A\rightarrow\R^d$ be a feature function. Then the policy class consists of all policies of the form:
\[\pi_\theta(a|s) = \frac{\exp(\theta^\top\phi(s,a))}{\sum_{a'\in\A}\exp(\theta^\top\phi(s,a'))}.\]

\textit{Natural Policy Gradient} --- We formulate NPG through mirror descent. The update at time $t+1$ is

\begin{equation}
	\label{eq:update}
	\nabla h(\pi^{t+1}_s) = \nabla h(\pi^t_s) + \eta_t Q^t_s\qquad \forall s\in\S, 
\end{equation}

where $h(\pi_s) = \sum_{a\in\A}\pi(a|s)\log\pi(a|s)$, is the entropy mirror map. This is equivalent to the update
\[\pi_{t+1}(s,a) \propto \pi_t(s,a) e^{\eta_t Q^t(s,a)}\qquad\forall s,a\in\S,\A,\]

or, as in Algorithm \ref{alg}, to requiring that $\theta_{t + 1}$ is such that
 \[\theta_{t + 1}^\top\phi(s,a) = \theta_t^\top\phi(s,a) + \eta_t Q^t(s,a) \qquad \forall s,a\in\S,\A.\]

In the tabular setting, we have $d = |\S||\A|$, $\phi(s,a)$ is a vector of all zeros except a one in the position assigned to $(s,a)$, and the update is equivalent to the one analyzed by \citep{RN265}. 
This mirror descent setup allows us to use standard mirror descent tools in the analysis \citep{RN186, RN279, RN266}, such as three-point descent lemma
\begin{equation}
	\label{eq:breg}
	D_h(\pi_s,\pi^t_s) - D_h(\pi_s,\pi^{t+1}_s) - D_h(\pi^{t+1}_s,\pi^t_s) = \langle \nabla h(\pi^t_s)-\nabla h(\pi^{t+1}_s),\pi^{t+1}_s - \pi_s\rangle\quad\forall\pi_s,
\end{equation}

which in this setting can be expressed as

\begin{equation}
	\label{eq:kl}
	\KL(\pi_s,\pi^t_s) - \KL(\pi_s,\pi^{t+1}_s) - \KL(\pi^{t+1}_s,\pi^t_s) = -\eta_t\langle Q^t_s,\pi^{t+1}_s - \pi_s\rangle\quad\forall\pi_s.	
\end{equation}

These tools, along with the performance difference lemma~\eqref{lemma:pdiff}, ensure that we can control the increase of the value function for each policy update. When we only have access to an approximation $\widetilde{Q}^\pi$ of $Q^\pi$, these tools allow controlling the error of this approximation by means of simple triangle inequality arguments, making possible the incorporation of the linear function approximation framework where $Q^\pi$ is approximated by a linear combination of the feature function $\phi$.




\section{Main result}
\label{sec:main}
In this section, we present our main result on the linear convergence of NPG. We start by introducing and discussing the assumptions and the algorithm.
\subsection{Algorithm and linear function approximation}
\label{sub:lin}
We make the following two assumptions on the linear approximation of $Q^\pi$, which are standard in the literature \citep{RN265,RN280}.
\begin{assumption} (Bias error)
	\label{ass:bias}
	Define the loss function
	\[L(w,\theta,v) := \E_{s, a\sim v}\left[\left(Q^{\pi_\theta} (s,a)-  w^\top \phi(s,a)\right)^2\right]\]
	and let
	\begin{equation}
		\label{eq:min}
		w_t\in\argmin_w L(w,\theta_t,d^t_\rho).
	\end{equation}
	Assume that $\forall t < T$ we have
	\[L(w_t,\theta_t,d^\star)\leq\varepsilon_\text{bias},\qquad L(w_t,\theta_t,d^{t+1}_\mu\circ\text{Unif}_\A)\leq\varepsilon_\text{bias}.\]
\end{assumption}
In order to better understand the implications of Assumption \ref{ass:bias}, we consider the trivial upper bound \citep{RN265}
\[L(w_t,\theta_t,d^\star)\leq \norm{\frac{d^\star}{d^t_\rho}}_\infty\!\!\!\!\! L(w_t,\theta_t,d^t_\rho)\leq \frac{1}{1-\gamma}\norm{\frac{d^\star}{\rho}}_\infty\!\!\!\!\! L(w_t,\theta_t,d^t_\rho).\]
This bound allows to think of $\varepsilon_\text{bias}$ in Assumption \ref{ass:bias} as controlling two quantities of interest. The first quantity is the loss incurred by the minimizer of $L(w,\theta_t,d^t_\rho)$, that is the best approximation $\widetilde{Q}^t =  w_t^\top \phi$ of $Q^t$ with respect to the squared error averaged over the distribution $d^t_\rho$. The second quantity is the shift in distribution from $d^t_\rho$ to $d^\star$ in the loss function. A similar conclusion for Assumption \ref{ass:bias} can be drawn for the the distribution $d^{t+1}_\mu\circ\text{Unif}_\A$. 



\begin{assumption} (Statistical error)
	Assume that our estimate $\widehat{w}_t$ of $w_t$ is such that
	\label{ass:stat}
	\[\E_{s,a\sim d^t_\rho}\left[\left(\langle w_t-\widehat{w}_t, \phi(s,a)\rangle\right)^2\right]\leq\varepsilon_\text{stat}.\]
\end{assumption}

Assumption \ref{ass:stat} concerns the statistical error incurred when solving the minimization problem in~\eqref{eq:min} and it can be used to describe the sample complexity of the algorithm. Let $\widehat{Q}^\pi(s,a) = \widehat{w}_t^\top \phi(s,a)$ be the sample-based estimate of $\widetilde{Q}^\pi$. Then Assumption \ref{ass:stat} is equivalent to assuming that 
\[\E_{s,a\sim d^t_\rho}\left[\left(\widehat{Q}^\pi(s,a)-\widetilde{Q}^\pi(s,a)\right)^2\right]\leq\varepsilon_\text{stat}.\]
Several algorithms have been shown to satisfy Assumption \ref{ass:stat} with a number of samples that depends only on the dimension $d$ of $\phi$ and not on $|\S|$ or $|\A|$, such as temporal difference learning \citep{bhandari2018finite}. This represent an improvement over the sample complexity of tabular algorithms, where the typical assumption $\lVert \widehat{Q}^\pi-Q^\pi \rVert_\infty\leq\varepsilon_\text{stat}$ \citep{RN266, RN269} causes the sample complexity to depend on $|\S||\A|$.

With this set-up, we can formulate NPG with linear function approximation as in Algorithm \ref{alg}. At time step $t$, let $\D(t,\rho)$ be an oracle such that $\widehat{w}_t = \D(t,\rho)$ satisfies Assumption \ref{ass:stat}.

\begin{algorithm}[h]
	\caption{NPG with linear function approximation}
	\label{alg}
	\begin{algorithmic}
		\STATE \textbf{Input:} Learning rate schedule $(\eta_t)_{t\geq 0}$; number of iterations $T$; initialized policy $\pi^{(0)}$; distribution $\rho$; oracle $\D$.
		\FOR{$t=0,\dots,T-1$}
		\STATE Obtain $\widehat{w}_t = \D(t,\rho)$.
		\STATE Update \[\theta_{t+1}=\theta_t+\eta_t \widehat{w}_t.\]
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\begin{remark} (Tabular setting)
	It is possible to recover the tabular case by setting $d = |\S||\A|$ and $\phi(s,a)$ to be a vector of all zeros except a one in the position assigned to $(s,a)$. In this case, we recover the same update as the tabular setting and we have that $\varepsilon_\text{bias} = 0$, as by setting $w = Q^t$ we obtain $\E_{s\sim \nu, a\sim \text{Unif}_\A}\left[\left(Q^t (s,a)-  w^\top \phi(s,a)\right)^2\right] = 0\,$ for any distribution $\nu$.
\end{remark}
\begin{remark} (Linear MDPs)
	Another setting for which the bias error $\varepsilon_\text{bias}$ is equal to $0$ is that of Linear MDPs \citep{RN162}, where it is assumed that the transition probability distribution and the reward function can be expressed as a linear function of the feature function $\phi$. Namely, assume there exist two feature maps $\phi:\S\times\A\rightarrow\R^d$ and $\mu:\S\rightarrow\R^d$ and a vector $v_r\in\R^d$ such that
	\[P(s'|s,a)=\langle\pi(s,a),\mu(s')\rangle,\qquad r(s,a) = \langle v_r, \phi(s,a)\rangle\qquad\forall s,s'\in\S,a\in\A.\]

	If this assumption is satisfied, then we have that $\forall s\in\S,a\in\A$
	\[Q^\pi(s,a) = r(s,a) + \gamma\int_\S V^\pi(s')P(s'|s,a)ds' = \left\langle\phi(s,a),v_r + \gamma\int_\S V^\pi(s')\mu(s')ds'\right\rangle,\]
	which means that at each time step $t$ there exists a $w_t\in\R^d$ such that $Q^t(s,a) = \langle w_t,\phi(s,a)\rangle$ and $L(w_t,\theta_t,d^t_\mu) = 0$.
\end{remark}

\subsection{Linear convergence}
\label{sub:thm}
In order to present the main result of our work, we need two additional assumptions on the distribution mismatch coefficient and the feature covariance matrix.
\begin{assumption}
	\label{ass:mismatch}
	Assume that the distribution mismatch coefficient \[\nu_\mu = \frac{1}{1-\gamma}\norm{\frac{d^\star_\mu}{\mu}}_\infty\]
	is finite, i.e. $\nu_\mu<\infty$.
\end{assumption}
Assumption \ref{ass:mismatch} is a standard assumption in the policy optimization literature~\citep{RN265,RN266}. As we will see in Theorem \ref{thm}, the iteration complexity of Algorithm \ref{alg} depends polynomially on this term, meaning that the convergence rate is faster when the starting state distribution $\mu$ covers the whole state space.
\begin{assumption} (Relative condition number)
	\label{ass:cond}
	With respect to a distribution $v$, define \[\Sigma_v = \E_{s,a\sim v}\left[\phi(s,a)\phi(s,a)^\top\right]\]
	and assume that there exists a $\kappa<\infty$ such that \[\sup_{w\in\R^d}\frac{w^\top\Sigma_{d^\star}w}{w^\top\Sigma_{\rho}w}\leq\kappa,\qquad\sup_{w\in\R^d}\frac{w^\top(\Sigma_{d^t_\mu\circ\text{Unif}_\A})w}{w^\top\Sigma_{\rho}w}\leq\kappa\quad\forall t\leq T.\]
\end{assumption}
Assumption \ref{ass:cond} is a standard assumption in the linear function approximation literature and highlights the importance of choosing a state-action distribution $\rho$ with good coverage over the feature space, as it can be enforced by choosing the appropriate $\rho$. In fact, if $\Phi = \{\phi(s,a)|s\in\S,a\in\A\}$ is a compact set, there always exists a state-action distribution $\rho$ such that $\kappa\leq d$ (see Lemma 23 in \cite{RN265}). In general, if $\norm{\phi(s,a)}_2^2\leq B$ for all $s\in\S,a\in\A$, we have the crude bound $\kappa\leq B/\sigma_\text{min}(\Sigma_{\rho})$, where $\sigma_\text{min}(A)$ is the minimum eigenvalue of matrix $A$.

We are now ready to state the following theorem on the linear convergence of Algorithm \ref{alg}.
\begin{theorem} (Linear convergence of NPG with log-linear parametrization)
	\label{thm}
	Consider NPG as in Algorithm \ref{alg} and let Assumptions \ref{ass:bias}, \ref{ass:stat}, \ref{ass:mismatch}, and \ref{ass:cond} hold. If the step-size schedule satisfies \[\eta_{t+1}\geq \frac{\nu_\mu}{\nu_\mu-1}\eta_t \qquad \forall t,\] and $\eta_0\geq\frac{1-\gamma}{\gamma}\KL_t^\star$, then for every $T\geq 0$ we have
	\[  V^\star(\mu)-V^T(\mu)\leq\left(1-\frac{1}{\nu_\mu}\right)^T\frac{2}{1-\gamma}+2\nu_\mu\sqrt{\frac{|\A|\kappa\varepsilon_\text{stat}}{(1-\gamma)^3}}+ 2\nu_\mu\frac{\sqrt{|\A|\varepsilon_\text{bias}}}{1-\gamma}.\]
\end{theorem}
To the best of our knowledge, Theorem~\ref{thm} represents the first result establishing linear convergence rates for NPG with unbounded updates and without entropy regularization for the log-linear policy class. The convergence rate has no explicit dependence on the cardinality of the state and action spaces, with the exception of the two $|\A|$ terms which, as already highlighted by \cite{RN265}, can be removed with a path-dependent bound. In the case where $\varepsilon_\text{bias} = 0$ and $\varepsilon_\text{stat} = 0$, the theorem recovers the same convergence rate as Theorem 10 in \cite{RN266}.

%and shows that, as long as $h$ is the entropy mirror map and $f_\theta$ can interpolate the true Q-function, linear convergence is possible in settings different from the tabular case.
%To obtain the sample complexity of the algorithm, we take advantage of the theory for temporal difference learning developed by \cite{RN282}. In particular, we have that in order to satisfy Assumption \ref{ass:stat} for all $T$ iterations with high probability we need $\widetilde{O}\left(\frac{T\norm{w_t}^2_2}{\varepsilon_\text{stat}(1-\gamma)^3}\right)$ samples. Combining this quantity with the iteration complexity from Theorem \ref{thm}, we obtain a total sample complexity of \[\widetilde{O}\left(\frac{\norm{w_t}^2_2}{\varepsilon^2(1-\gamma)^{10}}\norm{\frac{d^\star_\mu}{\mu}}_\infty\right).\]

\begin{remark} (Different policy parametrizations)
	\label{remark:1}
	While our work focuses on the log-linear policy class, it is possible to extend our framework and our analysis to general function approximation schemes. Let  $f_\theta:\S\times\A\rightarrow\R$ be a parameterized function and define the policy class $\{\pi_\theta|\theta\in\Theta\}$ as
	\[\pi_\theta(a|s) = \frac{\exp(f_\theta(s,a))}{\sum_{a'\in\A}\exp(f_\theta(s,a'))}.\]

	Let $g_\omega$ be a parametrized operator of $f_\theta$, define the loss function
	\[L(\omega,\theta,\nu) := \E_{s\sim \nu, a\sim \text{Unif}_\A}\left[\left(Q^{\pi_\theta} (s,a)- g_\omega(f_{\theta_t}(s,a))\right)^2\right]\]
	and let
	\[\omega_t\in\argmin_\omega L(\omega,\theta_t,d^t_\mu).\]
	Then, Assumption \ref{ass:bias} becomes \[L(\omega_t,\theta_t,d^\star_\mu)\leq\varepsilon_\text{bias}\qquad \forall t < T.\]
	The NPG update \eqref{eq:update} can then be formulated as requiring $\theta_{t + 1}$ to be such that \[f_{\theta_{t + 1}}(s,a) = f_{\theta_t}(s,a) + \eta_tg_{\omega_t}(f_{\theta_t}(s,a)) \qquad \forall s\in\S,a\in\A.\]

	Finding methods to solve this system of equations is beyond the scope of this work.
\end{remark}

\section{Analysis}
\label{sec:analysis}
In order to prove Theorem \ref{thm}, we need some intermediate results. The first one regards the decomposition of the statistical and the bias errors.
\begin{lemma}
	\label{lemma:appr}
	The expected error of the estimate $\widehat{Q}_s^t$ of $Q_s^t$ can be bounded as follows
	\[\left|E_{s\sim v}\langle Q_s^t-\widehat{Q}_s^t,\pi^t_s - \pi_s\rangle\right|\leq 2\sqrt{\frac{|\A|\kappa\varepsilon_\text{stat}}{1-\gamma}}+ 2\sqrt{|\A|\varepsilon_\text{bias}}\qquad \forall t < T,\]
	for both $v = d_\mu^{t+1}, \pi_s = \pi^{t+1}_s$ and $v = d_\mu^\star, \pi_s = \pi^\star_s$.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:appr}]
	We start by adding and subtracting $\widehat{Q}_s^t$
	\begin{align*}
		\left|E_{s\sim v}\langle Q_s^t-\widehat{Q}_s^t,\pi^t_s - \pi^{t+1}_s\rangle\right|&\leq \left|\E_{s\sim v}\langle \widetilde{Q}_s^t-\widehat{Q}_s^t,\pi^t_s-\pi_s\rangle\right|+ \left|\E_{s\sim v}\langle Q_s^t-\widetilde{Q}_s^t,\pi^t_s-\pi_s\rangle\right|.
	\end{align*}
	We then bound the two terms on the right-hand side separately. For the first term, we have that
	\begin{align*}
		&\left|\E_{s\sim v}\langle \widetilde{Q}_s^t-\widehat{Q}_s^t,\pi^t_s-\pi_s\rangle\right|\\ 
		&\leq\left|\E_{s\sim v, a \sim \pi^t_s}\left[( w_t-\widehat{w}_t)\phi(s,a)\right]\right|+\left|\E_{s\sim v, a \sim \pi_s}\left[( w_t-\widehat{w}_t)\phi(s,a)\right]\right|\\
		&\leq \sqrt{\E_{s\sim v, a \sim \pi^t_s}\left[\left(( w_t-\widehat{w}_t)\phi(s,a)\right)^2\right]}
		+\sqrt{\E_{s\sim v, a \sim \pi_s}\left[\left(( w_t-\widehat{w}_t)\phi(s,a)\right)^2\right]}\\
		&\leq2\sqrt{|\A|\E_{s\sim v, a \sim \text{Unif}_{\A}}\left[\left(( w_t-\widehat{w}_t)\phi(s,a)\right)^2\right]}= 2\sqrt{|\A|\norm{ w_t-\widehat{w}_t}^2_{\Sigma_{v\circ\text{Unif}_{\A}}}},
	\end{align*}
	where $\norm{w}^2_\Sigma:=w^\top\Sigma w$. Using Assumption \ref{ass:cond} and the fact that $(1-\gamma)\rho\leq d^t_\rho$ we have
	\[\norm{ w_t-\widehat{w}_t}^2_{\Sigma_{v\circ\text{Unif}_{\A}}}\leq\kappa\norm{ w_t-\widehat{w}_t}^2_{\Sigma_{\rho}}\leq\frac{\kappa}{1-\gamma}\norm{ w_t-\widehat{w}_t}^2_{\Sigma_{d_\rho^{t}}}\leq\frac{\kappa\varepsilon_\text{stat}}{1-\gamma}.\]
	Similarly, for the second term we have 
	\begin{align*}
		\left|\E_{s\sim v}\langle Q_s^t-\widetilde{Q}_s^t,\pi^t_s-\pi_s\rangle\right|&\leq2\sqrt{|\A|\E_{s\sim v, a \sim \text{Unif}_{\A}}\left[\left( Q^t(s,a)-\widetilde{Q}^t(s,a)\right)^2\right]}\leq 2\sqrt{|\A|\varepsilon_\text{bias}}.
	\end{align*}\end{proof}

For ease of exposition, in the rest of this section denote
\[\tau:=2\sqrt{\frac{|\A|\kappa\varepsilon_\text{stat}}{1-\gamma}}+ 2\sqrt{|\A|\varepsilon_\text{bias}}.\]

The next lemma regards the quasi-monotonic improvements of Algorithm~\ref{alg}. Let $\widehat{Q}^t_s = \widehat{w}_t^\top\phi(s,\cdot)$.
\begin{lemma}
	\label{lemma:2}
    The updates of Algorithm~\ref{alg} satisfy, for all $s\in\S$,
	\[\langle \widehat{Q}^t_s,\pi^{t+1}_s-\pi^t_s\rangle\geq 0\]
	and
	\begin{equation*}
		V^{t+1}(\mu)-V^t(\mu)\geq -\frac{\tau}{1-\gamma}.
	\end{equation*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:2}]
    By 1-strong convexity of $h$ on $(0,1)$, we have $\forall s\in\S$
	\[0\leq\norm{\pi^{t+1}_s - \pi^t_s}_2^2\leq\langle \nabla h(\pi^{t+1}_s) - \nabla h(\pi^t_s),\pi^{t+1}_s - \pi^t_s\rangle = \langle \widehat{Q}^t_s,\pi^{t+1}_s - \pi^t_s \rangle.\]
	As to the second inequality, we use the performance difference lemma \eqref{lemma:pdiff} and Lemma~\ref{lemma:appr} to obtain
	\begin{align*}
		(1-\gamma)(V^{t+1}(\mu)-V^t(\mu))&=\E_{s\sim d_\mu^{t+1}}\langle Q_s^t,\pi^{t+1}_s-\pi_s^t\rangle\\
		&=\E_{s\sim d_\mu^{t+1}}\langle \widehat{Q}_s^t,\pi^{t+1}_s-\pi_s^t\rangle + \E_{s\sim d_\mu^{t+1}}\langle Q_s^t-\widehat{Q}_s^t,\pi^{t+1}_s-\pi_s^t\rangle\\
		&\geq -\frac{\tau}{1-\gamma}.
	\end{align*}
\end{proof}

The last result we need is the following lemma, which can be straightforwardly proven by induction. 
\begin{lemma}
	\label{lemma:rec}
	Suppose $0<\alpha<1, b>0$ and a nonnegative sequence $\{a_k\}$ satisfies \[a_{k+1}\leq\alpha a_k + b\qquad\forall k\geq 0.\] Then for all $k\geq 0$,\[a_k\leq\alpha^k a_0+\frac{b}{1-\alpha}.\]
\end{lemma}

With these results in place, we are ready to prove Theorem \ref{thm}.

\begin{proof}[Proof of Theorem \ref{thm}]
Let \[ \nu_k = \norm{\frac{d^\star}{d_\mu^{t+1}}}_\infty\]
and consider the equality in \eqref{eq:kl}
\[\KL(\pi_s,\pi^t_s) - \KL(\pi_s,\pi^{t+1}_s) - \KL(\pi^{t+1}_s,\pi^t_s) = -\eta_t\langle Q^t_s,\pi^{t+1}_s - \pi_s\rangle\quad\forall\pi_s.\]
Then, for $\pi_s = \pi^\star_s$ we have that
\begin{equation}
	\label{eq:kl2}
	\E_{s\sim d_\mu^\star}\langle \widehat{Q}^t_s,\pi^t_s - \pi^{t+1}_s\rangle + \E_{s\sim d_\mu^\star}\langle \widehat{Q}^t_s,\pi^\star_s - \pi^t_s\rangle\leq \KL^\star_t - \KL^\star_{t+1}.
\end{equation}
We bound the two terms on the left-hand side separately. For the first one, we have that
\begin{align*}
	\E_{s\sim d_\mu^\star}&\langle \widehat{Q}^t_s,\pi^t_s - \pi^{t+1}_s\rangle\\&\geq \norm{\frac{d^\star}{d_\mu^{t+1}}}_\infty\E_{s\sim d_\mu^{t+1}}\langle \widehat{Q}_s^t,\pi^t_s - \pi^{t+1}_s\rangle\\
	&= \nu_{k+1}(1-\gamma)\left(V^t(\mu)-V^{t+1}(\mu)\right)+ \nu_{k+1}\E_{s\sim d_\mu^{t+1}}\langle \widehat{Q}_s^t-Q_s^t,\pi^t_s - \pi^{t+1}_s\rangle\\
	&\geq  \nu_{k+1}(1-\gamma)\left(V^t(\mu)-V^{t+1}(\mu)\right)- \nu_{k+1}\tau,
\end{align*}
where the first inequality is due to Lemma~\ref{lemma:2}, the equality is due to the performance difference lemma~\eqref{lemma:pdiff} and the second inequality is due to Lemma~\ref{lemma:appr}. We use Lemma~\ref{lemma:appr} again to bound the second term in the left-hand side of \eqref{eq:kl2}
\begin{align*}
	\E_{s\sim d_\mu^\star}\langle \widehat{Q}^t_s,\pi^\star_s - \pi^t_s\rangle &= \E_{s\sim d_\mu^\star}\langle Q^t_s,\pi^\star_s - \pi^t_s\rangle + \E_{s\sim d_\mu^\star}\langle \widehat{Q}^t_s-Q^t_s,\pi^\star_s - \pi^t_s\rangle\\
	&\geq (1-\gamma)\left(V^\star(\mu)-V^t(\mu)\right) - \tau.
\end{align*}
Plugging the two bounds in \eqref{eq:kl2} we obtain
\[ \nu_{k+1}\left(\Delta_{t+1}-\Delta_t-\frac{\tau}{1-\gamma}\right)+\Delta_t\leq\frac{\KL^\star_t}{(1-\gamma)\eta_t} -\frac{\KL^\star_{t+1}}{(1-\gamma)\eta_t}+\frac{\tau}{1-\gamma},\]
where $\Delta_t = V^\star(\mu)-V^t(\mu)$. From Lemma \ref{lemma:2} we have that $\Delta_{t+1}-\Delta_t-\frac{\tau}{1-\gamma}\leq0$, so, since $\nu_{t+1}\leq \nu_\mu$, we can replace $\nu_{t+1}$ with $\nu_\mu$ and write
\[ \nu_\mu\left(\Delta_{t+1}-\Delta_t\right)+\Delta_t\leq\frac{\KL^\star_t}{(1-\gamma)\eta_t} -\frac{\KL^\star_{t+1}}{(1-\gamma)\eta_t}+\frac{(1+\nu_\mu)\tau}{1-\gamma}.\]
Rearranging and dividing by $\nu_\mu$ we obtain
\[\Delta_{t+1} + \frac{\KL^\star_{t+1}}{(1-\gamma) \nu_\mu\eta_t} \leq \left(1-\frac{1}{ \nu_\mu}\right)\left(\Delta_t+\frac{\KL^\star_t}{(1-\gamma)\eta_t( \nu_\mu-1)}\right) +\left(1+\frac{1}{\nu_\mu}\right)\frac{\tau}{1-\gamma}.\]
If the step sizes satisfy $\eta_{t+1}( \nu_\mu-1)\geq\eta_t \nu_\mu$, then
\[\Delta_{t+1} + \frac{\KL^\star_{t+1}}{(1-\gamma)\eta_{t+1}( \nu_\mu-1)} \leq \left(1-\frac{1}{ \nu_\mu}\right)\left(\Delta_t+\frac{\KL^\star_t}{(1-\gamma)\eta_t( \nu_\mu-1)}\right) +\frac{2\tau}{1-\gamma}\]

where we used that $\nu_\mu\geq 1$. The proof of the theorem follows by applying Lemma \ref{lemma:rec}.
\end{proof}


\section{Conclusion}
\label{conc}
We show how unregularized NPG can be tuned to achieve linear convergence for the log-linear policy class up to an error floor that depends on the statistical error of our estimates of $Q^t$ and the bias error of the best linear approximation of $Q^t$. Our results fill the gap between the findings in the tabular setting and the log-linear policy setting, taking advantage of a mirror-descent type analysis, and address research directions outlined in previous works \citep{RN266}. The main future direction is that of extending our framework and results to general policy parametrizations, as we suggest in Remark \ref{remark:1}.


\bibliography{References}
\bibliographystyle{plainnat}
\end{document}