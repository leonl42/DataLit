\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and Sun]{RN220}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{arXiv preprint arXiv:2006.10814}, 2020.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and Mahajan]{RN265}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{J. Mach. Learn. Res.}, 22\penalty0 (98):\penalty0 1--76, 2021.

\bibitem[Bhandari and Russo(2021)]{RN273}
Jalaj Bhandari and Daniel Russo.
\newblock On the linear convergence of policy gradient methods for finite mdps,
  2021.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{bhandari2018finite}
Jalaj Bhandari, Daniel Russo, and Raghav Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In \emph{Conference on learning theory}, pages 1691--1692. PMLR,
  2018.

\bibitem[Bubeck(2015)]{RN186}
SÃ©bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends in Machine Learning}, 2015.

\bibitem[Cayci et~al.(2021)Cayci, He, and Srikant]{RN280}
Semih Cayci, Niao He, and R~Srikant.
\newblock Linear convergence of entropy-regularized natural policy gradient
  with linear function approximation.
\newblock \emph{arXiv preprint arXiv:2106.04096}, 2021.

\bibitem[Cen et~al.(2021)Cen, Cheng, Chen, Wei, and Chi]{RN150}
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock \emph{Operations Research}, 2021.

\bibitem[Hu et~al.(2021)Hu, Ji, and Telgarsky]{RN279}
Yuzheng Hu, Ziwei Ji, and Matus Telgarsky.
\newblock Actor-critic is implicitly biased towards high entropy optimal
  policies.
\newblock \emph{arXiv preprint arXiv:2110.11280}, 2021.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{RN162}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In Abernethy Jacob and Agarwal Shivani, editors, \emph{Conference on
  Learning Theory}, pages 2137--2143, 2020.

\bibitem[Kakade and Langfor(2002)]{RN281}
Sham Kakade and John Langfor.
\newblock Approximately optimal approximate reinforcement learning.
\newblock \emph{Proceedings of the 19th International Conference on Machine
  Learning}, 2002.

\bibitem[Kakade(2002)]{RN159}
Sham~M. Kakade.
\newblock A natural policy gradient.
\newblock \emph{Advances in Neural Information Processing Systems}, 2002.

\bibitem[Khodadadian et~al.()Khodadadian, Jhunjhunwala, Varma, and
  Maguluri]{RN268}
Sajad Khodadadian, Prakirt~Raj Jhunjhunwala, Sushil~Mahavir Varma, and
  Siva~Theja Maguluri.
\newblock On the linear convergence of natural policy gradient algorithm.
\newblock In \emph{2021 60th IEEE Conference on Decision and Control (CDC)},
  pages 3794--3799. IEEE.
\newblock ISBN 166543659X.

\bibitem[Lan(2022)]{RN270}
Guanghui Lan.
\newblock Policy mirror descent for reinforcement learning: Linear convergence,
  new sampling complexity, and generalized problem classes.
\newblock \emph{Mathematical programming}, pages 1--48, 2022.
\newblock ISSN 1436-4646.

\bibitem[Li et~al.(2021)Li, Chen, Chi, Gu, and Wei]{RN237}
Gen Li, Yuxin Chen, Yuejie Chi, Yuantao Gu, and Yuting Wei.
\newblock Sample-efficient reinforcement learning is feasible for linearly
  realizable mdps with limited revisiting.
\newblock \emph{arXiv preprint arXiv:2105.08024}, 2021.

\bibitem[Li et~al.(2022)Li, Zhao, and Lan]{RN269}
Yan Li, Tuo Zhao, and Guanghui Lan.
\newblock Homotopic policy mirror descent: Policy convergence, implicit
  regularization, and improved sample complexity.
\newblock \emph{arXiv preprint arXiv:2201.09457}, 2022.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{RN272}
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  6820--6829. PMLR, 2020.
\newblock ISBN 2640-3498.

\bibitem[Modi et~al.(2021)Modi, Chen, Krishnamurthy, Jiang, and Agarwal]{RN236}
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2102.07035}, 2021.

\bibitem[Qiu et~al.(2021)Qiu, Yang, Ye, and Wang]{RN274}
Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang.
\newblock On finite-time convergence of actor-critic algorithm.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  2\penalty0 (2):\penalty0 652--664, 2021.
\newblock ISSN 2641-8770.

\bibitem[Raskutti and Mukherjee(2015)]{RN184}
Garvesh Raskutti and Sayan Mukherjee.
\newblock The information geometry of mirror descent.
\newblock \emph{IEEE Transactions on Information Theory}, pages 1451--1457,
  2015.

\bibitem[Schulman et~al.()Schulman, Levine, Abbeel, Jordan, and Moritz]{RN214}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897. PMLR.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{RN182}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour]{RN158}
Richard~S. Sutton, David~A. McAllester, Satinder~P. Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1057--1063, 1999.

\bibitem[Uehara et~al.(2021)Uehara, Zhang, and Sun]{RN240}
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun.
\newblock Representation learning for online and offline rl in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2110.04652}, 2021.

\bibitem[Wagenmaker et~al.(2022)Wagenmaker, Chen, Simchowitz, Du, and
  Jamieson]{RN275}
Andrew~J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson.
\newblock First-order regret in reinforcement learning with linear function
  approximation: A robust estimation approach.
\newblock In \emph{International Conference on Machine Learning}, pages
  22384--22429. PMLR, 2022.
\newblock ISBN 2640-3498.

\bibitem[Xiao(2022)]{RN266}
Lin Xiao.
\newblock On the convergence rates of policy gradient methods.
\newblock \emph{arXiv preprint arXiv:2201.07443}, 2022.

\bibitem[Yuan et~al.(2023)Yuan, Du, Gower, Lazaric, and Xiao]{yuan2023linear}
Rui Yuan, Simon~Shaolei Du, Robert~M. Gower, Alessandro Lazaric, and Lin Xiao.
\newblock Linear convergence of natural policy gradient methods with log-linear
  policies.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Zanette et~al.(2021)Zanette, Cheng, and Agarwal]{RN277}
Andrea Zanette, Ching-An Cheng, and Alekh Agarwal.
\newblock Cautiously optimistic policy optimization and exploration with linear
  function approximation.
\newblock In \emph{Conference on Learning Theory}, pages 4473--4525. PMLR,
  2021.
\newblock ISBN 2640-3498.

\bibitem[Zhan et~al.(2021)Zhan, Cen, Huang, Chen, Lee, and Chi]{RN242}
Wenhao Zhan, Shicong Cen, Baihe Huang, Yuxin Chen, Jason~D Lee, and Yuejie Chi.
\newblock Policy mirror descent for regularized reinforcement learning: A
  generalized framework with linear convergence.
\newblock \emph{arXiv preprint arXiv:2105.11066}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Ren, Yang, Gonzalez, Schuurmans, and
  Dai]{RN276}
Tianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans,
  and Bo~Dai.
\newblock Making linear mdps practical via contrastive representation learning,
  2022.

\end{thebibliography}
