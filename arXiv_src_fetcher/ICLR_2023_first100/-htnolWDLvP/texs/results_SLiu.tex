\iffalse 
We elaborate on our \textbf{experiment results} below.
\fi 
% \begin{table*}[htb]
% \centering
% \caption{\footnotesize{Performance overview of various machine unlearning methods  on dense and 95\%-sparse models considering two unlearning scenarios:
% %. ResNet-18 \cite{he2016deep} are used across different unlearning settings: 
% forgetting one class and forgetting random data points.  
% The overview of unlearning methods and evaluation metrics are provided in Table\,\ref{tab: summary_MU_methods_metrics}, and sparse models are obtained using OMP \cite{ma2021sanity}. 
% %We carefully tune the hyperparameters for all machine unlearning methods to report the model which can achieve the best unlearning performance at different sparsity ratios. The results $a_{\pm{b}}$ represent mean $a$ and standard deviation $b$ over $10$ random trials.
% The $a\%$ performance gap of an approximate unlearning method against Retrain is provided  
% %. The relative drop or improvement represented 
% in (\textcolor{blue}{$a$}).
% %$a$ or \textcolor{blue}{$\LARGE\uparrow$}$a$. 
% %The best performance of each unlearning method in each evaluation metric is in bold.
% }} 
% \label{tab: overall_performance}
% \vspace*{0.1in} % Requirements, do not delete.
% \resizebox{0.95\textwidth}{!}{
% \begin{tabular}{c|cc|cc|cc|cc|c}
% \toprule[1pt]
% \midrule
%   \multirow{2}{*}{\MU}& \multicolumn{2}{c|}{{\UA}} & \multicolumn{2}{c|}{{\MIAF}}& \multicolumn{2}{c|}{{\RA}} & \multicolumn{2}{c|}{{\TA}}&{\RTE}  \\ 
%   & \multicolumn{1}{c|}{{\textsc{Dense}}}  & \multicolumn{1}{c|}{$\mathbf{95\%}$ \textbf{Sparsity}}
%     & \multicolumn{1}{c|}{\textsc{Dense}}  & \multicolumn{1}{c|}{$\mathbf{95\%}$ \textbf{Sparsity}}
%     & \multicolumn{1}{c|}{\textsc{Dense}}  & \multicolumn{1}{c|}{$\mathbf{95\%}$ \textbf{Sparsity}}
%       & \multicolumn{1}{c|}{\textsc{Dense}}  & \multicolumn{1}{c|}{$\mathbf{95\%}$ \textbf{Sparsity}} & (min)
%   \\
% % \cline{3-10}

% \midrule
% \rowcolor{Gray}
% \multicolumn{10}{c}{\Large Class-wise Forgetting} \\
% \midrule
% \retrain &\textcolor{blue}{$100.00_{\pm{0.00}}$}    & \textcolor{blue}{$100.00_{\pm{0.00}}$}
% &\textcolor{blue}{$100.00_{\pm{0.00}}$}   & \textcolor{blue}{$100.00_{\pm{0.00}}$}
% &\textcolor{blue}{$100.00_{\pm{0.00}}$}    & \textcolor{blue}{$99.99_{\pm{0.01}}$}
% &\textcolor{blue}{$94.83_{\pm{0.11}}$}   & \textcolor{blue}{$91.80_{\pm{0.89}}$}
%  &64.48\\
%   \FT &$22.53_{\pm{8.16}}$ (\textcolor{blue}{$\LARGE\downarrow$}$77.47$)&$\mathbf{73.64}_{\pm{9.46}}$  (\textcolor{blue}{$\LARGE\downarrow$}${26.36}$)&$75.00_{\pm{14.68}}$ (\textcolor{blue}{$\LARGE\downarrow$}${25.00}$)& $\mathbf{83.02}_{\pm{16.33}}$ (\textcolor{blue}{$\LARGE\downarrow$}${16.98}$) 
%   &$99.87_{\pm{0.04}}$ (\textcolor{blue}{$\LARGE\downarrow$}0.13) & $\mathbf{99.87}_{\pm{0.05}}$ (\textcolor{blue}{$\LARGE\downarrow$}${0.12}$)&$94.31_{\pm{0.19}}$ (\textcolor{blue}{$\LARGE\downarrow$}0.52)
%  &$94.32_{\pm{0.12}}$ (\textcolor{blue}{$\LARGE\uparrow$}2.52)
% &   4.04
  
  
  
%   \\
%  \GA &$93.08_{\pm{0.29}}$ (\textcolor{blue}{$\LARGE\downarrow$}6.92) &$\mathbf{98.09}_{\pm{0.11}}$ (\textcolor{blue}{$\LARGE\downarrow$}${1.91}$)
% & $93.08_{\pm{0.31}}$ (\textcolor{blue}{$\LARGE\downarrow$}6.92)& $\mathbf{94.67}_{\pm{0.25}}$ (\textcolor{blue}{$\LARGE\downarrow$}${5.33}$)
% & $92.60_{\pm{0.25}}$ (\textcolor{blue}{$\LARGE\downarrow$}7.40)& $87.74_{\pm{0.27}}$ (\textcolor{blue}{$\LARGE\downarrow$}12.26) 
% & $86.64_{\pm{0.28}}$ (\textcolor{blue}{$\LARGE\downarrow$}8.19)& $82.58_{\pm{0.27}}$ (\textcolor{blue}{$\LARGE\downarrow$}9.22) 
% &   1.07
%  \\
%   {\FF}  & $79.93_{\pm{8.92}}$ (\textcolor{blue}{$\LARGE\downarrow$}20.07)& $\mathbf{94.83}_{\pm{4.29}}$ (\textcolor{blue}{$\LARGE\downarrow$}${5.17}$) 
%   & $100.00_{\pm{0.00}}$ (\textcolor{blue}{$\LARGE\downarrow$}0.00)& $\mathbf{100.00}_{\pm{0.00}}$ (\textcolor{blue}{$\LARGE\downarrow$}0.00) 
%     & $99.45_{\pm{0.24}}$ (\textcolor{blue}{$\LARGE\downarrow$}0.55)& $\mathbf{99.48}_{\pm{0.33}}$ (\textcolor{blue}{$\LARGE\downarrow$}${0.51}$)
%         & $94.18_{\pm{0.08}}$ (\textcolor{blue}{$\LARGE\downarrow$}0.65)& $94.04_{\pm{0.10}}$ (\textcolor{blue}{$\LARGE\uparrow$}2.24)& 58.67
%   \\
%  \IU 
%   &$87.82_{\pm{2.15}} $ (\textcolor{blue}{$\LARGE\downarrow$}12.18)& $\mathbf{99.47}_{\pm{0.15}}$ (\textcolor{blue}{$\LARGE\downarrow$}${0.53}$)
%  & $95.96_{\pm0.21}$ (\textcolor{blue}{$\LARGE\downarrow$}4.04)
% &$\mathbf{99.93}_{\pm{0.04}}$ (\textcolor{blue}{$\LARGE\downarrow$}${0.07}$)
%  &$97.98_{\pm{0.21}}$ (\textcolor{blue}{$\LARGE\downarrow$}2.02) 
%  &$97.24_{\pm{0.13}}$ (\textcolor{blue}{$\LARGE\downarrow$}2.76) 
%  &$91.42_{\pm{0.21}}$ (\textcolor{blue}{$\LARGE\downarrow$}3.41)&$\mathbf{90.76_{\pm{0.18}}}$ (\textcolor{blue}{$\LARGE\downarrow$}${1.04}$) & 5.23
%  \\
% %  \FTSparse &$100.00_{\pm{0.00}}$  &$100.00_{\pm{0.00}}$  & $91.49_{\pm{1.21}}$&$87.17_{\pm1.31}$
% %   &$100.00_{\pm{0.00}}$  &$100.00_{\pm{0.00}}$  & $91.69_{\pm{1.57}}$&$87.30_{\pm1.39}$
% %   &$100.00_{\pm{0.00}}$  &$100.00_{\pm{0.00}}$  & $95.74_{\pm{0.54}}$&$88.97_{\pm1.00}$
% % \\
% % \FTAO  
% %   & -&-  & -&-
% % & $43.82_{\pm{11.68}}$& $98.64_{\pm{0.71}}$ & $99.96_{\pm{0.03}}$&$94.79_{\pm0.07}$
% %   &$99.80_{\pm{0.19}}$  &$100.00_{\pm{0.00}}$ & $99.86_{\pm{0.05}}$&$94.55_{\pm0.11}$
% % \\
% \midrule
% \rowcolor{Gray}
% \multicolumn{10}{c}{\Large Random  Data Forgetting} \\
% \midrule
% \retrain &\textcolor{blue}{$56.27_{\pm{0.07}}$}   & \textcolor{blue}{$57.86_{\pm{0.05}}$ }
% &\textcolor{blue}{$75.23_{\pm{0.14}}$}   & \textcolor{blue}{$76.14_{\pm{0.11}}$ }
% &\textcolor{blue}{$100.00_{\pm{0.00}}$}    & \textcolor{blue}{$99.99_{\pm{0.01}}$}
% &\textcolor{blue}{$89.54_{\pm{0.11}}$ }   & \textcolor{blue}{$88.41_{\pm{0.89}}$}
%  & 66.40 \\
%   \FT &$1.89_{\pm{0.79}}$({\textcolor{blue}{$\LARGE\downarrow$}}54.38)&$\mathbf{19.34}_{\pm{1.41}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}38.52)& $17.11_{\pm{2.21}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}58.22) &$\mathbf{35.18}_{\pm{2.12}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}40.96)
%   &$99.92_{\pm{0.03}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}0.08) & $99.21_{\pm{0.05}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}0.78)&$93.50_{\pm{0.52}}$ ({\textcolor{blue}{$\LARGE\uparrow$}}3.96)
%  &$\mathbf{91.20}_{\pm{0.12}}$ ({\textcolor{blue}{$\LARGE\uparrow$}}2.79)
% &  4.19 
  
  
  
%   \\
%  \GA
% &$50.75_{\pm{3.29}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}5.52)& $\mathbf{59.12}_{\pm{3.17}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}1.26)  
% &$69.27_{\pm{4.27}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}5.96)& $\mathbf{74.06}_{\pm{3.15}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}2.08)
% &$98.43_{\pm{0.20}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}1.57)& $\mathbf{98.59}_{\pm{0.17}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}1.41)
% &$87.56_{\pm{0.24}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}1.98)& $\mathbf{87.12}_{\pm{0.31}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}1.29) &1.03
%  \\
%   $\FF$  &$4.85_{\pm{4.20}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}51.42) &$\mathbf{6.92}_{\pm{3.72}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}50.94)
%   &$11.29_{\pm{5.12}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}63.94) &$\mathbf{12.37}_{\pm{4.54}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}63.77)
%   &$97.30_{\pm{0.52}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}2.70) &$96.13_{\pm{0.41}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}3.86)
%   &$88.94_{\pm{0.21}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}0.60) &$87.32_{\pm{0.15}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}1.09) & 60.17
%   \\
%  \IU 
%   &$53.95_{\pm{1.24}} $ ({\textcolor{blue}{$\LARGE\downarrow$}}$2.32$)& $\mathbf{57.48}_{\pm{0.17}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}$\mathbf{0.38}$) 
%  & $75.88_{\pm1.16}$ ({\textcolor{blue}{$\LARGE\uparrow$}}0.65)
%  &$\mathbf{76.73}_{\pm{0.74}}$ ({\textcolor{blue}{$\LARGE\uparrow$}}0.59)
%  &$99.68_{\pm{0.11}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}0.32)
%  &$99.67_{\pm{0.05}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}0.33)
%  &$88.93_{\pm{0.10}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}0.61)&$88.28_{\pm{0.14}}$ ({\textcolor{blue}{$\LARGE\downarrow$}}0.13) & 5.11 \\
% \midrule
% \bottomrule[1pt]
% \end{tabular}
% }
% \vspace*{-3mm}

% \end{table*}


% {$\LARGE\uparrow$}












\noindent \textbf{Model sparsity improves approximate unlearning.}
% \SL{[Remove 75\% results? Talk to me.]}
In \textbf{Tab.\,\ref{tab: overall_performance}}, we study the impact of model sparsity  on  the performance of various {\MU} methods 
%when applied to   class-wise forgetting   or  randomly  forgetting $9\%$ training set ($4400$ data points) 
in the `prune first, then unlearn' paradigm. 
The performance of the exact unlearning method  ({\retrain}) is also provided for comparison. 
Note that the better performance of    approximate unlearning  corresponds to the  smaller performance gap with  the gold-standard retrained model. 
%This also
%applies to other metrics.
%We summarize our   observations   below.
%Our key observations  and insights are illustrated below. 
%Two key insights can be drawn from Table\,\ref{tab: overall_performance}. 



\textit{First}, given an approximate unlearning  method ({\FT}, {\GA}, {\FF}, or {\IU}), we consistently observe that model sparsity improves {\UA} and {\MIAF} (\textit{i.e.}, the efficacy of approximate unlearning) without  much performance loss in  {\RA} (\textit{i.e.}, fidelity).
In particular, 
  the  performance gap between each approximate unlearning method and {\retrain} reduces as the model becomes sparser (see the `95\% sparsity' column vs. the `dense' column).
  %This also generally holds in other evaluation criteria, {\RA} and {\TA}. 
  Note that 
  the performance gap against {\retrain} is highlighted in $(\cdot)$ for each approximate unlearning.
  We also   observe that {\retrain}  on the 95\%-sparsity model encounters a  3\% {\TA} drop. Yet, from the perspective of approximate unlearning, this drop brings in 
  a more significant improvement  in    {\UA} and {\MIAF}  when model sparsity is promoted. Let us take {\FT} (the simplest unlearning method) for class-wise forgetting as an example. As the model sparsity   reaches   $95\%$, we obtain $51\%$ {\UA} improvement and $8\%$ {\MIAF} improvement. 
  Furthermore, {\FT} and {\IU} on the 95\%-sparsity model can better preserve  {\TA} compared to other  methods. 
Table\,\ref{tab: overall_performance} further indicates that sparsity reduces average disparity compared to a dense model across various approximate {\MU} methods and unlearning scenarios.
  %, and they can significantly improve their dense counterparts in {\UA} and {\MIAF}.



% and {\TA} (\textit{i.e.}, generalization). Let us take {\FT} (the simplest unlearning method) for class-wise forgetting as an example. As the model sparsity   reaches   $95\%$, we obtain $51\%$ {\UA} improvement and $8\%$ {\MIAF} improvement. %without any {\RA} and {\TA} drop. 

\iffalse 
  \textit{Second}, 
if we peer into the unlearning efficacy ({\UA} and {\MIAF}), 
  the  performance gap between each approximate unlearning method and {\retrain} reduces as the model becomes sparser (see the `95\% sparsity' column vs. the `dense' column). This also generally holds in other evaluation criteria, {\RA} and {\TA}. Note that the performance gap against {\retrain} is highlighted in $(\cdot)$ for each approximate unlearning.
On the other hand, we   observe that {\TA} of {\retrain}  on the 95\%-sparsity model may lead to  3\% accuracy drop. Yet, this drop allows a more significant improvement of approximate unlearning in    fidelity ({\UA} and {\MIAF})  on     sparse  models. Furthermore, {\FT} and {\IU} on the 95\%-sparsity model can better preserve  {\TA} compared to other approximate unlearning methods, and they can significantly improve their dense counterparts in {\UA} and {\MIAF}.
\fi 

  \iffalse 
\SL{The performance gap with {\retrain} is highlighted in $(\cdot)$ for each approximate unlearning method under each metric.  We also note that {\retrain} on the model of $95\%$ sparsity yields a slight {\TA} drop compared to the dense and $75\%$-sparse cases. This suggests that an extremely high sparsity may give rise to a tradeoff between unlearning efficacy and generalization.}
\YL{im thinking if this is a fair comparison. Sparser model does seem to have a significantly lower accuracy. I wonder if people can argue whether the reduced gap is due to the benefit of being sparse, or because of the drop of model performance and for example increasing uncertainty, which seem likely to be mediation factors.}
\fi 

\input{texs/figs/results_l1_sparse_unlearn}
\textit{Second},  existing approximate unlearning methods have different pros and cons. Let us focus on the regime of $95\%$ sparsity.  We observe that {\FT}  typically yields the best {\RA} and {\TA}, which has a tradeoff with its unlearning efficacy ({\UA} and {\MIAF}). Moreover, {\GA} yields the worst {\RA} since it is most loosely connected with the remaining dataset $\Dr$.  {{\FF} becomes ineffective when scrubbing random data points compared to its class-wise unlearning performance. 
%This is not surprising since {\FF} is allowed to modify the model architecture when unlearning a class.} 
Furthermore,    {\IU} causes a {\TA} drop but yields the smallest gap with exact unlearning across diverse metrics under the $95\%$ model sparsity.
%We refer readers to Appendix\,\ref{appendix: additional results} for more dataset results. 
% \SL{[details.]}
In Appendix\,\ref{appendix: additional results}, we provide additional results on CIFAR-100 and SVHN datasets, as shown in Tab.\,\ref{tab: overall_performance_ext_datasets}, as well as on the ImageNet dataset, depicted in Tab.\,\ref{tab: overall_performance_ImageNet}. Other results pertaining to the VGG-16 architecture are  provided in Tab.\,\ref{tab: overall_performance_ext_archs}.
% Tab.\,\ref{tab: overall_performance_ext_datasets}
% Tab.\,\ref{tab: overall_performance_ext_archs}
% Tab.\,\ref{tab: overall_performance_ImageNet}




\iffalse 
% \paragraph{Overall performance: Sparsity reduces the gap between exact unlearning and approximate unlearning.}
We look at the relationship between machine learning and sparsity in what follows. There is \textit{one key observation} drawn from our overall results: Sparsity can reduce the gap between exact unlearning and approximate unlearning across all datasets and machine unlearning methods (as shown in \textbf{Table\,\ref{tab: overall_performance})}. Table\,\ref{tab: overall_performance} shows different evaluation metrics of different unlearning methods at different sparsity levels across different datasets. For comparison, we also present the performance of exact unlearning methods Retrain. As we can see, the gap between imperfect unlearning and perfect unlearning is decreasing with sparsity growing, especially in {\UA} and {\MIAF} these two metrics. For example, the gap between {\FT} and Retrain of $95\%$ sparsity model drops $51.11\%$  compared to the dense model in  the {\UA} on the CIFAR-10 dataset. This phenomenon is also justified on different datasets shown in Table\,\ref{tab: overall_performance}. Although sparsity can reduce the gap between exact unlearning and approximate unlearning, the improvement is different in the diverse machine unlearning methods. From Table\,\ref{tab: overall_performance}, {\FT} will benefit from sparsity most, where {\MIAF} reduced $13\%$-$20\%$ across all datasets.  
Besides, we also can observe in most cases that {\IU} outperforms other unlearning methods at different sparsity levels, which is the most competitive method in different evaluation metrics. However, this method needs to be tuned carefully to choose suitable hyperparameters.
\fi 



% \noindent \textbf{\JC{Ablation study on parameter scheduler in \MUSparse.}}
% % However, as \citep{bach2012optimization} mentioned, the downside of $\ell_1$ regulation term will affect the is its loss  in {\RA} and {\TA} compared to {\FT} and {\retrain}. Therefore, we conducted a comprehensive study of the scheduler of $\lambda$ in {\MUSparse}. In \textbf{Tab.\,\ref{tab: ablation_l1_scheduler}}, we present the results of unlearning performance on different parameter schedulers: constant scheduler, linear growing scheduler, and linear decaying scheduler.
% % It shows that the decaying $\gamma$ scheduler performs the best among all the schedulers. If we directly apply a constant $\lambda$ to {\MUSparse}, it will either get a low {\UA} with lower $\lambda$ (for $\lambda = 0$, the method reduces to {\FT}) or worse {\RA} and {\TA} 
% %  under higher $\lambda$. In Tab.\,\ref{tab: ablation_l1_scheduler}, we picked a sweet point to get a balance between them. If we use the linear growing scheduler, which means the method focuses on the unlearning term first then moves the focus to sparsity. If we use the linear decaying scheduler, the method focuses on the unlearning term first then moves the focus to sparsity.
% As Sec.\,\ref{sec: sparsity_MU_alg} pointed out, the downside of the $\ell_1$ regularization term is its suppression on {\RA} and {\TA} compared to {\FT} and {\retrain}. To facilitate this deficiency, we introduced a well-designed scheduler to $\gamma$, the parameter of the regularization term, and conducted a comprehensive ablation study on designing the scheduler. The results of unlearning performance with different parameter schedulers, constant, linearly increasing, and linearly decreasing schedulers, are presented in \textbf{Tab.\,\ref{tab: ablation_l1_scheduler}}.

% \input{texs/figs/results_l1_sparse_unlearn}
% Directly applying a constant $\gamma$ to {\MUSparse} would either yield a low unlearning efficacy with lower $\gamma$ (for $\gamma = 0$, the method reduces to {\FT}) or degraded generalization performance under higher $\gamma$. In \textbf{Tab.\,\ref{tab: ablation_l1_scheduler}}, we have identified an optimal point that balances these metrics. Still, it cannot achieve as good performance as scheduled $\gamma$. The linearly increasing scheduler implies that the method initially emphasizes the unlearning term, then gradually shifts its focus towards sparsity. Conversely, the linearly decaying scheduler suggests that the focus initially lands on the sparsity, then gradually shifts to the unlearning term. The results reveal that the decaying scheduler outperforms all the others on both class-wise forgetting and random data forgetting, which aligned with the inspiration of the `prune first, then unlearn' paradigm. Therefore, the decaying scheduler will use in successive experiments except specified otherwise.



\noindent \textbf{{Effectiveness of sparsity-aware unlearning.}}
\iffalse
In  \textbf{Fig.\,\ref{fig: results_l1_sparse_unlearn}},
%and \textbf{Fig.\,\ref{fig: results_MU_pruning}},
%\textbf{Table\,\ref{tab: sparse_MU vs MU}} \SL{[or Fig.\,\ref{fig: results_MU_pruning}]}, 
we present the performance of proposed sparsity-aware unlearning methods (\textit{i.e.}, {\MUSparse}). 
%We      show the performance of {\retrain} and {\IU}-based approximate unlearning for comparison. 
To justify the effectiveness of our proposal, we implement  {\MUSparse} following the objective of {\FT},  the approximate unlearning method with the largest efficacy gap against exact unlearning on the original dense model as shown in Tab.\,\ref{tab: overall_performance}.

%As shown in Table\,\ref{tab: overall_performance}, {\IU} on sparse models can yield the    unlearning performance closest  to {\retrain}. 

%Moreover, to better justify the efficacy of our proposal, we adopt the  {\FT}-based unlearning objective and method to implement  {\MUSparse} and {\MUAO}. Recall that {\FT} is the simplest fine-tuning method with the worst unlearning efficacy on the dense model. 

As shown in \textbf{Fig.\,\ref{fig: results_l1_sparse_unlearn}}, {\MUSparse} improves the efficacy of unlearning (in terms of {\UA} and {\MIAF}) over {\IU}, and only has a quite small gap with {\retrain}  even if  the model considered for unlearning  is dense (without ever pruning). 
 This is because {\MUSparse} imposes a sparse regularization in  \eqref{eq: MUSparse} to penalize the model weights during unlearning, as shown in Fig.\,\ref{fig: l1_weight_magnitude}.
%outperforms {\IU}  in {\UA} and {\MIAF} and 
% Furthermore, 
% we note that {\MUAO} reduces to {\FT}  on dense model (\textit{i.e.}, there exists no alternating optimization between unlearning and pruning when $p\% = 0$). Thus, {\MUAO} only remains effective in the sparse regime  ($p >0$) and  outperforms {\MUSparse} in general, particularly in  {\RA} and {\TA}.
More results 
%vs. sparsity   
can be found in Fig.\,\ref{fig: results_l1_sparse_unlearn_others}. We also refer readers to Appendix \ref{appendix: additional results} for more unlearning scenarios. 
%in particular for the significant improvement in {\RA} and {\TA}.



% {\MUSparse} formulates unlearning as a sparsity-promoting optimization problem. 
\fi
In \textbf{Fig.\,\ref{fig: results_l1_sparse_unlearn}},
we   showcase the effectiveness of the proposed sparsity-aware unlearning method, \textit{i.e.}, {\MUSparse}. 
For ease of presentation, we focus on the comparison with  {\FT} and the optimal {\retrain}  strategy in both class-wise forgetting and random data forgetting  scenarios under (CIFAR-10, ResNet-18). As we can see, {\MUSparse}  outperforms {\FT} in  the unlearning efficacy ({\UA} and {\MIAF}), and closes the performance gap with {\retrain}  without losing the computation advantage of approximate unlearning. We refer readers to Appendix\,\ref{appendix: additional results} and Fig.\,\ref{fig: results_l1_sparse_unlearn_others} for further exploration of {\MUSparse} on additional datasets.
% \textbf{Fig.\,\ref{fig: results_l1_sparse_unlearn}} shows that {\MUSparse}  outperforms the conventional unlearning method such as {\FF}, and closes the performance gap with {\retrain} . 
% enhances the efficacy of unlearning (in terms of {\UA} and {\MIAF}) when compared to {\FF}, and the gap with {\retrain} is minimal, and the model considered for unlearning is dense (without any pruning). This is primarily attributed to {\MUSparse} imposing a sparse regularization, as per \eqref{eq: MUSparse}, to penalize the model weights during the unlearning process, as depicted in Fig.\,\ref{fig: l1_weight_magnitude}. Additional results are presented in Fig.\,\ref{fig: results_l1_sparse_unlearn_others}. 




\iffalse
\fi
%As a result, both {\IU} and {\GA} have advantages in privacy preservation of $\Dr$ after unlearning.




% \paragraph{Weight pruning gives rise to tradeoff in {\MU} between  efficacy and generalization.}
% One figure demonstrates that sparsity will bring degradation in the test accuracy, but improve efficacy.



% \iffalse 
% \paragraph{Prune first, then unlearn vs. Sparsity-infused MU.}
% We have demonstrated that sparsity will reduce the gap between exact unlearning and approximate unlearning in the previous results. Here we conduct experiments to verify the effectiveness of our proposed sparse-aware unlearning methods. \textbf{Table\,\ref{tab: sparse_MU vs MU}} shows the comparison between our proposed sparse-aware unlearning methods and the best `prune first, then unlearn' methods {\IU} on the CIFAR-10 dataset. We can observe that our proposed methods outperform the {\IU} in each metric.  
% \fi 



% \paragraph{Sparsification improves MU’s accuracy and efficacy across different models and unlearning scenarios}
% Table shows the relationship between sparsity and unlearning when using different arch and unlearning settings. 

%\SL{I stop here.}

\input{texs/figs/results_MU_pruning_backdoor}
\noindent \textbf{Application: {\MU} for Trojan model cleanse.}
We next present an application of {\MU} to remove the influence of poisoned backdoor data from a learned model,  following the backdoor attack setup   \cite{gu2017badnets}, where an adversary 
%The so-called backdoor (poisoning) attack \citep{gu2017badnets,goldblum2022dataset} 
manipulates a small portion of training data (\textit{a.k.a.}   poisoning ratio) by 
injecting a backdoor trigger (\textit{e.g.}, a small image patch) and modifying data labels towards a targeted incorrect label.  
%attack then serves as a ‘backdoor’ and enforces a spurious correlation between the Trojan trigger and the model training. 
The trained model is called \textit{Trojan model}, yielding the backdoor-designated incorrect prediction if the trigger is present at testing. Otherwise, it behaves normally. 
% That is, 
% training over the poisoned data set will enforce a spurious correlation between the Trojan trigger and the model prediction, so that the former  serves as a ‘backdoor’ for the trained model. 
%Backdoor Since To demonstrate the unlearning performance, we set up several backdoor attack experiments to


We then regard {\MU} as a defensive method to scrub the harmful influence of  poisoned training data in  the model's prediction, with a similar motivation as \citet{liu2022backdoor}.
%We assume that the set of poisoned data points is known \textit{a priori}, \textit{e.g.}, via Trojan trigger  detection \cite{wang2020practical}.
We evaluate the performance of the unlearned model from two perspectives, backdoor attack success rate (\textbf{ASR}) and standard accuracy (\textbf{SA}). 
\textbf{Fig.\,\ref{fig: results_MU_pruning_backdoor}} shows   ASR and {SA} of the   Trojan model (with poisoning ratio $10\%$)  and its unlearned version using the simplest {\FT} method against model sparsity. {Fig.\,\ref{fig: results_MU_pruning_backdoor} also includes the $\ell_1$-sparse {\MU} to demonstrate its effectiveness on  model cleanse. Since it is applied to a dense model (without using hard thresholding to force weight sparsity), it contributes just a single data point at the sparsity level 0\%.}
As we can see, the  original Trojan model maintains $100\%$ ASR and a similar SA across different model sparsity levels. By contrast, {\FT}-based unlearning can  reduce ASR without inducing much {SA} loss. Such a defensive advantage becomes more significant when sparsity reaches $90\%$. {Besides, $\ell_1$-sparse {\MU} can also effectively remove the backdoor effect while largely preserving the model’s generalization.} 
Thus, our proposed unlearning shows promise in  application of backdoor attack defense.

\noindent \textbf{Application: {\MU} to improve transfer learning.}
Further, we utilize the   {\MUSparse} method to mitigate the   impact of harmful data classes of   ImageNet    on transfer learning.  This approach is inspired by \citet{jain2022data}, which shows that  removing specific negatively-influenced ImageNet classes and retraining a source model  can enhance its transfer learning accuracy on    downstream   datasets  after finetuning. However, retraining the source model introduces additional computational overhead. {\MU} naturally addresses this limitation and offers a solution.
% a transfer influence score was proposed in \cite{jain2022data} to evaluate the usefulness of ImageNet data classes,  retraining the source model raises the computation overhead. 

% In the following section, we introduce an application of MU that removes the influence of harmful source classes in a pre-trained model, thereby enhancing the performance of transfer learning. Transfer learning, as it is well known, allows for the adaptation of a model trained on a \textit{source dataset} to optimize its performance on a \textit{downstream target task}.
% Recent research \cite{jain2022data} indicates that the exclusion of detrimental data from the source dataset can indeed augment the performance of transfer learning. Additionally, the same study provided a method capable of identifying beneficial subsets of the source dataset for different downstream tasks. However, the retraining of a model considering the optimal subset of the source dataset for each downstream task is computationally expensive and time-consuming.


\textbf{Tab.\,\ref{tab: transfer_results}} illustrates the transfer learning accuracy of the unlearned or retrained source model (ResNet-18) on ImageNet, with $n$ classes removed. The downstream target datasets used for evaluation are  SUN397 \cite{xiao2010sun} and OxfordPets \cite{parkhi2012cats}.
The  employed finetuning approach   is linear probing, which finetunes the classification head of the source model on target datasets while keeping the feature extraction network of the source model intact. 
% \JC{Our analysis focuses on fixed-weight transfer learning \cite{jain2022data}, which updates the linear classification head with the target domain data while freezing the feature extraction network.
% And we leverage transfer influences in \cite{jain2022data} to determine the classes to be unlearned. }%\SL{[Is the above true?]}
As we can see, removing data classes from the source ImageNet dataset    can lead to improved transfer learning accuracy compared to the conventional method of using the pre-trained model on the full ImageNet  (\textit{i.e.}, $n = 0$). Moreover,
our proposed 
\input{texs/tabs/results_MU_transfer}
{\MUSparse} method achieves comparable or even slightly better 
transfer learning accuracy than the retraining-based approach \citep{jain2022data}.  Importantly, {\MUSparse} offers the advantage of computational efficiency 2$\times$ speed up over previous method \citep{jain2022data} across all cases, making it an appealing choice for transfer learning using large-scale models.
Here we remark that in order to align with previous method \cite{jain2022data}, we employed a fast-forward computer vision training pipeline  (FFCV) \citep{leclerc2022ffcv}
to accelerate our ImageNet training on GPUs.
% \SL{[Missing FFCV discussion and citation. E.g., We remark that   a fast forward computer vision (FFCV) [refs] training pipeline is used to accelerate our ImageNet training on GPUs.]}
% demonstrates the performance of transfer learning on SUN397 \cite{xiao2010sun} and OxfordPets \cite{parkhi2012cats}, considering different numbers of excluded ImageNet classes as identified in \cite{jain2022data}. 
% As the table shown, {\FT}-based {\MU} consistently surpasses the performance of the full ImageNet pre-trained model, regardless of the number of ImageNet classes removed. For instance, {\FT}-based {\MU} enhances the accuracy of target tasks by $0.54\%$ compared to the full ImageNet pre-trained model when the number of ImageNet classes removed is $300$ on the OxfordPets with almost half time of retrain methods. Therefore, we propose that our unlearning method presents an effective strategy for enhancing transfer learning.


% To address this challenge, we employ {\FT}-based {\MU}  to neutralize the impact of detrimental source classes in the full-ImageNet pre-trained model, thereby improving transfer learning. The proposed methodology effectively eliminates the need for retraining the model with each optimal subset of the source dataset, making the process more efficient and scalable. 
% Here we operate under the assumption that the subset requiring removal is already known a priori, following the method detailed in the previous work \cite{jain2022data}. We adhere to the same training configuration for the full ImageNet pre-trained model as set out in \cite{jain2022data}, electing to use the linear probe (LP) as our transfer training protocol.

% \textbf{Tab. \ref{tab: transfer_results}} demonstrates the performance of transfer learning on SUN397 \cite{xiao2010sun} and OxfordPets \cite{parkhi2012cats}, considering different numbers of excluded ImageNet classes as identified in \cite{jain2022data}. 
% As the table shown, {\FT}-based {\MU} consistently surpasses the performance of the full ImageNet pre-trained model, regardless of the number of ImageNet classes removed. For instance, {\FT}-based {\MU} enhances the accuracy of target tasks by $0.54\%$ compared to the full ImageNet pre-trained model when the number of ImageNet classes removed is $300$ on the OxfordPets with almost half time of retrain methods. Therefore, we propose that our unlearning method presents an effective strategy for enhancing transfer learning.

% Here we assume that the subset should be scrubbed as a known prior via the method mentioned in previous work \cite{jain2022data}. We followed the same training setting for the full ImageNet pre-trained model in \cite{jain2022data}, and chose linear probe (LP) as our transfer training protocol. \textbf{Fig}\,\ref{fig: results_MU_transfer} shows the performance of transfer learning on SUN397 \cite{xiao2010sun} and OxfordPets \cite{parkhi2012cats} with different size of excluding ImageNet classes identified from \cite{jain2022data}. As we can see, the accuracy on the downstream task will increase first, then decrease with the number of ImageNet classes removed increasing. Besides, {\FT}-based {\MU} can outperform the full ImageNet pre-trained model at different numbers of ImageNet classes removed. For example, {\FT}-based {\MU} can boost $2\%$ target tasks accuracy compared to that of the full ImageNet pre-trained model. Thus, the proposed unlearning gives an effective method to improve transfer learning. 







% Since MU's purpose is to remove the effect of specific data, there is a straightforward application for MU. Backdoored neural network \citep{gu2017badnets}, or \textit{BadNet}, shows that when attackers poison part of the training dataset in a given pattern, the neural network, trained at the poisoned dataset, will misbehave on the images with the same pattern. Assuming we already know which part of the dataset was poisoned, we can apply MU methods to neutralize the poisoned images' influence to defend against data poisoning attacks on models. An effective MU method will get a relatively lower attack success rate (ASR) and be more stable in forgetting the poisoned data. To demonstrate the performance of the MU methods, we performed several experiments. 



% We follow the \textit{All-to-all attack} in \citep{gu2017badnets} to poison part of the training dataset. The model was trained and pruned over the same poisoned dataset in different sparsity. Then we deemed the poisoned dataset as the forgetting dataset, and applied both the `\textit{prune first, then unlearn}' methods and the `\textit{sparsity-aware unlearning}' methods. Additionally, In the `prune first, then unlearn' settings, we chose the OMP and Finetune as the backbones of pruning and unlearning. Fig \ref{fig: results_MU_pruning_backdoor} shows the results of experiments under backdoor attack settings. The curve indicates that under either unlearn method, the ASR decreased rapidly when the model became sparser, with a negligible trade-off in standard accuracy (SA).

% TODO: add results and analysis
% \vspace*{-4mm}
\noindent \textbf{Additional results.} 
{%We include more results in Appendix\,\ref{appendix: additional results}. In particular, 
We found that model sparsity also enhances the privacy of the unlearned model, as evidenced by a lower {\MIAR}. Refer to Appendix\,\ref{appendix: additional results} and Fig.\,\ref{fig: results_privacy} for more results. In addition, we have expanded our experimental scope to encompass the `prune first, then unlearn' approach across various datasets and architectures. The results can be found in Tab.\,\ref{tab: overall_performance_ext_datasets}, Tab.\,\ref{tab: overall_performance_ext_archs}, and Tab.\,\ref{tab: overall_performance_ImageNet}. Furthermore, we conducted experiments on the $\ell_1$-sparse {\MU} across different datasets, the Swin-Transformer architecture, and varying model sizes within the ResNet family. The corresponding findings are presented in Fig.\,\ref{fig: results_l1_sparse_unlearn_others} and Tab.\,\ref{tab: sparse_MU vs MU}, \ref{tab: vit}, \ref{tab: overall_perfoamnce_arch_20} and \ref{tab: overall_perfoamnce_arch_50}.}

