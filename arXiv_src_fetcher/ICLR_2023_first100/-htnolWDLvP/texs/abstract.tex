\begin{abstract}
In response to recent data regulation requirements, machine unlearning (\MU) has emerged as a critical process to remove the influence of specific examples from a given model. Although {exact} unlearning can be achieved through complete model retraining using the remaining dataset, the associated computational costs have driven the development of efficient, approximate unlearning techniques. Moving beyond data-centric MU approaches, our study introduces a novel model-based perspective: model sparsification via weight pruning, which is capable of reducing the gap between exact unlearning and approximate unlearning. 
We show in both theory and practice  that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. This   leads to a new MU paradigm,    termed {prune first, then unlearn}, which infuses a sparse model prior into the unlearning process. Building on this insight, we  also develop a sparsity-aware unlearning method that utilizes sparsity regularization to enhance the training process of   approximate unlearning.
Extensive experiments show that our  proposals consistently benefit MU in various unlearning scenarios. A notable highlight is the 77\% unlearning efficacy gain of fine-tuning (one of the simplest  unlearning methods) when    using sparsity-aware unlearning.
Furthermore, we  demonstrate the practical impact of  our proposed MU methods in addressing  other machine learning challenges, such as defending against backdoor attacks and  enhancing transfer learning.
{Codes are available at \url{https://github.com/OPTML-Group/Unlearn-Sparse}.}

%(by source class removal), demonstrating the usefulness  of our approaches to address machine learning challenges beyond unlearning for data privacy.

%Furthermore, we showcase the practical impact of our approaches via two applications, model cleanse against backdoor attacks and source data removal for  improved transfer learning. 
%including class-wise data scrubbing, random data scrubbing, and backdoor data
%forgetting.

% we propose an unlearning meta-strategy, termed ``\textbf{prune first, then unlearn}.'' Our research demonstrates, both theoretically and practically, that model sparsity enhances the multi-criteria unlearning performance of an approximate unlearner, bridging the approximation gap while maintaining efficiency. Building on this insight, we introduce a new sparsity-regulated unlearning method called ``\textbf{sparsity-aware unlearning}.'' Comprehensive experiments reveal that our insights and proposals consistently benefit {\MU} across various contexts, including class-wise data scrubbing and random data scrubbing. A notable highlight is the 77\% unlearning efficacy gain of fine-tuning, one of the simplest approximate unlearning methods, within the proposed sparsity-aware unlearning paradigm. Furthermore, our investigation delves into two practical application environments, trojan attacks and transfer learning, showcasing the adaptability of our approach.

% Furthermore, our investigation delves into two practical application environments, trojan attacks and transfer learning, demonstrating the versatility of our approach 
% \JC{Abstract revised.}One highlight is the 77\% unlearning efficacy gain of fine-tuning (one of the simplest approximate unlearning methods) in the proposed sparsity-aware unlearning paradigm.

% Recent data regulations necessitate machine unlearning (\MU): The removal of the effect of specific examples from the model. While \textit{exact} unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric {\MU} solutions, we advance {\MU} through a  novel model-based viewpoint: model sparsification via weight pruning. Based on this insight, we develop a straightforward unlearning meta-scheme: `prune first then unlearn'. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we proposed a new sparsity-regulized unlearning method termed `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit {\MU} in various scenarios including class-wise data scrubbing and random data scrubbing. Furthermore, our study delves into several practical application environments, which are trojan attacks and transfer learning, indicating our method can significantly improve the model's robustness and performance.

% One highlight is the 77\% unlearning efficacy gain of fine-tuning (one of the simplest approximate unlearning methods) in the proposed sparsity-aware unlearning paradigm.
\iffalse
Recent data regulations necessitate machine unlearning (\MU): The removal of the effect of specific examples from the model. While \textit{exact} unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond  data-centric  {\MU} solutions,
%Motivated by the model-based perspective
%improved generalization and privacy of sparse pruned models, 
we advance  {\MU}   through a  novel model-based  viewpoint:   sparsification via weight pruning.
%We take a thorough look at the connection between \textit{model sparsity} and unlearning. 
Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop   two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and  `sparsity-aware unlearning'.  Extensive experiments show that our findings and proposals consistently benefit   {\MU}    in various scenarios  including class-wise data scrubbing, random data scrubbing, and  backdoor data forgetting. One highlight is the 77\% unlearning efficacy  gain of   fine-tuning (one of the simplest approximate unlearning methods) in the proposed  sparsity-aware unlearning paradigm.
%a 77\%  efficacy gain for fine-tuning (one of the simplest approximate unlearning methods). 
%yields upto to 77\% improvement in 

%reduce the approximation gap by upto to xx\% across all unlearning criteria, while maintaining its yy\% improved efficiency over exact unlearning.
\fi


\iffalse 
Machine unlearning ({\MU}) has received increasing attention as a way to remove (or scrub) the impact of some training data points on the learned model. 
%and poses a general problem of understanding and analyzing the  influence of data points from trained models. 
Yet, it remains a major challenge to achieve {\MU} in a precise and efficient manner. In this work, we advance   {\MU}   through a fresh and novel viewpoint: model sparsification. We  show that model sparsity (achieved by weight pruning) enables us to close the gap between simple approximate  unlearning    and the ground-truth exact unlearning   (\textit{i.e.}, retraining from scratch on the training set after removing the data points to be scrubbed). To this end,  we   revisit and  taxonomize  {\MU} training and evaluation methods to establish   an integrated  testbed for exploring the interconnection between model unlearning and pruning. Theoretically, we prove the benefit of model sparsity in reducing the unlearning   error in a stochastic gradient descent (SGD)-based training process. We also empirically show that a simple pruning method (\textit{e.g.}, model pruning at random initialization or one-shot magnitude pruning) suffices to empower {\MU} to improve its efficacy and fidelity. With the aid of model pruning, we further propose two new unlearning paradigms, termed `prune first, then unlearn' and  `sparsity-aware unlearning', respectively. Extensive experiments show that our proposed approaches yield substantial improvements over unlearning baselines in multiple data-model setups and unlearning scenarios. \SL{As highlighted below, we show that unlearning a pruned ResNet-18 model with {95\%} sparsity on CIFAR-10 yields the closest unlearning performance   to exact unlearning than baseline methods.} 
\SL{[Fill in a key highlight in experiments.]} 
\fi 


%Also, our extensive experiments in different data-model setups and unlearning scenarios show that model sparisi    
\end{abstract}