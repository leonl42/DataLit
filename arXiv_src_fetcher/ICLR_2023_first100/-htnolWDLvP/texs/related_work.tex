\section{Related Work}
\vspace*{-2mm}
% \JC{Ref Sec 2. more thorough related work }
While Sec.\,\ref{sec: primer_MU} provides a summary of related works concerning exact and approximate unlearning methods and metrics,  a more comprehensive review  is provided below.

\noindent \textbf{Machine unlearning.}
%% sparsity in unlearning.
\iffalse 
{\MU} was first introduced in   \cite{cao2015towards}, which aims to make  an ML model to `forget'  data points upon completion of training.   {\MU}  was originally used  to prevent the leakage of  data privacy  from the trained model,   
in particular in coming forth with legislation like  General Data Protection Regulation (GDPR) \cite{hoofnagle2019european} and  California Consumer Privacy Act (CCPA) \cite{pardau2018california}. 
A straightforward approach  to unlearning is to retrain the model from scratch (\textit{i.e.}, {\retrain}) after removing the forgetting dataset from the original training set. Although {\retrain} yields the ground-truth unlearning strategy, it is least efficient in computation. Thus,   approximate but fast unlearning    becomes a significant research focus nowadays; examples include \cite{golatkar2020eternal,warnecke2021machine,graves2021amnesiac,thudi2021unrolling,becker2022evaluating,izzo2021approximate} as we reviewed in Sec.\,\ref{sec: primer_MU}. 
\fi 
In addition to exact and approximate unlearning methods as we have reviewed in Sec.\,\ref{sec: primer_MU}, there  exists other literature aiming to  develop  the probabilistic notion of unlearning \cite{ginart2019making,guo2019certified,neel2021descent,ullah2021machine,sekhari2021remember}, in particular through the lens of differential privacy (DP) \cite{dwork2006our}. Although DP enables unlearning  with provable error guarantees, %\cite{guo2019certified,neel2021descent,ullah2021machine,sekhari2021remember}, 
they typically require strong model and algorithmic assumptions and could lack effectiveness when facing practical  adversaries, \textit{e.g.}, membership inference attacks. Indeed, evaluating {\MU}  is far from trivial \cite{becker2022evaluating,thudi2022necessity,thudi2021unrolling}.  %as we described in Sec.\,\ref{sec: primer_MU}. 
Furthermore, the attention on  {\MU} has also   been raised in   different learning paradigms, \textit{e.g.}, federated learning   \cite{wang2022federated,liu2022right}, graph neural networks \cite{chen2022graph,chien2022certified,cheng2023gnndelete}, and adversarial ML \cite{marchant2022hard,di2022hidden}.
In addition to   preventing the leakage of  data privacy  from the trained models, the concept of {\MU} has also inspired  other emergent applications such as adversarial defense against backdoor attacks \cite{liu2022backdoor,warnecke2021machine} that we have studied and erasing image concepts of conditional generative models \cite{gandikota2023erasing,zhang2023forget}.
%Moreover, in addition to its primary applications, we have demonstrated \MU's potential in mitigating backdoor attacks \cite{liu2022backdoor}, as well as in augmenting the performance of transfer learning \cite{jain2022data}. These diverse applications underscore the versatility and utility of machine unlearning in various domains.

 


\noindent \textbf{Understanding data influence.}
The majority of {\MU} studies are motivated by data privacy. Yet, they  also closely relate to another line of research on understanding data influence in ML. For example, the influence function approach \cite{koh2017understanding} has been used as an algorithmic backbone of many unlearning methods  \cite{warnecke2021machine,izzo2021approximate}. From the viewpoint of data influence, {\MU}  has been used in the use case of adversarial defense against data poisoning backdoor attacks \cite{liu2022backdoor}. Beyond unlearning, evaluation of data influence  has also been studied in  fair learning  \cite{sattigeri2022fair,wang2022understanding},  transfer learning  \cite{jain2022data}, and   dataset pruning \cite{borsos2020coresets,yang2022dataset}. 
%to improve  the efficiency of ML. 


% It can also be viewed as a method of understanding dataset influence
% %\PR{dataset influence?}
% in model training


%%  In [16], Liu et al., for instance, utilize forgetting in order to remove backdoors that were induced into a model.

\noindent \textbf{Model pruning.}
%\iffalse 
The deployment constraints on \textit{e.g.}, computation, energy, and memory   necessitate the pruning of
today's ML models, \textit{i.e.}, promoting their weight sparsity. %\cite{han2015deep,chen2021lottery,frankle2018lottery,frankle2020linear,ma2021sanity,zhang2022advancing}.
%\fi 
The vast majority of existing works \cite{han2015deep,chen2021lottery,frankle2018lottery,frankle2020linear,ma2021sanity,zhang2022advancing,blalock2020state} focus on  developing model pruning methods that can strike a graceful balance between model's generalization and sparsity.
%the  generalization ability of pruned models against its sparsity; see the seminal work \cite{blalock2020state}. 
In particular, the existence of LTH (lottery ticket hypothesis) \cite{frankle2018lottery} demonstrates 
the feasibility of co-improving the model's generalization  and efficiency (in terms of sparsity) \cite{liu2018rethinking,tanaka2020pruning,wang2020picking,lee2018snip,zhang2023data}. 
%has been empirically justified  by LTH (lottery ticket hypothesis) \cite{frankle2018lottery},
\iffalse 
This has inspired 
many different kinds of pruning methods \cite{liu2018rethinking,tanaka2020pruning,wang2020picking,lee2018snip}. 
\fi 
In addition to generalization, model sparsity   achieved by pruning   can also be  leveraged to improve other performance metrics, such as   robustness \cite{sehwag2020hydra,chen2022quarantine,diffenderfer2021winning}, model explanation  \cite{wong2021leveraging,chen2022can},
%model connectivity \cite{frankle2020linear}, 
and privacy \cite{huang2020privacy,wang2020against,luo2021scalable,gong2020privacy}.

\iffalse 
out-of-distribution generalization \cite{diffenderfer2021winning}. 
In particular, the   relevant work to ours is model pruning for privacy-preserving learning \cite{huang2020privacy,wang2020against,luo2021scalable,gong2020privacy}. Yet,  nearly all the existing works  focus on how sparsity impacts data privacy, \textit{e.g.}, evaluated using MIAs (membership inference attacks) 
%against pruned models on training data points 
\cite{bagmar2021membership,yuan2022membership}. This is akin to MIA evaluation on the retained dataset in our work. Thus, pruning for privacy cannot   provide   a holistic and in-depth understanding of  how pruning  impacts {\MU}.  
\fi 

\iffalse
In addition,
a few recent works   \cite{wang2022federated,ye2022learning} attempt to draw insights from pruning for unlearning. In \cite{wang2022federated},  pruning channels of a  neural network shows   unlearning benefits in federated learning. And in \cite{ye2022learning}, filter pruning is introduced in lifelong learning to detect pruning identified exemplars (PIEs)   \cite{hooker2019compressed} that  are easy to forget. 
Different from the aforementioned literature to customize  pruning   for specific unlearning applications,  our work for the first time explores and exploits the connection between model pruning and unlearning systematically and in-depth. 
\fi 


%or membership inference attack and defense 

%\textsc{Grasp}\,\cite{wang2020pick}, \textsc{Snip}\,\cite{lee2018snip}, 

%%% pruning vs. privacy
%\paragraph{Pruning for.}


