\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Cao and Yang(2015)]{cao2015towards}
Yinzhi Cao and Junfeng Yang.
\newblock Towards making systems forget with machine unlearning.
\newblock In \emph{2015 IEEE Symposium on Security and Privacy}, pages
  463--480. IEEE, 2015.

\bibitem[Bourtoule et~al.(2021)Bourtoule, Chandrasekaran, Choquette-Choo, Jia,
  Travers, Zhang, Lie, and Papernot]{bourtoule2021machine}
Lucas Bourtoule, Varun Chandrasekaran, Christopher~A Choquette-Choo, Hengrui
  Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot.
\newblock Machine unlearning.
\newblock In \emph{2021 IEEE Symposium on Security and Privacy (SP)}, pages
  141--159. IEEE, 2021.

\bibitem[Nguyen et~al.(2022)Nguyen, Huynh, Nguyen, Liew, Yin, and
  Nguyen]{nguyen2022survey}
Thanh~Tam Nguyen, Thanh~Trung Huynh, Phi~Le Nguyen, Alan Wee-Chung Liew,
  Hongzhi Yin, and Quoc Viet~Hung Nguyen.
\newblock A survey of machine unlearning.
\newblock \emph{arXiv preprint arXiv:2209.02299}, 2022.

\bibitem[Rosen(2011)]{rosen2011right}
Jeffrey Rosen.
\newblock The right to be forgotten.
\newblock \emph{Stan. L. Rev. Online}, 64:\penalty0 88, 2011.

\bibitem[Hoofnagle et~al.(2019)Hoofnagle, van~der Sloot, and
  Borgesius]{hoofnagle2019european}
Chris~Jay Hoofnagle, Bart van~der Sloot, and Frederik~Zuiderveen Borgesius.
\newblock The european union general data protection regulation: what it is and
  what it means.
\newblock \emph{Information \& Communications Technology Law}, 28\penalty0
  (1):\penalty0 65--98, 2019.

\bibitem[Warnecke et~al.(2021)Warnecke, Pirch, Wressnegger, and
  Rieck]{warnecke2021machine}
Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck.
\newblock Machine unlearning of features and labels.
\newblock \emph{arXiv preprint arXiv:2108.11577}, 2021.

\bibitem[Graves et~al.(2021)Graves, Nagisetty, and Ganesh]{graves2021amnesiac}
Laura Graves, Vineel Nagisetty, and Vijay Ganesh.
\newblock Amnesiac machine learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 11516--11524, 2021.

\bibitem[Thudi et~al.(2021)Thudi, Deza, Chandrasekaran, and
  Papernot]{thudi2021unrolling}
Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot.
\newblock Unrolling sgd: Understanding factors influencing machine unlearning.
\newblock \emph{arXiv preprint arXiv:2109.13398}, 2021.

\bibitem[Becker and Liebig(2022)]{becker2022evaluating}
Alexander Becker and Thomas Liebig.
\newblock Evaluating machine unlearning via epistemic uncertainty.
\newblock \emph{arXiv preprint arXiv:2208.10836}, 2022.

\bibitem[Izzo et~al.(2021)Izzo, Smart, Chaudhuri, and Zou]{izzo2021approximate}
Zachary Izzo, Mary~Anne Smart, Kamalika Chaudhuri, and James Zou.
\newblock Approximate data deletion from machine learning models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2008--2016. PMLR, 2021.

\bibitem[Thudi et~al.(2022)Thudi, Jia, Shumailov, and
  Papernot]{thudi2022necessity}
Anvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot.
\newblock On the necessity of auditable algorithmic definitions for machine
  unlearning.
\newblock In \emph{31st USENIX Security Symposium (USENIX Security 22)}, pages
  4007--4022, 2022.

\bibitem[Golatkar et~al.(2020)Golatkar, Achille, and
  Soatto]{golatkar2020eternal}
Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Eternal sunshine of the spotless net: Selective forgetting in deep
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9304--9312, 2020.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Chen et~al.(2021)Chen, Frankle, Chang, Liu, Zhang, Carbin, and
  Wang]{chen2021lottery}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael
  Carbin, and Zhangyang Wang.
\newblock The lottery tickets hypothesis for supervised and self-supervised
  pre-training in computer vision models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16306--16316, 2021.

\bibitem[Frankle and Carbin(2018)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Frankle et~al.(2020{\natexlab{a}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  3259--3269. PMLR, 2020{\natexlab{a}}.

\bibitem[Ma et~al.(2021)Ma, Yuan, Shen, Chen, Chen, Chen, Liu, Qin, Liu, Wang,
  et~al.]{ma2021sanity}
Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan Chen, Ning
  Liu, Minghai Qin, Sijia Liu, Zhangyang Wang, et~al.
\newblock Sanity checks for lottery tickets: Does your winning ticket really
  win the jackpot?
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12749--12760, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Yao, Ram, Zhao, Chen, Hong, Wang, and
  Liu]{zhang2022advancing}
Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu~Zhao, Tianlong Chen, Mingyi Hong,
  Yanzhi Wang, and Sijia Liu.
\newblock Advancing model pruning via bi-level optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Blalock et~al.(2020)Blalock, Gonzalez~Ortiz, Frankle, and
  Guttag]{blalock2020state}
Davis Blalock, Jose~Javier Gonzalez~Ortiz, Jonathan Frankle, and John Guttag.
\newblock What is the state of neural network pruning?
\newblock \emph{Proceedings of machine learning and systems}, 2:\penalty0
  129--146, 2020.

\bibitem[Sehwag et~al.(2020)Sehwag, Wang, Mittal, and Jana]{sehwag2020hydra}
Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana.
\newblock Hydra: Pruning adversarially robust neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 19655--19666, 2020.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Zhang, Zhang, Chang, Liu, and
  Wang]{chen2022quarantine}
Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, and Zhangyang
  Wang.
\newblock Quarantine: Sparsity can uncover the trojan attack trigger for free.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 598--609, 2022{\natexlab{a}}.

\bibitem[Diffenderfer et~al.(2021)Diffenderfer, Bartoldson, Chaganti, Zhang,
  and Kailkhura]{diffenderfer2021winning}
James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya
  Kailkhura.
\newblock A winning hand: Compressing deep networks can improve
  out-of-distribution robustness.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 664--676, 2021.

\bibitem[Stoychev and Gunes(2022)]{stoychev2022effect}
Samuil Stoychev and Hatice Gunes.
\newblock The effect of model compression on fairness in facial expression
  recognition.
\newblock \emph{arXiv preprint arXiv:2201.01709}, 2022.

\bibitem[Xu and Hu(2022)]{xu2022can}
Guangxuan Xu and Qingyuan Hu.
\newblock Can model compression improve nlp fairness.
\newblock \emph{arXiv preprint arXiv:2201.08542}, 2022.

\bibitem[Wong et~al.(2021)Wong, Santurkar, and Madry]{wong2021leveraging}
Eric Wong, Shibani Santurkar, and Aleksander Madry.
\newblock Leveraging sparse linear layers for debuggable deep networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  11205--11216. PMLR, 2021.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Zhang, Wu, Huang, Liu, Chang, and
  Wang]{chen2022can}
Tianlong Chen, Zhenyu Zhang, Jun Wu, Randy Huang, Sijia Liu, Shiyu Chang, and
  Zhangyang Wang.
\newblock Can you win everything with a lottery ticket?
\newblock \emph{Transactions of Machine Learning Research}, 2022{\natexlab{b}}.

\bibitem[Huang et~al.(2020)Huang, Su, Ravi, Song, Arora, and
  Li]{huang2020privacy}
Yangsibo Huang, Yushan Su, Sachin Ravi, Zhao Song, Sanjeev Arora, and Kai Li.
\newblock Privacy-preserving learning via deep net pruning.
\newblock \emph{arXiv preprint arXiv:2003.01876}, 2020.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Wang, Wang, Zhou, Liu, Bi, Ding,
  and Rajasekaran]{wang2020against}
Yijue Wang, Chenghong Wang, Zigeng Wang, Shanglin Zhou, Hang Liu, Jinbo Bi,
  Caiwen Ding, and Sanguthevar Rajasekaran.
\newblock Against membership inference attack: Pruning is all you need.
\newblock \emph{arXiv preprint arXiv:2008.13578}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Guo, Xie, and
  Qi]{wang2022federated}
Junxiao Wang, Song Guo, Xin Xie, and Heng Qi.
\newblock Federated unlearning via class-discriminative pruning.
\newblock In \emph{Proceedings of the ACM Web Conference 2022}, pages 622--632,
  2022{\natexlab{a}}.

\bibitem[Ye et~al.(2022)Ye, Fu, Song, Yang, Liu, Jin, Song, and
  Wang]{ye2022learning}
Jingwen Ye, Yifang Fu, Jie Song, Xingyi Yang, Songhua Liu, Xin Jin, Mingli
  Song, and Xinchao Wang.
\newblock Learning with recoverable forgetting.
\newblock In \emph{European Conference on Computer Vision}, pages 87--103.
  Springer, 2022.

\bibitem[Hooker et~al.(2019)Hooker, Courville, Clark, Dauphin, and
  Frome]{hooker2019compressed}
Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome.
\newblock What do compressed deep neural networks forget?
\newblock \emph{arXiv preprint arXiv:1911.05248}, 2019.

\bibitem[Koh and Liang(2017)]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pages
  1885--1894. PMLR, 2017.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter]{parisi2019continual}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural Networks}, 113:\penalty0 54--71, 2019.

\bibitem[Cook and Weisberg(1982)]{cook1982residuals}
R~Dennis Cook and Sanford Weisberg.
\newblock \emph{Residuals and influence in regression}.
\newblock New York: Chapman and Hall, 1982.

\bibitem[Guo et~al.(2019)Guo, Goldstein, Hannun, and Van
  Der~Maaten]{guo2019certified}
Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der~Maaten.
\newblock Certified data removal from machine learning models.
\newblock \emph{arXiv preprint arXiv:1911.03030}, 2019.

\bibitem[Xu et~al.(2023)Xu, Zhu, Zhang, Zhou, and Yu]{xu2023machine}
Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip~S Yu.
\newblock Machine unlearning: A survey.
\newblock \emph{ACM Computing Surveys}, 56\penalty0 (1):\penalty0 1--36, 2023.

\bibitem[Singh and Alistarh(2020)]{singh2020woodfisher}
Sidak~Pal Singh and Dan Alistarh.
\newblock Woodfisher: Efficient second-order approximation for neural network
  compression.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18098--18109, 2020.

\bibitem[Song et~al.(2019)Song, Shokri, and Mittal]{song2019privacy}
Liwei Song, Reza Shokri, and Prateek Mittal.
\newblock Privacy risks of securing machine learning models against adversarial
  examples.
\newblock In \emph{Proceedings of the 2019 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 241--257, 2019.

\bibitem[Yeom et~al.(2018)Yeom, Giacomelli, Fredrikson, and
  Jha]{yeom2018privacy}
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha.
\newblock Privacy risk in machine learning: Analyzing the connection to
  overfitting.
\newblock In \emph{2018 IEEE 31st computer security foundations symposium
  (CSF)}, pages 268--282. IEEE, 2018.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and
  Ganguli]{tanaka2020pruning}
Hidenori Tanaka, Daniel Kunin, Daniel~L Yamins, and Surya Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6377--6389, 2020.

\bibitem[Frankle et~al.(2020{\natexlab{b}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020pruning}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock \emph{arXiv preprint arXiv:2009.08576}, 2020{\natexlab{b}}.

\bibitem[Bach et~al.(2012)Bach, Jenatton, Mairal, Obozinski,
  et~al.]{bach2012optimization}
Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et~al.
\newblock Optimization with sparsity-inducing penalties.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (1):\penalty0 1--106, 2012.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Song and Mittal(2020)]{song2020systematic}
Liwei Song and Prateek Mittal.
\newblock Systematic evaluation of privacy risks of machine learning models.
\newblock \emph{arXiv preprint arXiv:2003.10595}, 2020.

\bibitem[Gu et~al.(2017)Gu, Dolan-Gavitt, and Garg]{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock \emph{arXiv preprint arXiv:1708.06733}, 2017.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Fan, Chen, Liu, Ma, Wang, and
  Ma]{liu2022backdoor}
Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li~Wang, and Jianfeng
  Ma.
\newblock Backdoor defense with machine unlearning.
\newblock \emph{arXiv preprint arXiv:2201.09538}, 2022{\natexlab{a}}.

\bibitem[Jain et~al.(2022)Jain, Salman, Khaddaj, Wong, Park, and
  Madry]{jain2022data}
Saachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong, Sung~Min Park, and
  Aleksander Madry.
\newblock A data-based perspective on transfer learning.
\newblock \emph{arXiv preprint arXiv:2207.05739}, 2022.

\bibitem[Xiao et~al.(2010)Xiao, Hays, Ehinger, Oliva, and
  Torralba]{xiao2010sun}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In \emph{2010 IEEE computer society conference on computer vision and
  pattern recognition}, pages 3485--3492. IEEE, 2010.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and
  Jawahar]{parkhi2012cats}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV~Jawahar.
\newblock Cats and dogs.
\newblock In \emph{2012 IEEE conference on computer vision and pattern
  recognition}, pages 3498--3505. IEEE, 2012.

\bibitem[Leclerc et~al.(2022)Leclerc, Ilyas, Engstrom, Park, Salman, and
  Madry]{leclerc2022ffcv}
Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung~Min Park, Hadi Salman,
  and Aleksander Madry.
\newblock Ffcv: Accelerating training by removing data bottlenecks.
\newblock 2022.

\bibitem[Ginart et~al.(2019)Ginart, Guan, Valiant, and Zou]{ginart2019making}
Antonio Ginart, Melody Guan, Gregory Valiant, and James~Y Zou.
\newblock Making ai forget you: Data deletion in machine learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Neel et~al.(2021)Neel, Roth, and Sharifi-Malvajerdi]{neel2021descent}
Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi.
\newblock Descent-to-delete: Gradient-based methods for machine unlearning.
\newblock In \emph{Algorithmic Learning Theory}, pages 931--962. PMLR, 2021.

\bibitem[Ullah et~al.(2021)Ullah, Mai, Rao, Rossi, and Arora]{ullah2021machine}
Enayat Ullah, Tung Mai, Anup Rao, Ryan~A Rossi, and Raman Arora.
\newblock Machine unlearning via algorithmic stability.
\newblock In \emph{Conference on Learning Theory}, pages 4126--4142. PMLR,
  2021.

\bibitem[Sekhari et~al.(2021)Sekhari, Acharya, Kamath, and
  Suresh]{sekhari2021remember}
Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda~Theertha Suresh.
\newblock Remember what you want to forget: Algorithms for machine unlearning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 18075--18086, 2021.

\bibitem[Dwork et~al.(2006)Dwork, Kenthapadi, McSherry, Mironov, and
  Naor]{dwork2006our}
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
  Naor.
\newblock Our data, ourselves: Privacy via distributed noise generation.
\newblock In \emph{Annual international conference on the theory and
  applications of cryptographic techniques}, pages 486--503. Springer, 2006.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Xu, Yuan, Wang, and
  Li]{liu2022right}
Yi~Liu, Lei Xu, Xingliang Yuan, Cong Wang, and Bo~Li.
\newblock The right to be forgotten in federated learning: An efficient
  realization with rapid retraining.
\newblock \emph{arXiv preprint arXiv:2203.07320}, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2022{\natexlab{c}})Chen, Zhang, Wang, Backes, Humbert, and
  Zhang]{chen2022graph}
Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang
  Zhang.
\newblock Graph unlearning.
\newblock In \emph{Proceedings of the 2022 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 499--513, 2022{\natexlab{c}}.

\bibitem[Chien et~al.(2022)Chien, Pan, and Milenkovic]{chien2022certified}
Eli Chien, Chao Pan, and Olgica Milenkovic.
\newblock Certified graph unlearning.
\newblock \emph{arXiv preprint arXiv:2206.09140}, 2022.

\bibitem[Cheng et~al.(2023)Cheng, Dasoulas, He, Agarwal, and
  Zitnik]{cheng2023gnndelete}
Jiali Cheng, George Dasoulas, Huan He, Chirag Agarwal, and Marinka Zitnik.
\newblock Gnndelete: A general strategy for unlearning in graph neural
  networks.
\newblock \emph{arXiv preprint arXiv:2302.13406}, 2023.

\bibitem[Marchant et~al.(2022)Marchant, Rubinstein, and
  Alfeld]{marchant2022hard}
Neil~G Marchant, Benjamin~IP Rubinstein, and Scott Alfeld.
\newblock Hard to forget: Poisoning attacks on certified machine unlearning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 7691--7700, 2022.

\bibitem[Di et~al.(2022)Di, Douglas, Acharya, Kamath, and
  Sekhari]{di2022hidden}
Jimmy~Z Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, and Ayush Sekhari.
\newblock Hidden poison: Machine unlearning enables camouflaged poisoning
  attacks.
\newblock In \emph{NeurIPS ML Safety Workshop}, 2022.

\bibitem[Gandikota et~al.(2023)Gandikota, Materzynska, Fiotto-Kaufman, and
  Bau]{gandikota2023erasing}
Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau.
\newblock Erasing concepts from diffusion models.
\newblock \emph{arXiv preprint arXiv:2303.07345}, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Wang, Xu, Wang, and
  Shi]{zhang2023forget}
Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi.
\newblock Forget-me-not: Learning to forget in text-to-image diffusion models.
\newblock \emph{arXiv preprint arXiv:2303.17591}, 2023{\natexlab{a}}.

\bibitem[Sattigeri et~al.(2022)Sattigeri, Ghosh, Padhi, Dognin, and
  Varshney]{sattigeri2022fair}
Prasanna Sattigeri, Soumya Ghosh, Inkit Padhi, Pierre Dognin, and Kush~R.
  Varshney.
\newblock Fair infinitesimal jackknife: Mitigating the influence of biased
  training data points without refitting.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Wang, and
  Liu]{wang2022understanding}
Jialu Wang, Xin~Eric Wang, and Yang Liu.
\newblock Understanding instance-level impact of fairness constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  23114--23130. PMLR, 2022{\natexlab{b}}.

\bibitem[Borsos et~al.(2020)Borsos, Mutny, and Krause]{borsos2020coresets}
Zal{\'a}n Borsos, Mojmir Mutny, and Andreas Krause.
\newblock Coresets via bilevel optimization for continual learning and
  streaming.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14879--14890, 2020.

\bibitem[Yang et~al.(2022)Yang, Xie, Peng, Xu, Sun, and Li]{yang2022dataset}
Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li.
\newblock Dataset pruning: Reducing training data by examining generalization
  influence.
\newblock \emph{arXiv preprint arXiv:2205.09329}, 2022.

\bibitem[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock \emph{arXiv preprint arXiv:1810.05270}, 2018.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Zhang, and
  Grosse]{wang2020picking}
Chaoqi Wang, Guodong Zhang, and Roger Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock \emph{arXiv preprint arXiv:2002.07376}, 2020{\natexlab{b}}.

\bibitem[Lee et~al.(2018)Lee, Ajanthan, and Torr]{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock \emph{arXiv preprint arXiv:1810.02340}, 2018.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Kamath, Wu, Fan, Chen, Wang,
  Chang, Liu, and Hao]{zhang2023data}
Yimeng Zhang, Akshay~Karkal Kamath, Qiucheng Wu, Zhiwen Fan, Wuyang Chen,
  Zhangyang Wang, Shiyu Chang, Sijia Liu, and Cong Hao.
\newblock Data-model-circuit tri-design for ultra-light video intelligence on
  edge devices.
\newblock In \emph{Proceedings of the 28th Asia and South Pacific Design
  Automation Conference}, pages 745--750, 2023{\natexlab{b}}.

\bibitem[Luo et~al.(2021)Luo, Wu, Adeli, and Fei-Fei]{luo2021scalable}
Zelun Luo, Daniel~J Wu, Ehsan Adeli, and Li~Fei-Fei.
\newblock Scalable differential privacy with sparse network finetuning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 5059--5068, 2021.

\bibitem[Gong et~al.(2020)Gong, Zhan, Li, Niu, Ma, Wang, Ren, Ding, Lin, Xu,
  et~al.]{gong2020privacy}
Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao Wang, Bin
  Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, et~al.
\newblock A privacy-preserving-oriented dnn pruning and mobile acceleration
  framework.
\newblock In \emph{Proceedings of the 2020 on Great Lakes Symposium on VLSI},
  pages 119--124, 2020.

\bibitem[Gould et~al.(2016)Gould, Fernando, Cherian, Anderson, Cruz, and
  Guo]{gould2016differentiating}
Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo~Santa
  Cruz, and Edison Guo.
\newblock On differentiating parameterized argmin and argmax problems with
  application to bi-level optimization.
\newblock \emph{arXiv preprint arXiv:1607.05447}, 2016.

\end{thebibliography}
