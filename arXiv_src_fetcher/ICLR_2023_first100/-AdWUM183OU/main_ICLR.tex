\documentclass{article}

\PassOptionsToPackage{numbers}{natbib}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage{iclr2023_conference,times}
%\usepackage{tikz,graphics,color,fullpage,float,epsf}%,caption,subcaption}
\usepackage[font={small}]{caption}
\usepackage[font={scriptsize}]{subcaption}
\usepackage{paralist}
\usepackage{enumitem}

\usepackage{sidecap}
\usepackage{wrapfig}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
%\DeclareMathSymbol{\sh ortminus}{\mathbin}{AMSa}{"39}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\argmin}{argmin}
%\usepackage{amsthm}
%\usepackage{enumerate}
\usepackage{color}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{multirow}
%\usepackage{enumitem}
\definecolor{navy}{rgb}{0,0,0.0}
\usepackage[unicode=true] {hyperref}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\hypersetup{
    colorlinks=true,
    linkcolor=airforceblue,
    filecolor=airforceblue,      
    urlcolor=airforceblue,
    citecolor=airforceblue,
    }

\usepackage{amsfonts} %% <- also included by amssymb
\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}
%\usepackage{mathpazo} %% <- because the OP uses mathpazo, optional
%\usepackage{color}

\usepackage{color}
\usepackage{xcolor}

\definecolor{navy}{rgb}{0,0,0.0}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}

\newcommand{\daphna}[1]{{\color{magenta} #1}}
\newcommand{\daniel}[1]{{\color{blue} #1}}
\newcommand{\comment}[1]{{\color{navy} #1}}
\newcommand{\uri}[1]{{\color{red} #1}}
\newcommand{\prior}{\scriptsize{{\color{airforceblue} (with prior)}}}
\newcommand{\prtiny}{\tiny{{\color{airforceblue} (prior)}}}

\usepackage[ruled]{algorithm2e}
\usepackage{arydshln}

\definecolor{blue}{rgb}{0.0, 0.0, 1.0}
\definecolor{olive}{rgb}{0.5019607843137255, 0.5019607843137255, 0.0}
\definecolor{red}{rgb}{1.0, 0.0, 0.0}
\definecolor{green}{rgb}{0.0, 0.85, 0.0}
\definecolor{maroon}{rgb}{0.5019607843137255, 0.0, 0.0}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks 
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{bbm}

\newcommand{\ba}{\bm{a}}
\newcommand{\bz}{\bm{z}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bt}{\bm{t}}
\newcommand{\bv}{\bm{v}}
\newcommand{\by}{\bm{y}}
\newcommand{\bw}{\bm{w}}
\newcommand{\sQ}{\textsf{Q}}
\newcommand{\bW}{\bm{w}}
\newcommand{\X}{\mathbb{X}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\bX}{{\mathbf X}}
\newcommand{\hs}{{\hat s}}
\newcommand{\bE}{\bm{e}}
\newcommand{\var}{\mathrm{var}} 
\newcommand{\F}{\mathcal{F}}
\newcommand{\Cr}{\mathbb{C}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Mc}{\mathcal{M}}

% \newcommand{\hW}{{\tilde \bW}}
% \newcommand{\bW^s}{{\tilde \bW^s}}
\newcommand{\hQ}{\bm{Q}}
\newcommand{\hQrp}{{(\hQ^\rp_\rl)}}
\newcommand{\hhQ}{\hat\hQ}
\newcommand{\hW}{{\bW}}
\newcommand{\dW}{\Delta\bW}
\newcommand{\DW}{\Delta}
\newcommand{\iX}{\mathbb{X}}
\newcommand{\SXX}{\Sigma_{\scriptscriptstyle XX}}
\newcommand{\SYX}{\Sigma_{\scriptscriptstyle YX}}

\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\newcommand{\hlt}[1]{{\color{airforceblue} #1}}
\newcommand{\texp}[1]{\quad\mathrm{\hlt{(#1)}}}
\newcommand{\texq}[1]{\quad\mathrm{\hlt{#1}}}
%\newcommand{\comment}[1]{{}}

\newcommand{\myparagraph}[1]{\smallskip\noindent\textbf{#1}}
\newcommand{\myparagpar}[1]{\noindent\textbf{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{pcorollary}{Corollary}[proposition]
\newtheorem{lcorollary}{Corollary}[lemma]
\newtheorem{tcorollary}{Corollary}[theorem]
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{lconjecture}{Conjecture}[lemma]
\newtheorem{defn}{Definition}
\newtheorem{assumpt}{Assumption}
\newtheorem{result}{Result}
\newtheorem*{result*}{Theorem}



\newcommand\ca{.12}

%\title{\emph{DisagreeNet}: Using Training Agreement Dynamics to Identify Noisy Labels}
%\title{On the Dynamics of Learning in an Ensemble of Deep Networks, and the Identification of Noisy Labels}
%\title{On Learning in an Ensemble of Deep Networks, and the Identification of Noisy Labels}
%\title{\centering On the Dynamics of Learning in Deep Networks\\ and the Identification of Noisy Labels}
%\title{\centering \hspace{-.35cm}On the Dynamics of Consensus in Deep Learning\\ and the Identification of Noisy Labels}
\title{\centering The Dynamic of Consensus in Deep Networks\\ and the Identification of Noisy Labels}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
% }


\author{Daniel Shwartz, Uri Stern \& Daphna Weinshall\\
School of Computer Science and Engineering\\
Hebrew University of Jerusalem\\
Jerusalem, Israel \\
\texttt{\{Daniel.Shwartz1,Uri.Stern,daphna\}@mail.huji.ac.il} \\
}


\begin{document}


\maketitle


\begin{abstract}

Deep neural networks have incredible capacity and expressibility, and can seemingly memorize any training set. This introduces a problem when training in the presence of noisy labels, as the noisy examples cannot be distinguished from clean examples by the end of training. Recent research has dealt with this challenge by utilizing the fact that deep networks seem to memorize clean examples much earlier than noisy examples. Here we report a new empirical result: for each example, when looking at the time it has been memorized by each model in an ensemble of networks, the diversity seen in noisy examples is much larger than the clean examples. We use this observation to develop a new method for noisy labels filtration. The method is based on a statistics of the data, which captures the differences in ensemble learning dynamics between clean and noisy data. We test our method on three tasks: (i) noise amount estimation; (ii) noise filtration; (iii) supervised classification. We show that our method improves over existing baselines in all three tasks using a variety of datasets, noise models, and noise levels. Aside from its improved performance, our method has two other advantages. (i) Simplicity, which implies that no additional hyperparameters are introduced. (ii) Our method is modular: it does not work in an end-to-end fashion, and can therefore be used to clean a dataset for any other future usage.

\end{abstract}

\section{Introduction}
\label{sec:intro}

% Modern Deep Neural Networks (DNN) include a large number of parameters and non-linear operations, resulting in a highly non-convex, high-dimensional optimization landscape. As a result, since each model $f_{i}^e$ in ensemble $\F^e(\iX)$ is initialized and trained independently, their weights vary drastically and it is hard to compare them directly \citep{li2015convergent, yosinski2015understanding}. 

Deep neural networks dominate the state of the art in an ever increasing list of application domains, but for the most part, this incredible success relies on very large datasets of annotated examples available for training. Unfortunately, large amounts of high-quality annotated data are hard and expensive to acquire, whereas cheap alternatives (obtained by way of crowd-sourcing or automatic labeling, for example) often introduce noisy labels into the training set. By now there is much empirical evidence that neural networks can memorize almost every training set, including ones with noisy and even random labels \cite{zhang2017ICLR}, which in turn increases the generalization error of the model. As a result, the problems of identifying the existence of label noise and the separation of noisy labels from clean ones, are becoming more urgent and therefore attract increasing attention. 

Henceforth, we will call the set of examples in the training data whose labels are correct "clean data", and the set of examples whose labels are incorrect "noisy data". While all labels can be eventually learned by deep models, it has been empirically shown that most noisy datapoints are learned by deep models late, after most of the clean data has already been learned \citep{arpit2017closer}. Therefore many methods focus on the learning time of an example in order to classify it as noisy or clean, by looking at its loss \citep{pleiss2020identifying,arazo2019unsupervised} or loss per epoch \citep{li2020dividemix} in a single model. However, these methods struggle to classify correctly clean and noisy datapoints that are learned at the same time, or worse - noisy datapoints that are learned early. Additionally, many of these methods work in an end-to-end manner, and thus neither provide noise level estimation nor do they deliver separate sets of clean and noisy data for novel future usages.

\begin{wrapfigure}{R}{0.6\textwidth}
\centering
\vspace{-0.45cm}
\fbox{\includegraphics[width=0.565\textwidth]{figs/dyn.png}
\vspace{-0.5cm}}
\caption{With noisy labels models show higher disagreement. The noisy examples are not only learned at a later stage, but each model learns the example at its own different time.}
\vspace{-0.25cm}
\label{fig:1}
\end{wrapfigure}
%}

Our first contribution is a new empirical results regarding the learning dynamics of an ensemble of deep networks, showing that the dynamics is different when training with clean data vs. noisy data. The dynamics of clean data has been studied in \citep{hacohen2020let,pliushch2021deep}, where it is reported that different deep models learn examples in the same order and pace. This means that when training a few models and comparing their predictions, a binary occurrence (approximately) is seen at each epoch $e$: either all the networks correctly predict the example's label, or none of them does. This further implies that for the most part, the distribution of predictions across points is bimodal. Additionally, a variety of studies showed that the bias and variance of deep networks decrease as the networks complexity grow \citep{nakkiran2021deep,neal2018modern}, providing additional evidence that different deep networks learn data at the same time simultaneously. 

In Section~\ref{sec:agg-overfit} we describe a new empirical result: when training an ensemble of deep models with noisy data, and \emph{in contrast to what happens when using clean data, different models learn different datapoints at different times} (see Fig.~\ref{fig:1}). This empirical finding tells us that in an ensemble of networks, the learning dynamics of clean data and noisy data can be distinguished. When training such an ensemble with a mixture of clean and noisy data, the emerging dynamics reflects this observation, as well as the tendency of clean data to be learned faster as previously observed. 

In our second contribution, we use this result to develop a new algorithm for noise level estimation and noise filtration, which we call \emph{DisagreeNet} (see Section~\ref{sec:approach}). Importantly, unlike most alternative methods, our algorithm is simple (it does not introduce any new hyperparameters), parallelizable, easy to integrate with any supervised or semi-supervised learning method and any loss function, and does not rely on prior knowledge of the noise amount. When used for noise filtration, our empirical study (see Section~\ref{sec:empirical}) shows the superiority of \emph{DisagreeNet} as compared to the state of the art, using different datasets, different noise models and different noise levels. When used for supervised classification by way of pre-processing the training set prior to training a deep model, it provides a significant boost in performance, more so than alternative methods. 

\myparagpar{Relation to prior art}

Work on the dynamics of learning in deep models has received increased attention in recent years \citep[e.g.,][]{nguyen2020wide,hacohen2020let,baldock2021deep}. Our work adds a new observation to this body of knowledge, which is seemingly unique to an ensemble of deep models (as against an ensemble of other commonly used classifiers). Thus, while there exist other methods that use ensembles to handle label noise  \citep[e.g.,][]{SABZEVARI20182374,Feng2020Adaptive,Ying21Destect,Moura18Ensemble}, for the most part they cannot take advantage of this characteristic of deep models, and as a result are forced to use additional knowledge, typically the availability of a clean validation set and/or prior knowledge of the noise amount. 

Work on deep learning with noisy labels (see \cite{song2022learning} for a recent survey) can be coarsely divided to two categories: general methods that use a modified loss or network's architecture, and methods that focus on noise identification. The first group includes methods that aim to estimate the underlying noise transition matrix \citep{goldberger2016training,patrini2017making}, employ a noise-robust loss \citep{ghosh2017robust,zhang2018generalized,wang2019symmetric,xu2019l_dmi}, or achieve robustness to noise by way of regularization \citep{tanno2019learning,jenni2018deep}. Methods in the second group, which is more inline with our approach, focus more directly on noise identification. Some methods assume that clean examples are usually learned faster than noisy examples \citep[e.g.][]{liu2020early}. Others \citep{arazo2019unsupervised,li2020dividemix} generate soft labels by interpolating the given labels and the model's predictions during training. Yet other methods \citep{jiang2018mentornet,han2018co,malach2017decoupling,yu2019does,lec}, like our own, inspect an ensemble of networks, usually in order to transfer information between networks and thus avoid agreement bias. 

Notably, we also analyze the behavior of ensembles in order to identify the noisy examples, resembling \citep{pleiss2020identifying,nguyen2019self,lec}. But unlike these methods, which track the loss of the networks, we track the dynamics of the agreement between multiple networks over epochs. We then show that this statistics is more effective, and achieves superior results. Additionally (and not less importantly), unlike these works, we do not assume prior knowledge of the noise amount or the presence of a clean validation set, and do not introduce new hyper-parameters in our algorithm. 

Recently, the emphasis has somewhat shifted to the use of semi-supervised learning and contrastive learning \citep{li2020dividemix,liu2020early,MOIT, JoCoR,JoSRC,zheltonozhskii2022contrast, Sel-CL,UNICON}. Semi-supervised learning is an effective paradigm for the prediction of missing labels. This paradigm is especially useful when the identification of noisy points cannot be done reliably, in which case it is advantageous to remove labels whose likelihood to be true is not negligible. The effectiveness of semi-supervised learning in providing reliable pseudo-labels for unlabeled points will compensate for the loss of clean labels. 

However, semi-supervised learning is not universally practical as it often relies on the extraction of effective representations based on unsupervised learning tasks, which typically introduces implicit priors (e.g., that contrastive loss is appropriate). In contrast, our goal is to reliably identify noisy points, to be subsequently removed. Thus, our method can be easily incorporated into any SOTA method which uses supervised or semi-supervised learning (with or without contrastive learning), and may provide benefit even when semi-supervised learning is not viable.

\section{Inter-Network Agreement: Definition and Scores}


Measuring the similarity between deep models is not a trivial challenge, as modern deep neural networks are complex functions defined by a huge number of parameters, which are invariant to transformations hidden in the model's architecture. Here we measure the similarity between deep models in an ensemble by measuring inter-model prediction agreement at each datapoint. Accordingly, in Section~\ref{sec:per-epoch} we describe scores that are based on the state of the networks at each epoch $e$, while in Section~\ref{sec:cum-scores} we describe cumulative scores that integrate these states through many epochs. Practically (see Section~\ref{sec:approach}), our proposed method relies on the cumulative scores, which are shown empirically to provide more accurate results in the noise filtration task. These scores promise added robustness, as it is no longer necessary to identify the epoch at which the score is to be evaluated.

\vspace{-.5em}
\subsection{Preliminaries}
%
\myparagpar{Notations} 
Let $f^e:\mathbb{R}^{d}\to {[0,1]}^{|C|}$ denote a deep model, trained with Stochastic Gradient Descent (SGD) for $e$ epochs on training set $\iX = \left\{(\bx_i,y_i)\right \}_{i=1}^M$, where $\bx_i\in\mathbb{R}^{d}$ denotes a single example and $y_i\in [C]$ its corresponding label. Let $\F^e(\iX)=\{f_1^e,...,f_{N}^e\}$ denote an ensemble of $N$ such models, where each model $f_{i\in [N]}^e$ is initialized and trained independently on $\iX$. %The datapoints in $\iX$ are sampled iid, where $(\bx,y)\sim\mathcal{D}_{X,Y}$. 

\myparagpar{Noise model} We analyze the training dynamics of an ensemble of models in the presence of label noise. Label noise is different from data noise (like image distortion or additive Gaussian noise). Here it is assumed that after the training set $\iX = \left\{(\bx_i,l_i)\right \}_{i=1}^M$ is sampled, the labels $\left\{l_i\right \}$ are corrupted by some noise function $g:[C]\to[C]$, and the training set becomes $\iX = \left\{(\bx_i,y_i)\right \}_{i=1}^M,~y_i=g(l_i)$. The two most common models of label noise are termed \emph{symmetric noise} and \emph{asymmetric noise} \citep{patrini2017making}. In both cases it is assumed that some fixed percentage of the labels are corrupted by $g(l)$. With symmetric noise, $g(l)$ assigns any new label from the set $[C]\setminus\{l\}$ with equal probability. With asymmetric noise, $g(l)$ is the deterministic permutation function (see App.~\ref{app:tech} for details). Note that the asymmetric noise model is considered much harder than the symmetric noise model. 

\vspace{-.5em}
\subsection{Per-Epoch Agreement Score}
\label{sec:per-epoch}


Following \cite{hacohen2020let}, we define the \textit{True Positive Agreement} (TPA) score of ensemble $\F^e(\iX)$ at each datapoint $(\bx,y)$, where \fbox{$TPA(\bx,y;\F^e(\iX)) = \frac{1}{N}\sum_{i=1}^{N}\mathbbm{1}_{[f_{i}^{e}(\bx) = y]}$}.
% \begin{eqnarray}
% \label{eq:TPA}
% TPA(\bx,y;\F^e(\iX)) = \frac{1}{N}\sum_{i=1}^{N}\mathbbm{1}_{[f_{i}^{e}(\bx) = y]}
% \end{eqnarray}
The TPA score measures the average accuracy of the models in the ensemble, when seeing $\bx$, after each model has been trained for exactly $e$ epochs on $\iX$. Note that $TPA$ measures the average accuracy of multiple models on one example, as opposed to the generalization error that measures the average error of one model on multiple examples. 
\vspace{-.5em}
\subsection{Cumulative Scores}
\label{sec:cum-scores}

When inspecting the dynamics of the TPA score on clean data, we see that at the beginning the distribution of $\{TPA(\bx_i,y_i)\}$ is concentrated around 0, and then quickly shifts to 1 as training proceeds (see side panels in Fig.~\ref{fig:global}). This implies that empirically, data is learned in a specific order by all models in the ensemble. To measure this phenomenon we use the \textit{Ensemble Learning Pace} (ELP) score defined below, which essentially integrates the TPA score over a set of epochs $\Ec$:
\begin{eqnarray}
\label{eq:ELP}
ELP(\bx,y) = \frac{1}{\vert\Ec\vert}\sum_{e\in\Ec}TPA(\bx,y;\F^e(\iX))
\vspace{-1.0em}
\end{eqnarray}
$ELP(\bx,y)$ captures both the time of learning by a single model, and its consistency across models. For example, if all the models learned the example early, the score would be high. It would be significantly lower if some of them learned it later than others (see pseudo-code in App.~\ref{app:ELP-pseudo}).

In our study we evaluated two additional cumulative scores of inter-model agreement:
\begin{enumerate}[leftmargin=0.65cm]
    \item Cumulative loss:
\vspace{-1em}
\begin{equation*}
CumLoss(\bx,y) = \frac{1}{N\vert\Ec\vert}\sum_{i,{e\in\Ec}}CE(f_{i}^{e}(\bx),y)   
\end{equation*}
Above $CE$ denotes the cross entropy function. This score is very similar to ELP, engaging the average of the cross-entropy loss instead of the accuracy indicator $\mathbbm{1}_{[f_{i}^{e}(\bx) = y]}$.% over all epochs and all networks.
\item Area under the margin:
following \citep{pleiss2020identifying}, the MeanMargin score is defined as follows
\begin{equation*}
MeanMargin(\bx,y)=\frac{1}{N\vert\Ec\vert}\sum_{i,{e\in\Ec}}[f^{e}_{i}(\bx)]_{y_i}-\argmax_{j\neq y_{i}}[f^{e}_{i}(\bx)]_j    
\end{equation*}
The MeanMargin score is the mean of the 'margin', the difference between the value of the ground-truth logit (before softmax) and the value of the otherwise maximal logit.
%\vspace{-1.0em}
\end{enumerate}


\section{The Dynamics of Agreement: New Empirical Observation}
\label{sec:agg-overfit}

In this section we analyze, both theoretically and empirically, how measures of inter-network agreement may indicate the detrimental phenomenon of 'overfit'. \emph{Overfit} is a condition that can occur during the training of deep neural networks. It is characterized by the co-occurring decrease of \emph{train error or loss} and the increase of \emph{test error or loss}. Recall that train loss is the quantity that is being continuously minimized during the training of deep models, while the test error is the quantity linked to generalization error. When these quantities change in opposite directions, training harms the final performance and thus early stopping is recommended. 

We begin by showing in Section~\ref{sec:theory} that in an ensemble of linear regression models, overfit and the agreement between models are negatively correlated. When this is the case, an epoch in which the agreement between networks reaches its maximal value is likely to indicate the beginning of overfit. 

Our next goal is to examine the relevance of this result to deep learning in practice. Yet inexplicably, at least as far as image datasets are concerned, overfit rarely occurs in practice when deep learning is used for image recognition. However, when label noise is introduced, significant overfit occurs. Capitalizing on this observation, we report in Section~\ref{sec:overfit-empical} that when overfit occurs in the independent training of an ensemble of deep networks, the agreement between the networks starts to decrease.

The approach we describe  in Section~\ref{sec:approach} is motivated by these results:
Since it has been observed that noisy data are memorized later than clean data, we hypothesize that overfit occurs when the memorization of noisy labels becomes dominant. This suggests that measuring the dynamics of agreement between networks, which is correlated with overfit as shown below,  can be effectively used for the identification of label noise. 



\subsection{Overfit and Agreement: Theoretical Result}
\label{sec:theory}

Since deep learning models are not amenable to a rigorous theoretical analysis, and in order to gain computational insight into such general phenomena as overfit, simpler models are sometimes analyzed \citep[e.g.][]{weinshall2020theory}.  %\citep{fukumizu1998effect,arora2018optimization,LeiEtAlNIPS19,du2019width,YunKMICLR21}. 
Accordingly, in App.~\ref{sec:overfit-app} we formally analyze the relation between overfit and inter-model agreement in an ensemble of linear regression models. In this framework, it can be shown that the two phenomena are negatively correlated, namely, increase in overfit implies decrease in inter-model agreement. Thus, we prove (under some assumptions) the following result:
\begin{result*}
% \newline
% \textbf{Result.}
Assume an ensemble of models obtained by solving linear regression with gradient descent and random initialization. If overfit \emph{increases} at time $t$ in all the models in the ensemble, then the agreement between the models in the ensemble at time $t$ \emph{decreases}.
\end{result*}


\subsection{Measuring the Agreement between Models}
\label{sec:BI-def}

In order to obtain a score that captures the level of disagreement between networks, we inspect more closely the distribution of $TPA(\bx,y;\F^e(\iX))$, defined in Section~\ref{sec:per-epoch}, over a sample of datapoints, and analyze its dynamics as training proceeds. First, note that if all of the models in ensemble $\F^e(\iX)$ give identical predictions at each point, the TPA score would be either 0 (when all the networks predict a false label) or 1 (when all the networks predict the correct label). In this case, the TPA distribution is perfectly bimodal, with only two peaks at 0 and 1. If the predictions of the models at each point are independent with mean accuracy $p$, then it can be readily shown that TPA is approximately the binomial random variable with a unimodal distribution around $p$. 

Empirically, \citep{hacohen2020let} showed that in ensembles of deep models trained on ‘real’ datasets as we use here, the TPA distribution is highly bimodal. Since commonly used measures of bimodality, such as the Pearson bimodality score, are ill-fitted for the discrete TPA distribution, we measure bimodality with the following \textit{Bimodal Index} score:
\begin{eqnarray}
\label{eq:BI}
BI(e) = \sqrt{\frac{1}{M}\sum_{i=1}^M\mathbbm{1}_{[TPA(\bx_i,y_i;\F^e(\iX))=N]}} + \sqrt{\frac{1}{M}\sum_{i=1}^M\mathbbm{1}_{[TPA(\bx_i,y_i;\F^e(\iX))=0]}}
\end{eqnarray}
$BI(e)$ measures how many examples are either correctly or incorrectly classified by \emph{all} the models in the ensemble, rewarding distributions where points are (roughly) equally divided between 0 and 1. Here we use this score to measure the agreement between networks at epoch $e$.

\begin{figure}[ht]%{0.8\textwidth}
\begin{center}
\begin{subfigure}{0.96\textwidth}
  \centering{
  %\captionsetup[subfigure]{labelformat=empty}
\valign{#\cr
\hbox{\begin{subfigure}{0.12\textwidth}
  \centering{\tiny\color{blue}{1}} \\
  \includegraphics[width=\textwidth]{figs/bim_dense_1.png}
%   \vspace{-0.6cm}
%   \caption{\tiny\color{blue}{1}}
%   \label{subfig:first}
  \end{subfigure}}
%\vfill
\hbox{\begin{subfigure}{0.12\textwidth}
  \centering{\tiny\color{olive}{2} max BI}
  \includegraphics[width=\textwidth]{figs/bin_dense_2.png}
%   \vspace{-0.6cm}
%   \caption{\tiny\color{olive}{2} max BI}
%  \label{subfig:second}
  \end{subfigure}}
%\vfill
\hbox{\begin{subfigure}{0.12\textwidth}
  \centering{\tiny\color{red}{3}}
  \includegraphics[width=\textwidth]{figs/bin_dense_3.png}
%   \vspace{-0.6cm}
%   \caption{\tiny\color{red}{3}}
%   \label{subfig:third}
  \end{subfigure}}
\cr%\noalign{\hfill}
%\captionsetup[subfigure]{labelformat=parens}
\hbox{\begin{subfigure}[b]{0.45\textwidth}
%  \includegraphics[height=0.35\textheight,width=\textwidth]{figs/bim_dense.png}
  \includegraphics[width=\textwidth]{figs/bim_dense.png}
  \vspace{-0.6cm}
  \caption{BI ($Y$-axis) vs. epochs ($X$-axis)}  \label{fig:global}
  \end{subfigure}}
\cr%\noalign{\hfill}
%\captionsetup[subfigure]{labelformat=empty}
\hbox{\begin{subfigure}{0.12\textwidth}
  \centering{\tiny\color{green}{4}}
  \includegraphics[width=\textwidth]{figs/bim_dense_4.png}
%   \vspace{-0.6cm}
%   \caption{\tiny\color{green}{4}}
%   \label{subfig:fourth}
  \end{subfigure}}
%\vfill
\hbox{\begin{subfigure}{0.12\textwidth}
  \centering{\tiny\color{maroon}{5}}
  \includegraphics[width=\textwidth]{figs/bim_dense_5.png}
%   \vspace{-0.6cm}
%   \caption{\tiny\color{maroon}{5}}
%   \label{subfig:fifth}
  \end{subfigure}}
%\vfill
\hbox{\begin{subfigure}{0.12\textwidth}
  \centering{\tiny{6}}
  \includegraphics[width=\textwidth]{figs/bim_dense_6.png}
%   \vspace{-0.6cm}
%   \caption{\tiny{6}}
%   \label{subfig:sixth}
  \end{subfigure}}
\cr
\hspace{-0.65\textwidth}
%\hbox{
\begin{subfigure}{0.25\textwidth}
  \includegraphics[width=\textwidth]{figs/ah2.png}
  \vspace{-0.45cm}
  \caption{}
  \label{subfig:biaccsym}
  \end{subfigure}
%  }
\cr
}
}
\end{subfigure}
\vspace{-1em}
\caption{(a) Main panel: bimodality in an ensemble of 10 DenseNet networks, trained to classify Cifar10 with 20\% symmetric noise. Side panels: TPA distribution in 6 epochs (blue - clean examples, orange - noisy ones). (b) Scatter plots of test accuracy vs train bimodality, measured by $BI(e)$ as defined in (\ref{eq:BI}), where changes in color from blue to yellow correspond with advancing epochs.}
%\vspace{-2em}
%\label{fig:global}
\end{center}
\end{figure}

If we were to draw the \emph{Bimodality Index (BI)} of the TPA score as a function of the epochs (Fig.~\ref{fig:global}), we often see two distinct phases. Initially (phase 1), BI is monotonically increasing, namely, both test accuracy and agreement are on the rise. We call it the ‘learning’ phase. Empirically, in this phase most of the clean examples are being learned (or memorized), as can also be seen in the left side panels of Fig.~\ref{fig:global} %. This interpretation is based on empirical evidence, that examples with noisy labels are learned much later than clean examples 
\citep[cf.][]{li2015convergent}. At some point BI may start to decrease, followed by another possible ascent. This is phase 2, in which empirically the memorization of noisy examples dominates the learning (see the right side panels of Fig.~\ref{fig:global}). This fall and rise is explained by another set of empirical observations, that noisy labels are \textbf{not} being learned in the same order by an ensemble of networks (see App.~\ref{app:noisy-IB}), which therefore predicts a decline in BI when noisy labels are being learned. 



\subsection{Overfit and Agreement: Empirical Evidence}
\label{sec:overfit-empical}

Earlier work, investigating the dynamics of learning in deep networks, suggests that examples with noisy labels are learned later \citep{KruegerBJAKMBFC17,zhang2017ICLR,arpit2017closer,arora2019fine}. Since the learning of noisy labels is unlikely to improve the model's test accuracy, we hypothesize that this may be correlated with the occurrence (or increase) of \emph{overfit}. The theoretical result in Section~\ref{sec:theory} suggests that this may be correlated with a decrease in the agreement between networks. Our goal now is to test this prediction empirically.

We next outline empirical evidence that this is indeed the case in actual deep models. In order to boost the strength of overfit, we adopt the scenario of recognition with label noise, where the occurrence of overfit is abundant. When overfit indeed occurs, our experiments show that if the test accuracy drops, then the disagreement score BI also decreases (see example in Fig.~\ref{subfig:biaccsym}-bottom). This observation is confirmed with various noise models and different datasets. When overfit does not occur, the prediction is no longer observed (see example in Fig.~\ref{subfig:biaccsym}-top).

These results suggest that a consistent drop in the $BI$ index of some training set $\iX$ can be used to estimate the occurrence of overfit, and possibly even the beginning of noisy label memorization. %Empirical risk can be estimated this way without any external source of data besides the training data. 





\section{Dealing with Noisy Labels: Proposed Approach}
\label{sec:approach}
%\vspace{-0.5em}

When dealing with noisy labels, there are essentially three intertwined problems that may require separate treatment:
\setlist{nolistsep}
\begin{enumerate}[leftmargin=0.65cm,noitemsep]
\item	\textbf{Noise level estimation}: estimate the number of noisy examples.
\item	\textbf{Noise filtration}: flag points whose label is to be removed.
\item	\textbf{Classifier construction}: train a model without the examples that are flagged as noisy.%, using either a supervised or semi-supervised learning method.
\end{enumerate}


\subsection{DisagreeNet, for Noise Level Estimation and Noise Filtration}
\label{sec:noise-est-fli}

Guided by Section~\ref{sec:agg-overfit}, we propose a method to estimate the noise level in a training set denoted \emph{DisagreeNet}, which is further used to filter out the noisy examples (see pseudo-code below in Alg.~\ref{alg:cap}):
\begin{enumerate}[leftmargin=0.65cm]
\item Compute the ELP score from (\ref{eq:ELP}) at each training example.
\item Fit a two component BMM to the ELP distribution 
(see Fig.~\ref{fig:bmm-fit}).
\item Use the intersection between the 2 components of the BMM fit to divide the data to two groups.
\item Call the group with lower ELP 'noisy data'.
\item Estimate noise level by counting the number of datapoints in the noisy group. 
\end{enumerate}

% \vspace{-1.5em}
\begin{algorithm}
\SetKw{Init}{initialization}
\SetKwComment{Comment}{/* }{ */}
\caption{\emph{DisagreeNet}} \label{alg:cap}% estimating noise levels and flagging noisy points
\KwIn{ELP\_arr, specifying the ELP score of each point in training set $\iX$}
\KwOut{Noise level estimate, and the list of indices of noisy points}
$\{G_{low-ELP},G_{high-ELP}\} \gets$ divide the data to two groups using \textbf{fit\_BMM}(ELP\_arr)\;
noise\_indices $\gets$ indices of ELP\_arr assigned to $G_{low-ELP}$\;%where \textbf{predict\_BMM}(ELP\_arr)) == noise\;
%peak\_left\_index, peak\_right\_index  $\gets$ \textbf{findpeaks}(hist\_arr)\;
%min\_point  $\gets$ \textbf{min\_index}(hist\_arr[peak\_left\_index: peak\_right\_index])\;
noise\_estim  $\gets \frac{|G_{low-ELP}|}{|\mathrm{ELP\_arr}|}$\;
\Return noise\_estim, noise\_indices
\end{algorithm}
% \vspace{-2.0em}

\begin{figure}[htbp]
\centering
\begin{subfigure}[tb]{0.3\textwidth}
  \includegraphics[width=\linewidth]{figs/BMMcifar10symmetricnoise40.png}
  \centering
  \scriptsize{Cifar10, 40\% Symm. noise}
%  \caption{\tiny{Cifar10, 40\% Symm. noise}}
  \end{subfigure}
% \hfill
\hspace{.2cm}
  \begin{subfigure}[tb]{0.3\textwidth}
  \includegraphics[width=\linewidth]{figs/BMMcifar100asymmetricnoise20.png}
  \centering
  \scriptsize{Cifar100, 20\% Asymm. noise}
%  \caption{\tiny{Cifar100, 20\% Asymm. noise}}
  \end{subfigure}
%  \hfill
\hspace{.2cm}
  \begin{subfigure}[tb]{0.3\textwidth}
  \includegraphics[width=\linewidth]{figs/BMMTinyImagenetsymmetric40.png}
  \centering
  \scriptsize{TinyImagenet, 40\% Symm. noise}
%  \caption{\tiny{TinyImagenet, 40\% Symm. noise}}
  \end{subfigure}
%  \hfill
%   \begin{subfigure}[tb]{0.245\textwidth}
%   \includegraphics[width=\linewidth]{figs/BMMImagenet100symmetric20.png}
%   \centering
%   \scriptsize{Imagenet100, 20\% Symm. noise}
% %  \caption{\tiny{Imagenet100, 20\% Symm. noise}}
%   \end{subfigure}
      \caption[Estimation]{ELP distribution, shown separately for the clean data in blue and the noisy data in orange. Superimposed, in blue and orange lines, is the bi-modal BMM fit to the ELP total (not separated) distribution}
      \label{fig:bmm-fit}
%      \vspace{-1.5em}
\end{figure}

As part of our ablation study, we evaluated the two alternative scores defined in Section~\ref{sec:cum-scores}: CumLoss and MeanMargin, where Step 2 of \emph{DisagreeNet} is executed using one of them instead of the ELP score. Results are shown in Section~\ref{sec:ablation}, revealing the superiority of the ELP score in  noise filtration. 

\subsection{Classifier construction}

Aiming to achieve modular handling of noisy labels, we propose the following two-step approach:  
\begin{enumerate}[leftmargin=0.65cm]
\item Run \emph{DisagreeNet}.
\item Run SOTA supervised learning method using the filtered data.
\end{enumerate}
In step 2 it is possible to invoke semi-supervised SOTA methods, using the noisy group as unsupervised data. However, given that semi-supervised learning typically involves additional assumptions (or prior knowledge) as well as high computational complexity (that restricts its applicability to smaller datasets), as discussed in Section~\ref{sec:intro}, we do not consider this scenario here. 

\vspace{-.5em}
\section{Empirical Evaluation}
\label{sec:empirical}
\vspace{-0.5em}

We evaluate our method in the following scenarios and tasks:
\begin{enumerate}[leftmargin=0.65cm]
    \item Noise identification (Section~\ref{sec:noise-identification}), with two complementary sub-tasks: (i) estimate the noise level in the given dataset; (ii) identify the noisy examples. 
    \item Supervised classification (Section~\ref{sec:supervised}), after the removal of the noisy examples.
    %\item Semi-supervised classification (Section~\ref{sec:semi-supervised}), after the removal of the noisy labels and the incorporation of the flagged examples into the unlabeled set.
\end{enumerate}

\vspace{-.4em}
\subsection{Dataset and Baselines (\small{details are deferred to App.~\ref{app:tech})}}
\label{sec:prelem}
\vspace{-.4em}


\myparagpar{Datasets} We evaluate our method on a few standard image classification datasets, including Cifar10 and Cifar100  \citep{krizhevsky2009learning}, Tiny imagenet \citep{le2015tiny}, subsets of Imagenet \citep{deng2009imagenet}, Clothing1M \citep{clothing} and Animal10N \citep{animal10n}, see App.~\ref{app:tech} for details. These datasets were used in earlier work to evaluate the success of noise estimation \citep{pleiss2020identifying,arazo2019unsupervised,li2020dividemix,liu2020early}. 

\myparagpar{Baselines and comparable methods} We report results with the following supervised learning methods for learning from noisy data: 
\textbf{\emph{DY-BMM}} and \textbf{\emph{DY-GMM}} \citep{arazo2019unsupervised},  \textbf{\emph{INCV}} \citep{chen2019understanding}, \textbf{\emph{AUM}} \citep{pleiss2020identifying}, \textbf {\emph{Bootstrap}} \citep{reed2014training}, \textbf{\emph{D2L}} \citep{ma2018dimensionality},  \textbf{\emph{MentorNet}} \citep{jiang2018mentornet}. We also report the results of two absolute baselines: \begin{inparaenum}[(i)] \item \textbf{\emph{Oracle}}, which trains the model on the clean dataset; \item \textbf{\emph{Random}}, which trains the model after the removal of a random fraction of the whole data, equivalent to the noise level. \end{inparaenum}

\myparagpar{Other methods} The following methods use additional prior information, such as a clean validation set or known level of noise: \textbf{\emph{Co-teaching}}  \citep{han2018co}, \textbf{\emph{O2U}} \citep{o2u}, \textbf{\emph{LEC}} \citep{lec} and \textbf{\emph{SELFIE}} \citep{song2019selfie}. Direct comparison does injustice to the previous group of methods, and is therefore deferred to App. ~\ref{app:diff_assump}. Another group of methods is excluded from the comparison because they invoke semi-supervised or contrastive learning  \citep[e.g.,][]{MOIT, li2020dividemix, UNICON, Sel-CL, JoCoR,JoSRC}, which is a different learning paradigm (see discussion of prior art in Section~\ref{sec:intro}).

\begin{figure}[b!]
    \centering
    \begin{subfigure}[b]{.95\textwidth}
%        \centering
        \includegraphics[width=.8\textwidth]{figs/F1legend_bar.png}
%    \vspace{-.25cm}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figs/cifar100f1_bar.png}
    \vspace{-.5cm}
        \caption{Cifar100 sym}
        \label{cifar_sym}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figs/cifar100af1_bar.png}
    \vspace{-.5cm}
        \caption{Cifar100 asym}
        \label{cifar_asym}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figs/CIfa100NE_bar.png}
    \vspace{-.5cm}
        \caption{Cifar100 sym}
        \label{cifar_sym_2}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figs/cifar100NE_asym1.png}
    \vspace{-.5cm}
        \caption{Cifar100 asym}
        \label{cifar_asym_2}
    \end{subfigure}

    \caption[Estimation]{(a)-(b) \textbf{Noise identification}: F1 score for noisy label identification task, using different noise levels ($X$-axis), with asymmetric (a) and asymmetric (b) noise models. Results reflect  3 repetitions involving an ensemble of 10 Densenets each. (c)-(d) \textbf{Noise level estimation}: different noise levels are evaluated ($X$-axis), with asymmetric (c) and asymmetric (d) noise models (the 3 comparison baselines did not report this estimate).}
     \vspace{-1.0em}
\end{figure}

%Baseline methods falling into the second category include: \textbf{\emph{SELF}} \citep{nguyen2019self}, \textbf{\emph{Meta learning}} \citep{li2019learning}, \textbf{\emph{DivideMix}} \citep{li2020dividemix}, \textbf{\emph{ELR}} \citep{liu2020early} and \textbf{\emph{C2D}} \citep{zheltonozhskii2022contrast}. 
\myparagraph{Implementation details} We used DenseNet \citep{iandola2014densenet}, ResNet-18 and ResNet50 \citep{he2016deep}  when training on CIFAR-10/100 and Tiny imagenet , and ResNet50 for Clothing1M and Animal10N.


\vspace{-.4em}
\subsection{Results: Noise Identification}
\label{sec:noise-identification}
\vspace{-.4em}

The performance of \emph{DisagreeNet} is evaluated in two tasks: \begin{inparaenum}[(i)] \item The detection of noisy examples, shown in Fig.~\ref{cifar_sym}-\ref{cifar_asym} (see also Figs.~\ref{fig:NEcifar10} and \ref{fig:precisionrecallfig} in App.~\ref{sec: additinal}), where \emph{DisagreeNet} is seen to outperform the three baselines - \emph{AUM}, \emph{DY-GMM} and \emph{DY-BMM}. \item Noise level estimation, shown in Fig.~\ref{cifar_sym_2}-\ref{cifar_asym_2}\end{inparaenum}, showing good noise level estimation especially in the case of symmetric noise. We also compare \emph{DisagreeNet} to MeanMargin and CumLoss, see Fig.~\ref{fig:score-comparisons-no-ext}.

\vspace{-.4em}
\subsection{Result: Supervised Classifications}%: Supervised Learning} 
\label{sec:supervised}
\vspace{-.4em}


\begin{table}[thb]
  \caption{Test accuracy (\%), average and standard error, in the best epoch of retraining after filtration. %, Imagenet50 and Imagenet100. 
  Results of benchmark methods (see Section~\ref{sec:prelem}) are taken from \citep{pleiss2020identifying}. %We report our results on Imagenet50 and Imagenet100 without comparison to our methods, as none reported results on these datasets. 
 The top and middle tables show CIFAR-10, CIFAR-100 and Tiny Imagenet, with simulated noise. The bottom table shows three `real noise' datasets, and includes in addition results of noise level estimation (when applicable). The presumed noise level for these datasets is indicated in the top line following \citep{huang2019o2u, song2019selfie}.}

\vspace{-.5em}

  \label{table:noise-supervised}
\footnotesize
%\scriptsize
%  \centering
  \begin{tabular}{l| c|c|c||c | c|c}
%    \multicolumn{1}{ c |}{} & \multicolumn{3}{ c ||}{} & \multicolumn{3}{ c }{}   \\ 
    \toprule
%    \multicolumn{1}{ c |}{\textbf{Dataset}} & \multicolumn{4}{ c ||}{\textbf{CIFAR-10}} & \multicolumn{4}{ c }{\textbf{CIFAR-100}}   \\ 
    \multicolumn{1}{ c |}{Method/\textbf{Dataset}} & \multicolumn{3}{ c ||}{\textbf{CIFAR-10 sym}} & \multicolumn{3}{ c }{\textbf{CIFAR-100 sym}}   \\ 
    \hline
 %   \midrule
    \multicolumn{1}{ l |}{Noise level}    & 20\% & 40\% & 60\%  &  20\% & 40\% & 60\% \\
    \hline
    random   &$87.18 \pm 0.6$ & $81.59 \pm 0.4$&  $64.35 \pm 0.4$& $65.49 \pm 0.4$& $49.1 \pm 0.2$& $28.7 \pm 0.5$  \\
    
    \emph{Bootstrap}   & $77.6 \pm 0.2$& $62.6 \pm 0.4$& $48.0 \pm 0.2$&  $51.4 \pm 0.2$& $41.1 \pm 0.2$& $29.7 \pm 0.2$ \\
    
    \emph{MentorNet}   & $86.7 \pm 0.1$& $81.9 \pm 0.2$& -- & $64.2 \pm 0.3$& $57.5 \pm 0.2$& -- \\

    \emph{D2L} & $87.7 \pm 0.2$& $84.4 \pm 0.3$& $72.7 \pm 0.6$& $54.0 \pm 1.0$& $29.7 \pm 1.8$& -- \\
    \emph{INCV}  & $89.5 \pm 0.1$& $86.8 \pm 0.1$& $81.1 \pm 0.3$ & $58.6 \pm 0.5$& $55.4 \pm 0.2$& $43.7 \pm 0.3$\\
    \emph{AUM}  & $90.2 \pm 0.0$& $87.5 \pm 0.1$& $82.1 \pm 0.0$ & $65.5 \pm 0.2$& $61.3 \pm 0.1$& $53.0 \pm 0.5$\\[0.5ex]
    \emph{DisagreeNet}+SL & $\mathbf{93.1 \pm 0.2}$  &  $\mathbf{91.1 \pm 0.1}$ & $\mathbf{83.9 \pm 0.08}$  &  $\mathbf{77.3 \pm 0.2}$  & $\mathbf{71.8 \pm 0.3}$   & $\mathbf{64.7 \pm 0.3}$ \\
 
    \hline
    oracle   & $95.1 \pm 0.2$ & $94.1 \pm 0.2$  & $92.4 \pm 0.1$  & $78.2 \pm 0.3$  & $75.4 \pm 0.1$ & $70.3 \pm 0.2$ \\
    \bottomrule
  \end{tabular}
  \vspace{-.5em}
  \begin{tabular}{l| c|c || c|c || c|c}
    \multicolumn{1}{ c }{} & \multicolumn{2}{ c }{} & \multicolumn{2}{ c }{} & \multicolumn{2}{ c}{}  \\ 
    \toprule
    \multicolumn{1}{ c |}{Method/\textbf{Dataset}} & \multicolumn{2}{ c ||}{\textbf{CIFAR-10 constant asym}} & \multicolumn{2}{ c ||}{\textbf{CIFAR-100 constant asym}}  & \multicolumn{2}{ c }{\textbf{Tiny Imagenet sym}}  \\ 
    \hline
%    \midrule
    \multicolumn{1}{ l |}{Noise level}    & 20\% & 40\% &  20\% & 40\%  &  20\% & 40\% \\
    \hline
    \emph{random}   & $89.5 \pm 0.2$& $79.3 \pm 0.4$& $65.2 \pm 0.1$& $44.64 \pm 0.2$ & $49.8 \pm 0.4$ & $29.9 \pm 0.3$\\
    \emph{Bootstrap}  & $76.2 \pm 0.2$& $55.0 \pm 0.6$& $53.4 \pm 0.3$& $38.7 \pm 0.3$ & - & -\\
    \emph{D2L}  &  $88.6 \pm 0.2$& $76.4 \pm 1.5$& $43.6 \pm 0.7$& $16.9 \pm 1.2$ & - & - \\
    \emph{DY-BMM}  & $77.9 \pm 0.1$& $59.4 \pm 0.6$& $53.2 \pm 0.0$& $37.9 \pm 0.0$ & $41.8 \pm 0.1$& $36.3 \pm 0.2$ \\
    \emph{INCV}  &  $88.3 \pm 0.1$& $79.8 \pm 0.4$& $56.8 \pm 0.1$& $44.4 \pm 0.7$ & $45.2 \pm 0.1$& $42.6 \pm 0.1$\\
    \emph{AUM}  & $89.7 \pm 0.1$& $58.7 \pm 0.2$& $59.7 \pm 0.2$& $40.2 \pm 0.1$ & $48.9 \pm 0.2$& $44.7 \pm 0.1$\\[0.5ex]
% \hdashline[1pt/2pt]\noalign{\vskip 0.5ex}
    % \emph{LEC} \prior  & 89.4& 86.5& 58.9&  47.8& -& - \\ [0.5ex]
% \hdashline[1pt/2pt]\noalign{\vskip 0.5ex}
    \emph{DisagreeNet}+SL   & $\textbf{94.4} \pm \textbf{0.1}$  & $\textbf{91.9} \pm \textbf{0.0}$  & $\textbf{73.9} \pm \textbf{0.5}$ & $\textbf{61.3} \pm \textbf{0.2}$  & $\textbf{64.5} \pm \textbf{0.1}$& $\textbf{58.5} \pm \textbf{0.2}$\\
    \hline
%    \hline
    oracle   &  $95.2 \pm 0.0$ & $94.3 \pm 0.0$  & $78.1 \pm 0.1$ & $75 \pm 0.1$  & $65.4 \pm 0.0$& $60.8 \pm 0.2$  \\
    
    \bottomrule
  \end{tabular}
  \vspace{-.5em}
  \begin{tabular}{l| c|c || c|c|}
    \multicolumn{1}{ c }{} &  \multicolumn{2}{ c }{} & \multicolumn{2}{ c}{}  \\ 
    \toprule
    \multicolumn{1}{ c |}{Method/\textbf{Dataset}} & \multicolumn{2}{ c ||}{\textbf{animal10N, 8\% noise}} & \multicolumn{2}{ c |}{\textbf{Clothing1M, 38\% noise}}  \\ 
    \hline
%    \midrule
    \multicolumn{1}{ l |}{Noise level}    & noise est & test accuracy &  noise est & test accuracy \\
    \hline
    \emph{Cross-Entropy}   & - & $84.1 \pm 0.3$& - & $69$\\
%    \emph{INCV}  & -  & - &  -&-  \\
    \emph{AUM}  & -  & - & 10.7& 70.4  \\[0.5ex]
    \emph{DisagreeNet}+SL   & 7.8  & $85.1 \pm 0.1$ & 17& $70.8$  \\

    
    \bottomrule
  \end{tabular}
% \vspace{-0.5em}
%    \vspace{-2.0em}
\end{table}


\emph{DisagreeNet} is used to remove noisy examples, after which we train a deep model from scratch using the remaining examples only. We report our main results using the Densenet architecture, and report results with other architectures in the ablation study. Table~\ref{table:noise-supervised} summarizes the results for simulated symmetric and asymmetric noise on 5 datasets, and 3 repetitions. It also shows results on 2 real datasets, which are assumed (in previous work) to contain significant levels of 'real' label noise. Additional results are reported in App.~\ref{app:diff_assump}, including methods that require additional prior knowledge.

Not surprisingly, dealing with datasets that are presumed to include inherent label noise proved more difficult, and quite different, than dealing with synthetic noise. As claimed in \citep{MOIT}, non-malicious label noise does less damage to networks' generalization than random label noise: on Clothing1M, for example, hardly any overfit is seen during training, even though the data is believed to contain more than 35\% noise. Still, here too, \emph{DisagreeNet} achieves improved accuracy without access to a clean validation set or known noise level  (see Table \ref{table:noise-supervised}). In App.~\ref{app:diff_assump}, Table ~\ref{Table:diff_assump} we compare \emph{Disagreenet} to methods that \emph{do} use such prior knowledge. Surprisingly, we see that \emph{DisagreeNet} still achieves better results even  without using any additional prior knowledge. 

\vspace{-.5em}
\subsection{Ablation Study}
\label{sec:ablation}
\vspace{-0.2em}

\paragraph{How many networks are needed?} We report in Table.~\ref{Table:ablation_main} the F1 score for noisy label identification, using \emph{DisagreeNet} with varying numbers of networks. %The performance is already competitive with a relatively small number of networks. 
The main boost in performance provided by the use of additional networks is seen when using \emph{DisagreeNet} on hard noise scenarios, such as the asymmetric noise, or with small amounts of noise.

\begin{table}[th]
    \caption{F1 score of DisagreeNet, using different numbers of models.  %The asymmetric noise is malicious     noise
    }
    \label{Table:ablation_main}
\vspace{-1em}
\scriptsize
  \centering
  \begin{tabular}{c| c|  c|c|c|c|c|c}
     &  & \multicolumn{6}{ c }{}  \\ 
    \toprule
    \multicolumn{1}{ c |}{\textbf{Dataset}} & \multicolumn{1}{ c |}{\textbf{Noise}} & \multicolumn{6}{ c }{\textbf{size of ensemble (number of networks)}}  \\ 
    
    \midrule
    Method &  & 1 & 2 & 3 & 4 & 7 & 10  \\
    \hline
    
    \multirow{3}{*}{Cifar10 sym} 
    &$10\%$ &$0.605 \pm 0.01$ &$0.77 \pm 0.0$ &$0.862 \pm 0.0$ &$0.906 \pm 0.0$ &$0.941 \pm 0.0$ &$0.936 \pm 0.0$ \\
    &$20\%$ &$0.861 \pm 0.0$ &$0.939 \pm 0.0$ &$0.95 \pm 0.0$ &$0.949 \pm 0.0$ &$0.943 \pm 0.0$ &$0.941 \pm 0.0$ \\
    &$40\%$ &$0.954 \pm 0.0$ &$0.953 \pm 0.0$ &$0.952 \pm 0.0$ &$0.952 \pm 0.0$ &$0.951 \pm 0.0$ &$0.951 \pm 0.0$ \\
    \hline
    \multirow{3}{*}{Cifar100 sym} 
    &$10\%$ &$0.225 \pm 0.05$ &$0.855 \pm 0.0$ &$0.855 \pm 0.01$ &$0.854 \pm 0.0$ &$0.860 \pm 0.01$ &$0.864 \pm 0.01$ \\
    &$20\%$ &$0.89 \pm 0.0$ &$0.895 \pm 0.0$ &$0.896 \pm 0.0$ &$0.897 \pm 0.0$ &$0.901 \pm 0.0$ &$0.899 \pm 0.0$ \\
    &$40\%$ &$0.89 \pm 0.0$ &$0.917 \pm 0.0$ &$0.921 \pm 0.0$ &$0.924 \pm 0.0$ &$0.924 \pm 0.0$ &$0.927 \pm 0.0$ \\

    \hline
    \multirow{3}{*}{Cifar10 asym} 
    &$10\%$ &$0.355 \pm 0.0$ &$0.469 \pm 0.01$ &$0.568 \pm 0.01$ &$0.631 \pm 0.01$ &$0.748 \pm 0.0$ &$0.814 \pm 0.0$ \\
    &$20\%$ &$0.553 \pm 0.0$ &$0.642 \pm 0.01$ &$0.703 \pm 0.01$ &$0.734 \pm 0.0$ &$0.799 \pm 0.01$ &$0.829 \pm 0.01$ \\
    &$40\%$ &$0.739 \pm 0.0$ &$0.795 \pm 0.0$ &$0.816 \pm 0.0$ &$0.826 \pm 0.0$ &$0.824 \pm 0.0$ &$0.812 \pm 0.0$ \\
    \hline
    \multirow{3}{*}{Cifar100 asym} 
    &$10\%$ &$0.703 \pm 0.0$ &$0.708 \pm 0.0$ &$0.712 \pm 0.0$ &$0.716 \pm 0.0$ &$0.718 \pm 0.0$ &$0.717 \pm 0.0$ \\
    &$20\%$ &$0.727 \pm 0.0$ &$0.732 \pm 0.0$ &$0.732 \pm 0.0$ &$0.735 \pm 0.0$ &$0.736 \pm 0.0$ &$0.737 \pm 0.0$ \\
    &$40\%$ &$0.594 \pm 0.0$ &$0.606 \pm 0.0$ &$0.614 \pm 0.0$ &$0.614 \pm 0.0$ &$0.618 \pm 0.0$ &$0.62 \pm 0.0$ \\

    \hline
    \bottomrule
    \end{tabular}
    \end{table}


\paragraph{Additional ablation results}
Results in App.~\ref{app:ablationstudy}, Table~\ref{Table:ablation_secondry} indicate robustness to architecture, scheduler, and usage of augmentation, although the standard training procedures achieve the best results. Additionally, we see robustness to changing the backbone architecture of \emph{DisagreeNet}, using ResNet18 and ResNet50, see Table \ref{table:acc_resnet}. Finally, in Fig.~\ref{fig:score-comparisons-no-ext} we compare \emph{DisagreeNet} using ELP to disagreeNet using the MeanMargin and CumLoss scores, as defined in Section~\ref{sec:cum-scores}. In symmetric noise scenarios all scores perform well, while in asymmetric noise scenarios the ELP score performs much better, as can be seen in Figs.~\ref{cifar_asym:t},\ref{cifar_asym_2:t}. Additional comparisons of the 3 scores are reported in Apps.~\ref{app:ablationstudy} and \ref{app:uri}.  


\begin{table}[thb]
  \caption{Final accuracy results when changing the backbone architecture.
  }


  \label{table:acc_resnet}
\vspace{-1em}
\footnotesize
%\scriptsize
  \centering
  \begin{tabular}{l| c|c|c||c | c|c}
%    \multicolumn{1}{ c |}{} & \multicolumn{3}{ c ||}{} & \multicolumn{3}{ c }{}   \\ 
    \toprule
%    \multicolumn{1}{ c |}{\textbf{Dataset}} & \multicolumn{4}{ c ||}{\textbf{CIFAR-10}} & \multicolumn{4}{ c }{\textbf{CIFAR-100}}   \\ 
    \multicolumn{1}{ c |}{Method/\textbf{Dataset}} & \multicolumn{3}{ c ||}{\textbf{CIFAR-10 sym}} & \multicolumn{3}{ c }{\textbf{CIFAR-100 sym}}   \\ 
    \hline
 %   \midrule
    \multicolumn{1}{ l |}{Noise level}    & 20\% & 40\% & 60\%  &  20\% & 40\% & 60\% \\
    \hline
       \emph{\scriptsize{DisagreeNet}+SL R18}   & ${93.3 \pm 0.1}$  &  ${91.1 \pm 0.6}$ & ${87.6 \pm 0.1}$  &  ${75.1 \pm 0.1}$  & ${71.5 \pm 0.1}$   & ${63.1 \pm 0.4}$ \\
       
     \emph{\scriptsize{DisagreeNet}+SL R50} & ${93.4 \pm 0.1}$  &  ${91.0 \pm 0.2}$ & ${87.0 \pm 0.1}$  &  ${75.7 \pm 0.3}$  & ${70.2 \pm 1.2}$   & ${61.0 \pm 0.4}$ \\
 

    \bottomrule
  \end{tabular}
\end{table}





\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{.8\textwidth}
        \includegraphics[width=\textwidth]{figs/F1legend_bar_noexternals.png}
    \vspace{-.5cm}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figs/cifar100f1_noexternals_bars.png}
    \vspace{-.5cm}
        \caption{Cifar100 sym}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figs/cifar100af1_noexternals_bars.png}
    \vspace{-.5cm}
        \caption{Cifar100 asym}
        \label{cifar_asym:t}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figs/cifar100NE_bar_mae.png}
    \vspace{-.5cm}
        \caption{Cifar100 sym}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figs/cifar100NE_asym_bar_mae.png}
    \vspace{-.5cm}
        \caption{Cifar100 asym}
        \label{cifar_asym_2:t}
    \end{subfigure}
     \caption[Estimation]{(a)-(b): F1 score ($Y$-axis) for the noisy label identification task, using different noise levels ($X$-axis), with asymmetric (a) and asymmetric (b) noise models. Results with 3 variants of \emph{DisagreeNet} are shown, based on 3 scores: MeanMargin, ELP and CumLoss. (c)-(d): Error in noise level estimation ($Y$-axis) using different noise levels ($X$-axis), with asymmetric (c) and asymmetric (d) noise models. As can be seen, ELP very significantly outperforms the other 2 scores when handling asymmetric noise. } 
     \label{fig:score-comparisons-no-ext}
     \vspace{-1.0em}
\end{figure}



\section{Summary and Discussion}

We presented a new empirical observation, that the variability in the predictions of an ensemble of deep networks is much larger when labels are noisy, than it is when labels are clean. This observation is used as a basis for a new method for classification with noisy labels, addressing along the way the tasks of noise level estimation, noisy labels identification, and classifier construction. Our method is easy to implement, and can be readily incorporated into existing methods for deep learning with label noise, including semi-supervised methods, to improve the outcome of the methods.

Importantly, our method achieves this improvement without making additional assumptions, which are commonly made by alternative methods: \begin{inparaenum}[(i)] \item Noise level is expected to be unknown. \item There is no need for a clean validation set, which many other methods require, but which is very difficult to acquire when the training set is corrupted. \item Almost no additional hyperparameters are introduced. \end{inparaenum}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage

\section*{Acknowledgments}
This work was supported by the Israeli Ministry of Science and Technology, and by the Gatsby Charitable Foundations. 


\bibliographystyle{plainnat}
\bibliography{bib}

\clearpage
\appendix

\section*{Appendix}


\section{Overfit and inter-model correlation}
\label{sec:overfit-app}

In this section we formally analyze the relation between two type of scores, which  measure either overfit or inter-model agreement. \emph{Overfit} is a condition that can occur during the training of deep neural networks. It is characterized by the co-occurring decrease of train error or loss, which is continuously minimized during the training of a deep model, and the increase of test error or loss, which is the ideal measure one would have liked to minimize and which determines the network's generalization error. An \emph{agreement} score measures how similar the models are in their predictions. 

We start by introducing the model and some notations in Section~\ref{sec:notations}.  In Section~\ref{sec:overfit-agreement} we prove the main result (Prop.~\ref{lem:agreement}): the occurrence of overfit at time s in all the models of the ensemble implies that the agreement
between the models decreases.


\subsection{Model and notations}
\label{sec:notations}

\myparagraph{Model.}
We analyze the agreement between an ensemble of $Q$ models, computed by solving the linear regression problem with Gradient Descent (GM) and random initialization. In this problem, the learner estimates a linear function $f(\bx): \R^d\to \R$, where $\bx\in\R^d$ denotes an input vector and $y\in\R$ the desired output. Given a training set of $M$ pairs $\{\bx_m,y_m\}_{m=1}^M$, let $X\in\R^{d\times M}$ denote the training input - a matrix whose $m^\mathrm{th}$ column is $\bx_m\in\R^d$, and let row vector $\by\in\R^M$ denote the output vector whose $m^\mathrm{th}$ element is $y_m$. When solving a linear regression problem, we seek a row vector $\hat\hW\in\R^M$ that satisfies
\begin{equation}
\label{eq:lin-problem}
%\begin{split}
\hat\hW = \argmin\limits_\hW L(\hW), \qquad\qquad L(\hW) =  \frac{1}{2} \Vert \hW X-\by  \Vert_F^2
%\end{split}
\end{equation}
To solve (\ref{eq:lin-problem}) with GD, we perform at each iterative step $s\geq 1$ the following computation:
\begin{equation}
\label{eq:gradient-step}
\begin{split}
\hW^{s+1} &= \hW^s -\mu \dW^s \\
\dW^s &= \frac{\partial L(\iX)}{\partial\hW}\bigg\vert_{\hW=\hW^s} = \hW^s\SXX-\SYX \qquad\qquad \SXX=X X^\top, ~\SYX=\by  X^\top
\end{split}
\end{equation}
for some random initialization vector $\hW_0\in\R^M$ where usually $\E[\hW_0]=0$, and learning rate $\mu$. Henceforth we omit the index $s$ when self evident from context.

\myparagraph{Additional notations}
Below, index $i$ denotes a network instance and $t$ denotes the test data. 
\begin{itemize}
    \item Let $X(i)$ denotes the training matrix used to train network $i\in[Q]$, while $X(t)$ denotes the test matrix. Accordingly, 
\begin{equation*}
\SXX(i)=X(i) X(i)^\top, \quad\SYX (i)=\by (i) X(i)^\top
\end{equation*}
    \item Let $\dW (i)$ denote the gradient step of $\hW(i)$, the $i^\mathrm{th}$ mdoel learned from training set $\{X(i),\by (i)\}$, and let $\bE(i,j)$ denote the cross error of $\hW(i)$ on $\{X(j),\by (j)\}$. Then
\begin{equation*}
\begin{split}
\dW (i) &= \hW (i) \SXX (i) -\SYX (i)  \\
\bE(i,j) &= \hW(i)X(j)-\by (j) \qquad\implies\qquad  \dW(i) = \bE(i,i) X(i)^\top
\end{split}
\end{equation*}
    \item Let $\DW(i,j)$ denote the cross gradient:
\begin{equation}
\label{eq:cross}
\begin{split}
\DW(i,j) = \bE(i,j)X(j)^\top = \hW(i)\SXX(j)-\SYX(j) \quad
\implies\quad  \dW(i) = \DW(i,i)
\end{split}
\end{equation}
 \end{itemize}
After each GD step, the model and the error are updated as follows:
\begin{equation*}
\begin{split}
\tilde\hW(i) &= \hW (i) - \mu\dW (i) \\ %= \hW (i) - \mu\bE(i,i) X(i)^\top\\
\tilde\bE(i,j) &= \tilde\hW(i)X(j)-\by (j) = \bE(i,j)- \mu\DW (i,i)X(j)
\end{split}
\end{equation*}
We note that at step $s$ and $\forall i,j$, $\tilde\hW(i)$ is a random vector in  $\R^d$, and $\tilde\bE(i,j)$ is a random vector in $\R^M$. 

\myparagraph{Test error random variable.}
Let $N$ denote the number of test examples. Note that $\{\bE(i,t)\}_{i=1}^Q$ is a set of $Q$ test errors vectors in $\R^N$, where the $n^\mathrm{th}$ component of the $i^\mathrm{th}$ vector $\bE(i,t)_n$ captures the test error of model $i$ on test example $n$. In effect, it is a sample of size $Q$ from the random variable $\bE(*,t)_n$. This random variable captures the error over test point $n$ of a model computed from a random sample of $M$ training examples. The empirical variance of this random variable will be used to estimate the agreement between the models.

\myparagraph{Overfit.}
Overfit occurs at step $s$ if 
\begin{equation}
\label{eq:overfit-def}
\Vert \tilde\bE(i,t) \Vert_F^2 >  \Vert \bE(i,t)  \Vert_F^2
\end{equation}

\myparagraph{Measuring inter-model agreement.}
In classification problems, bi-modality of the ELP score captures the agreement between a set of classifiers, all trained on the same training matrix $X(i)=X$. Since here we are analyzing a regression problem, we need a comparable score to measure agreement between the predictions of $Q$ linear functions. This measure is chosen to be the variance of the test error among models. Accordingly, we will measure \emph{disagreement} by the empirical variance of the test error random variable $\tilde\bE(*,t)_n$, average over all test examples $n\in[N]$.

More specifically, consider an ensemble of linear models $\{w(i)\}_{i=1}^Q$ trained on set $\iX$ to minimize (\ref{eq:lin-problem}) with $s$ gradient steps, where $i$ denotes the index of a network instance and $Q$ the number of network instances. Using the test error vectors of these models $e(i,t)$, we compute the empirical variance of each element $\var[\bE(*,t)_n]$, and sum over the test examples $n\in[N]$:
\begin{equation*}
\begin{split}
\sum_{n=1}^N\sigma^2[e(*,t)_n ] =\sum_{n =1}^N\frac{1}{2Q^2}\sum_{i=1}^Q\sum_{j=1}^Q \vert e(i,t)_n -e(j,t)_n \vert^2 =\frac{1}{2Q^2}\sum_{i=1}^Q\sum_{j=1}^Q \Vert e(i,t)-e(j,t)\Vert^2 
\end{split}
\end{equation*}

\begin{defn}[Inter-model DisAgreement.]
The disagreement among a set of $Q$ linear models $\{w(i)\}_{i=1}^Q$ at step $s$ is defined as follows
\begin{equation}
\label{eq:agreement-s}
DisAg(s) = \frac{1}{2Q^2}\sum_{i=1}^Q\sum_{j=1}^Q \Vert e(i,t)-e(j,t)\Vert^2 
\end{equation}
\end{defn}



\subsection{Overfit and Inter-Network Agreement}
\label{sec:overfit-agreement}

\begin{lemma}
\label{lem:overfit}
Assume that the learning rate $\mu$ is small enough so that we can neglect terms that are $O(\mu^2)$. Then in each gradient descent step $s$, overfit occurs iff the gradient step $\dW(i) $ of network $i$ is negatively correlated with the cross gradient $\DW(i,t)$.
\end{lemma}
\begin{proof}
Starting from (\ref{eq:overfit-def})
\begin{equation}
\label{eq:overfit}
\begin{split}
\texp{overfit} \iff & \Vert \tilde\bE(i,t) \Vert_F^2 >  \Vert \bE(i,t)  \Vert_F^2  \\
\iff & \Vert \tilde\bE(i,t) \Vert_F^2 -  \Vert \bE(i,t)  \Vert_F^2 = \Vert \bE(i,t) - \mu\DW (i,i)X(t)\Vert_F^2 -  \Vert \bE(i,t)  \Vert_F^2 > 0\\
%\implies & \left \Vert \big [\hW(i)-\mu\DW(i,i)\big ] X(t)-Y(t) \right \Vert_F^2 - \left \Vert \hW(i) X(t)-Y(t) \right \Vert_F^2 > 0\\
\iff & -2\mu\DW(i,i)X(t)\bE(i,t)^\top + O(\mu^2) > 0 \\
\iff &  \DW(i,i)\cdot\DW(i,t)  < 0  
\end{split}
\end{equation}
\end{proof}

\begin{lemma}
\label{lem:2}
Assume that $\Vert I-\mu\SXX\Vert < 1$ and $\SXX$ is invertible. If the number of gradient steps $s$ is large enough so that $\Vert I-\mu\SXX\Vert^{s}$ is negligible, then
\begin{equation}
\label{eq:lim}
\E[\hW^{s}]  = \SYX \SXX^{-1} 
\end{equation}
\end{lemma}
\begin{proof}
Starting from (\ref{eq:gradient-step}), we can show that 
\begin{equation*}
\hW^{s} = \hW^0(I-\mu\SXX)^{s-1} + \mu\SYX \sum_{k=1}^{s-1} (I-\mu\SXX)^{k-1} 
\end{equation*}
Since $\E(\hW^0)=0$
\begin{equation*}
\E(\hW^{s}) = \E(\hW^0)(I-\mu\SXX)^{s-1} + \mu\SYX \sum_{k=1}^{s-1} (I-\mu\SXX)^{k-1}  = \mu\SYX \sum_{k=1}^{s-1} (I-\mu\SXX)^{k-1} 
\end{equation*}
Given the lemma's assumptions, this expression can be evaluated and simplified:
\begin{equation}
\label{eq:shaky}
\begin{split}
\E(\hW^{s}) &=  \mu\SYX  [I-(I-\mu\SXX)]^{-1} [I-(I-\mu\SXX)^{s-1}]  \\
&= \SYX \SXX^{-1}-\SYX \SXX^{-1}(I-\mu\SXX)^{s-1}  \\ 
&\approx \SYX \SXX^{-1}  
\end{split}
\end{equation}
\end{proof}


From (\ref{eq:agreement-s}) it follows that a decrease in inter-model agreement at step $s$, which is implied by increased test variance among models, is indicated by the following inequality:
\begin{equation}
\label{eq:agreement}
%\texp{agreement~decrease} \iff  
\Cr = DisAg(s)-DisAg(s-1) = \frac{1}{2Q^2}\sum_{i,j=1}^Q  \Vert \tilde\bE(i,t)-\tilde\bE(j,t)\Vert^2  - \frac{1}{2Q^2}\sum_{i,j=1}^Q \Vert \bE(i,t)-\bE(j,t)\Vert^2  ~>~0
\end{equation}

\begin{result*}
\label{lem:agreement}
We make the following asymptotic assumptions:
\begin{enumerate}
\item
All models see the same training set, where $X(i)=X~\forall i\in[Q]$.
\item
The learning rate $\mu$ is small enough so that we can neglect terms that are $O(\mu^2)$. 
\item
$\Vert I-\mu\SXX\Vert < 1$, $\SXX$ is invertible, and the number of gradient steps $s$ is large enough so that $\Vert I-\mu\SXX\Vert^{s}$ can be neglected. 
\item
The number of models $Q$ is large enough so that $\frac{1}{Q}\sum_{i=1}^Q  \hW(i) \approx \E[\hW] $.
\end{enumerate}
When these assumptions hold, the occurrence of overfit at time $s$ in all the models of the ensemble implies that the agreement between the models decreases.
\end{result*}
\begin{proof}

(\ref{eq:agreement}) can be rearranged as follows
\begin{equation*}
\begin{split}
\Cr &= \frac{1}{2Q^2}\sum_{i,j=1}^Q  \Vert [\bE(i,t)-\mu\DW (i,i)X(t)]-[\bE(j,t)-\mu\DW (j,j)X(t)]\Vert^2  - \frac{1}{Q^2}\sum_{i,j=1}^Q \Vert \bE(i,t)-\bE(j,t)\Vert^2  \\
& =\frac{1}{Q^2} \sum_{i,j=1}^Q  -\mu [\bE(i,t)-\bE(j,t)]\cdot[\DW (i,i)X(t)]-\DW (j,j)X(t)] +O(\mu^2) \\%\texq{~insert~\bE(i,t)X(t)^\top\!\!=\!\DW(i,t)}\\
&=  \frac{\mu}{Q^2}\sum_{i,j=1}^Q \left [ \DW(i,i)\cdot \DW(j,t)+ \DW(j,j)\cdot \DW(i,t)\right ] - \left [ \DW(i,i)\cdot \DW(i,t)+ \DW(j,j)\cdot \DW(j,t)\right ] +O(\mu^2)\\
\end{split}
\end{equation*}
% $\DW(i,t) = \bE(i,j)X(t)^\top$
where the last transition follows from $\bE(i,t)X(t)^\top\!=\DW(i,t)$. Using assumption 2
\begin{equation}
\Cr =\mu (\Cr' - \Cr'')+O(\mu^2) \approx\mu (\Cr' - \Cr'')
\label{eq:C}
\end{equation}
where
\begin{equation}
\label{eq:cr''}
\Cr'' =\frac{1}{Q^2}\sum_{i,j=1}^Q\left [ \DW(i,i)\cdot \DW(i,t)+ \DW(j,j)\cdot \DW(j,t)\right ] = \frac{2}{Q}\sum_{i=1}^Q\DW(i,i)\cdot \DW(i,t)
\end{equation}
and
\begin{equation}
\label{eq:cr'}
\begin{split}
\Cr' &=\frac{1}{Q^2}\sum_{i,j=1}^Q \left [ \DW(i,i)\cdot \DW(j,t)+ \DW(j,j)\cdot \DW(i,t)\right ] \\
&=\frac{1}{Q}\sum_{i=1}^Q  \DW(i,i)\cdot \frac{1}{Q}\sum_{j=1}^Q\DW(j,t)+ \frac{1}{Q}\sum_{j=1}^Q\DW(j,j)\cdot \frac{1}{Q}\sum_{i=1}^Q\DW(i,t) \\
&=\frac{1}{Q}\sum_{i=1}^Q  \DW(i,i)\cdot \frac{2}{Q}\sum_{j=1}^Q\DW(j,t)
\end{split}
\end{equation}



Next, we prove that $\Cr'$ is approximately 0. We first deduce from assumptions 1 and 4 that
\begin{equation*}
\frac{1}{Q}\sum_{i=1}^Q  \DW(i,i) = \frac{1}{Q}\sum_{i=1}^Q  \hW (i) \SXX (i) -\SYX (i) = \left (\frac{1}{Q}\sum_{i=1}^Q  \hW (i) \right ) \SXX -\SYX \approx \E[\hW] \SXX -\SYX
\end{equation*}
From assumption 3 and Lemma~\ref{lem:2}, we have that $\E[\hW] \approx \SYX \SXX^{-1}$. Thus
\begin{equation*}
\frac{1}{Q}\sum_{i=1}^Q  \DW(i,i) \approx \E[\hW] \SXX -\SYX \approx \SYX \SXX^{-1}\SXX -\SYX = 0
\end{equation*}
From this derivation and (\ref{eq:cr'}) we may conclude that $\Cr' \approx 0$. Thus
\begin{equation}
\label{eq:c-agree}
\Cr \approx -\mu \Cr''= -\mu\frac{2}{Q}\sum_{i=1}^Q\DW(i,i)\cdot \DW(i,t)
\end{equation}

If overfit occurs at time $s$ in all the models of the ensemble, then $\Cr>0$ from Lemma~\ref{lem:overfit} and (\ref{eq:c-agree}). From (\ref{eq:agreement}) we may conclude that the inter-model agreement decreases, which concludes the proof.

\end{proof}

\section{Noisy Labels and Inter-Model Agreement}
\label{app:noisy-IB}

\begin{figure}[htb]
\centering
\begin{subfigure}[tb]{0.49\textwidth}
    \includegraphics[width=\linewidth]{figs/hoeC.png}
  \centering
  \caption{Empirical distribution - Clean examples}
\end{subfigure}
\begin{subfigure}[tb]{0.49\textwidth}
    \includegraphics[width=\linewidth]{figs/hoeCBIN.png}
  \centering
  \caption{Empirical distribution - Noisy examples}
\end{subfigure}
\begin{subfigure}[tb]{0.49\textwidth}
    \includegraphics[width=\linewidth]{figs/hoeN.png}
  \centering
  \caption{Binomial distribution - Clean examples}
\end{subfigure}
\begin{subfigure}[tb]{0.49\textwidth}
    \includegraphics[width=\linewidth]{figs/hoeNBIN.png}
  \centering
  \caption{Binomial distribution - Noisy examples}
\end{subfigure}
\begin{subfigure}[tb]{.5\textwidth}
  \includegraphics[width=\linewidth]{figs/wass_dist.png}
  \centering
\vspace{-1em}
\caption{}%
 \end{subfigure}
\vspace{-1em}
      \caption{(a)-(d): The empirical distribution of the agreement values over epochs ($X$-axis: epochs, $Y$-axis: agreement, color code: blue for low and red for high). Clearly, the distribution of noisy examples resembles the binomial distribution with matched expected value, while the clean examples distribution is far from binomial. (e) Wasserstein distance between the binomial distribution and the empirical agreement distribution over epochs. }
      \label{fig:binomical}
\end{figure}




Here we show empirical evidence, that noisy labels are \textbf{not} being learned in the same order by an ensemble of networks. To see this, we measure the distance between the TPA distribution, computed separately for clean examples and for noisy examples, and the binomial distribution, which is the expected distribution of iid classifiers with the same overall accuracy. Specifically, we compute the Wasserstein distance between the agreement distribution at each epoch and the binomials BIN (k,$p_{clean}$) and BIN(k,$p_{noisy}$), where $p_{clean}$ is the average accuracy on the clean examples, and $p_{noisy}$ is the average accuracy on the noisy examples, see Fig.~\ref{fig:binomical}. We see that while the distribution of model agreement on clean examples is very far from the binomial distribution, the distribution of model agreement  on noisy examples is much closer, which means that each DNN memorizes noisy examples at an approximately independent rate.

\section{Computing the ELP Score: Pseudo-Code}
\label{app:ELP-pseudo}


\begin{algorithm}[ht]
\SetKwComment{Comment}{/* }{ */}
\SetKw{Init}{initialization}
\caption{Computing the ELP score} \label{alg:compELP}
\KwIn{Training dataset $\iX$ with $N$ examples, potentially noisy, network architecture $\mathcal{A}$, batch size $b$, learning rate $\eta$, number of networks K, number of epochs E}
\KwOut{Array, containing the ELP score for each data point}

compute agreement during training\;
\Init{$\theta_{1}^{0} ... \theta_{K}^{0}$} different initialization of $\mathcal{A}$\;
Initialize \textit{agreement\_arr}[E,N]$\gets 0$\;
\For{$e=0; E$}
{
    \For{$k=0; K$}
    {
        sample $indices_b$ with size $b$ from [1...N] uniformly\;
        $x_b,y_b\gets \mathcal{X}[indices_b],\mathcal{Y}[indices_b]$\Comment*[r]{Gets a mini batch}
        
        compute $p_b$ on $x_b$ using $\theta_{k}^{e}$\;
        
        compute loss $l_b$ with respect to $p_b$ and $y_b$\;
        
        $\theta_k^{e+1} \gets SGD(\theta_k^e;l_b)$\;
        $agreement\_arr[e,indice_b]$ += $(\textbf{argmax}(p_b) ==y_b)$\Comment*[r]{Store whether the network k predicted correctly on the examples at epoch e}
    }
}
$agreement\_arr \gets agreement\_arr/(E\cdot K)$\Comment*[r]{normaliztion}
$ELP\_arr \gets mean\_over\_epochs(agreement\_arr)$\Comment*[r]{mean over $X$-axis}

\Return $ELP\_arr$
\label{alg:computeELP}
\end{algorithm}




\section{Additional results}
\label{sec: additinal}


\subsection{Noise level estimation on additional datasets}
\label{app:noise-filt}


%See Fig.~\ref{fig:NEcifar10}
\label{fdi:cifar10NE}
\begin{figure}[h]
\label{fig:rescifar10}
    \centering
    \begin{subfigure}[b]{.88\textwidth}
        \includegraphics[width=\textwidth]{figs/F1legend.png}
        \vspace{-.5cm}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c10f1bar.png}
        \vspace{-.5cm}
        \caption{CIFAR10 F1}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/cifar10NE_bar_mae.png}
        \vspace{-.5cm}
        \caption{CIFAR10 estimation error}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c10af1bar.png}
        \vspace{-.5cm}
        \caption{CIFAR10 asym F1}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Cifar10NE_asym_bar_mae.png}
        \vspace{-.5cm}
        \caption{CIFAR10 asym estimation error}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Tinyf1bar.png}
        \vspace{-.5cm}
        \caption{TinyImagenet F1}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/TinyNE_bar_mae.png}
        \vspace{-.5cm}
        \caption{TinyImagenet estimation error}
    \end{subfigure}
    \caption[Estimation]{Additional results on CIFAR10 and Tiny Imagenet.}
    \label{fig:NEcifar10}
\end{figure}

\newpage






\subsection{Precision and Recall results}
\label{app:precisionandrecall}


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{.88\textwidth}
        \includegraphics[width=\textwidth]{figs/F1legend.png}
    \vspace{-.5cm}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c10pbar.png}
        \vspace{-.5cm}
        \caption{ CIFAR10}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c100pbar.png}
        \vspace{-.5cm}
        \caption{CIFAR100}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c10apbar.png}
        \vspace{-.5cm}
        \caption{CIFAR10 asym}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c100apbar.png}
        \vspace{-.5cm}
        \caption{CIFAR100 asym}
    \end{subfigure}
        \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c10rbar.png}
        \vspace{-.5cm}
        \caption{ CIFAR10}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c100rbar.png}
        \vspace{-.5cm}
        \caption{CIFAR100}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c10arbar.png}
        \vspace{-.5cm}
        \caption{CIFAR10 asym}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/c100arbar.png}
        \vspace{-.5cm}
        \caption{CIFAR100 asym}
    \end{subfigure}
    \caption[Estimation]{Noisy label identification. Top: precision; bottom: recall.}
    \label{fig:precisionrecallfig}
\end{figure}



\section{Ablation study}
\label{app:ablationstudy}

Table ~\ref{Table:ablation_secondry} summarize experiments relating to architecture, scheduler,
and augmentation usage.
\begin{table}[h]
  \caption{F1 score for Cifar100 with 2 levels of symmetric noise. Different ablation conditions are marked in columns: \emph{ResNet34} indicates a change of architecture, \emph{no Aug} indicates that image augmentations are not used, and \emph{lr 0.01} indicates that no scheduler or learning rate drop are used during training.}
  \label{Table:ablation_secondry}
\vspace{-1em}
\small
  \centering
  \begin{tabular}{c|  c|c|c|c|c}
    & & & & &\\ 
    \toprule
%    \multicolumn{1}{ c |}{\textbf{Dataset}} & 
    \textbf{Noise level} & \textbf{No change} & \textbf{ResNet34} &  \textbf{No Aug} &  \textbf{Constant lr 0.01} &  \textbf{Lr 0.01 + no Aug} \\ 
    
    \midrule
%    \multirow{3}{*}{Cifar100 sym constant lr 0.01 no augmentation} 
    $20\%$ &$0.903 \pm 0.01$ &$0.839 \pm 0.0$ &$0.832 \pm 0.0$ &$0.859 \pm 0.01$ & $0.869 \pm 0.01$\\
    $40\%$ &$0.918 \pm 0.01$ &$0.887 \pm 0.0$ &$0.887 \pm 0.01$ &$0.877 \pm 0.0$ &$0.888 \pm 0.01$\\
    \hline
    \bottomrule
  \end{tabular}
\end{table}



\paragraph{Alternative scores}

We evaluate the two alternative scores defined in Section~\ref{sec:cum-scores}: CumLoss and MeanMargin, in which case Step 2 of \emph{DisagreeNet} is executed using one of them instead of the ELP score.  Fig.~\ref{fig:score-comparisons} shows the Probability Distribution Function (PDF) of the three scores, revealing that ELP is more consistency bimodal (especially in the difficult asymmetric case), with modes (peaks) that appear more separable. This  benefit translates to superior performance in the noise filtration task (Figs.~\ref{cifar_asym:t},\ref{cifar_asym_2:t}). 

\begin{figure}[htbp]
\centering
\begin{subfigure}[tb]{0.485\textwidth}
  \includegraphics[width=\linewidth]{figs/Dense100-40-hist.png}
  \centering
  \caption{Cifar10 with 40\% Symmetric noise}
      \label{fig:score-comparisons-easy}
  \end{subfigure}
  \hfill
  \begin{subfigure}[tb]{0.485\textwidth}
  \includegraphics[width=\linewidth]{figs/dense100-40.png}
  \centering
  \caption{Cifar100 with 20\% Asymmetric noise}
      \label{fig:score-comparisons-hard}
  \end{subfigure}
      \caption[Estimation]{Distribution of the CumLoss, MeanMargin and ELP scores during training. ELP remains bimodal even for hard noise models, where the other scores become unimodal. }
      \label{fig:score-comparisons}
%      \vspace{-1.5em}
\end{figure}

We believe that this empirical observation, of increased mode separation, is due to significant difference in the pace of change in agreement values during training between clean and noisy data, in contrast with the pace of change in smoother measures of confidence like \emph{Margin} and \emph{Loss} (see App.~\ref{app:uri}). Note that with the easier symmetric noise, we do not see this difference, and indeed the other scores exhibit two nicely separated modes, sometimes achieving even better results in noise filtration than ELP (Fig.~\ref{fig:NEcifar10} in App.~\ref{app:noise-filt}). However, when comparing the test accuracy after retraining (see App.~\ref{sec: additinal}), we observe that ELP still achieves superior results. 


\section{Methodology and Technical details}
\label{app:tech}

\myparagraph{Datasets} We evaluated our method on a few standard image classification datasets, including Cifar10 and Cifar100  \citep{krizhevsky2009learning} and Tiny imagenet \citep{le2015tiny}. Cifar10/100 consist of 60k $32\times 32$ color images of 10 and 100 classes respectivaly. Tiny ImageNet consists of 100,000 images from 200 classes of ImageNet \citep{deng2009imagenet}, downsampled to size $64\times 64$. Animal10N  dataset contains 5 pairs of confusing animals with a total of 55,000 64x64 images. %Imagenet50 and Imagenet100 are subsets of Imagenet \citep{deng2009imagenet}, consisting of 50 and 100 classes respectively. 
Clothing1M \citep{clothing} contains 1M clothing images in 14 classes. These datasets were used in earlier work to evaluate the success of noise estimation \citep{pleiss2020identifying,arazo2019unsupervised,li2020dividemix,liu2020early}.

\myparagraph{Baseline methods for comparison} We evaluate our method in the context of two approaches designed to deal with label noise: methods that focus on improving the supervised learning by identifying noisy labels and removing/reducing their influence on the training, and methods that use iterative methods and utilize semi-supervised algorithms in order to learn with noisy labels.
\myparagraph{First approach:} 
\begin{inparadesc} \item[$\diamond$] \textbf{\emph{DY-BMM}} and \textbf{\emph{DY-GMM}} \citep{arazo2019unsupervised} estimate mixture models on the loss to separate noisy and clean examples. 
    \item[$\diamond$]  \textbf{\emph{INCV}} \citep{chen2019understanding} iteratively filter out noisy examples by using cross-validation.
    \item[$\diamond$]  \textbf{\emph{AUM}} \citep{pleiss2020identifying} inserts corrupted examples to determine a filtration threshold, using the mean margin  as a score.
    \item[$\diamond$] \textbf {\emph{Bootstrap}} \citep{reed2014training} interpolates between the net predictions and the given label.
    \item[$\diamond$]  \textbf{\emph{D2L}} \citep{ma2018dimensionality} follows \emph{Bootstrap}, and uses the examples dimensional attributes for the interpolation.
    % \item[$\diamond$]  \textbf{\emph{decoupling}} \citep{malach2017decoupling} use two networks and their disagreement during training to filter noise from the dataset.
    \item[$\diamond$]  \textbf{\emph{Co-teaching}} \citep{han2018co} use two networks to filter clean data for the other net training.
    \item[$\diamond$]  \textbf{\emph{O2U}} \citep{o2u} varies the learning rate to identify the noisy samples, based on a loss-based metric.
    \item[$\diamond$]  \textbf{\emph{MentorNet}} \citep{jiang2018mentornet} trains a mentor network, whose outputs are used as a curriculum to the student network. \item[$\diamond$]  \textbf{\emph{LEC}} \citep{lec} trains multiple networks, and uses the intersection of their small loss examples (using a given noise rate as a threshold) to construct a filtered dataset for the next epoch.
    \end{inparadesc}

\myparagraph{Second approach:} 
\begin{inparadesc} \item[$\diamond$]  \textbf{\emph{SELF}} \citep{nguyen2019self} iteratively uses an exponential moving average of a net prediction over the epochs, compared to the ground truth labels, to filter noisy labels and retrain . \item[$\diamond$]  \textbf{\emph{Meta learning}} \citep{li2019learning} uses a gradient based technique to update the networks weights with noise tolerance.
    \item[$\diamond$] \textbf{\emph{DivideMix}} \citep{li2020dividemix} uses 2 networks to flag examples as noisy and clean with two component mixture, after which the SSL technique MixMatch \citep{berthelot2019mixmatch} is used.
    \item[$\diamond$] \textbf{\emph{ELR}} \citep{liu2020early} identifies early learned example, and uses them to regulate the learning process. \item[$\diamond$] \textbf{\emph{C2D}} \citep{zheltonozhskii2022contrast} uses the same algorithm as ELR and Dividemix, and uses a pretrain net with unsupervised loss. \end{inparadesc}


\myparagraph{Technical Details}
\label{Technical}
Unless stated otherwise, we used an SGD optimizer with 0.9 momentum and a learning rate of 0.01, weight decay of 5e-4, and batch size of 32. We used a Cosine-annealing scheduler in all of our experiments and used standard augmentation (horizontal flips, random crops) during training. We inspected the effect of different hyperparameters in the ablation study. All of our experiments were conducted on the internal cluster of the Hebrew University, on GPU type AmpereA10.
 
 %\myparagraph{Semi-supervised technical details} when using Flexmatch we adapted the setups suggested by \citet{flexmatch2021}. Thus, we trained WRN-28-2 for CIFAR10, WRN-28-8 for CIFAR100, and ResNet50 for webvision. For baselines \emph{DivideMix} and \emph{ELR} with the pre-trained model, we followed the same hyperparameters suggested in \citep{zheltonozhskii2022contrast} and used PreAct ResNet-18.
 
\section{Comparing agreement to confidence in noise filtration}
\label{app:uri}

While the learning time of an example has been shown to be effective for noise filtration, it fails to separate noisy and clean data that are learned more or less at the same time. To tackle this problem, one needs additional information, beyond the learning time of a single network. When using an ensemble, we can use the TPA score, or else the average probability assigned to the ground truth label (denoted the "correct" logit) by the networks. The latter score conveys the model's confidence in the ground truth label, and is used by our two alternative scores - CumLoss and MeanMargin. 

Going beyond learning time, we propose to look at "how quickly" the agreement value rises from 0 to 1, denoted as the "slope" of the agreement. Since our empirical results indicate that the learning time of noisy data is much more varied, we expect a slower rise in agreement over noisy data as compared to clean data. In our experiments, ELP achieved superior results in noise filtration. We hypothesize that the difference in slope between clean and noisy data may underlie the superiority of ELP in noise filtration.

To check this hypothesis, we compare between two scores computed at each data example: ELP and Logits Mean (denoted LM for simplicity). LM is defined as follows:
\[LM(x) = \frac{\sum_{i=1}^k \sum_{j=1}^T{[p_{i,j}(x)]_y}}{kT}\]
where $k$ is the number of networks, $T$ is the number of epochs during training, $(x,y)$ is a data example and its assigned label, and $[p_{i,j}(x)]_y$ is the probability assigned by network $i$ in epoch $j$ to $y$ (the ground truth label). 

In order to compare between the pace of increase (slope) of ELP and LM, we conduct the following analysis: We select the two groups of clean and noisy data that are learned (roughly) at the same time by some net in the ensemble, and then compute the average agreement and "correct" logit functions as a function of epoch, separately for clean and noisy data. We then compute the difference per epoch between the noisy and clean average agreement, which we denote as $\Delta Agreement$ and $\Delta logit$. Note that $\Delta Agreement$ and $\Delta logit$ encode the difference in the slope between noisy and clean data, since they begin to rise at (roughly) the same time. Finally, we plot in Fig.~\ref{fig:uri} the difference between $\Delta Agreement$ and $\Delta logit$, recalling that larger $\Delta$ indicates stronger separation between the clean and noisy data. %Our results appear in Fig.~\ref{fig:uri}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Dense100_20_plot.png}
        \vspace{-.5cm}
        \caption{20\% symmetric noise}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Dense100_40_plot.png}
        \vspace{-.5cm}
        \caption{40\% symmetric noise}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Dense100_asym_new_plot_20.png}
        \vspace{-.5cm}
        \caption{20\% Asymmetric noise}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Dense100_asym_new_40_plot.png}
        \vspace{-.5cm}
        \caption{40\% Asymmetric noise}
    \end{subfigure}
        \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Dense100_20_scatter_2.png}
        \vspace{-.5cm}
        \caption{20\% symmetric noise}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Dense100_40_scatter.png}
        \vspace{-.5cm}
        \caption{40\% symmetric noise}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Dense100_asym_20_scatter.png}
        \vspace{-.5cm}
        \caption{20\% Asymmetric noise}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{figs/Dense100_40_scatter.png}
        \vspace{-.5cm}
        \caption{40\% Asymmetric noise}
    \end{subfigure}
    \caption[Estimation]{Top: $X$-axis is the learning time of the chosen clean and noisy data; $Y$-axis is the difference between $\Delta Agreement$ and $\Delta logit$. We see that for most of the training, the difference is positive, implying that ELP provides stronger separation between these groups. Bottom: $X$-axis is the difference between $\Delta Agreement$ and $\Delta logit$; $Y$-axis is the ratio between the amount of clean and noisy data. The color represents the learning time of the groups. These graphs show that while at the end of the training the difference between $\Delta Agreement$ and $\Delta logit$ is negative, implying that LM would be better at separating these groups, these are in fact very small sets of data, as most of the data is learned by some network at an earlier stage of the training}
    \label{fig:uri}
\end{figure}

Indeed, our analysis shows that with asymmetric noise, the difference between the agreement slope on clean and noisy data of the ELP score is consistently larger than the agreement slope difference between the average logits on clean and noisy data. This, we believe, is the key reason as to why ELP outperforms LM in noise filtration. Note that this effect is much less pronounced when using the easier symmetric noise, and indeed, our empirical results show that ELP does not outperform LM significantly in this case.

To conclude, we believe that the signal encoded by the agreement values is stronger than the signal encoded in measures of confidence in the networks' prediction when true labels are concerned, which explains its capability to classify correctly even some hard-clean examples and easy-noisy examples as clean and noise (respectively). This, we believe, is a result of the polarization effect caused by the binary indicators inside TPA, which disregard misleading positive probabilities assigned to noisy labels even before they are learned by the networks.


\section{Comparing to methods with different assumptions}
\label{app:diff_assump}

Here we compare DisagreeNet to methods that assume known noise level - O2U \citep{o2u} and LEC \citep{lec}, using a 9-layered CNN for the training (with standard hyper parameters as detailed in App. ~\ref{app:tech}). Since the noise level is assumed known, we replace the estimation provided by DisagreeNet with the actual noise level. The results are summarized in Table.~\ref{Table:diff_assump}. We also compare DisagreeNet to other methods that use prior knowledge, where DisagreeNet does not use prior knowledge. The results are summarized in Table.~\ref{table:noise-supervised-prior}   

\begin{table}[h]
\scriptsize
  \centering
  \begin{tabular}{c| c|  c|c|c|}
    \toprule
    \multicolumn{1}{ c |}{\textbf{Dataset}} & \multicolumn{1}{ c |}{\textbf{Noise level}} & \multicolumn{3}{ c }{\textbf{Method}}  \\ 
    \hline
    \midrule
    Dataset - noise type& Noise level & O2U & LEC & DisagreeNet  \\
    \hline
    
    \multirow{4}{*}{Cifar10 sym} 
    &$10\%$  &$87.64\%$ &-         & $\mathbf{92.57\%}$  \\
    &$20\%$  &$85.24\%$ &$88.31\%$ &$\mathbf{91.44\%}$  \\
    &$40\%$  &$79.64\%$ &-         &$\mathbf{88.48\%}$  \\
    &$60\%$  &  -    &$80.52\%$.   &$\mathbf{81.81\%}$  \\

    \hline
    \multirow{4}{*}{Cifar100 sym} 
    &$10\%$ &$62.32\%$ & -          & $\mathbf{69.86\%}$  \\
    &$20\%$  &$60.53\%$ & $59.98\%$ &$\mathbf{67.99\%}$  \\
    &$40\%$  &$52.47\%$ &-          &$\mathbf{62.89\%}$  \\
    &$60\%$  &  -  &$46.63\%$       &$\mathbf{53.76\%}$  \\
    \hline
    \multirow{3}{*}{Cifar10 asym} 
    &$10\%$ &$88.22\%$ &-          &$\mathbf{91.96\%}$  \\
    &$20\%$  &  -   &$89.41\%$.    &$\mathbf{90.94\%}$  \\
    &$40\%$  &  -   &$86.50\%$     &$\mathbf{86.93\%}$  \\
    \hline
    \multirow{3}{*}{Cifar100 asym} 
    &$10\%$ &$64.50\%$ & -       &$\mathbf{69.83\%}$  \\
    &$20\%$  &  -   &$58.86\%$.  &$\mathbf{67.99\%}$  \\
    &$40\%$  &  -   &$47.82\%$.  &$\mathbf{62.89\%}$  \\
    \hline
    \bottomrule
  \end{tabular}
  \caption{Test accuracy (\%) comparison with methods that utilize prior knowledge with 9-layered CNN}
\label{Table:diff_assump}
\end{table}




\begin{table}[thb]
\footnotesize
%\scriptsize
  \centering
  \begin{tabular}{l| c|c|c||c | c|c}
    \multicolumn{1}{ c |}{} & \multicolumn{3}{ c ||}{} & \multicolumn{3}{ c }{}   \\ 
    \toprule
%    \multicolumn{1}{ c |}{\textbf{Dataset}} & \multicolumn{4}{ c ||}{\textbf{CIFAR-10}} & \multicolumn{4}{ c }{\textbf{CIFAR-100}}   \\ 
    \multicolumn{1}{ c |}{Method/\textbf{Dataset}} & \multicolumn{3}{ c ||}{\textbf{CIFAR-10 sym}} & \multicolumn{3}{ c }{\textbf{CIFAR-100 sym}}   \\ 
%    \hline
 %   \midrule
    \multicolumn{1}{ l |}{Noise level}    & 20\% & 40\% & 60\%  &  20\% & 40\% & 60\% \\
    \hline
    % Co-teaching  \prtiny & $88.8 \pm 0.1$& $86.5 \pm 0.1$& $80.7 \pm 0.1$& $64.1 \pm 0.1$& $60.2 \pm 0.2$& $48.0 \pm 0.3$\\
    % LEC \prior & $88.3$& -& 80.5&  60& -& 46.63 \\
    % O2U  \prior  & $92.5$& 90.3& -&  74.1& 69.2& - \\[0.5ex]
 
%    \hline
\rule{0pt}{2ex}      Co-teaching   & $88.8 \pm 0.1$& $86.5 \pm 0.1$& $80.7 \pm 0.1$& $64.1 \pm 0.1$& $60.2 \pm 0.2$& $48.0 \pm 0.3$\\
    LEC  & $88.3$& -& 80.5&  60& -& 46.63 \\
    O2U   & $92.5$& 90.3& -&  74.1& 69.2& - \\[0.5ex]
    \hdashline[1pt/2pt]\noalign{\vskip 0.5ex}
    \begin{tabular}{@{}l@{}}\emph{DisagreeNet}+SL \\ {\scriptsize{{\color{airforceblue} (no prior knowledge)}}}\end{tabular} & $\mathbf{93.1 \pm 0.2}$  &  $\mathbf{91.1 \pm 0.1}$ & $\mathbf{83.9 \pm 0.08}$  &  $\mathbf{77.3 \pm 0.2}$  & $\mathbf{71.8 \pm 0.3}$   & $\mathbf{64.7 \pm 0.3}$ \\
    % \midrule
    % \hline
    % \hline
%    oracle   & $95.1 \pm 0.2$ & $94.1 \pm 0.2$  & $92.4 \pm 0.1$  & $78.2 \pm 0.3$  & $75.4 \pm 0.1$ & $70.3 \pm 0.2$ \\
    
    % \hline    
    % \hline 
    \bottomrule
  \end{tabular}

  \begin{tabular}{l| c|c || c|c || c|c}
    \multicolumn{1}{ c |}{} & \multicolumn{2}{ c ||}{} & \multicolumn{2}{ c ||}{} & \multicolumn{2}{ c}{}  \\ 
    \toprule
    \multicolumn{1}{ c |}{Method/\textbf{Dataset}} & \multicolumn{2}{ c ||}{\textbf{CIFAR-10 asym}} & \multicolumn{2}{ c ||}{\textbf{CIFAR-100 asym}}  & \multicolumn{2}{ c }{\textbf{Tiny Imagenet sym}}  \\ 
%    \hline
%    \midrule
    \multicolumn{1}{ l |}{Noise level}    & 20\% & 40\% &  20\% & 40\%  &  20\% & 40\% \\
    \hline
\rule{0pt}{2ex}      \emph{LEC}  & 89.4& 86.5& 58.9&  47.8& -& - \\ [0.5ex]
   \hdashline[1pt/2pt]\noalign{\vskip 0.5ex}
      \begin{tabular}{@{}l@{}}\emph{DisagreeNet}+SL \\ {\color{airforceblue} (no prior knowledge)}\end{tabular} & $\textbf{94.4} \pm \textbf{0.1}$  & $\textbf{91.9} \pm \textbf{0.0}$  & $\textbf{73.9} \pm \textbf{0.5}$ & $\textbf{61.3} \pm \textbf{0.2}$  & $\textbf{62.5} \pm \textbf{0.2}$& $\textbf{55.7} \pm \textbf{0.4}$\\
%    \hline
%    \hline
%    oracle   &  $95.2 \pm 0.0$ & $94.3 \pm 0.0$  & $78.1 \pm 0.1$ & $75 \pm 0.1$  & $65.4 \pm 0.0$& $60.8 \pm 0.2$  \\
    
    \bottomrule
  \end{tabular}


  \begin{tabular}{l| c|c }
    \multicolumn{1}{ c |}{} & \multicolumn{2}{ c }{}\\ 
    \toprule
    \multicolumn{1}{ c |}{Method/\textbf{Dataset}} & \multicolumn{2}{ c }{\textbf{animal10N, 8\% noise}}  \\ 
%    \hline
%    \midrule
    \multicolumn{1}{ l |}{Noise level}    & noise est & test accuracy\\
    \hline
%    \emph{Cross-Entropy}   & - & $84.1 \pm 0.3$\\
    \emph{Co-teaching}  & -  & $82.5 \pm 0.1$   \\
    \emph{SELFIE} & -  & $83 \pm 0.1$   \\
   \hdashline[1pt/2pt]\noalign{\vskip 0.5ex}
    \emph{DisagreeNet}+SL {\color{airforceblue} (no prior knowledge)}  & 7.8  & $\mathbf{85.1 \pm 0.1}$ \\

    
    \bottomrule
  \end{tabular}
%  \vspace{0.2cm}
  \caption{Test accuracy (\%) comparison with methods that utilize prior knowledge of the real noise level. }


  \label{table:noise-supervised-prior}
%    \vspace{-2.0em}
\end{table}

\end{document}

