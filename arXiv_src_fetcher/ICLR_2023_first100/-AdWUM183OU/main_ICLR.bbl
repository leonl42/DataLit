\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O’Connor, and
  McGuinness]{arazo2019unsupervised}
Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness.
\newblock Unsupervised label noise modeling and loss correction.
\newblock In \emph{International conference on machine learning}, pages
  312--321. PMLR, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger,
  Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron
  Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International conference on machine learning}, pages
  233--242. PMLR, 2017.

\bibitem[Baldock et~al.(2021)Baldock, Maennel, and Neyshabur]{baldock2021deep}
Robert Baldock, Hartmut Maennel, and Behnam Neyshabur.
\newblock Deep learning through the lens of example difficulty.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 10876--10889, 2021.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin~A Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Chai et~al.(2021)Chai, Wu, and Zeng]{Ying21Destect}
Ying Chai, Chengrong Wu, and Jianping Zeng.
\newblock Detect noisy label based on ensemble learning.
\newblock In Hongying Meng, Tao Lei, Maozhen Li, Kenli Li, Ning Xiong, and Lipo
  Wang, editors, \emph{Advances in Natural Computation, Fuzzy Systems and
  Knowledge Discovery}, pages 1843--1850, Cham, 2021. Springer International
  Publishing.
\newblock ISBN 978-3-030-70665-4.

\bibitem[Chen et~al.(2019)Chen, Liao, Chen, and Zhang]{chen2019understanding}
Pengfei Chen, Ben~Ben Liao, Guangyong Chen, and Shengyu Zhang.
\newblock Understanding and utilizing deep neural networks trained with noisy
  labels.
\newblock In \emph{International Conference on Machine Learning}, pages
  1062--1070. PMLR, 2019.

\bibitem[de~Moura et~al.(2018)de~Moura, Prudencio, and
  Cavalcanti]{Moura18Ensemble}
Kecia~G. de~Moura, Ricardo~B.C. Prudencio, and George~D.C. Cavalcanti.
\newblock Ensemble methods for label noise detection under the noisy at random
  model.
\newblock In \emph{2018 7th Brazilian Conference on Intelligent Systems
  (BRACIS)}, pages 474--479, 2018.
\newblock \doi{10.1109/BRACIS.2018.00088}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Feng et~al.(2020)Feng, Quan, and Dauphin]{Feng2020Adaptive}
Wei Feng, Yinghui Quan, and Gabriel Dauphin.
\newblock Label noise cleaning with an adaptive ensemble method based on noise
  detection metric.
\newblock \emph{Sensors}, 20\penalty0 (23), 2020.
\newblock ISSN 1424-8220.
\newblock \doi{10.3390/s20236718}.
\newblock URL \url{https://www.mdpi.com/1424-8220/20/23/6718}.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{ghosh2017robust}
Aritra Ghosh, Himanshu Kumar, and P~Shanti Sastry.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~31, 2017.

\bibitem[Goldberger and Ben-Reuven(2016)]{goldberger2016training}
Jacob Goldberger and Ehud Ben-Reuven.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock 2016.

\bibitem[Hacohen et~al.(2020)Hacohen, Choshen, and Weinshall]{hacohen2020let}
Guy Hacohen, Leshem Choshen, and Daphna Weinshall.
\newblock Let’s agree to agree: Neural networks share classification order on
  real datasets.
\newblock In \emph{Int. Conf. Machine Learning ICML}, pages 3950--3960, 2020.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
  Masashi Sugiyama.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Huang et~al.(2019{\natexlab{a}})Huang, Qu, Jia, and
  Zhao]{huang2019o2u}
Jinchi Huang, Lie Qu, Rongfei Jia, and Binqiang Zhao.
\newblock O2u-net: A simple noisy label detection approach for deep neural
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 3326--3334, 2019{\natexlab{a}}.

\bibitem[Huang et~al.(2019{\natexlab{b}})Huang, Qu, Jia, and Zhao]{o2u}
Jinchi Huang, Lie Qu, Rongfei Jia, and Binqiang Zhao.
\newblock O2u-net: A simple noisy label detection approach for deep neural
  networks.
\newblock In \emph{2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 3325--3333, 2019{\natexlab{b}}.
\newblock \doi{10.1109/ICCV.2019.00342}.

\bibitem[Iandola et~al.(2014)Iandola, Moskewicz, Karayev, Girshick, Darrell,
  and Keutzer]{iandola2014densenet}
Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell,
  and Kurt Keutzer.
\newblock Densenet: Implementing efficient convnet descriptor pyramids.
\newblock \emph{arXiv preprint arXiv:1404.1869}, 2014.

\bibitem[Jenni and Favaro(2018)]{jenni2018deep}
Simon Jenni and Paolo Favaro.
\newblock Deep bilevel learning.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pages 618--633, 2018.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2018mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{International Conference on Machine Learning}, pages
  2304--2313. PMLR, 2018.

\bibitem[Karim et~al.(2022)Karim, Rizve, Rahnavard, Mian, and Shah]{UNICON}
Nazmul Karim, Mamshad~Nayeem Rizve, Nazanin Rahnavard, Ajmal Mian, and Mubarak
  Shah.
\newblock Unicon: Combating label noise through uniform selection and
  contrastive learning, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.14542}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Online}, 2009.

\bibitem[Krueger et~al.(2017)Krueger, Ballas, Arpit, Kanwal, Maharaj, Bengio,
  Fischer, and Courville]{KruegerBJAKMBFC17}
David Krueger, Nicolas Ballas, Stanislaw~Devansh Arpit, Maxinder~S. Kanwal,
  Tegan Maharaj, Emmanuel Bengio, Asja Fischer, and Aaron~C. Courville.
\newblock Deep nets don't learn via memorization.
\newblock In \emph{Int. Conf. Learning Representations ICLR}, 2017.

\bibitem[Le and Yang(2015)]{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[Lee and Chung(2019)]{lec}
Jisoo Lee and Sae-Young Chung.
\newblock Robust training with ensemble consensus.
\newblock \emph{arXiv preprint arXiv:1910.09792}, 2019.

\bibitem[Li et~al.(2019)Li, Wong, Zhao, and Kankanhalli]{li2019learning}
Junnan Li, Yongkang Wong, Qi~Zhao, and Mohan~S Kankanhalli.
\newblock Learning to learn from noisy labeled data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 5051--5059, 2019.

\bibitem[Li et~al.(2020)Li, Socher, and Hoi]{li2020dividemix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2002.07394}, 2020.

\bibitem[Li et~al.(2022)Li, Xia, Ge, and Liu]{Sel-CL}
Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu.
\newblock Selective-supervised contrastive learning with noisy labels, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.04181}.

\bibitem[Li et~al.(2015)Li, Yosinski, Clune, Lipson, and
  Hopcroft]{li2015convergent}
Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John~E Hopcroft.
\newblock Convergent learning: Do different neural networks learn the same
  representations?
\newblock In \emph{Adv. Neural Inform. Process. Syst. NeurIPS}, pages 196--212,
  2015.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and
  Fernandez-Granda]{liu2020early}
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20331--20342, 2020.

\bibitem[Ma et~al.(2018)Ma, Wang, Houle, Zhou, Erfani, Xia, Wijewickrema, and
  Bailey]{ma2018dimensionality}
Xingjun Ma, Yisen Wang, Michael~E Houle, Shuo Zhou, Sarah Erfani, Shutao Xia,
  Sudanthi Wijewickrema, and James Bailey.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In \emph{International Conference on Machine Learning}, pages
  3355--3364. PMLR, 2018.

\bibitem[Malach and Shalev-Shwartz(2017)]{malach2017decoupling}
Eran Malach and Shai Shalev-Shwartz.
\newblock Decoupling" when to update" from" how to update".
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2021deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2021\penalty0 (12):\penalty0 124003, 2021.

\bibitem[Neal et~al.(2018)Neal, Mittal, Baratin, Tantia, Scicluna,
  Lacoste-Julien, and Mitliagkas]{neal2018modern}
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna,
  Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock \emph{arXiv preprint arXiv:1810.08591}, 2018.

\bibitem[Nguyen et~al.(2019)Nguyen, Mummadi, Ngo, Nguyen, Beggel, and
  Brox]{nguyen2019self}
Duc~Tam Nguyen, Chaithanya~Kumar Mummadi, Thi Phuong~Nhung Ngo, Thi Hoai~Phuong
  Nguyen, Laura Beggel, and Thomas Brox.
\newblock Self: Learning to filter noisy labels with self-ensembling.
\newblock \emph{arXiv preprint arXiv:1910.01842}, 2019.

\bibitem[Nguyen et~al.(2020)Nguyen, Raghu, and Kornblith]{nguyen2020wide}
Thao Nguyen, Maithra Raghu, and Simon Kornblith.
\newblock Do wide and deep networks learn the same things? uncovering how
  neural network representations vary with width and depth.
\newblock \emph{arXiv preprint arXiv:2010.15327}, 2020.

\bibitem[Ortego et~al.(2020)Ortego, Arazo, Albert, O'Connor, and
  McGuinness]{MOIT}
Diego Ortego, Eric Arazo, Paul Albert, Noel~E. O'Connor, and Kevin McGuinness.
\newblock Multi-objective interpolation training for robustness to label noise.
\newblock \emph{CoRR}, abs/2012.04462, 2020.
\newblock URL \url{https://arxiv.org/abs/2012.04462}.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Giorgio Patrini, Alessandro Rozza, Aditya Krishna~Menon, Richard Nock, and
  Lizhen Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1944--1952, 2017.

\bibitem[Pleiss et~al.(2020)Pleiss, Zhang, Elenberg, and
  Weinberger]{pleiss2020identifying}
Geoff Pleiss, Tianyi Zhang, Ethan Elenberg, and Kilian~Q Weinberger.
\newblock Identifying mislabeled data using the area under the margin ranking.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17044--17056, 2020.

\bibitem[Pliushch et~al.(2021)Pliushch, Mundt, Lupp, and
  Ramesh]{pliushch2021deep}
Iuliia Pliushch, Martin Mundt, Nicolas Lupp, and Visvanathan Ramesh.
\newblock When deep classifiers agree: Analyzing correlations between learning
  order and image statistics.
\newblock \emph{arXiv preprint arXiv:2105.08997}, 2021.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,
  and Andrew Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{arXiv preprint arXiv:1412.6596}, 2014.

\bibitem[Sabzevari et~al.(2018)Sabzevari, Martínez-Muñoz, and
  Suárez]{SABZEVARI20182374}
Maryam Sabzevari, Gonzalo Martínez-Muñoz, and Alberto Suárez.
\newblock A two-stage ensemble method for the detection of class-label noise.
\newblock \emph{Neurocomputing}, 275:\penalty0 2374--2383, 2018.
\newblock ISSN 0925-2312.
\newblock \doi{https://doi.org/10.1016/j.neucom.2017.11.012}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0925231217317265}.

\bibitem[Song et~al.(2019{\natexlab{a}})Song, Kim, and Lee]{animal10n}
Hwanjun Song, Minseok Kim, and Jae-Gil Lee.
\newblock {SELFIE}: Refurbishing unclean samples for robust deep learning.
\newblock In \emph{ICML}, 2019{\natexlab{a}}.

\bibitem[Song et~al.(2019{\natexlab{b}})Song, Kim, and Lee]{song2019selfie}
Hwanjun Song, Minseok Kim, and Jae-Gil Lee.
\newblock Selfie: Refurbishing unclean samples for robust deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5907--5915. PMLR, 2019{\natexlab{b}}.

\bibitem[Song et~al.(2022)Song, Kim, Park, Shin, and Lee]{song2022learning}
Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee.
\newblock Learning from noisy labels with deep neural networks: A survey.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem[Tanno et~al.(2019)Tanno, Saeedi, Sankaranarayanan, Alexander, and
  Silberman]{tanno2019learning}
Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel~C Alexander, and
  Nathan Silberman.
\newblock Learning from noisy labels by regularized estimation of annotator
  confusion.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 11244--11253, 2019.

\bibitem[Wang et~al.(2019)Wang, Ma, Chen, Luo, Yi, and
  Bailey]{wang2019symmetric}
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 322--330, 2019.

\bibitem[Wei et~al.(2020)Wei, Feng, Chen, and An]{JoCoR}
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo~An.
\newblock Combating noisy labels by agreement: A joint training method with
  co-regularization, 2020.
\newblock URL \url{https://arxiv.org/abs/2003.02752}.

\bibitem[Weinshall and Amir(2020)]{weinshall2020theory}
Daphna Weinshall and Dan Amir.
\newblock Theory of curriculum learning, with convex loss functions.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (222):\penalty0 1--19, 2020.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{clothing}
Tong Xiao, Tian Xia, Yi~Yang, Chang Huang, and Xiaogang Wang.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{2015 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 2691--2699, 2015.
\newblock \doi{10.1109/CVPR.2015.7298885}.

\bibitem[Xu et~al.(2019)Xu, Cao, Kong, and Wang]{xu2019l_dmi}
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang.
\newblock L\_dmi: A novel information-theoretic loss function for training deep
  nets robust to label noise.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Yao et~al.(2021)Yao, Sun, Zhang, Shen, Wu, Zhang, and Tang]{JoSRC}
Yazhou Yao, Zeren Sun, Chuanyi Zhang, Fumin Shen, Qi~Wu, Jian Zhang, and
  Zhenmin Tang.
\newblock Jo-src: A contrastive approach for combating noisy labels, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.13029}.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Xingrui Yu, Bo~Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{International Conference on Machine Learning}, pages
  7164--7173. PMLR, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017ICLR}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{Int. Conf. Learning Representations ICLR}, 2017.

\bibitem[Zhang and Sabuncu(2018)]{zhang2018generalized}
Zhilu Zhang and Mert Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Zheltonozhskii et~al.(2022)Zheltonozhskii, Baskin, Mendelson,
  Bronstein, and Litany]{zheltonozhskii2022contrast}
Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex~M Bronstein, and
  Or~Litany.
\newblock Contrast to divide: Self-supervised pre-training for learning with
  noisy labels.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pages 1657--1667, 2022.

\end{thebibliography}
