@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst. NeurIPS})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})
@String(ICML = {Int. Conf. Machine Learning ICML})
@String(ICLR = {Int. Conf. Learning Representations ICLR})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})

@inproceedings{hacohen2020let,
  title={Let’s Agree to Agree: Neural Networks Share Classification Order on Real Datasets},
  author={Hacohen, Guy and Choshen, Leshem and Weinshall, Daphna},
  booktitle=ICML,
  pages={3950--3960},
  year={2020},
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  journal={Online},
  year={2009},
  publisher={Citeseer}
}

@article{le2015tiny,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Ya and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

% comparing networks is hard
@inproceedings{li2015convergent,
  title={Convergent learning: Do different neural networks learn the same representations?},
  author={Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John E},
  booktitle=NIPS,
  pages={196--212},
  year={2015}
}

@article{yosinski2015understanding,
  title={Understanding neural networks through deep visualization},
  author={Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  journal={arXiv preprint arXiv:1506.06579},
  year={2015}
}

@article{fukumizu1998effect,
  title={Effect of batch learning in multilayer neural networks},
  author={Fukumizu, Kenji},
  journal={Gen},
  volume={1},
  number={04},
  pages={1E--03},
  year={1998}
}

@inproceedings{arora2018optimization,
  title={On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle=ICML,
  pages={244--253},
  year={2018}
}

@inproceedings{LeiEtAlNIPS19,
  author    = {Lei Wu and Qingcan Wang and Chao Ma},
  editor    = {Hanna M. Wallach andHugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
  title     = {Global Convergence of Gradient Descent for Deep Linear Residual Networks},
  booktitle = NIPS,
  pages     = {13368--13377},
  year      = {2019},
  }
  
  @inproceedings{du2019width,
  title={Width provably matters in optimization for deep linear neural networks},
  author={Du, Simon and Hu, Wei},
  booktitle=ICML,
  pages={1655--1664},
  year={2019},
}

@inproceedings{YunKMICLR21,
  author    = {Chulhee Yun and Shankar Krishnan and Hossein Mobahi},
  title     = {A unifying view on implicit bias in training linear neural networks},
  booktitle = ICLR,
  year      = {2021},
}

@inproceedings{KruegerBJAKMBFC17,
  author    = {David Krueger and Nicolas Ballas and Stanislaw Devansh Arpit and Maxinder S. Kanwal and Tegan Maharaj and Emmanuel Bengio and Asja Fischer and Aaron C. Courville},
  title     = {Deep Nets Don't Learn via Memorization},
  booktitle = ICLR,
  year      = {2017},
}

@article{hacohen2021principal,
  title={Principal components bias in deep neural networks},
  author={Hacohen, Guy and Weinshall, Daphna},
  journal={arXiv preprint arXiv:2105.05553},
  year={2021}
}

@article{flexmatch2021,
  author    = {Bowen Zhang and Yidong Wang and Wenxin Hou and Hao Wu and Jindong Wang and Manabu Okumura and Takahiro Shinozaki},
  title     = {FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo
               Labeling},
  journal   = {CoRR},
  volume    = {abs/2110.08263},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2110.08263},
}

@inproceedings{arazo2019unsupervised,
  title={Unsupervised label noise modeling and loss correction},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and O’Connor, Noel and McGuinness, Kevin},
  booktitle={International conference on machine learning},
  pages={312--321},
  year={2019},
  organization={PMLR}
}

@inproceedings{chen2019understanding,
  title={Understanding and utilizing deep neural networks trained with noisy labels},
  author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
  booktitle={International Conference on Machine Learning},
  pages={1062--1070},
  year={2019},
  organization={PMLR}
}

@article{pleiss2020identifying,
  title={Identifying mislabeled data using the area under the margin ranking},
  author={Pleiss, Geoff and Zhang, Tianyi and Elenberg, Ethan and Weinberger, Kilian Q},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17044--17056},
  year={2020}
}

@article{reed2014training,
  title={Training deep neural networks on noisy labels with bootstrapping},
  author={Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
  journal={arXiv preprint arXiv:1412.6596},
  year={2014}
}

@inproceedings{ma2018dimensionality,
  title={Dimensionality-driven learning with noisy labels},
  author={Ma, Xingjun and Wang, Yisen and Houle, Michael E and Zhou, Shuo and Erfani, Sarah and Xia, Shutao and Wijewickrema, Sudanthi and Bailey, James},
  booktitle={International Conference on Machine Learning},
  pages={3355--3364},
  year={2018},
  organization={PMLR}
}

@inproceedings{jiang2018mentornet,
  title={Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels},
  author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International Conference on Machine Learning},
  pages={2304--2313},
  year={2018},
  organization={PMLR}
}

@article{han2018co,
  title={Co-teaching: Robust training of deep neural networks with extremely noisy labels},
  author={Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{yu2019does,
  title={How does disagreement help generalization against label corruption?},
  author={Yu, Xingrui and Han, Bo and Yao, Jiangchao and Niu, Gang and Tsang, Ivor and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={7164--7173},
  year={2019},
  organization={PMLR}
}

@inproceedings{li2019learning,
  title={Learning to learn from noisy labeled data},
  author={Li, Junnan and Wong, Yongkang and Zhao, Qi and Kankanhalli, Mohan S},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5051--5059},
  year={2019}
}

@article{li2020dividemix,
  title={Dividemix: Learning with noisy labels as semi-supervised learning},
  author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2002.07394},
  year={2020}
}
@article{liu2020early,
  title={Early-learning regularization prevents memorization of noisy labels},
  author={Liu, Sheng and Niles-Weed, Jonathan and Razavian, Narges and Fernandez-Granda, Carlos},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20331--20342},
  year={2020}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International conference on machine learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{iandola2014densenet,
  title={Densenet: Implementing efficient convnet descriptor pyramids},
  author={Iandola, Forrest and Moskewicz, Matt and Karayev, Sergey and Girshick, Ross and Darrell, Trevor and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1404.1869},
  year={2014}
}

@article{li2017webvision,
  title={Webvision database: Visual learning and understanding from web data},
  author={Li, Wen and Wang, Limin and Li, Wei and Agustsson, Eirikur and Van Gool, Luc},
  journal={arXiv preprint arXiv:1708.02862},
  year={2017}
}

@inproceedings{zheltonozhskii2022contrast,
  title={Contrast to divide: Self-supervised pre-training for learning with noisy labels},
  author={Zheltonozhskii, Evgenii and Baskin, Chaim and Mendelson, Avi and Bronstein, Alex M and Litany, Or},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1657--1667},
  year={2022}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}

@article{neal2018modern,
  title={A modern take on the bias-variance tradeoff in neural networks},
  author={Neal, Brady and Mittal, Sarthak and Baratin, Aristide and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:1810.08591},
  year={2018}
}


@software{Taskesen_findpeaks_is_for_2020,
author = {Taskesen, Erdogan},
license = {MIT},
month = {10},
title = {{findpeaks is for the detection of peaks and valleys in a 1D vector and 2D array (image).}},
url = {https://erdogant.github.io/findpeaks},
version = {2.3.1},
year = {2020}
}

@inproceedings{patrini2017making,
  title={Making deep neural networks robust to label noise: A loss correction approach},
  author={Patrini, Giorgio and Rozza, Alessandro and Krishna Menon, Aditya and Nock, Richard and Qu, Lizhen},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1944--1952},
  year={2017}
}

@article{goldberger2016training,
  title={Training deep neural-networks using a noise adaptation layer},
  author={Goldberger, Jacob and Ben-Reuven, Ehud},
  year={2016}
}


@article{malach2017decoupling,
  title={Decoupling" when to update" from" how to update"},
  author={Malach, Eran and Shalev-Shwartz, Shai},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{jenni2018deep,
  title={Deep bilevel learning},
  author={Jenni, Simon and Favaro, Paolo},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={618--633},
  year={2018}
}


@inproceedings{tanno2019learning,
  title={Learning from noisy labels by regularized estimation of annotator confusion},
  author={Tanno, Ryutaro and Saeedi, Ardavan and Sankaranarayanan, Swami and Alexander, Daniel C and Silberman, Nathan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11244--11253},
  year={2019}
}


@inproceedings{wang2019symmetric,
  title={Symmetric cross entropy for robust learning with noisy labels},
  author={Wang, Yisen and Ma, Xingjun and Chen, Zaiyi and Luo, Yuan and Yi, Jinfeng and Bailey, James},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={322--330},
  year={2019}
}

@article{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{ghosh2017robust,
  title={Robust loss functions under label noise for deep neural networks},
  author={Ghosh, Aritra and Kumar, Himanshu and Sastry, P Shanti},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{xu2019l_dmi,
  title={L\_dmi: A novel information-theoretic loss function for training deep nets robust to label noise},
  author={Xu, Yilun and Cao, Peng and Kong, Yuqing and Wang, Yizhou},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{song2022learning,
  title={Learning from noisy labels with deep neural networks: A survey},
  author={Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

% continued lets agree to agree
@article{pliushch2021deep,
  title={When Deep Classifiers Agree: Analyzing Correlations between Learning Order and Image Statistics},
  author={Pliushch, Iuliia and Mundt, Martin and Lupp, Nicolas and Ramesh, Visvanathan},
  journal={arXiv preprint arXiv:2105.08997},
  year={2021}
}

@inproceedings{zhang2017ICLR,
  author    = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  title     = {Understanding deep learning requires rethinking generalization},
  booktitle = ICLR,
  year      = {2017}
}


@article{nguyen2019self,
  title={Self: Learning to filter noisy labels with self-ensembling},
  author={Nguyen, Duc Tam and Mummadi, Chaithanya Kumar and Ngo, Thi Phuong Nhung and Nguyen, Thi Hoai Phuong and Beggel, Laura and Brox, Thomas},
  journal={arXiv preprint arXiv:1910.01842},
  year={2019}
}

@INPROCEEDINGS{clothing,  author={Tong Xiao and Tian Xia and Yi Yang and Chang Huang and Xiaogang Wang},  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Learning from massive noisy labeled data for image classification},   year={2015},  volume={},  number={},  pages={2691-2699},  doi={10.1109/CVPR.2015.7298885}}

@inproceedings{animal10n,
title={{SELFIE}: Refurbishing Unclean Samples for Robust Deep Learning},
author={Song, Hwanjun and Kim, Minseok and Lee, Jae-Gil},
booktitle={ICML},
year={2019}}


@INPROCEEDINGS{o2u,
  author={Huang, Jinchi and Qu, Lie and Jia, Rongfei and Zhao, Binqiang},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={3325-3333},
  doi={10.1109/ICCV.2019.00342}}

@article{lec,
  title={Robust training with ensemble consensus},
  author={Lee, Jisoo and Chung, Sae-Young},
  journal={arXiv preprint arXiv:1910.09792},
  year={2019}
}

@article{SABZEVARI20182374,
title = {A two-stage ensemble method for the detection of class-label noise},
journal = {Neurocomputing},
volume = {275},
pages = {2374-2383},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217317265},
author = {Maryam Sabzevari and Gonzalo Martínez-Muñoz and Alberto Suárez},
keywords = {Noise detection, Ensemble learning, Subsampling, Robust classification, Random forest},
}


@Article{Feng2020Adaptive,
AUTHOR = {Feng, Wei and Quan, Yinghui and Dauphin, Gabriel},
TITLE = {Label Noise Cleaning with an Adaptive Ensemble Method Based on Noise Detection Metric},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {23},
ARTICLE-NUMBER = {6718},
URL = {https://www.mdpi.com/1424-8220/20/23/6718},
ISSN = {1424-8220},
ABSTRACT = {Real-world datasets are often contaminated with label noise; labeling is not a clear-cut process and reliable methods tend to be expensive or time-consuming. Depending on the learning technique used, such label noise is potentially harmful, requiring an increased size of the training set, making the trained model more complex and more prone to overfitting and yielding less accurate prediction. This work proposes a cleaning technique called the ensemble method based on the noise detection metric (ENDM). From the corrupted training set, an ensemble classifier is first learned and used to derive four metrics assessing the likelihood for a sample to be mislabeled. For each metric, three thresholds are set to maximize the classifying performance on a corrupted validation dataset when using three different ensemble classifiers, namely Bagging, AdaBoost and k-nearest neighbor (k-NN). These thresholds are used to identify and then either remove or correct the corrupted samples. The effectiveness of the ENDM is demonstrated in performing the classification of 15 public datasets. A comparative analysis is conducted concerning the homogeneous-ensembles-based majority vote method and consensus vote method, two popular ensemble-based label noise filters.},
DOI = {10.3390/s20236718}
}

@InProceedings{Ying21Destect,
author="Chai, Ying
and Wu, Chengrong
and Zeng, Jianping",
editor="Meng, Hongying
and Lei, Tao
and Li, Maozhen
and Li, Kenli
and Xiong, Ning
and Wang, Lipo",
title="Detect Noisy Label Based on Ensemble Learning",
booktitle="Advances in Natural Computation, Fuzzy Systems and Knowledge Discovery",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="1843--1850",
abstract="The success of machine learning relies on high-quality labeled training data. If there are incorrectly labeled data in the training data, the performance of the best classifier will be greatly reduced in a wide range of classification problems, and noisy tags are also often more harmful than noisy attributes. Unfortunately, large datasets almost contain incorrect or inaccurate labels. This paper proposes a simple and effective method that can identify noisy data in text classification datasets to a certain extent. We analyze the characteristics of the noisy data, and design the new method based on the idea of ensemble learning. The method combines with the majority voting and iterative methods to select the noisy data hidden in the dataset. Under the same conditions, our method can select more noisy data, and perform corresponding evaluations on the recall and precision of noise. The experimental results show that this method is better than some previous methods.",
isbn="978-3-030-70665-4"
}

@INPROCEEDINGS{Moura18Ensemble,
  author={de Moura, Kecia G. and Prudencio, Ricardo B.C. and Cavalcanti, George D.C.},
  booktitle={2018 7th Brazilian Conference on Intelligent Systems (BRACIS)}, 
  title={Ensemble Methods for Label Noise Detection Under the Noisy at Random Model}, 
  year={2018},
  volume={},
  number={},
  pages={474-479},
  doi={10.1109/BRACIS.2018.00088}
}
  
@article{nguyen2020wide,
  title={Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth},
  author={Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
  journal={arXiv preprint arXiv:2010.15327},
  year={2020}
}

@article{baldock2021deep,
  title={Deep learning through the lens of example difficulty},
  author={Baldock, Robert and Maennel, Hartmut and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10876--10889},
  year={2021}
}

@article{weinshall2020theory,
  title={Theory of curriculum learning, with convex loss functions},
  author={Weinshall, Daphna and Amir, Dan},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={222},
  pages={1--19},
  year={2020}
}

@article{MOIT,
  author    = {Diego Ortego and
               Eric Arazo and
               Paul Albert and
               Noel E. O'Connor and
               Kevin McGuinness},
  title     = {Multi-Objective Interpolation Training for Robustness to Label Noise},
  journal   = {CoRR},
  volume    = {abs/2012.04462},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.04462},
  eprinttype = {arXiv},
  eprint    = {2012.04462},
  timestamp = {Wed, 09 Dec 2020 15:29:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-04462.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{UNICON,
  doi = {10.48550/ARXIV.2203.14542},
  
  url = {https://arxiv.org/abs/2203.14542},
  
  author = {Karim, Nazmul and Rizve, Mamshad Nayeem and Rahnavard, Nazanin and Mian, Ajmal and Shah, Mubarak},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{Sel-CL,
  doi = {10.48550/ARXIV.2203.04181},
  
  url = {https://arxiv.org/abs/2203.04181},
  
  author = {Li, Shikun and Xia, Xiaobo and Ge, Shiming and Liu, Tongliang},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Selective-Supervised Contrastive Learning with Noisy Labels},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{JoSRC,
  doi = {10.48550/ARXIV.2103.13029},
  
  url = {https://arxiv.org/abs/2103.13029},
  
  author = {Yao, Yazhou and Sun, Zeren and Zhang, Chuanyi and Shen, Fumin and Wu, Qi and Zhang, Jian and Tang, Zhenmin},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Jo-SRC: A Contrastive Approach for Combating Noisy Labels},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{JoCoR,
  doi = {10.48550/ARXIV.2003.02752},
  
  url = {https://arxiv.org/abs/2003.02752},
  
  author = {Wei, Hongxin and Feng, Lei and Chen, Xiangyu and An, Bo},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Combating noisy labels by agreement: A joint training method with co-regularization},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{huang2019o2u,
  title={O2u-net: A simple noisy label detection approach for deep neural networks},
  author={Huang, Jinchi and Qu, Lie and Jia, Rongfei and Zhao, Binqiang},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={3326--3334},
  year={2019}
}

@inproceedings{song2019selfie,
  title={Selfie: Refurbishing unclean samples for robust deep learning},
  author={Song, Hwanjun and Kim, Minseok and Lee, Jae-Gil},
  booktitle={International Conference on Machine Learning},
  pages={5907--5915},
  year={2019},
  organization={PMLR}
}

