\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[lea()]{leanprover}
Lean theorem prover.
\newblock \url{https://leanprover.github.io/about/}.

\bibitem[Bansal et~al.(2019{\natexlab{a}})Bansal, Loos, Rabe, Szegedy, and
  Wilcox]{bansal2019holist}
Bansal, K., Loos, S., Rabe, M., Szegedy, C., and Wilcox, S.
\newblock Holist: An environment for machine learning of higher order logic
  theorem proving.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  454--463, 2019{\natexlab{a}}.

\bibitem[Bansal et~al.(2019{\natexlab{b}})Bansal, Loos, Rabe, and
  Szegedy]{bansal2019learning}
Bansal, K., Loos, S.~M., Rabe, M.~N., and Szegedy, C.
\newblock Learning to reason in large theories without imitation.
\newblock \emph{arXiv preprint arXiv:1905.10501}, 2019{\natexlab{b}}.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., D{\k{e}}biak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Browne et~al.(2012)Browne, Powley, Whitehouse, Lucas, Cowling,
  Rohlfshagen, Tavener, Perez, Samothrakis, and Colton]{browne2012survey}
Browne, C.~B., Powley, E., Whitehouse, D., Lucas, S.~M., Cowling, P.~I.,
  Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., and Colton, S.
\newblock A survey of monte carlo tree search methods.
\newblock \emph{IEEE Transactions on Computational Intelligence and AI in
  games}, 4\penalty0 (1):\penalty0 1--43, 2012.

\bibitem[Buzzard et~al.(2019)Buzzard, Commelin, and Massot]{perfectoidspaces}
Buzzard, K., Commelin, J., and Massot, P.
\newblock Lean perfectoid spaces.
\newblock \url{https://leanprover-community.github.io/lean-perfectoid-spaces/},
  2019.

\bibitem[Carbone \& Semmes(1996)Carbone and Semmes]{carbone1996cuts}
Carbone, A. and Semmes, S.
\newblock Making proofs without modus ponens: An introduction to the
  combinatorics and complexity of cut elimination.
\newblock \emph{Bulletin of the American Mathematical Society}, 34:\penalty0
  131--159, 1996.

\bibitem[Czechowski et~al.(2021)Czechowski, Odrzyg{\'o}{\'z}d{\'z},
  Zbysi{\'n}ski, Zawalski, Olejnik, Wu, Kucinski, and
  Mi{\l}o{\'s}]{czechowski2021subgoal}
Czechowski, K., Odrzyg{\'o}{\'z}d{\'z}, T., Zbysi{\'n}ski, M., Zawalski, M.,
  Olejnik, K., Wu, Y., Kucinski, L., and Mi{\l}o{\'s}, P.
\newblock Subgoal search for complex reasoning tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[de~Moura et~al.(2015)de~Moura, Kong, Avigad, Van~Doorn, and von
  Raumer]{de2015lean}
de~Moura, L., Kong, S., Avigad, J., Van~Doorn, F., and von Raumer, J.
\newblock The lean theorem prover (system description).
\newblock In \emph{International Conference on Automated Deduction}, pp.\
  378--388. Springer, 2015.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Firoiu et~al.(2021)Firoiu, Aygun, Anand, Ahmed, Glorot, Orseau, Zhang,
  Precup, and Mourad]{firoiu2021training}
Firoiu, V., Aygun, E., Anand, A., Ahmed, Z., Glorot, X., Orseau, L., Zhang, L.,
  Precup, D., and Mourad, S.
\newblock Training a first-order theorem prover from synthetic data.
\newblock \emph{arXiv preprint arXiv:2103.03798}, 2021.

\bibitem[Han et~al.(2021)Han, Rute, Wu, Ayers, and Polu]{han2021proof}
Han, J.~M., Rute, J., Wu, Y., Ayers, E.~W., and Polu, S.
\newblock Proof artifact co-training for theorem proving with language models.
\newblock \emph{arXiv preprint arXiv:2102.06203}, 2021.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang,
  Song, and Steinhardt]{hendrycks2021measuring}
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song,
  D., and Steinhardt, J.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{arXiv preprint arXiv:2103.03874}, 2021.

\bibitem[Huang et~al.(2018)Huang, Dhariwal, Song, and
  Sutskever]{huang2018gamepad}
Huang, D., Dhariwal, P., Song, D., and Sutskever, I.
\newblock Gamepad: A learning environment for theorem proving.
\newblock \emph{arXiv preprint arXiv:1806.00608}, 2018.

\bibitem[Irving et~al.(2016)Irving, Szegedy, Alemi, E{\'e}n, Chollet, and
  Urban]{irving2016deepmath}
Irving, G., Szegedy, C., Alemi, A.~A., E{\'e}n, N., Chollet, F., and Urban, J.
\newblock Deepmath-deep sequence models for premise selection.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2235--2243, 2016.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019style}
Karras, T., Laine, S., and Aila, T.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  4401--4410, 2019.

\bibitem[Lehoczky \& Rusczyk({\natexlab{a}})Lehoczky and Rusczyk]{aopsv1}
Lehoczky, S. and Rusczyk, R.
\newblock \emph{The Art of Problem Solving, Volume 1: the Basics},
  {\natexlab{a}}.
\newblock ISBN:978-0-9773045-6-1.

\bibitem[Lehoczky \& Rusczyk({\natexlab{b}})Lehoczky and Rusczyk]{aopsv2}
Lehoczky, S. and Rusczyk, R.
\newblock \emph{The Art of Problem Solving, Volume 2: and Beyond},
  {\natexlab{b}}.
\newblock ISBN:978-0-9773045-8-5.

\bibitem[Li et~al.(2020)Li, Yu, Wu, and Paulson]{li2020modelling}
Li, W., Yu, L., Wu, Y., and Paulson, L.~C.
\newblock Modelling high-level mathematical reasoning in mechanised declarative
  proofs.
\newblock \emph{arXiv preprint arXiv:2006.09265}, 2020.

\bibitem[Loos et~al.(2017)Loos, Irving, Szegedy, and Kaliszyk]{loos2017deep}
Loos, S., Irving, G., Szegedy, C., and Kaliszyk, C.
\newblock Deep network guided proof search.
\newblock \emph{arXiv preprint arXiv:1701.06972}, 2017.

\bibitem[Megill \& Wheeler(2019)Megill and Wheeler]{megill2019metamath}
Megill, N.~D. and Wheeler, D.~A.
\newblock \emph{Metamath: A Computer Language for Pure Mathematics}, 2019.
\newblock URL \url{http://us.metamath.org/downloads/metamath.pdf}.

\bibitem[Polu \& Sutskever(2020)Polu and Sutskever]{polu2020generative}
Polu, S. and Sutskever, I.
\newblock Generative language modeling for automated theorem proving.
\newblock \emph{arXiv preprint arXiv:2009.03393}, 2020.

\bibitem[Rabe et~al.(2020)Rabe, Lee, Bansal, and Szegedy]{rabe2020language}
Rabe, M.~N., Lee, D., Bansal, K., and Szegedy, C.
\newblock Mathematical reasoning via self-supervised skip-tree training.
\newblock \emph{arXiv preprint arXiv:2006.04757}, 2020.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and
  Sutskever, I.
\newblock Zero-shot text-to-image generation.
\newblock \emph{arXiv preprint arXiv:2102.12092}, 2021.

\bibitem[Scholze(2020)]{liquidtensor}
Scholze, P.
\newblock Liquid tensor experiment.
\newblock
  \url{https://xenaproject.wordpress.com/2020/12/05/liquid-tensor-experiment/},
  2020.

\bibitem[Selsam et~al.(2018)Selsam, Lamm, B{\"u}nz, Liang, de~Moura, and
  Dill]{selsam2018learning}
Selsam, D., Lamm, M., B{\"u}nz, B., Liang, P., de~Moura, L., and Dill, D.~L.
\newblock Learning a sat solver from single-bit supervision.
\newblock \emph{arXiv preprint arXiv:1802.03685}, 2018.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2017mastering}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6105--6114. PMLR, 2019.

\bibitem[Urban \& Jakubův(2020)Urban and Jakubův]{urban2020first}
Urban, J. and Jakubův, J.
\newblock First neural conjecturing datasets and experiments.
\newblock \emph{arXiv preprint arXiv:2005.14664}, 2020.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2017)Wang, Tang, Wang, and Deng]{wang2017premise}
Wang, M., Tang, Y., Wang, J., and Deng, J.
\newblock Premise selection for theorem proving by deep graph embedding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2786--2796, 2017.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao,
  Gao, Macherey, et~al.]{wu2016google}
Wu, Y., Schuster, M., Chen, Z., Le, Q.~V., Norouzi, M., Macherey, W., Krikun,
  M., Cao, Y., Gao, Q., Macherey, K., et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock \emph{arXiv preprint arXiv:1609.08144}, 2016.

\bibitem[Wu et~al.(2020)Wu, Jiang, Ba, and Grosse]{wu2020int}
Wu, Y., Jiang, A.~Q., Ba, J., and Grosse, R.
\newblock Int: An inequality benchmark for evaluating generalization in theorem
  proving.
\newblock \emph{arXiv preprint arXiv:2007.02924}, 2020.

\bibitem[Yang \& Deng(2019)Yang and Deng]{yang2019learning}
Yang, K. and Deng, J.
\newblock Learning to prove theorems via interacting with proof assistants.
\newblock \emph{arXiv preprint arXiv:1905.09381}, 2019.

\bibitem[Zheng et~al.(2021)Zheng, Han, and Polu]{zheng2021minif2f}
Zheng, K., Han, J.~M., and Polu, S.
\newblock Minif2f: a cross-system benchmark for formal olympiad-level
  mathematics.
\newblock \emph{arXiv preprint arXiv:2109.00110}, 2021.

\end{thebibliography}
