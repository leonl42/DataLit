\begin{thebibliography}{33}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Arani et~al.(2019)Arani, Sarfraz, and Zonooz}]{Arani2019ImprovingGA}
E.~Arani, Fahad Sarfraz, and Bahram Zonooz. 2019.
\newblock Improving generalization and robustness with noisy collaboration in
  knowledge distillation.
\newblock \emph{ArXiv}, abs/1910.05057.

\bibitem[{Barikeri et~al.(2021)Barikeri, Lauscher, Vuli'c, and
  Glavas}]{Barikeri2021RedditBiasAR}
Soumya Barikeri, Anne Lauscher, Ivan Vuli'c, and Goran Glavas. 2021.
\newblock Redditbias: A real-world resource for bias evaluation and debiasing
  of conversational language models.
\newblock In \emph{ACL/IJCNLP}.

\bibitem[{Bartoldson et~al.(2020)Bartoldson, Morcos, Barbu, and
  Erlebacher}]{Bartoldson2020TheGT}
Brian Bartoldson, Ari~S. Morcos, Adrian Barbu, and Gordon Erlebacher. 2020.
\newblock The generalization-stability tradeoff in neural network pruning.
\newblock \emph{ArXiv}, abs/1906.03728.

\bibitem[{Cheong(2019)}]{Cheong2019transformersZ}
Robin Cheong. 2019.
\newblock transformers . zip : Compressing transformers with pruning and
  quantization.

\bibitem[{Fan et~al.(2020)Fan, Grave, and Joulin}]{Fan2020ReducingTD}
Angela Fan, Edouard Grave, and Armand Joulin. 2020.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock \emph{ArXiv}, abs/1909.11556.

\bibitem[{Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith}]{Gehman2020RealToxicityPromptsEN}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A. Smith.
  2020.
\newblock Realtoxicityprompts: Evaluating neural toxic degeneration in language
  models.
\newblock \emph{ArXiv}, abs/2009.11462.

\bibitem[{Goldblum et~al.(2020)Goldblum, Fowl, Feizi, and
  Goldstein}]{Goldblum2020AdversariallyRD}
Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Goldstein. 2020.
\newblock Adversarially robust distillation.
\newblock In \emph{AAAI}.

\bibitem[{Gupta and Agrawal(2020)}]{Gupta2020CompressionOD}
Manish Gupta and Puneet Agrawal. 2020.
\newblock Compression of deep learning models for text: A survey.
\newblock \emph{ArXiv}, abs/2008.05221.

\bibitem[{Hanu and {Unitary team}(2020)}]{Detoxify}
Laura Hanu and {Unitary team}. 2020.
\newblock Detoxify.
\newblock Github. https://github.com/unitaryai/detoxify.

\bibitem[{Hendrycks and Dietterich(2019)}]{Hendrycks2019BenchmarkingNN}
Dan Hendrycks and Thomas~G. Dietterich. 2019.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{ArXiv}, abs/1903.12261.

\bibitem[{Hinton et~al.(2015)Hinton, Vinyals, and
  Dean}]{Hinton2015DistillingTK}
Geoffrey~E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{ArXiv}, abs/1503.02531.

\bibitem[{Hooker et~al.(2020)Hooker, Moorosi, Clark, Bengio, and
  Denton}]{Hooker2020CharacterisingBI}
Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily~L. Denton.
  2020.
\newblock Characterising bias in compressed models.
\newblock \emph{ArXiv}, abs/2010.03058.

\bibitem[{Iandola et~al.(2020)Iandola, Shaw, Krishna, and
  Keutzer}]{Iandola2020SqueezeBERTWC}
Forrest~N. Iandola, Albert~Eaton Shaw, Ravi Krishna, and Kurt Keutzer. 2020.
\newblock Squeezebert: What can computer vision teach nlp about efficient
  neural networks?
\newblock \emph{ArXiv}, abs/2006.11316.

\bibitem[{Jiao et~al.(2020)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu}]{Jiao2020TinyBERTDB}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu. 2020.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock \emph{ArXiv}, abs/1909.10351.

\bibitem[{Jord{\~a}o and Pedrini(2021)}]{Jordo2021OnTE}
Artur Jord{\~a}o and H{\'e}lio Pedrini. 2021.
\newblock On the effect of pruning on adversarial robustness.
\newblock \emph{2021 IEEE/CVF International Conference on Computer Vision
  Workshops (ICCVW)}, pages 1--11.

\bibitem[{Joseph et~al.(2020)Joseph, Siddiqui, Bhaskara, Gopalakrishnan,
  Muralidharan, Garland, Ahmed, and Dengel}]{Joseph2020GoingBC}
Vinu Joseph, Shoaib~Ahmed Siddiqui, Aditya Bhaskara, Ganesh Gopalakrishnan,
  Saurav Muralidharan, Michael Garland, Sheraz Ahmed, and Andreas~R. Dengel.
  2020.
\newblock Going beyond classification accuracy metrics in model compression.

\bibitem[{Kaya et~al.(2019)Kaya, Hong, and Dumitras}]{Kaya2019ShallowDeepNU}
Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. 2019.
\newblock Shallow-deep networks: Understanding and mitigating network
  overthinking.
\newblock In \emph{ICML}.

\bibitem[{Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut}]{Lan2020ALBERTAL}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut. 2020.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{ArXiv}, abs/1909.11942.

\bibitem[{Michel et~al.(2019)Michel, Levy, and Neubig}]{Michel2019AreSH}
Paul Michel, Omer Levy, and Graham Neubig. 2019.
\newblock Are sixteen heads really better than one?
\newblock In \emph{NeurIPS}.

\bibitem[{Nadeem et~al.(2021)Nadeem, Bethke, and Reddy}]{Nadeem2021StereoSetMS}
Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.
\newblock Stereoset: Measuring stereotypical bias in pretrained language
  models.
\newblock In \emph{ACL/IJCNLP}.

\bibitem[{Pang et~al.(2021)Pang, Yang, Dong, Su, and Zhu}]{Pang2021BagOT}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. 2021.
\newblock Bag of tricks for adversarial training.
\newblock \emph{ArXiv}, abs/2010.00467.

\bibitem[{Papernot et~al.(2016)Papernot, Mcdaniel, Wu, Jha, and
  Swami}]{Papernot2016DistillationAA}
Nicolas Papernot, Patrick Mcdaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
  2016.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock \emph{2016 IEEE Symposium on Security and Privacy (SP)}, pages
  582--597.

\bibitem[{Prasanna et~al.(2020)Prasanna, Rogers, and
  Rumshisky}]{Prasanna2020WhenBP}
Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020.
\newblock When bert plays the lottery, all tickets are winning.
\newblock \emph{ArXiv}, abs/2005.00561.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{Radford2019LanguageMA}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever. 2019.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Roller et~al.(2021)Roller, Dinan, Goyal, Ju, Williamson, Liu, Xu,
  Ott, Shuster, Smith, Boureau, and Weston}]{Roller2021RecipesFB}
Stephen Roller, Emily Dinan, Naman Goyal, Da~Ju, Mary Williamson, Yinhan Liu,
  Jing Xu, Myle Ott, Kurt Shuster, Eric~Michael Smith, Y.-Lan Boureau, and
  Jason Weston. 2021.
\newblock Recipes for building an open-domain chatbot.
\newblock In \emph{EACL}.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and
  Wolf}]{Sanh2019DistilBERTAD}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{ArXiv}, abs/1910.01108.

\bibitem[{Shejwalkar and Houmansadr(2019)}]{Shejwalkar2019ReconcilingUA}
Virat Shejwalkar and Amir Houmansadr. 2019.
\newblock Reconciling utility and membership privacy via knowledge
  distillation.
\newblock \emph{ArXiv}, abs/1906.06589.

\bibitem[{Sheng et~al.(2019)Sheng, Chang, Natarajan, and Peng}]{Sheng2019TheWW}
Emily Sheng, Kai-Wei Chang, P.~Natarajan, and Nanyun Peng. 2019.
\newblock The woman worked as a babysitter: On biases in language generation.
\newblock \emph{ArXiv}, abs/1909.01326.

\bibitem[{Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov}]{Voita2019AnalyzingMS}
Elena Voita, David Talbot, F.~Moiseev, Rico Sennrich, and Ivan Titov. 2019.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock In \emph{ACL}.

\bibitem[{Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush}]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush. 2020.
\newblock \href {https://www.aclweb.org/anthology/2020.emnlp-demos.6}
  {Transformers: State-of-the-art natural language processing}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online.
  Association for Computational Linguistics.

\bibitem[{Zhang et~al.(2020)Zhang, Sun, Galley, Chen, Brockett, Gao, Gao, Liu,
  and Dolan}]{Zhang2020DIALOGPTL}
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,
  Jianfeng Gao, Jingjing Liu, and William~B. Dolan. 2020.
\newblock Dialogpt : Large-scale generative pre-training for conversational
  response generation.
\newblock In \emph{ACL}.

\bibitem[{Zhao and Caragea(2021)}]{Zhao2021KnowledgeDW}
Chenye Zhao and Cornelia Caragea. 2021.
\newblock Knowledge distillation with bert for image tag-based privacy
  prediction.
\newblock In \emph{RANLP}.

\bibitem[{Zhao et~al.(2018)Zhao, Wang, Yatskar, Ordonez, and
  Chang}]{Zhao2018GenderBI}
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang.
  2018.
\newblock Gender bias in coreference resolution: Evaluation and debiasing
  methods.
\newblock In \emph{NAACL}.

\end{thebibliography}
