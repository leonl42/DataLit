
\noindent \textbf{Fairness Evaluation} \quad We will add more experiment to verify our observation about bias reduction. The current evaluation dataset is too small, and the method may also be a bit naive. We would consider using bias regard metric \cite{Sheng2019TheWW} and try measuring bias in word embedding;\\

\noindent \textbf{Compression} \quad We will investigate the effectiveness of the current technique of pruning attention heads, and maybe try weights pruning and structured dropout. The toxicity and bias experiments for pruning methods will also be added. Moreover, the training time has become a large constraint given the large GPT2 model size; we will try to design experiments on smaller architectures in the future.\\

 \noindent \textbf{Theoretical explanation} \quad We have thus far unable to verify our hypothesis about the connection between regularization and the observed fairness improvement. However, we believe that if Knowledge Distillation and Pruning can effectively improve model robustness against adversarial attacks\cite{Papernot2016DistillationAA}, it is highly likely that they can also improve LM fairness. The prospect of developing universal fairness techniques for LMs based on compression is very promising. 