
\noindent \textbf{Result for Toxicity Evaluation} \quad In order to make the result exhaustive and convincing, we conducted experiments using two different datasets, with four different sampling strategies. The result is surprisingly coherent and uniform, that toxicity decreases with the intensity of distillation. We first experiment with toxic triggers from RealToxicityPrompts, which is sourced from the training dataset of GPT2. If the full GPT2 model produces more toxicity on this dataset, it could be because that full GPT2 memorizes more toxic training data; this hypothesis is supported by the fact that GPT2 has much longer generation length than the distilled models as in Figure \ref{fig: length}, implying that it remembers more information regard this given prompts. However, if this is the case, the toxicity reduction pattern may not persist under another prompt sampling setting;\\

So, we added experiments using the TCCC datasets, which the GPT2 has never seen. We also experimented with three sampling strategy: toxic, safe, and random. The result is shown in Figure \ref{fig: tox_random_safe}, in which the same toxicity reduction pattern does persist. This result suggests that a simple memorization explanation can't account for this phenomenon, since no models has seen the TCCC data before. We are unable to explain this result now, and will try to give interpretation in the future.\\ 

\noindent \textbf{Result for Bias Evaluation} \quad The experiment for bias reduction is recorded in Figure \ref{fig: stereobias}, but is still short of being conclusive. Gender bias as measured by Winobias dataset clearly decreases with distillation. However, the social bias as measured by the Stereoset have no such pattern and largely stays flat. I think there are three main possibilities. The first one is that any pattern recorded is due to randomness, since the testing dataset is relatively small(1.5k). The second possibility is that Stereoset did not give good reflection of model bias, because models make choices not based on bias, but baed on coherence. Stereoset is not a very rigorous benchmark for bias. The third possibility is that Winobias did not give good measurement of bias, which is quite unlikely, since the only variable in Winobias is the gender pronoun. Winobias results should be much more reliable than that of the Stereoset. \\

% Distill conclusion
\noindent \textbf{In conclusion,} \quad this paper evaluates the effect of model compression techniques (knowledge distillation and pruning) on NLP fairness. The pattern that distillation improves model fairness is being recorded, but we could not very well explain this phenomenon. We do hypothesize that this effect could be connected to regularization and model robustness, but still needs further experiments and theoretical support to verify this connection. 


% Pruning conclusion
%We also choose pruning heads to study the impact of pruning over fairness. We conduct pruning experiments on a GPT-2 model and evaluate the perplexity of a limited number of pruned models. Our experiment shows that our pruned model achieved comparable PPL as the DistillGPT although higher than the pretrained model. Our next step is to continue the pruning experiment by varying the number of heads kept at the end and conduct the fairness study with more pruning results.


