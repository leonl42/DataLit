Neural Language Models, such as GPT and Bert, are fundamental not only in natural language processing (NLP) research but also in reality. Language model based applications has become an integral part of our everyday life, including AI agent Siri, Alexa, typing assistant, and Google search suggestions, etc.

However, the current neural language models come with some serious limitations - long inference time and large model size, which could limit their real-life deployment on edge devices. Model compression techniques thus becomes crucial to address such drawbacks. However, despite its importance, the  model compression's effect on fairness is still under-explored, and to the best of our knowledge, this is the first work to ever examine this issue for NLP models. 

Despite need for practical application, his topic is also intuitively interesting because there are two plausible but contradictory hypotheses regarding the effect of model compression:

(1) Memorization: Large neural models, due to over-parameterization, learns, or memorizes too well the undesired traits of the corpus they trained on. So, as the model gets compressed, it forgets some biased or toxic contents, and becomes less toxic and biased. 

(2) Winner Takes it All: Model compression reinforces heuristic bias, because as model becomes smaller, it tends to simplify the reasoning process, ignore the under-represented features, and rely on a small and highly biased subset of parameters to make predictions, making compressed models more biased.

This paper evaluates model compression's effect on toxicity and social bias. Our experiments shows that Knowledge Distillation causes monotonic reduction of model toxicity, and model bias also seem to follow a trend of reduction as model size decreases with distillation. So, the answer seems to be veering more towards the direction of the first hypothesis about memorization.

Following the intuition from previous studies that model compression could potentially improve generalization\cite{Bartoldson2020TheGT} \cite{Arani2019ImprovingGA} and robustness\cite{Goldblum2020AdversariallyRD}, we wonder if similar factors are at work here, leading to the reduction in bias and toxicity. However, this link is difficult to establish, because we don't know whether overfitting could lead to toxicity and bias increase. But this is a worthwhile question to study, and could lead to the discovery of a universal fairness improvement method for NLP; we will further investigate this connection in our next step.

Our main contributions are as follows:\\
(1) Show empirical evidence that distilled models are less toxic, and maybe less biased.

(2) Extends discussion of "model compression as a regularizer" to generative language models, and deliberate the possible applications of compression to enhance LM robustness to toxicity and bias.
