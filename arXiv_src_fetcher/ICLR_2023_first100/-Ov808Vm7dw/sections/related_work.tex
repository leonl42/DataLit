
\noindent \textbf{Compression} \quad According the survey paper \cite{Gupta2020CompressionOD}, model compression methods for NLP currently include: pruning(\citet{Michel2019AreSH},\citet{Voita2019AnalyzingMS},\citet{Prasanna2020WhenBP}), quantization\cite{Cheong2019transformersZ}, knowledge distillation(\citet{Jiao2020TinyBERTDB},\citet{Iandola2020SqueezeBERTWC}), parameter sharing(\citet{Lan2020ALBERTAL},\citet{Lan2020ALBERTAL}), tensor decomposition and sub-quadratic complexity transformers. \\

\noindent \textbf{Fairness} \quad Google Brain \cite{Hooker2020CharacterisingBI} tries to characterize compression's impact on fairness for vision models. They tests quantization and pruning techniques and argue that though compressed models achieve similar overall error rate, but fairness is compromised because performance of samples with under-represented features is sacrificed after compression. Researchers from University of Utah \cite{Joseph2020GoingBC} proposes adding fairness into the compression objective function for vision tasks. However, to the best of our knowledge, no prior work has been done studying Knowledge Distillation method, nor are there any compression fairness studies on NLP models.\\

\noindent \textbf{Compression as regularization} \quad 
\cite{Fan2020ReducingTD} introduces a compression method for transformers named structured dropout, which is shown to achieve higher performance than distillation and weight pruning. The method assumes that transformer models are over-parametrized and sub-structures of the original model could achieve equivalent performances, plus that smaller networks will enjoy the benefit of regularization. Many studies (\citet{Jordo2021OnTE}, \citet{Bartoldson2020TheGT}) also argue that pruning of Convolutional Neural Networks serves as a way of regularization. \\

\noindent \textbf{Compression for robust learning} \quad
The seminal work of \cite{Papernot2016DistillationAA} introduces Knowledge Distillation as a defense against adversarial perturbations. Following works continue to use Knowledge Distillation to improve generalization \cite{Arani2019ImprovingGA} and robustness (\citet{Goldblum2020AdversariallyRD}). Knowledge Distillation is also used to improve models on privacy protection (\citet{Shejwalkar2019ReconcilingUA}, \citet{Zhao2021KnowledgeDW}). Moreover, pruning can improve model robustness according to the following studies (\citet{Jordo2021OnTE}, \citet{Pang2021BagOT},\citet{Hendrycks2019BenchmarkingNN} ). \cite{Kaya2019ShallowDeepNU} shows that stopping at earlier layers during inference can improve model robustness. The intuition is still that smaller and shallower networks are more robust.\\

\noindent \textbf{Compression for fairness} \quad Our experiments demonstrate monotonic reduction of model toxicity and biases as the model size decreases with distillation. The gold question is whether the regularization and robustness effect of model compression incur the toxicity and bias reduction that we observed in distilled generative language models. If yes, can we also develop techniques to improve NLP fairness using model compression? If not, what is the cause of the monotonic toxicity and bias reduction?
