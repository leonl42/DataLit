<?xml version="1.0" encoding="UTF-8"?>
<?latexml searchpaths="/home/miri/Documents/university/DataLit/arXiv_src_fetcher/ICLR_2023_first100/-Ov808Vm7dw"?>
<!--  %This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended. --><!--  %In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines. --><?latexml class="article" options="11pt"?>
<?latexml package="naacl2021"?>
<?latexml package="times"?>
<?latexml package="latexsym"?>
<?latexml package="xcolor"?>
<?latexml package="fontenc" options="T1"?>
<?latexml package="inputenc" options="utf8"?>
<?latexml package="graphicx"?>
<?latexml package="microtype"?>
<!--  %If␣the␣title␣and␣author␣information␣does␣not␣fit␣in␣the␣area␣allocated,␣uncomment␣the␣following --><!--  %\setlength\titlebox{&lt;dim&gt;} --><!--  %and␣set␣&lt;dim&gt;␣to␣something␣5cm␣or␣larger. --><!--  %Author␣information␣can␣be␣set␣in␣various␣styles: --><!--  %For␣several␣authors␣from␣the␣same␣institution: --><!--  %\author{Author␣1␣\and␣...␣\and␣Author␣n␣\\ --><!--  %Address␣line␣\\␣...␣\\␣Address␣line} --><!--  %if␣the␣names␣do␣not␣fit␣well␣on␣one␣line␣use --><!--  %Author␣1␣\\␣{\bf␣Author␣2}␣\\␣...␣\\␣{\bf␣Author␣n}␣\\ --><!--  %For␣authors␣from␣different␣institutions: --><!--  %\author{Author␣1␣\\␣Address␣line␣\\␣␣...␣\\␣Address␣line --><!--  %\And␣␣...␣\And --><!--  %Author␣n␣\\␣Address␣line␣\\␣...␣\\␣Address␣line} --><!--  %To␣start␣a␣seperate␣‘‘row’’␣of␣authors␣use␣\AND,␣as␣in --><!--  %\author{Author␣1␣\\␣Address␣line␣\\␣␣...␣\\␣Address␣line --><!--  %\AND --><!--  %Author␣2␣\\␣Address␣line␣\\␣...␣\\␣Address␣line␣\And --><!--  %****␣naacl2021.tex␣Line␣50␣**** --><!--  %Author␣3␣\\␣Address␣line␣\\␣...␣\\␣Address␣line} --><?latexml RelaxNGSchema="LaTeXML"?>
<document xmlns="http://dlmf.nist.gov/LaTeXML" class="ltx_authors_1line">
  <resource src="LaTeXML.css" type="text/css"/>
  <resource src="ltx-article.css" type="text/css"/>
  <title>Can Model Compression Improve NLP Fairness?</title>
  <creator role="author">
    <personname>Guangxuan Xu <break/>Department of Computer Science,<break/>University of California, Los Angeles<break/><text font="typewriter">gxu21@cs.ucla.edu</text> <break/><ERROR class="undefined">\And</ERROR>Qingyuan Hu <break/>Department of Computer Science,<break/>University of California, Los Angeles<break/><text font="typewriter">hu528@cs.ucla.edu</text> <break/></personname>
  </creator>
  <abstract name="Abstract">
    <p>Model compression techniques are receiving increasing attention; however, the effect of compression on model fairness is still under explored. This is the first paper to examine the effect of distillation and pruning on the toxicity and bias of generative language models. We test Knowledge Distillation and Pruning methods on the GPT2 model and found a consistent pattern of toxicity and bias reduction after model distillation; this result can be potentially interpreted by existing line of research which describes model compression as a regularization technique; our work not only serves as a reference for safe deployment of compressed models, but also extends the discussion of "compression as regularization" into the setting of neural LMs, and hints at the possibility of using compression to develop fairer models.</p>
  </abstract>
  <section inlist="toc" xml:id="S1">
    <tags>
      <tag>1</tag>
      <tag role="autoref">section 1</tag>
      <tag role="refnum">1</tag>
      <tag role="typerefnum">§1</tag>
    </tags>
    <title><tag close=" ">1</tag>Introduction</title>
    <para xml:id="S1.p1">
      <p>Neural Language Models, such as GPT and Bert, are fundamental not only in natural language processing (NLP) research but also in reality. Language model based applications has become an integral part of our everyday life, including AI agent Siri, Alexa, typing assistant, and Google search suggestions, etc.</p>
    </para>
    <para xml:id="S1.p2">
      <p>However, the current neural language models come with some serious limitations - long inference time and large model size, which could limit their real-life deployment on edge devices. Model compression techniques thus becomes crucial to address such drawbacks. However, despite its importance, the model compression’s effect on fairness is still under-explored, and to the best of our knowledge, this is the first work to ever examine this issue for NLP models.</p>
    </para>
    <para xml:id="S1.p3">
      <p>Despite need for practical application, his topic is also intuitively interesting because there are two plausible but contradictory hypotheses regarding the effect of model compression:</p>
    </para>
    <para xml:id="S1.p4">
      <p>(1) Memorization: Large neural models, due to over-parameterization, learns, or memorizes too well the undesired traits of the corpus they trained on. So, as the model gets compressed, it forgets some biased or toxic contents, and becomes less toxic and biased.</p>
    </para>
    <para xml:id="S1.p5">
      <p>(2) Winner Takes it All: Model compression reinforces heuristic bias, because as model becomes smaller, it tends to simplify the reasoning process, ignore the under-represented features, and rely on a small and highly biased subset of parameters to make predictions, making compressed models more biased.</p>
    </para>
    <para xml:id="S1.p6">
      <p>This paper evaluates model compression’s effect on toxicity and social bias. Our experiments shows that Knowledge Distillation causes monotonic reduction of model toxicity, and model bias also seem to follow a trend of reduction as model size decreases with distillation. So, the answer seems to be veering more towards the direction of the first hypothesis about memorization.</p>
    </para>
    <para xml:id="S1.p7">
      <p>Following the intuition from previous studies that model compression could potentially improve generalization<cite class="ltx_citemacro_cite"><bibref bibrefs="Bartoldson2020TheGT" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> <cite class="ltx_citemacro_cite"><bibref bibrefs="Arani2019ImprovingGA" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> and robustness<cite class="ltx_citemacro_cite"><bibref bibrefs="Goldblum2020AdversariallyRD" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>, we wonder if similar factors are at work here, leading to the reduction in bias and toxicity. However, this link is difficult to establish, because we don’t know whether overfitting could lead to toxicity and bias increase. But this is a worthwhile question to study, and could lead to the discovery of a universal fairness improvement method for NLP; we will further investigate this connection in our next step.</p>
    </para>
    <para xml:id="S1.p8">
      <p>Our main contributions are as follows:<break/>(1) Show empirical evidence that distilled models are less toxic, and maybe less biased.</p>
    </para>
    <para xml:id="S1.p9">
      <p>(2) Extends discussion of "model compression as a regularizer" to generative language models, and deliberate the possible applications of compression to enhance LM robustness to toxicity and bias.</p>
    </para>
  </section>
  <section inlist="toc" xml:id="S2">
    <tags>
      <tag>2</tag>
      <tag role="autoref">section 2</tag>
      <tag role="refnum">2</tag>
      <tag role="typerefnum">§2</tag>
    </tags>
    <title><tag close=" ">2</tag>Related Work</title>
    <para class="ltx_noindent" xml:id="S2.p1">
      <p><text font="bold">Compression</text>  According the survey paper <cite class="ltx_citemacro_cite"><bibref bibrefs="Gupta2020CompressionOD" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>, model compression methods for NLP currently include: pruning(<cite class="ltx_citemacro_citet"><bibref bibrefs="Michel2019AreSH" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>,<cite class="ltx_citemacro_citet"><bibref bibrefs="Voita2019AnalyzingMS" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>,<cite class="ltx_citemacro_citet"><bibref bibrefs="Prasanna2020WhenBP" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>), quantization<cite class="ltx_citemacro_cite"><bibref bibrefs="Cheong2019transformersZ" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>, knowledge distillation(<cite class="ltx_citemacro_citet"><bibref bibrefs="Jiao2020TinyBERTDB" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>,<cite class="ltx_citemacro_citet"><bibref bibrefs="Iandola2020SqueezeBERTWC" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>), parameter sharing(<cite class="ltx_citemacro_citet"><bibref bibrefs="Lan2020ALBERTAL" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>,<cite class="ltx_citemacro_citet"><bibref bibrefs="Lan2020ALBERTAL" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>), tensor decomposition and sub-quadratic complexity transformers. <break/></p>
    </para>
    <para class="ltx_noindent" xml:id="S2.p2">
      <p><text font="bold">Fairness</text>  Google Brain <cite class="ltx_citemacro_cite"><bibref bibrefs="Hooker2020CharacterisingBI" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> tries to characterize compression’s impact on fairness for vision models. They tests quantization and pruning techniques and argue that though compressed models achieve similar overall error rate, but fairness is compromised because performance of samples with under-represented features is sacrificed after compression. Researchers from University of Utah <cite class="ltx_citemacro_cite"><bibref bibrefs="Joseph2020GoingBC" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> proposes adding fairness into the compression objective function for vision tasks. However, to the best of our knowledge, no prior work has been done studying Knowledge Distillation method, nor are there any compression fairness studies on NLP models.<break/></p>
    </para>
    <para class="ltx_noindent" xml:id="S2.p3">
      <p><text font="bold">Compression as regularization</text>  <cite class="ltx_citemacro_cite"><bibref bibrefs="Fan2020ReducingTD" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> introduces a compression method for transformers named structured dropout, which is shown to achieve higher performance than distillation and weight pruning. The method assumes that transformer models are over-parametrized and sub-structures of the original model could achieve equivalent performances, plus that smaller networks will enjoy the benefit of regularization. Many studies (<cite class="ltx_citemacro_citet"><bibref bibrefs="Jordo2021OnTE" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>, <cite class="ltx_citemacro_citet"><bibref bibrefs="Bartoldson2020TheGT" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>) also argue that pruning of Convolutional Neural Networks serves as a way of regularization. <break/></p>
    </para>
    <para class="ltx_noindent" xml:id="S2.p4">
      <p><text font="bold">Compression for robust learning</text>  The seminal work of <cite class="ltx_citemacro_cite"><bibref bibrefs="Papernot2016DistillationAA" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> introduces Knowledge Distillation as a defense against adversarial perturbations. Following works continue to use Knowledge Distillation to improve generalization <cite class="ltx_citemacro_cite"><bibref bibrefs="Arani2019ImprovingGA" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> and robustness (<cite class="ltx_citemacro_citet"><bibref bibrefs="Goldblum2020AdversariallyRD" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>). Knowledge Distillation is also used to improve models on privacy protection (<cite class="ltx_citemacro_citet"><bibref bibrefs="Shejwalkar2019ReconcilingUA" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>, <cite class="ltx_citemacro_citet"><bibref bibrefs="Zhao2021KnowledgeDW" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>). Moreover, pruning can improve model robustness according to the following studies (<cite class="ltx_citemacro_citet"><bibref bibrefs="Jordo2021OnTE" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>, <cite class="ltx_citemacro_citet"><bibref bibrefs="Pang2021BagOT" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>,<cite class="ltx_citemacro_citet"><bibref bibrefs="Hendrycks2019BenchmarkingNN" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> ). <cite class="ltx_citemacro_cite"><bibref bibrefs="Kaya2019ShallowDeepNU" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> shows that stopping at earlier layers during inference can improve model robustness. The intuition is still that smaller and shallower networks are more robust.<break/></p>
    </para>
    <para class="ltx_noindent" xml:id="S2.p5">
      <p><text font="bold">Compression for fairness</text>  Our experiments demonstrate monotonic reduction of model toxicity and biases as the model size decreases with distillation. The gold question is whether the regularization and robustness effect of model compression incur the toxicity and bias reduction that we observed in distilled generative language models. If yes, can we also develop techniques to improve NLP fairness using model compression? If not, what is the cause of the monotonic toxicity and bias reduction?</p>
    </para>
  </section>
  <section inlist="toc" xml:id="S3">
    <tags>
      <tag>3</tag>
      <tag role="autoref">section 3</tag>
      <tag role="refnum">3</tag>
      <tag role="typerefnum">§3</tag>
    </tags>
    <title><tag close=" ">3</tag>Approach</title>
    <subsection inlist="toc" xml:id="S3.SS1">
      <tags>
        <tag>3.1</tag>
        <tag role="autoref">subsection 3.1</tag>
        <tag role="refnum">3.1</tag>
        <tag role="typerefnum">§3.1</tag>
      </tags>
      <title><tag close=" ">3.1</tag>Compression Techniques</title>
      <para xml:id="S3.SS1.p1">
        <p>We conduct experiments using two methods: Knowledge Distillation and Pruning. We choose Knowledge Distillation because it’s popular in NLP: Distill-Bert<cite class="ltx_citemacro_cite"><bibref bibrefs="Sanh2019DistilBERTAD" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, Distill-GPT, Distilled Blenderbot, and distil-T5 are all distilled generative models publicly available to download and deploy on Hugginface.co <cite class="ltx_citemacro_cite"><bibref bibrefs="wolf-etal-2020-transformers" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>; those distilled models could achieve testing performance on par with original models, while cutting inference time and model size by more than half; we also test on pruning because it’s the most commonly used compression techniques, and there are also works that prunes NLP models(Insert more about pruning here);<break/></p>
      </para>
      <para class="ltx_noindent" xml:id="S3.SS1.p2">
        <p><text font="bold">Knowledge Distillation</text>  Standard training objective minimizes the cross-entropy between the model’s predicted distribution and the one-hot empirical distribution of training labels. In Distillation, rather than training with one-hot encoding, we train with the soft targets (probabilities of the teacher). In practice, following <cite class="ltx_citemacro_cite"><bibref bibrefs="Hinton2015DistillingTK" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, we used a softmax-temperature to smoothen the soft target. A trick is to initialize student with teacher’s weight, and our experiments confirms that random initialization scores much lower performance compared with using pretrained weights. For intuition, you can view Knowledge Distillation as fine-tuning with a truncated architecture, based on soft-targets from a teacher model rather than one-hot labels. <break/></p>
      </para>
      <figure inlist="lof" labels="LABEL:fig:_distill" placement="ht" xml:id="S3.F1">
        <tags>
          <tag>Figure 1</tag>
          <tag role="autoref">Figure 1</tag>
          <tag role="refnum">1</tag>
          <tag role="typerefnum">Figure 1</tag>
        </tags>
        <graphics candidates="graphs/white_distill.png" graphic="graphs/white_distill.png" options="width=213.39566pt,height=128.0374pt" xml:id="S3.F1.g1"/>
        <toccaption><tag close=" ">1</tag>Different classes of distilled models</toccaption>
        <caption><tag close=": ">Figure 1</tag>Different classes of distilled models</caption>
      </figure>
      <para class="ltx_noindent" xml:id="S3.SS1.p3">
        <p><text font="bold">Pruning</text>  Pruning removes elements of a network to reduce the model size and increase inference speed. Researchers have proposed different methods to prune weights, neurons, blocks, as well as head and layers. Research shows that pruning head and layers without finetuning the models can still achieve limited performance loss. Thus, to kick off our pruning experiments, we started with pruning attention heads of GPT-2 <cite class="ltx_citemacro_cite"><bibref bibrefs="Michel2019AreSH" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>. We first sort the attention heads in all layers by a proxy importance score, then mask the heads with the lowest importance score. We can also iterate this process until the loss reaches a threshold.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="S3.SS2">
      <tags>
        <tag>3.2</tag>
        <tag role="autoref">subsection 3.2</tag>
        <tag role="refnum">3.2</tag>
        <tag role="typerefnum">§3.2</tag>
      </tags>
      <title><tag close=" ">3.2</tag>Retraining Compressed Models</title>
      <para class="ltx_noindent" xml:id="S3.SS2.p1">
        <p><text font="bold">Distilled Models</text>  We retrained 6 distilled GPT2 language models, with different parameters sizes or training strategy. GPT-2 <cite class="ltx_citemacro_cite"><bibref bibrefs="Radford2019LanguageMA" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> is a unidirectional, transformer-based model to predict the next word in a sentence. GPT2’s architecture can be characterized as a stack of decoding blocks, each of which is made of self-attention mask and a fully connected 2-layer neural network.Knowledge Distillation cuts the number of decoding blocks of the model, and result in smaller models as illustrated in the Figure <ref labelref="LABEL:fig:_distill"/>.</p>
      </para>
      <para xml:id="S3.SS2.p2">
        <p>If we ignore the positional embedding(relatively small), the total parameters in a model is given by the sum of the word-embedding size and number-of-blocks * 7M; by this formula, the smallest A-class model is only half the size of GPT, and its inference time only 1/3 of the original GPT2.<break/></p>
      </para>
      <para class="ltx_noindent" xml:id="S3.SS2.p3">
        <p><text font="bold">Pruned Models</text><!--  %The␣pretrained␣model␣has␣a␣total␣number␣of␣1.24e+08␣parameters,␣after␣pruning,␣2.2␣percent␣of␣parameters␣have␣been␣removed. -->  We load a pretrained GPT-2 model from HuggingFace and pruned the attention heads based on different data subsets. In our preliminary experiment, we conducted 3 iterations of pruning and kept 85 percent of the heads, namely reduced the number of heads from 144 to 122. In general, pruning GPT-2 models takes significant time especially on large datasets. In this paper, we report the preliminary results by varying the size of the datasets. We will continue the pruning experiment with more conditions in the future.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="S3.SS3">
      <tags>
        <tag>3.3</tag>
        <tag role="autoref">subsection 3.3</tag>
        <tag role="refnum">3.3</tag>
        <tag role="typerefnum">§3.3</tag>
      </tags>
      <title><tag close=" ">3.3</tag>Fairness Evaluation</title>
      <para xml:id="S3.SS3.p1">
        <p>We run experiments to evaluate the toxicity and bias level of the pruned models; the scope of our experiments goes beyond model we retrained ourselves, but also includes some open source pre-trained models(Blenderbot<cite class="ltx_citemacro_cite"><bibref bibrefs="Roller2021RecipesFB" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> and Dialogpt <cite class="ltx_citemacro_cite"><bibref bibrefs="Zhang2020DIALOGPTL" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>) available on Hugginface.co. (Pruned models isn’t evaluated yet due to time constraints.)<break/></p>
      </para>
      <para class="ltx_noindent" xml:id="S3.SS3.p2">
        <p><text font="bold">Toxicity Evaluation</text>  For toxicity evaluation, we use the benchmark RealToxicityPrompts <cite class="ltx_citemacro_cite"><bibref bibrefs="Gehman2020RealToxicityPromptsEN" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> dataset and Toxic Comment Classification Challenge(TCCC) dataset by Google Jigsaw. RealToxicityPrompts’s data are sourced from the OpenWebText dataset, which is the training dataset for the original GPT2 model. The TCCC dataset is sourced from the comments pages of Wikipedia. RealToxicityPrompts contain half-sentence prompts that can be directly used to trigger Language Model generation; we curate a prompts dataset based on TCCC by cutting the number of tokens in each sentence by half. Then, we score model generations using a popular Bert-based toxicity classifier–Detoxify<cite class="ltx_citemacro_cite"><bibref bibrefs="Detoxify" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, which reported high accuracy score in Kaggle Toxic Comment Classification Challenge and Jigsaw Unintended Bias in Toxicity Classification.</p>
      </para>
      <figure inlist="lof" labels="LABEL:fig:_stereo" placement="ht!" xml:id="S3.F2">
        <tags>
          <tag>Figure 2</tag>
          <tag role="autoref">Figure 2</tag>
          <tag role="refnum">2</tag>
          <tag role="typerefnum">Figure 2</tag>
        </tags>
        <graphics candidates="graphs/stereoset.png" graphic="graphs/stereoset.png" options="scale=0.22" xml:id="S3.F2.g1"/>
        <toccaption><tag close=" ">2</tag>Stereoset sample</toccaption>
        <caption><tag close=": ">Figure 2</tag>Stereoset sample</caption>
      </figure>
      <para class="ltx_noindent" xml:id="S3.SS3.p3">
        <p><text font="bold">Bias Evaluation</text>  For Bias Evaluation, we measure social bias using the Stereoset <cite class="ltx_citemacro_cite"><bibref bibrefs="Nadeem2021StereoSetMS" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> dataset, which covers racial, national,
gender, and professional stereotypes; The format of the dataset is given in Figure <ref labelref="LABEL:fig:_stereo"/>; it feeds the three sentences which are either, stereotyped, anti-stereotyped, or unrelated to the LM; if the model prefers more anti-stereotyped sentences, then we could regard it as less biased. This dataset may be not as convincing and rigorous, because it can’t rule out the possibility that LMs chose stereotyped sentences not for bias, but for coherence. We measure gender bias of language models using the WinoBias Dataset <cite class="ltx_citemacro_cite"><bibref bibrefs="Zhao2018GenderBI" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, which is the largest manually labelled gender bias dataset; Still, we evaluate the model by observing whether it prefers biased or anti-bias sentences, which only differ by the gender pronoun. An example of the dataset is given below:</p>
      </para>
      <para class="ltx_noindent" xml:id="S3.SS3.p4">
        <p><text font="bold">Anti-bias:[The farmer] asked the designer what <text color="#FF8000">[she]</text> could do to help. </text></p>
      </para>
      <para class="ltx_noindent" xml:id="S3.SS3.p5">
        <p><text font="bold">Biased: [The farmer] asked the designer what <text color="#FF8000">[he]</text> could do to help.</text></p>
      </para>
    </subsection>
  </section>
  <section inlist="toc" xml:id="S4">
    <tags>
      <tag>4</tag>
      <tag role="autoref">section 4</tag>
      <tag role="refnum">4</tag>
      <tag role="typerefnum">§4</tag>
    </tags>
    <title><tag close=" ">4</tag>Experiments</title>
    <subsection inlist="toc" xml:id="S4.SS1">
      <tags>
        <tag>4.1</tag>
        <tag role="autoref">subsection 4.1</tag>
        <tag role="refnum">4.1</tag>
        <tag role="typerefnum">§4.1</tag>
      </tags>
      <title><tag close=" ">4.1</tag>Model Training Details</title>
      <para class="ltx_noindent" xml:id="S4.SS1.p1">
        <p><text font="bold">Knowledge Distillation</text>  We conduct distillation using the method described in Distill-Bert <cite class="ltx_citemacro_cite"><bibref bibrefs="Sanh2019DistilBERTAD" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>. The six distilled models we trained are named (A, B, <Math mode="inline" tex="B_{not}" text="B _ (n * o * t)" xml:id="S4.SS1.p1.m1">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">B</XMTok>
                <XMApp>
                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                </XMApp>
              </XMApp>
            </XMath>
          </Math>, C, D, <Math mode="inline" tex="D_{not}" text="D _ (n * o * t)" xml:id="S4.SS1.p1.m2">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">D</XMTok>
                <XMApp>
                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                </XMApp>
              </XMApp>
            </XMath>
          </Math>), with respectively (4,6,6,8,10,10) number of decoder blocks, as shown in Figure <ref labelref="LABEL:fig:_distill"/>; among them, <Math mode="inline" tex="B_{not}" text="B _ (n * o * t)" xml:id="S4.SS1.p1.m3">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">B</XMTok>
                <XMApp>
                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                </XMApp>
              </XMApp>
            </XMath>
          </Math> and <Math mode="inline" tex="D_{not}" text="D _ (n * o * t)" xml:id="S4.SS1.p1.m4">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">D</XMTok>
                <XMApp>
                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                </XMApp>
              </XMApp>
            </XMath>
          </Math> did not use pretrained weights as initialization, while all others initialized with truncated weights from the original GPT2 model. The original training dataset of OpenAI GPT2 is the the 48GB OpenWebText. However, training on full 48GB is out of our capacity, so we randomly sampled around 1/100 of the OpenWebText data to train the distilled models. Training takes 3 epochs, and it automatically saves the best performing model; The other training hyper-parameters are the same as the GPT-2 Model in huggingface. The average training time is 6 hours on 2 A100 GPUs, and the smaller the student model, the faster the training. The perplexity of trained models is shown in Figure <ref labelref="LABEL:fig:_ppl"/>.<break/></p>
      </para>
      <figure inlist="lof" labels="LABEL:fig:_ppl" placement="ht" xml:id="S4.F3">
        <tags>
          <tag>Figure 3</tag>
          <tag role="autoref">Figure 3</tag>
          <tag role="refnum">3</tag>
          <tag role="typerefnum">Figure 3</tag>
        </tags>
        <graphics candidates="graphs/distill_ppl.png" graphic="graphs/distill_ppl.png" options="scale=0.4" xml:id="S4.F3.g1"/>
        <toccaption><tag close=" ">3</tag>Perplexity of distilled models</toccaption>
        <caption><tag close=": ">Figure 3</tag>Perplexity of distilled models</caption>
      </figure>
      <para class="ltx_noindent" xml:id="S4.SS1.p2">
        <p><text font="bold">Pruning</text>  Pruning applies the method proposed in <cite class="ltx_citemacro_cite"><bibref bibrefs="Michel2019AreSH" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>. We start with pretrained GPT2 model from HuggingFace which has 144 attention heads in total. In each prune iteration, we sort the heads by a proxy importance score and mask the lowest important 5 percent. And we repeat the pruning 3 times which results in keeping 85 percent of the heads. We choose to use WikiText which contains a reasonable number of data ( 35K). In this experiment, we sample 5 subsets with various sizes from the WikiText and conduct pruning using these subsets, namely 256, 1280, 2560, 5120, and 12800. They all have the same size, and report their perplexity (PPL) score along with the original model in Figure <ref labelref="LABEL:fig:_prune_ppl"/>. As a result of the pruning, we observed 1.2x to 1.5x computation speed improvement. We observe that using a larger dataset as validation can help improve the pruning performance, as expected, since larger datasets results in more representative proxy importance scores.</p>
      </para>
<!--  %As␣illustrated␣in␣Figure␣\ref{fig:␣prune_ppl},␣the␣results␣show␣that,␣the␣PPL␣of␣the␣pruned␣models␣decreases␣as␣the␣size␣of␣the␣data␣increases.␣For␣example,␣PPL␣of␣the␣model␣using␣12800␣data␣samples␣is␣significantly␣smaller␣than␣the␣model␣with␣256␣samples.␣Further␣for␣sample␣sizes␣with␣more␣than␣1280,␣we␣don’t␣observe␣obvious␣decrease␣of␣the␣PPL.␣This␣could␣imply␣that␣a␣small␣subset␣of␣the␣dataset␣(1280␣out␣of␣~35000)␣is␣sufficient␣to␣prune␣a␣model␣without␣loosing␣performance.␣If␣this␣observation␣stands,␣the␣pruning␣can␣be␣significantly␣accelerated.␣Moreover,␣comparing␣with␣the␣pretrained␣model␣which␣scored␣29.9,␣both␣the␣PPL␣of␣pruned␣models␣and␣the␣distilled␣model␣have␣similar␣but␣higher␣values. -->      <figure inlist="lof" labels="LABEL:fig:_prune_ppl" placement="ht" xml:id="S4.F4">
        <tags>
          <tag>Figure 4</tag>
          <tag role="autoref">Figure 4</tag>
          <tag role="refnum">4</tag>
          <tag role="typerefnum">Figure 4</tag>
        </tags>
        <graphics candidates="graphs/prune_ppl.png" graphic="graphs/prune_ppl.png" options="scale=0.48" xml:id="S4.F4.g1"/>
        <toccaption><tag close=" ">4</tag>Perplexity of pruned models</toccaption>
        <caption><tag close=": ">Figure 4</tag>Perplexity of pruned models</caption>
      </figure>
      <figure inlist="lof" labels="LABEL:fig:_tox_random_safe" placement="ht" xml:id="S4.F5">
        <tags>
          <tag>Figure 5</tag>
          <tag role="autoref">Figure 5</tag>
          <tag role="refnum">5</tag>
          <tag role="typerefnum">Figure 5</tag>
        </tags>
        <graphics candidates="graphs/tox_random_safe.png" graphic="graphs/tox_random_safe.png" options="width=433.62pt,keepaspectratio=true" xml:id="S4.F5.g1"/>
        <toccaption><tag close=" ">5</tag>Toxicity tested on TCCC </toccaption>
        <caption><tag close=": ">Figure 5</tag>Toxicity tested on TCCC </caption>
      </figure>
    </subsection>
    <subsection inlist="toc" xml:id="S4.SS2">
      <tags>
        <tag>4.2</tag>
        <tag role="autoref">subsection 4.2</tag>
        <tag role="refnum">4.2</tag>
        <tag role="typerefnum">§4.2</tag>
      </tags>
      <title><tag close=" ">4.2</tag>Toxicity in Model Generation</title>
      <para class="ltx_noindent" xml:id="S4.SS2.p1">
        <p><text font="bold">Experiment Setup</text>  (Note that the following experiments are all conducted on Knowledge Distillation Models, because we do not have enough usable pruned models yet.) Using two prompt datasets, RealToxicityPrompts <cite class="ltx_citemacro_cite"><bibref bibrefs="Gehman2020RealToxicityPromptsEN" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> and the Toxic Comments Classification Challenge(TCCC) , we run generation with different generative models and note their respective toxic count and toxic score; all models uses top-k search = 10, sample=True; as described in the previous section, we use Detoxify classifier to detect toxic examples, counting the total number of samples with toxic probability over 0.5(toxic count), and calculating the accumulated toxic probability of all samples(toxic score). For both datasets, we tried different sampling strategies, and produced the toxicity plot of different models under those settings.<break/></p>
      </para>
      <para class="ltx_noindent" xml:id="S4.SS2.p2">
        <p><text font="bold">RealToxicityPrompts</text>  For this dataset, we sampled 1200 toxicity triggers, which are non-toxic prompts but leads to toxic continuation in the actual data. Each model generates three sentences for each prompt, so, we are evaluating 3600 sentences per model. The toxicity level in the model generation is illustrated in Figure <ref labelref="LABEL:fig:_real"/>. Note that RealToxicityPrompts is sourced from the training dataset of GPT2, which means the GPT2 model has seen those toxic prompts before. If the GPT2 models seems very toxic in this setting, the reason may lie in its memorization of toxic training data: distilled models are heavily regularized and less likely to output the toxic training data than the GPT2 model.<break/></p>
      </para>
      <figure inlist="lof" labels="LABEL:fig:_real" placement="h" xml:id="S4.F6">
        <tags>
          <tag>Figure 6</tag>
          <tag role="autoref">Figure 6</tag>
          <tag role="refnum">6</tag>
          <tag role="typerefnum">Figure 6</tag>
        </tags>
        <graphics candidates="graphs/detoxify2.png" graphic="graphs/detoxify2.png" options="scale=0.6" xml:id="S4.F6.g1"/>
        <toccaption><tag close=" ">6</tag>Real Toxicity Prompts</toccaption>
        <caption><tag close=": ">Figure 6</tag>Real Toxicity Prompts</caption>
      </figure>
      <para class="ltx_noindent" xml:id="S4.SS2.p3">
        <p><text font="bold">TCCC Prompts</text>  Different from the RealToxicityPrompts dataset, GPT2 has never trained on the TCCC dataset. And we adopted three sampling strategies to build three test datasets, each containing 4k samples, named as(tox4k, random4k, safe4k). The tox4k contains prompts cropped from originally toxic sentences; the random4k’s prompts are cropped from randomly sampled sentences in TCCC, in which around 10 percent data is toxic; the safe4k’s prompts are cropped from very safe sentences, with toxic score lower than 0.001. The Figure <ref labelref="LABEL:fig:_tox_random_safe"/> illustrate the toxicity distribution of different models under different sampling settings. To note, toxicity detection is performed on full sentence for random4k and safe4k; but only the generated part is rendered for toxicity detection in tox4k, because otherwise, all samples will be found toxic.<break/></p>
      </para>
      <table inlist="lot" labels="LABEL:fig:_length" placement="hb" xml:id="S4.T1">
        <tags>
          <tag>Table 1</tag>
          <tag role="autoref">Table 1</tag>
          <tag role="refnum">1</tag>
          <tag role="typerefnum">Table 1</tag>
        </tags>
        <tabular class="ltx_guessed_headers" vattach="middle">
          <thead>
            <tr>
              <td border="l r t" thead="column"/>
              <td align="center" border="r t" thead="column">Tox4k</td>
              <td align="center" border="r t" thead="column">Random4k</td>
              <td align="center" border="r t" thead="column">Safe4k</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" border="l r t">prelay4</td>
              <td align="center" border="r t">93.7</td>
              <td align="center" border="r t">110.7</td>
              <td align="center" border="r t">113.3</td>
            </tr>
            <tr>
              <td align="center" border="l r t">prelay6</td>
              <td align="center" border="r t">91.8</td>
              <td align="center" border="r t">108.4</td>
              <td align="center" border="r t">110.9</td>
            </tr>
            <tr>
              <td align="center" border="l r t">prelay8</td>
              <td align="center" border="r t">90.4</td>
              <td align="center" border="r t">107.8</td>
              <td align="center" border="r t">109.6</td>
            </tr>
            <tr>
              <td align="center" border="l r t">prelay10</td>
              <td align="center" border="r t">89.4</td>
              <td align="center" border="r t">107.6</td>
              <td align="center" border="r t">110.1</td>
            </tr>
            <tr>
              <td align="center" border="l r t">distillgpt2</td>
              <td align="center" border="r t">165.3</td>
              <td align="center" border="r t">174.9</td>
              <td align="center" border="r t">175.6</td>
            </tr>
            <tr>
              <td align="center" border="b l r t">gpt2</td>
              <td align="center" border="b r t">185.2</td>
              <td align="center" border="b r t">197.5</td>
              <td align="center" border="b r t">199.0</td>
            </tr>
          </tbody>
        </tabular>
        <toccaption><tag close=" ">1</tag>Average Length of Generation</toccaption>
        <caption><tag close=": ">Table 1</tag>Average Length of Generation</caption>
      </table>
      <para class="ltx_noindent" xml:id="S4.SS2.p4">
        <p><text font="bold">Observation</text>  We observe that toxicity level consistently decrease as the model size becomes smaller, in all 4 depicted sampling settings, and in two different toxicity evaluation dataset.</p>
      </para>
      <figure inlist="lof" labels="LABEL:fig:_stereobias" placement="ht!" xml:id="S4.F7">
        <tags>
          <tag>Figure 7</tag>
          <tag role="autoref">Figure 7</tag>
          <tag role="refnum">7</tag>
          <tag role="typerefnum">Figure 7</tag>
        </tags>
        <graphics candidates="graphs/social_bias_2.png" graphic="graphs/social_bias_2.png" options="scale=0.57" xml:id="S4.F7.g1"/>
<!--  %****␣experiments.tex␣Line␣75␣**** -->        <toccaption><tag close=" ">7</tag>Stereoset Bias</toccaption>
        <caption><tag close=": ">Figure 7</tag>Stereoset Bias</caption>
      </figure>
    </subsection>
    <subsection inlist="toc" xml:id="S4.SS3">
      <tags>
        <tag>4.3</tag>
        <tag role="autoref">subsection 4.3</tag>
        <tag role="refnum">4.3</tag>
        <tag role="typerefnum">§4.3</tag>
      </tags>
      <title><tag close=" ">4.3</tag>Bias in Model Generation</title>
      <para class="ltx_noindent" xml:id="S4.SS3.p1">
        <p><text font="bold">Stereoset</text>  We first test the models on the Stereoset dataset, which was specifically curated by <cite class="ltx_citemacro_cite"><bibref bibrefs="Nadeem2021StereoSetMS" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> to measure social bias in language models. For each model, we plot the total number of times that it chooses anti-stereotype word and unrelated words. The more anti-stereotypes it chose, the less biased it is. And the more unrelated words it choose, the more unreliable it is. The result is shown in Figure <ref labelref="LABEL:fig:_stereobias"/>.<break/></p>
      </para>
      <para class="ltx_noindent" xml:id="S4.SS3.p2">
        <p><text font="bold">Winobias</text>  Winobias dataset has a similar format as Stereoset, and is used to measure gender bias. It contains a total of 1584 samples, and we count the number of times each model chooses anti-bias sentences. For both datasets, the more anti-stereotypes it chose, the less biased it is. The result is shown in Figure <ref labelref="LABEL:fig:_stereobias"/>. Other papers <cite class="ltx_citemacro_cite"><bibref bibrefs="Barikeri2021RedditBiasAR" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> calculate mean perplexity difference between the anti-bias and biased sentences to measure the bias level of language models; however,since there’s a significant inequality in the original perplexity of different models we compare, mean perplexity difference can be directly applied to our setting. <break/></p>
      </para>
      <para class="ltx_noindent" xml:id="S4.SS3.p3">
        <p><text font="bold">Observation</text>  We find that gender bias level consistently decrease as the model size becomes smaller; but distillation’s the effect on Stereoset dataset isn’t obvious. The current experiment may be too weak to give any convincing conclusion now, but there does seem to be a trend of bias reduction after distillation.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="S4.SS4">
      <tags>
        <tag>4.4</tag>
        <tag role="autoref">subsection 4.4</tag>
        <tag role="refnum">4.4</tag>
        <tag role="typerefnum">§4.4</tag>
      </tags>
      <title><tag close=" ">4.4</tag>Ablation Study</title>
      <para xml:id="S4.SS4.p1">
        <p>We conduct an ablation study to mitigate the concern that toxicity reduction is only the result of smaller model size. As in Figure <ref labelref="LABEL:fig:_Ablation"/>, the DialoGPT <cite class="ltx_citemacro_cite"><bibref bibrefs="Zhang2020DIALOGPTL" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> models (small, medium, large) are publicly available pretraiend models on Huggingface, trained from scratch using different architectures, rather than being distilled. The trend of toxicity and bias reduction is not observed in the DialoGPT models. Rather, toxicity decreases is observed in Distilled-400M Blenderbot versus orginal Blenderbot 3B. This study confirms that it’s distillation, rather than simple model size that causes the observed fairness pattern. <break/></p>
      </para>
      <figure inlist="lof" labels="LABEL:fig:_Ablation" placement="ht!" xml:id="S4.F8">
        <tags>
          <tag>Figure 8</tag>
          <tag role="autoref">Figure 8</tag>
          <tag role="refnum">8</tag>
          <tag role="typerefnum">Figure 8</tag>
        </tags>
        <graphics candidates="graphs/bias_toxicity.png" graphic="graphs/bias_toxicity.png" options="scale=0.48" xml:id="S4.F8.g1"/>
        <toccaption><tag close=" ">8</tag>Ablation Study</toccaption>
        <caption><tag close=": ">Figure 8</tag>Ablation Study</caption>
<!--  %****␣experiments.tex␣Line␣100␣**** -->      </figure>
    </subsection>
    <subsection inlist="toc" xml:id="S4.SS5">
      <tags>
        <tag>4.5</tag>
        <tag role="autoref">subsection 4.5</tag>
        <tag role="refnum">4.5</tag>
        <tag role="typerefnum">§4.5</tag>
      </tags>
      <title><tag close=" ">4.5</tag>Perplexity vs Bias <Math mode="inline" tex="/" text="/" xml:id="S4.SS5.m1">
          <XMath>
            <XMTok meaning="divide" role="MULOP">/</XMTok>
          </XMath>
        </Math> Toxicity</title>
      <para xml:id="S4.SS5.p1">
        <p>Figure <ref labelref="LABEL:fig:_ppl_bias_t"/> illustrates a potential weakness of our experiments; that is as the PPL decreases, model toxicity and bias level increases. Then, it can be argued that it’s low perplexity that caused models to seem less biased and less toxic. However, in general, it’s difficult to match heavily compressed model’s PPL with the original model, so this concern couldn’t be easily addressed. What we can do in the future is to narrow down the perplexity differences, and observe if there’s any decrease in bias/toxicity reduction.</p>
      </para>
      <figure inlist="lof" labels="LABEL:fig:_ppl_bias_t" placement="ht!" xml:id="S4.F9">
        <tags>
          <tag>Figure 9</tag>
          <tag role="autoref">Figure 9</tag>
          <tag role="refnum">9</tag>
          <tag role="typerefnum">Figure 9</tag>
        </tags>
        <graphics candidates="graphs/ppl_bias_toxicity.png" graphic="graphs/ppl_bias_toxicity.png" options="scale=0.4" xml:id="S4.F9.g1"/>
        <toccaption><tag close=" ">9</tag>Perplexity vs Bias <Math mode="inline" tex="/" text="/" xml:id="S4.F9.m1">
            <XMath>
              <XMTok meaning="divide" role="MULOP">/</XMTok>
            </XMath>
          </Math> Toxicity</toccaption>
        <caption><tag close=": ">Figure 9</tag>Perplexity vs Bias <Math mode="inline" tex="/" text="/" xml:id="S4.F9.m2">
            <XMath>
              <XMTok meaning="divide" role="MULOP">/</XMTok>
            </XMath>
          </Math> Toxicity</caption>
      </figure>
    </subsection>
  </section>
  <section inlist="toc" xml:id="S5">
    <tags>
      <tag>5</tag>
      <tag role="autoref">section 5</tag>
      <tag role="refnum">5</tag>
      <tag role="typerefnum">§5</tag>
    </tags>
    <title><tag close=" ">5</tag>Discussion and Conclusion</title>
    <para class="ltx_noindent" xml:id="S5.p1">
      <p><text font="bold">Result for Toxicity Evaluation</text>  In order to make the result exhaustive and convincing, we conducted experiments using two different datasets, with four different sampling strategies. The result is surprisingly coherent and uniform, that toxicity decreases with the intensity of distillation. We first experiment with toxic triggers from RealToxicityPrompts, which is sourced from the training dataset of GPT2. If the full GPT2 model produces more toxicity on this dataset, it could be because that full GPT2 memorizes more toxic training data; this hypothesis is supported by the fact that GPT2 has much longer generation length than the distilled models as in Figure <ref labelref="LABEL:fig:_length"/>, implying that it remembers more information regard this given prompts. However, if this is the case, the toxicity reduction pattern may not persist under another prompt sampling setting;<break/></p>
    </para>
    <para xml:id="S5.p2">
      <p>So, we added experiments using the TCCC datasets, which the GPT2 has never seen. We also experimented with three sampling strategy: toxic, safe, and random. The result is shown in Figure <ref labelref="LABEL:fig:_tox_random_safe"/>, in which the same toxicity reduction pattern does persist. This result suggests that a simple memorization explanation can’t account for this phenomenon, since no models has seen the TCCC data before. We are unable to explain this result now, and will try to give interpretation in the future.<break/></p>
    </para>
    <para class="ltx_noindent" xml:id="S5.p3">
      <p><text font="bold">Result for Bias Evaluation</text>  The experiment for bias reduction is recorded in Figure <ref labelref="LABEL:fig:_stereobias"/>, but is still short of being conclusive. Gender bias as measured by Winobias dataset clearly decreases with distillation. However, the social bias as measured by the Stereoset have no such pattern and largely stays flat. I think there are three main possibilities. The first one is that any pattern recorded is due to randomness, since the testing dataset is relatively small(1.5k). The second possibility is that Stereoset did not give good reflection of model bias, because models make choices not based on bias, but baed on coherence. Stereoset is not a very rigorous benchmark for bias. The third possibility is that Winobias did not give good measurement of bias, which is quite unlikely, since the only variable in Winobias is the gender pronoun. Winobias results should be much more reliable than that of the Stereoset. <break/></p>
    </para>
    <para class="ltx_noindent" xml:id="S5.p4">
<!--  %Distill␣conclusion -->      <p><text font="bold">In conclusion,</text>  this paper evaluates the effect of model compression techniques (knowledge distillation and pruning) on NLP fairness. The pattern that distillation improves model fairness is being recorded, but we could not very well explain this phenomenon. We do hypothesize that this effect could be connected to regularization and model robustness, but still needs further experiments and theoretical support to verify this connection.</p>
    </para>
<!--  %Pruning␣conclusion 
     %We␣also␣choose␣pruning␣heads␣to␣study␣the␣impact␣of␣pruning␣over␣fairness.␣We␣conduct␣pruning␣experiments␣on␣a␣GPT-2␣model␣and␣evaluate␣the␣perplexity␣of␣a␣limited␣number␣of␣pruned␣models.␣Our␣experiment␣shows␣that␣our␣pruned␣model␣achieved␣comparable␣PPL␣as␣the␣DistillGPT␣although␣higher␣than␣the␣pretrained␣model.␣Our␣next␣step␣is␣to␣continue␣the␣pruning␣experiment␣by␣varying␣the␣number␣of␣heads␣kept␣at␣the␣end␣and␣conduct␣the␣fairness␣study␣with␣more␣pruning␣results. -->  </section>
  <section inlist="toc" xml:id="S6">
    <tags>
      <tag>6</tag>
      <tag role="autoref">section 6</tag>
      <tag role="refnum">6</tag>
      <tag role="typerefnum">§6</tag>
    </tags>
    <title><tag close=" ">6</tag>Future Work</title>
    <para class="ltx_noindent" xml:id="S6.p1">
      <p><text font="bold">Fairness Evaluation</text>  We will add more experiment to verify our observation about bias reduction. The current evaluation dataset is too small, and the method may also be a bit naive. We would consider using bias regard metric <cite class="ltx_citemacro_cite"><bibref bibrefs="Sheng2019TheWW" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> and try measuring bias in word embedding;<break/></p>
    </para>
    <para class="ltx_noindent" xml:id="S6.p2">
      <p><text font="bold">Compression</text>  We will investigate the effectiveness of the current technique of pruning attention heads, and maybe try weights pruning and structured dropout. The toxicity and bias experiments for pruning methods will also be added. Moreover, the training time has become a large constraint given the large GPT2 model size; we will try to design experiments on smaller architectures in the future.<break/></p>
    </para>
    <para class="ltx_noindent" xml:id="S6.p3">
      <p><text font="bold">Theoretical explanation</text>  We have thus far unable to verify our hypothesis about the connection between regularization and the observed fairness improvement. However, we believe that if Knowledge Distillation and Pruning can effectively improve model robustness against adversarial attacks<cite class="ltx_citemacro_cite"><bibref bibrefs="Papernot2016DistillationAA" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>, it is highly likely that they can also improve LM fairness. The prospect of developing universal fairness techniques for LMs based on compression is very promising.</p>
    </para>
    <pagination role="newpage"/>
  </section>
  <bibliography citestyle="authoryear" files="anthology,custom" xml:id="bib">
    <title>References</title>
  </bibliography>
<!--  %\section{Example␣Appendix} 
     %\input{sections/appendix} --></document>
