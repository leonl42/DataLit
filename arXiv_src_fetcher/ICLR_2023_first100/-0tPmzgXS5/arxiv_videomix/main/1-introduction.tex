\section{Introduction}
\label{section:introduction}
Video action classification models have achieved remarkable performance improvements in recent years. 
The main innovations have stemmed from the introduction of large-scale video dataset like Kinetics~\cite{kinetics} and Sports-1M~\cite{KarpathyCVPR14} and the development of powerful network architectures using 3D convolutional neural networks (3D CNNs). 

As the architecture of video models become deeper and more complex, overfitting and the resulting loss of generalizability become greater concerns. For example, models trained on large-scale video dataset still suffer from the object and scene biases: models rely heavily on specific discriminative objects and scene elements~\cite{sevilla2019only,li2019repair,weinzaepfel2019mimetics}. This has led to sub-optimal generalization performances and the decrease in localization abilities of video action classifiers. Ideally, a model should extract cues for recognition from diverse sources to enhance generalizability and the robustness to missing features.

The above problems are already identified and studied in static image recognition tasks, especially in the context of modern models based on 2D CNN architectures. For 2D image recognition, data augmentation has proven to be effective, while not requiring extra annotation or training time~\cite{singh2017hide,choe2019attention,cutmix}.

In contrast, there is a lack of extensive studies on the data augmentation strategies for video recognition tasks. In this paper, we examine the efficacy of image-domain data augmentation strategies on video data, especially the ones based on feature erasing that are known to improve model robustness and generalizability~\cite{devries2017cutout,zhong2017randomerase}.
In particular, we consider a generalization of CutMix~\cite{cutmix} to video sequences.
We show experiments to decide which axis (spatial or temporal) CutMix needs to be extended for the best performance on video sequences. 
As a result of our analysis, we introduce the \textbf{VideoMix} augmentation strategy. A new training video sample is constructed by cutting and pasting a random video cuboid patch from one video to the other. The ground truth label for this video is a volume-proportional combination of the source video labels.


\input{arxiv_videomix/main/tables/title}

We show the effectiveness of VideoMix with extensive evaluations on various 3D CNN architectures, datasets, and tasks.
Table~\ref{table:intro} summarizes the improvements by VideoMix.
VideoMix consistently improves video classification models without any additional parameter and a significant amount of computation.
