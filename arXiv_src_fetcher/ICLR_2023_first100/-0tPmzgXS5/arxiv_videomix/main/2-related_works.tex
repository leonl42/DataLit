\section{Related Works}
\label{section:related_works}

We briefly discuss the related works of the video classification task and the data augmentation in this section. 

\subsection{Video Classification}

The key difference between video and image classifications is that the former must capture temporal information.
Some prior works~\cite{simonyan2014two,feichtenhofer2016convolutional,carreira2017quo} have extracted temporal cues explicitly (e.g. via optical flow).
With the development of deep learning and large-scale video datasets~\cite{kinetics,sports1M}, 3D convolutional neural networks (3D CNNs)~\cite{tran2015learning_spatiotemporal,hara2018can_spatiotemporal,carreira2017quo,tran2019CSN} and non-local modules~\cite{wang2018non}  
are proposed to learn the temporal cues automatically.
Recently proposed SlowFast network~\cite{feichtenhofer2019slowfast} and CSN~\cite{tran2019CSN} show the state-of-the-art performances on the video classification using 3D CNNs. 
SlowFast proposed a dual-branch architecture to combine a slow path for static spatial features and a fast path for dynamic motion features, and CSN utilizes depth-wise convolution for lightweight 3D CNN architecture. 
While the advances in video classification have been focused on the architectural axis, we explore the orthogonal \textbf{data} axis, which is seldom explored in the context of video recognition tasks. 
We show the effectiveness of VideoMix by conducting experiments on top of the state-of-the-art SlowFast and CSN networks.


\subsection{Data Augmentation}

\paragraph{Data augmentation for image classification.}
There are many augmentation strategies for static image classification tasks. 
Horizontal flipping, random resizing, and cropping have been used in training ImageNet classifiers and are now considered the standard set of augmentation strategies~\cite{Inceptionv3}.
There have been regional dropout methods~\cite{devries2017cutout,zhong2017randomerase} which remove random regions of an image to enhance robustness and generalization.
Other single-image augmentation strategies include RandAugment~\cite{randaugment} and AutoAugment~\cite{autoaugment}. They consider the combination of extensive pixel-level image augmentation types, such as rotation, shear, translation, and color jittering. RandAugment and AutoAugment train classifiers with above operations via random selection and learned policy, respectively.
Augmentation strategies that combine more than one image include Mixup~\cite{zhang2017mixup} and CutMix~\cite{cutmix}. Mixup averages the RGB values of two images and the ground truth labels to create new samples.
CutMix~\cite{cutmix} has improved upon regional dropout by filling in image patches from other images in the dropped-out region, thereby maximizing the pixel efficiency during training.
The labels are mixed among the source images as in Mixup. We discuss and experiment with the above static-image augmentation strategies on video classification tasks in our analysis.

\paragraph{Data augmentation for video classification.}
There do exist a few attempts to apply data augmentation strategies on videos. On the spatial side, the standard single-image augmentation strategies for image classification have been considered: horizontal flipping and random cropping~\cite{wang2015towards}. 
Along the temporal axis, a widely-used augmentation strategy is to randomly sub-sample a shorter video clip from the full sequence.
However, there has been an overall lack of extensive studies on video augmentation methods. This work contributes the first studies on the impact of video augmentation strategies on the generalization, localization, and transfer learning capabilities.