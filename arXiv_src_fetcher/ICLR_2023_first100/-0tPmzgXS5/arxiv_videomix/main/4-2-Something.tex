
\subsection{Something-Something-V2}
\label{section:experiments:something}

\input{arxiv_videomix/main/tables/something}

\paragraph{Dataset.}
Something-Something-V2 dataset~\cite{goyal2017something} contains 169k training and 25k validation videos with 174 action classes.
We evaluate the performances with top-1 and top-5 accuracies. 
Something-Something-V2 is known for the fine-grainedness of actions, the diversity of contexts. It poses new challenges for action recognition not covered by Kinetics~\cite{kinetics}.

\paragraph{Implementation details.}
We use SlowOnly-50 ($8\times8$) and SlowFast-50 ($8\times8$) models.
The models are pre-trained on Kinetics-400 with the standard training strategy, and the final fully-connected layer is replaced with the new one with $174$ output dimensions. 
The entire models are then fine-tuned for the Something-Something-V2 dataset for $40$ epochs with the batch size $64$ and learning rate $0.01$, which is decayed by a factor of 10 after $26$ and $33$ epochs.
Other implementation details are in Appendix~\ref{appendix:somethingv2}. 

\paragraph{Results.}
We investigate how well VideoMix improves the generalizability of action recognition models in the challenging benchmark beyond the Kinetics.
To separate the fine-tuning effects of VideoMix, it is applied only during the fine-tuning stage. The pretrained model is the same as the baseline. 
Table~\ref{table:experiment:something} shows the results. 
We observe that VideoMix improves the top-1 accuracies of SlowOnly-50 ($8\times8$) and SlowFast-50 ($8\times8$) by $+\mathbf{0.9}\%$ and $+\mathbf{0.8}\%$ against the baselines, respectively. 
VideoMix is effective on Something-Something-V2 as well.