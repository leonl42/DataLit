

\subsection{Kinetics Action Recognition}
\label{section:experiments:kinetics}

\paragraph{Dataset.}
Kinetics-400~\cite{kinetics} is a widely used large-scale action recognition benchmark consisting of 240k training videos and 20k validation videos in 400 human action classes.
The performances are evaluated with the top-1 and top-5 accuracies. 
Note that about 10\% of the Kinetics-400 videos are not available on YouTube to be downloaded. It is not possible to reproduce the exact accuracies reported in the original paper; we train the action classifier over the available subset and treat this result as the baseline.

\paragraph{Training and evaluation.}
We follow the original training recipes of the baseline architectures in~\cite{feichtenhofer2019slowfast}.
We train models from scratch using the stochastic gradient descent optimizer for 250 epochs with batch size $64$ and initial learning rate $0.1$, which is decayed by cosine annealing. 
For a training video clip, $64$ consecutive frames are randomly sampled from a video and $T$ frames are sub-sampled with $\tau$ temporal stride as an input for video models. 
For every model, random resize crop and random horizontal flip are applied on training video clips as standard augmentations.
For evaluation, we use $3\times10$ view ensembles as in~\cite{feichtenhofer2019slowfast}, where $10$ clips are uniformly sampled along the temporal dimension from the entire video sequences and $3$ spatial regions are uniformly sampled along the longer side of the frames.


\paragraph{Network architecture.}
We use the SlowOnly, SlowFast~\cite{feichtenhofer2019slowfast}, and interaction preserved CSN (ip-CSN)~\cite{tran2019CSN} to show the impact of VideoMix on the video classification task. 
Every model is based on the ResNet architecture~\cite{resnet}. We denote the specific ResNet type with the suffix ``-(\#\ignorespaces depth)''.
We also denote each video model with frame length $T$ and temporal stride $\tau$ in the trailing bracket $(T\times\tau)$.
For example, SlowOnly-50 ($4\times16$) is based on the ResNet-50 architecture, considers $T=4$ input frames sub-sampled from the original $64$ frames with temporal stride $\tau=16$.
SlowFast-50 ($8\times8$) takes two separate input streams, the slow and fast pathways, each with $8$ and $32$ total number of input frames ($T$) with temporal strides ($\tau$) $8$ and $2$, respectively.


\paragraph{Kinetics-400 results.}

\input{arxiv_videomix/main/tables/kinetics400}

We evaluate VideoMix on Kinetics-400 with SlowOnly and SlowFast networks~\cite{feichtenhofer2019slowfast} as the base network architectures.
SlowFast combines two branches: the slow branch for static spatial features and the fast branch for dynamic motion features.
SlowOnly only has the slow branch that is similar to the ResNet~\cite{resnet} architecture with 3D convolutional kernels.
The experimental results are shown in Table~\ref{table:experiment:kinetics400}.
The accuracies in the table are reproduced results\footnote{The original paper has reported the top-1 accuracies for {SlowOnly-50 ($4 \times 16$), SlowOnly-50 ($8 \times 8$), and SlowFast-50 ($8 \times 8$)} as $72.6$, $74.8$, and $77.0$, respectively. The difference is due to the 10\% unavailable videos in Kinetics-400 and smaller batch sizes due to GPU limitations.}.
We also report the inference cost (GFlops) of a single view (a temporal clip with spatial crop) as well as the number of views for the prediction of a single video.
We observe that VideoMix consistently improves the accuracy of baseline models. 
VideoMix achieves the top-1 accuracy of $72.7\%$, $74.9\%$, and $76.6\%$ for SlowOnly-50 ($4 \times 16$), SlowOnly-50 ($8 \times 8$), and SlowFast-50 ($8 \times 8$) with improvements of $+\mathbf{0.9}\%$, $+\mathbf{1.3}\%$, and $+\mathbf{0.7}\%$, respectively.
We also show that VideoMix-augmented SlowFast recognizer achieves a competitive performance ($76.6\%$) against other methods such as I3D~\cite{carreira2017quo} ($72.1\%$), Two-Stream I3D~\cite{carreira2017quo} ($75.7\%$), and Nonlocal-ResNet50~\cite{wang2018non} ($76.5\%$) which require more computational costs (GFlops) than the SlowFast architecture. 

\paragraph{Mini-Kinetics results.}

\input{arxiv_videomix/main/tables/mini-kinetics}

We also evaluate VideoMix on Mini-Kinetics as shown in Table~\ref{table:experiment:mini-kinetics}.
We observe that VideoMix improves the performance of various baseline architectures: ip-CSN-50~\cite{tran2019CSN}, SlowOnly-50 ($4\times16$), SlowOnly-50 ($8\times8$), and SlowOnly-50 ($8\times8$) with $\mathbf{75.9}\%$ ($\mathbf{+1.1}\%$), $\mathbf{76.0}\%$ ($\mathbf{+1.6}\%$), $\mathbf{79.2}\%$ ($\mathbf{+1.7}\%$) and $\mathbf{81.9}\%$ ($\mathbf{+2.4}\%$), respectively.








\paragraph{Ablation Studies.}

We conduct ablation studies on the Mini-Kinetics dataset.
SlowOnly-34 is used as the running baseline, where the BasicBlock is used as in ResNet-34~\cite{resnet}.
The results are shown in Table~\ref{table:experiment:albation}.

\input{arxiv_videomix/main/tables/ablation}

We first examine the impact of the mixture area hyperparameter $\alpha \in \{0.2,0.5,$ $1.0\text{(ours)},2.0\}$.
VideoMix at various $\alpha$ values exhibits stable performances, uniformly outperforming the vanilla baseline. VideoMix is not sensitive to the hyperparameter $\alpha$. 
When we reduce the chance of applying VideoMix on a minibatch to $prob$=$0.5$ (default is $prob$=$1.0$), the top-1 accuracy drops by $0.6$ percent point. The simple strategy of applying VideoMix on every video in the batch is a better choice.
When we increase the number of mixing videos (``\#videos") more than two, the performance significantly drops, which indicates that two videos are enough for VideoMix and mixing more than two videos may hinder the training convergence.

Temporal VideoMix and Spatio-temporal VideoMix are not as effective as the VideoMix (Spatial VideoMix), with only $+0.4\%$ and $+1.5\%$ boosts over the baseline.
Per-frame VideoMix independently applies VideoMix at every frame, ruining the temporal continuity of the VideoMix operation. It shows an even worse accuracy of 74.8\%, lower than the original model. Temporal continuity is an important ingredient for the video augmentation.

More ablation studies to see the effect of dataset sizes and applying other data augmentation methods together with VideoMix are provided in Appendix~\ref{appendix:more_baseline}.