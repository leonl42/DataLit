\section{Something-Something-V2 Action Recognition}
\label{appendix:somethingv2}
We describe the implementation details for Something-Something-V2 Action Recognition here. 

We train models on Something-Something-V2 dataset for 40 epochs with the batch size 64 and learning rate 0.01.
The learning rate is decayed by a factor of 10 after 26 and 33 epochs. 
Other training details are the same as Kinetics (Section 4.1 of the main paper), except for the standard data augmentation. 

The standard data augmentation of Kinetics experiments is that horizontal flipping, random cropping, and temporal uniform sampling. 
Temporal uniform sampling samples a random clip of the entire sequences with a uniform frame interval. 

For Something-Something-V2, we do not use horizontal flipping augmentation since the action's direction is critical for this dataset (e.g., there is  a `pushing something from left to right' action category). 
Also, we sample frames with temporally perturbed interval instead of temporal uniform sampling. 
In detail, we first split the entire frames with $T$ bins ($T$ is the number of sampled frames), and we select a frame from each bin and aggregate $T$ frames.

For inference, we use 3 spatial crops and single temporal crop.