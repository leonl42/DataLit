\section{THUMOS'14 Weakly Supervised Temporal Action Localization}
\label{appendix:wstal}

We describe the implementation details for THUMOS'14 Weakly Supervised Temporal Action Localization (WSTAL) task. 

We utilize the codebase\footnote{https://github.com/Pilhyeon/BaSNet-pytorch} of \cite{lee2020bas} for I3D~\cite{carreira2017quo} baseline.
We extract I3D~\cite{carreira2017quo} features from training video using this repository\footnote{https://github.com/piergiaj/pytorch-i3d}.
We sample $750$ video segments from a training video.
Each segment has $16$ frames and the segments are not overlapped. 

The input video has RGB frames and also optical flows, and they are separately fed into dual-path I3D network. 
Each segment of RGB and optical flow frames results in 1024-dimensional feature.
Thus the dimension of extracted feature for a video (i.e., $750$ segments) is $750\times1024$ for RGB input, and $750\times2048$ for both using RGB and optical flow input (RGB+flow).
We apply \textbf{Temporal VideoMix} along the temporal dimension (i.e., the first axis of the feature $750\times2048$) on the extracted feature.

To see the effectiveness of VideoMix with more stronger baseline, we conducted VideoMix with W-TALC~\cite{paul2018w} using the official pytorch codebase\footnote{https://github.com/sujoyp/wtalc-pytorch}. 
We follow the original codebase's setting and we apply \textbf{Temporal VideoMix} along the temporal dimension as in the I3D experiment.  

