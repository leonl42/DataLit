@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{li2020fourier,
  title={Fourier Neural Operator for Parametric Partial Differential Equations},
  author={Li, Zongyi and Kovachki, Nikola Borislavov and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{Zwicker2020,
  doi = {10.21105/joss.02158},
  url = {https://doi.org/10.21105/joss.02158},
  year = {2020},
  publisher = {The Open Journal},
  volume = {5},
  number = {48},
  pages = {2158},
  author = {David Zwicker},
  title = {py-pde: A Python package for solving partial differential equations},
  journal = {Journal of Open Source Software}
}

@book{thuerey2021pbdl,
title={Physics-based Deep Learning},
author={Nils Thuerey and Philipp Holl and Maximilian Mueller and Patrick Schnell and Felix Trost and Kiwon Um},
url={https://physicsbaseddeeplearning.org},
year={2021},
publisher={WWW}
}

@inproceedings{holl2020phiflow,
  title={phiflow: A Differentiable PDE Solving Framework for Deep Learning via Physical Simulations},
  author={Holl, Philipp and Koltun, Vladlen and Um, Kiwon and LTCI, Telecom Paris and Paris, IP and Thuerey, Nils},
  booktitle={NeurIPS Workshop},
  year={2020}
}


@article{karniadakis2021physics,
  title={Physics-informed machine learning},
  author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal={Nature Reviews Physics},
  volume={3},
  number={6},
  pages={422--440},
  year={2021},
  publisher={Nature Publishing Group}
}



@article{zhu2018bayesian,
  title={Bayesian deep convolutional encoder--decoder networks for surrogate modeling and uncertainty quantification},
  author={Zhu, Yinhao and Zabaras, Nicholas},
  journal={Journal of Computational Physics},
  volume={366},
  pages={415--447},
  year={2018},
  publisher={Elsevier}
}


@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}


@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational Physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}


@article{lu2019deeponet,
  title={Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1910.03193},
  year={2019}
}

@article{bhattacharya2020model,
author = {Kaushik Bhattacharya and Bamdad Hosseini and Nikola B. Kovachki and Andrew M. Stuart},
title = {Model {Reduction} {And} {Neural} {Networks} {For} {Parametric} {PDEs}},
journal = {The SMAI journal of computational mathematics},
pages = {121--157},
publisher = {Soci\'et\'e de Math\'ematiques Appliqu\'ees et Industrielles},
volume = {7},
year = {2021},
doi = {10.5802/smai-jcm.74},
language = {en},
url = {https://smai-jcm.centre-mersenne.org/articles/10.5802/smai-jcm.74/}
}



@article{nelsen2021random,
  title={The random feature model for input-output maps between banach spaces},
  author={Nelsen, Nicholas H and Stuart, Andrew M},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={5},
  pages={A3212--A3243},
  year={2021},
  publisher={SIAM}
}


@article{li2020neural,
  title={Neural operator: Graph kernel network for partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2003.03485},
  year={2020}
}


@article{patel2021physics,
  title={A physics-informed operator regression framework for extracting data-driven continuum models},
  author={Patel, Ravi G and Trask, Nathaniel A and Wood, Mitchell A and Cyr, Eric C},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={373},
  pages={113500},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{kingma2014adam,
author = {Kingma, Diederick P and Ba, Jimmy},
title = {Adam: A method for stochastic optimization},
booktitle = { International Conference on Learning Representations (ICLR) },
year = {2015}
}


@article{ha2016hypernetworks,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
}

@article{von2019continual,
  title={Continual learning with hypernetworks},
  author={von Oswald, Johannes and Henning, Christian and Sacramento, Jo{\~a}o and Grewe, Benjamin F},
  journal={arXiv preprint arXiv:1906.00695},
  year={2019}
}

@inproceedings{DBLP:conf/iclr/HaDL17,
  author    = {David Ha and
               Andrew M. Dai and
               Quoc V. Le},
  title     = {HyperNetworks},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=rkpACe1lx},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HaDL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
Oswald2020Continual,
title={Continual learning with hypernetworks},
author={Johannes von Oswald and Christian Henning and Benjamin F. Grewe and Jo√£o Sacramento},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgwNerKvB}
}

@inproceedings{rotman2020electric,
  title={Electric analog circuit design with hypernetworks and a differential simulator},
  author={Rotman, Michael and Wolf, Lior},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4157--4161},
  year={2020},
  organization={IEEE}
}
@inproceedings{littwin2019deep,
  title={Deep meta functionals for shape representation},
  author={Littwin, Gidi and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1824--1833},
  year={2019}
}
@inproceedings{suarez2017language,
  title={Language modeling with recurrent highway hypernetworks},
  author={Suarez, Joseph},
  booktitle={Advances in neural information processing systems},
  pages={3267--3276},
  year={2017}
}

@article{nachmani2019hyper,
  title={Hyper-graph-network decoders for block codes},
  author={Nachmani, Eliya and Wolf, Lior},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={2329--2339},
  year={2019}
}

@article{li2020multipole,
  title={Multipole graph neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Stuart, Andrew and Bhattacharya, Kaushik and Anandkumar, Anima},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6755--6766},
  year={2020}
}

@book{frisch1995turbulence,
  title={Turbulence: the legacy of AN Kolmogorov},
  author={Frisch, Uriel},
  year={1995},
  publisher={Cambridge university press}
}

@article{li2021markov,
  title={Markov Neural Operators for Learning Chaotic Systems},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2106.06898},
  year={2021}
}

@article{kovachki2021neural,
  title={Neural operator: Learning maps between function spaces},
  author={Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2108.08481},
  year={2021}
}

@article{LuLu2019DLno,
author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
copyright = {http://creativecommons.org/licenses/by-nc-sa/4.0},
language = {eng},
abstract = {While it is widely known that neural networks are universal approximators of
continuous functions, a less known and perhaps more powerful result is that a
neural network with a single hidden layer can approximate accurately any
nonlinear continuous operator. This universal approximation theorem is
suggestive of the potential application of neural networks in learning
nonlinear operators from data. However, the theorem guarantees only a small
approximation error for a sufficient large network, and does not consider the
important optimization and generalization errors. To realize this theorem in
practice, we propose deep operator networks (DeepONets) to learn operators
accurately and efficiently from a relatively small dataset. A DeepONet consists
of two sub-networks, one for encoding the input function at a fixed number of
sensors $x_i, i=1,\dots,m$ (branch net), and another for encoding the locations
for the output functions (trunk net). We perform systematic simulations for
identifying two types of operators, i.e., dynamic systems and partial
differential equations, and demonstrate that DeepONet significantly reduces the
generalization error compared to the fully-connected networks. We also derive
theoretically the dependence of the approximation error in terms of the number
of sensors (where the input function is defined) as well as the input function
type, and we verify the theorem with computational results. More importantly,
we observe high-order error convergence in our computational tests, namely
polynomial rates (from half order to fourth order) and even exponential
convergence with respect to the training dataset size.},
title = {DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
year = {2019},
}

@article{DBLP:journals/corr/abs-1910-03193,
  author    = {Lu Lu and
               Pengzhan Jin and
               George Em Karniadakis},
  title     = {DeepONet: Learning nonlinear operators for identifying differential
               equations based on the universal approximation theorem of operators},
  journal   = {CoRR},
  volume    = {abs/1910.03193},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.03193},
  eprinttype = {arXiv},
  eprint    = {1910.03193},
  timestamp = {Wed, 11 Dec 2019 00:00:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-03193.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
anandkumar2020neural,
title={Neural Operator: Graph Kernel Network for Partial Differential Equations},
author={Anima Anandkumar and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Nikola Kovachki and Zongyi Li and Burigede Liu and Andrew Stuart},
booktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},
year={2020},
url={https://openreview.net/forum?id=fg2ZFmXFO3}
}

@article{DBLP:journals/corr/abs-2106-06898,
  author    = {Zongyi Li and
               Nikola B. Kovachki and
               Kamyar Azizzadenesheli and
               Burigede Liu and
               Kaushik Bhattacharya and
               Andrew M. Stuart and
               Anima Anandkumar},
  title     = {Markov Neural Operators for Learning Chaotic Systems},
  journal   = {CoRR},
  volume    = {abs/2106.06898},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.06898},
  eprinttype = {arXiv},
  eprint    = {2106.06898},
  timestamp = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-06898.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2108-08481,
  author    = {Nikola B. Kovachki and
               Zongyi Li and
               Burigede Liu and
               Kamyar Azizzadenesheli and
               Kaushik Bhattacharya and
               Andrew M. Stuart and
               Anima Anandkumar},
  title     = {Neural Operator: Learning Maps Between Function Spaces},
  journal   = {CoRR},
  volume    = {abs/2108.08481},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.08481},
  eprinttype = {arXiv},
  eprint    = {2108.08481},
  timestamp = {Mon, 23 Aug 2021 14:07:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-08481.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}