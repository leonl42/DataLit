\newpage
\appendix
\onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem \ref{thm:static_gradient}}

First note from Assumption~\ref{assume:risk_envelope} that
\begin{description}
\item [(i)] Slater's condition holds in the primal optimization problem~\eqref{eq:coherent_as_optimization},
\item [(ii)] $L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$ is convex in $\xi$ and concave in $(\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$.
\end{description}
Thus by the duality result in convex optimization \cite{boyd2009convex}, the above conditions imply strong duality and we have $\rho(Z) = \max_{\xi\geq 0}\min_{\lambda^{\mathcal P},\lambda^{\mathcal I}\geq 0,\lambda^{\mathcal E}} L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})= \min_{\lambda^{\mathcal P},\lambda^{\mathcal I}\geq 0,\lambda^{\mathcal E}}\max_{\xi\geq 0} L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$. From Assumption~\ref{assume:risk_envelope}, one can also see that the family of functions $\{L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\}_{(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\in\reals^{|\Omega|}\times\reals\times\reals^{|\mathcal E|}\times\reals^{|\mathcal I|}}$ is equi-differentiable in $\theta$, $ L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$ is Lipschitz, as a result, an absolutely continuous function in $\theta$, and thus, $\nabla_\theta L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$ is continuous and bounded at each $(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$. Then for every selection of saddle point $(\xi^*_{\theta},\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta}) \in \spset$ of~\eqref{eq:Lagrangian}, using the Envelop theorem for saddle-point problems (see Theorem~4~of \citealt{milgrom2002envelope}), we have
%
\begin{equation}\label{eq:envelope_theorem}
\nabla_\theta\max_{\xi\geq 0}\;\min_{\lambda^{\mathcal P},\lambda^{\mathcal I}\geq 0,\lambda^{\mathcal E}} L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I}) = \nabla_\theta L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\!\!\mid_{(\xi^*_{\theta},\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta})}.
\end{equation}
%
The result follows by writing the gradient in~\eqref{eq:envelope_theorem} explicitly, and using the likelihood-ratio trick:
%
\begin{equation*}
   \sum_{\omega \in \Omega} \!\! \xi(\omega) \dt P_\theta(\omega) Z(\omega)\!-\!\lambda^{\mathcal P}\sum_{\omega \in \Omega}\xi(\omega)\dt P_\theta(\omega) = \sum_{\omega \in \Omega} \!\! \xi(\omega) P(\omega) \dt \log P(\omega) \left(Z(\omega)\!-\!\lambda^{\mathcal P}\right),
\end{equation*}
%
where the last equality is justified by Assumption~\ref{ass:LR_well_behaved}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Gradient Results for Static Mean-Semideviation}\label{sec:MSD_supp}

In this section we consider the mean-semideviation risk measure, defined as follows:
\begin{equation}\label{eq:mean-sd_def}
    \msd(Z) = \Exp{Z} + c\left( \Exp{(Z - \Exp{Z})_{+}^{2}} \right)^{1 / 2},
\end{equation}
Following the derivation in \cite{Shapiro2009}, note that $\left( \Exp{|Z|^{2}} \right)^{1 / 2} = \|Z\|_2$, where $\|\cdot\|_2$ denotes the $L_2$ norm of the space $\mathcal L_2 (\Omega,\mathcal F,P_\theta)$. The norm may also be written as:
\begin{equation*}
    \|Z\|_2 = \sup_{\|\xi\|_2 \leq 1} \dotp{\xi}{Z},
\end{equation*}
and hence
\begin{equation*}
\begin{split}
  \left( \Exp{(Z - \Exp{Z})_{+}^{2}} \right)^{1 / 2} = \sup_{\|\xi\|_2 \leq 1} \dotp{\xi}{(Z - \Exp{Z})_{+}} &= \sup_{\|\xi\|_2 \leq 1, \xi \geq 0} \dotp{\xi}{Z - \Exp{Z}} \\
    &= \sup_{\|\xi\|_2 \leq 1, \xi \geq 0} \dotp{\xi - \Exp{\xi}}{Z}.
\end{split}
\end{equation*}
It follows that Eq. \eqref{eq:coherent_as_optimization} holds with
\begin{equation*}
    \U = \left\{ \xi' \in \cZ^* : \quad \xi' = 1 + c \xi - c \Exp{\xi}, \quad \|\xi\|_q \leq 1, \quad \xi \geq 0  \right\}.
\end{equation*}
For this case it will be more convenient to write Eq. \eqref{eq:coherent_as_optimization} in the following form
\begin{equation}\label{eq:msd_as_optimization}
    \msd(Z) = \sup_{\|\xi\|_q \leq 1, \xi \geq 0} \dotp{1 + c \xi - c \Exp{\xi}}{Z}.
\end{equation}
Let $\bar{\xi}$ denote an optimal solution for \eqref{eq:msd_as_optimization}. In \cite{Shapiro2009} it is shown that $\bar{\xi}$ is a contact point of $(Z - \Exp{Z})_{+}$, that is
\begin{equation*}
    \bar{\xi} \in \arg \max \left\{ \dotp{\xi}{(Z - \Exp{Z})_{+}}: \|\xi\|_2 \leq 1 \right\},
\end{equation*}
and we have that
\begin{equation}\label{eq:bar_xi_p2}
\bar{\xi} = \frac{(Z - \Exp{Z})_{+}}{\|(Z - \Exp{Z})_{+}\|_2} = \frac{(Z - \Exp{Z})_{+}}{\SD(Z)}.
\end{equation}
Note that $\bar{\xi}$ is not necessarily a probability distribution, but for $c\in[0,1]$, it can be shown \cite{Shapiro2009} that $1 + c \bar{\xi} - c \Exp{\bar{\xi}}$ always is.

In the following we show that $\bar{\xi}$ may be used to write the gradient $\dt{\msd(Z)}$ as an expectation, which will lead to a sampling algorithm for the gradient.

\begin{proposition}\label{prop:msd_grad_supp}
    Under Assumption \ref{ass:LR_well_behaved}, we have that
\begin{equation*}
    \dt{\msd(Z)} = \dt{\Exp{Z}} + \frac{c}{\SD(Z)}\Exp{(Z - \Exp{Z})_{+} \left(\dt \log P(\omega) (Z - \Exp{Z}) - \dt{\Exp{Z}}\right)},
\end{equation*}
and, according to the standard likelihood-ratio method,
\begin{equation*}
    \dt{\Exp{Z}} = \Exp{\dt \log P(\omega) Z}.
\end{equation*}
\end{proposition}
\begin{proof}
Note that in Eq. \eqref{eq:msd_as_optimization} the constraints do not depend on $\theta$. Therefore, using the envelope theorem we obtain that
\begin{equation}\label{eq:msd_proof_1}
\begin{split}
  \dt{\rho(Z)} &= \dt{ \dotp{1 + c\bar{\xi} - c\Exp{\bar{\xi}}}{Z} } \\
    &= \dt{ \dotp{1}{Z} } +c \dt{ \dotp{\bar{\xi}}{Z} } - c \dt{ \dotp{\Exp{\bar{\xi}}}{Z} }.
\end{split}
\end{equation}
We now write each of the terms in Eq. \eqref{eq:msd_proof_1} as an expectation.
We start with the following standard likelihood-ratio result:
\begin{equation*}
    \dt{\dotp{1}{Z}} = \dt{\Exp{Z}} = \Exp{\dt \log P(\omega) Z}.
\end{equation*}
Also, we have that
\begin{equation*}
    \dotp{\Exp{\bar{\xi}}}{Z} = \Exp{\bar{\xi}}\Exp{Z},
\end{equation*}
therefore, by the derivative of a product rule:
\begin{equation*}
    \dt{\dotp{\Exp{\bar{\xi}}}{Z}} = \dt{\Exp{\bar{\xi}}}\Exp{Z} + \Exp{\bar{\xi}}\dt{\Exp{Z}}.
\end{equation*}
By the likelihood-ratio trick and Eq. \eqref{eq:bar_xi_p2} we have that
\begin{equation*}
    \dt{\Exp{\bar{\xi}}} = \frac{1}{\SD(Z)} \Exp{\dt \log P(\omega) (Z - \Exp{Z})_{+} }.
\end{equation*}


Also, by the likelihood-ratio trick
\begin{equation*}
\dt{\Exp{\bar{\xi} Z}} = \Exp{\dt \log P(\omega) \bar{\xi} Z}.
\end{equation*}
Plugging these terms back in Eq. \eqref{eq:msd_proof_1}, we have that
\begin{equation*}
\begin{split}
  \dt{\rho(Z)} &= \dt{\Exp{Z}} + c\dt{\Exp{\bar{\xi} Z}} - c\dt{\Exp{\bar{\xi}}}\Exp{Z} -c\Exp{\bar{\xi}}\dt{\Exp{Z}} \\
    &= \dt{\Exp{Z}} + c\Exp{\bar{\xi} \left(\dt \log P(\omega) Z - \dt{\Exp{Z}}\right)} - c\dt{\Exp{\bar{\xi}}}\Exp{Z} \\
    &= \dt{\Exp{Z}} + \frac{c}{\SD(Z)}\Exp{(Z - \Exp{Z})_{+} \left(\dt \log P(\omega) Z - \dt{\Exp{Z}}\right)} - c\dt{\Exp{\bar{\xi}}}\Exp{Z} \\
    &= \dt{\Exp{Z}} + \frac{c}{\SD(Z)}\Exp{(Z - \Exp{Z})_{+} \left(\dt \log P(\omega) (Z - \Exp{Z}) - \dt{\Exp{Z}}\right)}.\\
\end{split}
\end{equation*}
\end{proof}

Proposition \ref{prop:msd_grad} naturally leads to a sampling-based gradient estimation algortihm, which we term \texttt{GMSD} (Gradient of Mean Semi-Deviation). The algorithm is described in Algorithm \ref{alg:Gmsd}.

\begin{algorithm}
\caption{\texttt{GMSD}}\label{alg:Gmsd}
1: \alggiven
\begin{itemize}
\item Risk level $c$
\item An i.i.d. sequence $z_{1},\dots,z_{N} \sim P_\theta$.
\end{itemize}
2: Set
\begin{equation*}
    \widehat{\Exp{Z}} = \avgN z_i.
\end{equation*}
3: Set
\begin{equation*}
    \widehat{\SD(Z)} = \left( \avgN ( z_i - \widehat{\Exp{Z}} )_{+}^{2} \right)^{1 / 2}.
\end{equation*}
4: Set
\begin{equation*}
    \widehat{\dt{\Exp{Z}}} = \avgN \dt \log P(z_i) z_i.
\end{equation*}
5: \algreturn
\begin{equation*}
    \hat{\dt{\rho(Z)}} = \widehat{\dt{\Exp{Z}}} + \frac{c}{\widehat{\SD(Z)}}\avgN (z_i - \widehat{\Exp{Z}})_{+} \left(\dt \log P(z_i) (z_i - \widehat{\Exp{Z}}) - \widehat{\dt{\Exp{Z}}}\right).
\end{equation*}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Consistency Proof}

Let $\left( \Omega_{SAA},\mathcal F_{SAA},P_{SAA} \right)$ denote the probability space of the SAA functions (i.e., the randomness due to sampling).

Let $L_{\theta;N}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$ denote the Lagrangian of the SAA problem
\begin{equation}\label{eq:SAA_Lagrangian}
\begin{split}
L_{\theta;N}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})=&\sum_{\omega \in \Omega} \!\! \xi(\omega) P_{\theta;N}(\omega) Z(\omega)\!-\!\lambda^{\mathcal P}\left(\sum_{\omega \in \Omega}\xi(\omega)P_{\theta;N}(\omega)\!-\!1\!\right)\\
&-\sum_{e\in\mathcal E}\lambda^{\mathcal E}(e) f_e(\xi,P_{\theta;N})-\sum_{i\in\mathcal I}\lambda^{\mathcal I}(i) f_i(\xi,P_{\theta;N}).
\end{split}
\end{equation}
Recall that $\spset \subset \reals^{|\Omega|}\times \reals\times \reals^{|\mathcal E|}\times \reals^{|\mathcal I|}_{+}$ denotes the set of saddle points of the true Lagrangian \eqref{eq:Lagrangian}.
Let $\saaspset\subset \reals^{|\Omega|}\times \reals\times \reals^{|\mathcal E|}\times \reals^{|\mathcal I|}_{+}$ denote the set of SAA Lagrangian \eqref{eq:SAA_Lagrangian} saddle points.

Suppose that there exists a compact set $C \equiv C_\xi \times C_\lambda$, where $C_\xi \subset \reals^{|\Omega|}$ and $C_\lambda \subset \reals\times \reals^{|\mathcal E|}\times \reals^{|\mathcal I|}_{+}$ such that:
\begin{description}
\item[(i)] The set of Lagrangian saddle points $\spset \subset C$ is non-empty and bounded.
\item[(ii)] The functions $f_e(\xi,P_\theta)$ for all $e\in\mathcal E$ and $f_i(\xi,P_\theta)$ for all $i\in\mathcal I$ are finite valued and continuous (in $\xi$) on $C_\xi$.
\item[(iii)] For $N$ large enough the set $\saaspset$ is non-empty and $\saaspset \subset C$ w.p.~1.
\end{description}

Recall from Assumption \ref{assume:risk_envelope} that for each fixed $\xi\in\mathcal B$, both $f_i(\xi,p)$ and $g_e(\xi,p)$ are continuous in $p$. Furthermore, by the S.L.L.N. of Markov chains, for each policy parameter, we have $P_{\theta,N}\rightarrow P_{\theta}$ w.p.~1. From the definition of the Lagrangian function and continuity of constraint functions,  one can easily see that for each $(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\in\reals^{|\Omega|}\times\reals\times\reals^{|\mathcal E|}\times\reals_{+}^{|\mathcal I|}$, $L_{\theta;N}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\rightarrow L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$ w.p.~1. Denote with $\setdist{A}{B}$ the deviation of set $A$ from set $B$, i.e., $\setdist{A}{B}=\sup_{x\in A}\inf_{y\in B}\|x-y\|$.  Further assume that:
\begin{description}
\item[(iv)] If $\xi_N \in \mathcal U(\pemp)$ and $\xi_N$ converges w.p.~1 to a point $\xi$, then $\xi\in\mathcal U(\pprob)$.
\end{description}
According to the discussion in Page 161 of \citet{Shapiro2009}, the Slater condition of Assumption \ref{assume:risk_envelope} guarantees the following condition:
\begin{description}
\item[(v)] For some point $\xi \in \optset$ there exists a sequence $\xi_N \in \mathcal U(\pemp)$ such that $\xi_N \to \xi$ w.p.~1,
\end{description}
and from Theorem 6.6 in \citet{Shapiro2009}, we know that both sets $ \mathcal U(\pemp)$ and $ \mathcal U(P_\theta)$ are convex and compact.
Furthermore, note that we have
\begin{description}
\item[(vi)] The objective function on \eqref{eq:coherent_as_optimization} is linear, finite valued and continuous in $\xi$ on $C_\xi$ (these conditions obviously hold for almost all $\omega\in\Omega$ in the integrand function $\xi(\omega)Z(\omega)$).
\item[(vii)] S.L.L.N. holds point-wise for any $\xi$.
\end{description}
From (i,iv,v,vi,vii), and under the same lines of proof as in Theorem 5.5 of \citet{Shapiro2009}, we have that
\begin{equation}\label{eq:consistency_prf_1}
    \rho_N(Z) \to \rho(Z) \textrm{ w.p. } 1 \textrm{ as } N \to \infty,
\end{equation}
\begin{equation}\label{eq:consistency_prf_2}
    \setdist{\saaoptset}{\optset} \to 0 \textrm{ w.p. } 1 \textrm{ as } N \to \infty,
\end{equation}

In part 1 and part 2 of the following proof, we show, by following similar derivations as in Theorem 5.2, Theorem 5.3 and Theorem 5.4 of \citet{Shapiro2009}, that $L_{\theta;N}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N})\rightarrow L_{\theta}(\xi^*_{\theta},\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta})$ w.p.~1 and $\setdist{\saaspset}{\spset} \to 0 \textrm{ w.p. } 1 \textrm{ as } N \to \infty$. Based on the definition of the deviation of sets, the limit point of any element in $\saaspset$ is also an element in $\spset$.

Assumptions (i) and (iii) imply that we can restrict our attention to the set $C$.

\paragraph{Part 1}
We first show that $L_{\theta;N}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N})$ converges to $L_{\theta}(\xi^*_{\theta},\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta})$ w.p.~1 as $N\to \infty$.


%%%%%%%%%%%%%%%%%%%%%%%%%
For each fixed $(\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\in C_\lambda$, the function $L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$ is convex and continuous in $\xi$.  Together with the point-wise S.L.L.N. property, Theorem 7.49 of \citet{Shapiro2009} implies that $L_{\theta;N}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I}) - L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\overset{e}{\rightarrow}0$, where $\overset{e}{\rightarrow}$ denotes epi-convergence.  Furthermore, since the objective and constraint functions are convex in $\xi$ and are finite valued on $C_\xi$, the set $\text{dom} L_{\theta}(\cdot,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$ has non-empty interior. It follows from Theorem 7.27 of  \citet{Shapiro2009} that epi-convergence of $L_{\theta,N}$ to $L_\theta$ implies uniform convergence on $C_\xi$, i.e., $ \sup_{\xi \in C_\xi} \left| L_{\theta;N}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I}) - L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\right| \leq \epsilon
$.
On the other hand, for each fixed $\xi\in C_\xi$,
the function $L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$ is linear and thus continuous in $(\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})$ and $\text{dom} L_{\theta}(\xi,\cdot,\cdot,\cdot)=\reals\times\reals^{|\mathcal E|}\times\reals^{|\mathcal I|}$ has non-empty interior. It follows from analogous arguments that $ \sup_{(\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I}) \in C_{\lambda}} \left| L_{\theta;N}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I}) - L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\right| \leq \epsilon$. Combining these results implies
 that for any $\epsilon > 0$ and a.e. $\omega_{SAA} \in \Omega_{SAA}$ there is a $N^*(\epsilon,\omega_{SAA})$ such that
\begin{equation}\label{eq:consistency_prf_3}
    \sup_{(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I}) \in C} \left| L_{\theta;N}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I}) - L_{\theta}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\right| \leq \epsilon.
\end{equation}
Now, assume by contradiction that for some $N > N^*(\epsilon,\omega_{SAA})$ we have $\saaLag - \Lag > \epsilon$. Then by definition of the saddle points
\begin{equation*}
\begin{split}
    L_{\theta;N}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta}) &\geq \saaLag  \\ &> \Lag + \epsilon \geq L_{\theta}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta}) + \epsilon,
\end{split}
\end{equation*}
contradicting \eqref{eq:consistency_prf_3}.

Similarly, assuming by contradiction that $ \Lag - \saaLag> \epsilon$ gives
\begin{equation*}
\begin{split}
    L_{\theta}(\xi^*_{\theta},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N}) &\geq \Lag \\ &> \saaLag  + \epsilon \geq L_{\theta;N}(\xi^*_{\theta},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N}) + \epsilon,
\end{split}
\end{equation*}
also contradicting \eqref{eq:consistency_prf_3}.

It follows that $\left| \saaLag - \Lag \right| \leq \epsilon$ for all $N > N^*(\epsilon,\omega_{SAA})$, and therefore
\begin{equation}\label{eq:consistency_prf_L_converges}
  \lim_{N\to \infty} L_{\theta;N}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N}) = L_{\theta}(\xi^*_{\theta},\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta}),
\end{equation}
w.p.~1.

\paragraph{Part 2}
Let us now show that $\setdist{\saaspset}{\spset} \to 0$. We argue by a contradiction. Suppose that $\setdist{\saaspset}{\spset} \nrightarrow 0$. Since $C$ is compact, we can assume that there exists a sequence $(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N}) \in \saaspset$ that converges to a point $(\bar{\xi}^*,\bar{\lambda}^{*,\mathcal P},\bar{\lambda}^{*,\mathcal E},\bar{\lambda}^{*,\mathcal I})\in C$ and
$(\bar{\xi}^*,\bar{\lambda}^{*,\mathcal P},\bar{\lambda}^{*,\mathcal E},\bar{\lambda}^{*,\mathcal I}) \not\in \spset$. However, from \eqref{eq:consistency_prf_2} we must have that $\bar{\xi}^* \in \optset$. Therefore, we must have that
\begin{equation*}
    L_{\theta}(\bar{\xi}^*,\bar{\lambda}^{*,\mathcal P},\bar{\lambda}^{*,\mathcal E},\bar{\lambda}^{*,\mathcal I}) > L_{\theta}(\bar{\xi}^*,\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta}),
\end{equation*}
by definition of the saddle point set.

Now,
\begin{equation}\label{eq:consistency_prf_4}
\begin{split}
    &L_{\theta;N}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N}) - L_{\theta}(\bar{\xi}^*,\bar{\lambda}^{*,\mathcal P},\bar{\lambda}^{*,\mathcal E},\bar{\lambda}^{*,\mathcal I}) \\=& \left[L_{\theta;N}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N}) - L_{\theta}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N})\right] + \\ &+ \left[L_{\theta}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N}) - L_{\theta}(\bar{\xi}^*,\bar{\lambda}^{*,\mathcal P},\bar{\lambda}^{*,\mathcal E},\bar{\lambda}^{*,\mathcal I})\right].
\end{split}
\end{equation}
The first term in the r.h.s. of \eqref{eq:consistency_prf_4} tends to zero, using the argument from  \eqref{eq:consistency_prf_3}, and the second by continuity of $L_{\theta}$ guaranteed by (ii). We thus obtain that $L_{\theta;N}(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N})$ tends to $L_{\theta}(\bar{\xi}^*,\bar{\lambda}^{*,\mathcal P},\bar{\lambda}^{*,\mathcal E},\bar{\lambda}^{*,\mathcal I}) > L_{\theta}(\xi^*_{\theta},\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta})$, which is a contradiction to \eqref{eq:consistency_prf_L_converges}.

\paragraph{Part 3}
We now show the consistency of $\dtN \rho(Z)$.

Consider Eq. \eqref{eq:SAA_gradient}. Since $\dt \log P(\cdot)$ is bounded by Assumption \ref{ass:LR_well_behaved}, and $\dt{f_i(\cdot;P_{\theta})}$ and $\dt{g_e(\cdot;P_{\theta})}$ are bounded by Assumption \ref{assume:risk_envelope}, and using our previous result  $\setdist{\saaspset}{\spset} \to 0$, we have that for a.e. $\omega_{SAA}\in \Omega_{SAA}$

\begin{equation*}
\begin{split}
  \lim_{N\to\infty}\dtN{\rho(Z)}  &=  \sum_{\omega\in\Omega} \pprob(\omega) \xi^*_{\theta}(\omega) \dt \log P(\omega) (Z(\omega) - \lambda^{*,\mathcal P}_{\theta}) \\
    & - \sum_{e\in\mathcal E} \lambda^{*,\mathcal E}_{\theta}(e) \dt{g_e(\xi^*_{\theta};P_{\theta})}\\
    & -\sum_{i\in\mathcal I} \lambda^{*,\mathcal I}_{\theta}(i) \dt{f_i(\xi^*_{\theta};P_{\theta})} \\
    &= \dt{\rho(Z)}.
\end{split}
\end{equation*}
where the first equality is obtained from the Envelop theorem (see Theorem \ref{thm:static_gradient}) with $(\xi^*_{\theta},\lambda^{*,\mathcal P}_{\theta},\lambda^{*,\mathcal E}_{\theta},\lambda^{*,\mathcal I}_{\theta})\in \saaspset\cap\spset$ is the limit point of the converging sequence $\{(\xi^*_{\theta;N},\lambda^{*,\mathcal P}_{\theta;N},\lambda^{*,\mathcal E}_{\theta;N},\lambda^{*,\mathcal I}_{\theta;N})\}_{N\in\mathbb N}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem \ref{thm:dynamic_risk}}

Similar to the proof of Theorem \ref{thm:static_gradient}, recall the saddle point definition of $(\xi^*_{\theta,x},\lambda^{*,\mathcal P}_{\theta,x},\lambda^{*,\mathcal E}_{\theta,x},\lambda^{*,\mathcal I}_{\theta,x}) \in \spset$ and strong duality result, i.e.,
\begin{equation*}
\begin{split}
\max_{\xi\,:\,\xi P_\theta(\cdot|x)\in\mathcal U(x,P_\theta(\cdot|x))}\sum_{x'  \in \mathcal X} \, \xi(x') P_\theta(x'|x)V_\theta(x')&=\max_{\xi\geq 0}\min_{\lambda^{\mathcal P},\lambda^{\mathcal I}\geq 0,\lambda^{\mathcal E}}L_{\theta,x}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\\&=\min_{\lambda^{\mathcal P},\lambda^{\mathcal I}\geq 0,\lambda^{\mathcal E}}\max_{\xi\geq 0}L_{\theta,x}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I}).
\end{split}
\end{equation*}
the gradient formula in \eqref{eq:envelope_theorem} can be written as
\[
\begin{split}
\nabla_\theta V_\theta(x)&=\nabla_\theta \left[C_\theta(x) \!+\! \gamma\max_{\xi\,:\,\xi P_\theta(\cdot |x)\in \U(x,P_\theta(\cdot |x))}\mathbb E_{\xi}[V_\theta]\right]\\
&=\gamma\! \sum_{x'  \in \mathcal X} \! \xi^*_{\theta,x}(x') P_\theta(x'|x)\nabla_\theta V_\theta(x')+\sum_{a\in\mathcal A}\mu_\theta(a|x)\nabla_\theta\log\mu_\theta(a|x)h_\theta(x,a),
\end{split}
\]
where the stage-wise cost function $h_\theta(x,a)$ is defined in \eqref{eq:h}. By defining $\widehat{h}_\theta(x)=\sum_{a\in\mathcal A}\mu_\theta(a|x)\nabla_\theta\log\mu_\theta(a|x)h_\theta(x,a)$ and unfolding the recursion, the above expression implies
\[
\begin{split}
\nabla_\theta V_\theta(x_0)=&\widehat{h}_\theta(x_0)+\gamma\sum_{x_1\in\mathcal X}P_\theta(x_1|x_0)\xi_\theta^*(x_1)\Bigg[\widehat{h}_\theta(x_1)+\gamma \sum_{x_2\in\mathcal X}P_\theta(x_2|x_1)\xi_\theta^*(x_2) \nabla_\theta V_\theta\left(x_2\right)\Bigg].
\end{split}
\]
Now since $\nabla_\theta V_\theta$ is continuously differentiable with bounded derivatives, when $t\rightarrow\infty$, one obtains $\gamma^t\nabla_\theta V_\theta(x)\rightarrow 0$ for any $x\in\mathcal X$. Therefore, by Bounded Convergence Theorem, $\lim_{t\rightarrow\infty}\rho(\gamma^t V_{\theta}(x_t))=0$, when $x_0=x$ the above expression implies the result of this theorem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Gradient Formula for Dynamic Risk - Full Results}
\label{sec:dynamic}

In this section, we first derive a new formula for the gradient of a general Markov-coherent dynamic risk measure $\dt \rho_\infty(\mdp)$ that involves the \emph{value function} of the risk objective $\rho_\infty(\mdp)$ (e.g.,~the value function proposed by~\citealt{ruszczynski2010risk}). This formula extends the well-known ``policy gradient theorem"~\cite{sutton_policy_2000,konda2000actor} developed for the expected return to Markov-coherent dynamic risk measures. Using this formula, we suggest the following actor-critic style algorithm for estimating $\dt \rho_\infty(\mdp)$:

%\begin{itemize}
%\item
\textbf{Critic:} For a given policy $\theta$, calculate the \emph{risk-sensitive value function} of $\rho_\infty(\mdp)$ (see Section~\ref{sec:val_fn_rpprox}), and \\
%\item
\textbf{Actor:} Using the critic's value function, estimate $\dt \rho_\infty(\mdp)$ by sampling (see Section~\ref{sec:pol_grad}).
%\end{itemize}

The value function proposed by~\citet{ruszczynski2010risk} assigns to each state a particular value that encodes the long-term risk starting from that state. When the state space $\mathcal X$ is large, calculating the value function by dynamic programming (as suggested by~\citealt{ruszczynski2010risk}) becomes intractable due to the ``curse of dimensionality". For the risk-neutral case, a standard solution to this problem is to approximate the value function by a set of state-dependent features, and use sampling to calculate the parameters of this approximation~\citep{BT96}. In particular, \emph{temporal difference} (TD) learning methods~\cite{sutton_reinforcement_1998} are popular for this purpose, which have been recently extended to robust MDPs by~\citet{tamar2014robust}. We use their (robust) TD algorithm and show how our critic use it to approximates the {\em risk-sensitive} value function. We then discuss how the error introduced by this approximation affects the gradient estimate of the actor.
%by projected \emph{risk-sensitive Bellman iteration}
% Explain general actor-critic approach - value function estimation + policy gradient theorem

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dynamic Risk}
We provide a multi-period generalization of the concepts presented in Section~\ref{subsec:coherent}. Here we closely follow the discussion in~\citet{ruszczynski2010risk}.

Consider a probability space $(\Omega, \mathcal F, P_\theta)$, a filtration $\mathcal F_0\subset \mathcal F_1\subset \mathcal F_2 \cdots \subset \mathcal F_T \subset \mathcal F$, and an adapted sequence of real-valued random variables $Z_t$, $t\in \{0, \ldots,T\}$. We assume that $\mathcal F_0 = \{\Omega, \emptyset\}$, i.e.,~$Z_0$ is deterministic. For each $t\in\{0, \ldots, T\}$, we denote by $\mathcal Z_t$ the space of random variables defined over the probability space $(\Omega, \mathcal F_t, P_\theta)$, and also let $\mathcal Z_{t, T}:=\mathcal Z_t \times \cdots \times \mathcal Z_T$ be a sequence of these spaces. The sequence of random variables $Z_t$ can be interpreted as the stage-wise costs observed along a trajectory generated by an MDP parameterized by a parameter $\theta$, i.e.,~$Z_{0,T} \doteq \big(Z_0=\gamma^0C(x_0,a_0),\dots,Z_T=\gamma^TC(x_T,a_T)\big)\in\mathcal Z_{0,T}$.

% Present the trajectory as the fundamental random variable

In particular, we are interested in the sequence of random variables induced by the trajectories from a Markov decision process (MDP) parameterized by parameter $\theta$.

Explicitly, for any $t\geq 0$ and state dependent random variable $Z(x_{t+1})\in\mathcal Z_{t+1}$, the risk evaluation is given by

%\vspace{-0.2in}
\begin{small}
\begin{equation}
\label{eq:representation-result}
\rho\big(Z(x_{t+1})\big)=\max_{\xi\,:\, \xi P_\theta(\cdot |x_t)\in \U(x_t,P_\theta(\cdot |x_t))}\mathbb E_{\xi}\big[Z(x_{t+1})\big],
\end{equation}
\end{small}
%\vspace{-0.2in}

where we let $\U(x_t,P_\theta(\cdot |x_t))$ denote the risk-envelope \eqref{eq:U_as_optimization} with $\pprob$ replaced with $P_\theta(\cdot |x_t)$. The Markovian assumption on the risk measure $\rho_T(\mdp)$ allows us to optimize it using dynamic programming techniques.
\subsection{Risk-Sensitive Bellman Equation}
\label{subsec:Risk-Bellman}

Our value-function estimation method is driven by a Bellman-style equation for Markov coherent risks. Let $B(\mathcal X)$ denote the space of real-valued bounded functions on $\mathcal X$ and $C_\theta(x)=\sum_{a\in\mathcal A}C(x,a)\mu_\theta(a|x)$ be the stage-wise cost function induced by policy $\mu_\theta$. We now define the risk sensitive Bellman operator $T_{\theta}[V] : B(\mathcal X) \mapsto B(\mathcal X)$ as
%
\begin{equation}\label{eq:T_supp}
T_{\theta}[V](x) :=C_\theta(x) + \gamma\max_{\xi P_\theta(\cdot |x)\in \U(x,P_\theta(\cdot |x))}\mathbb E_{\xi}[V].
\end{equation}
%
According to Theorem~1 in~\citet{ruszczynski2010risk}, the operator $T_\theta$ has a unique fixed-point $V_\theta$, i.e.,~ $T_{\theta}[V_\theta](x)=V_\theta(x),\;\forall x\in\mathcal{X}$, that is equal to the risk objective function induced by $\theta$, i.e.,~$V_\theta(x_0)=\rho_\infty(\mdp)$. However, when the state space $\mathcal X$ is large, exact enumeration of the Bellman equation is intractable due to ``curse of dimensionality''. Next, we provide an iterative approach to approximate the risk sensitive value function.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Value Function Approximation}
\label{sec:val_fn_rpprox}

Consider the linear approximation of the risk-sensitive value function $V_\theta(x)\approx v^\top\phi(x)$, where $\phi(\cdot)\in\mathbb R^{\kappa_2}$ is the $\kappa_2$-dimensional state-dependent feature vector. Thus, the approximate value function belongs to the low dimensional sub-space $\mathcal{V}=\left\{\Phi v|v\in\mathbb R^{\kappa_2}\right\}$, where $\Phi:\mathcal X\rightarrow\mathbb R^{\kappa_2}$ is a function mapping such that $\Phi(x)=\phi(x)$. %Let $v_\theta^*\in\mathbb R^{\kappa_2}$ be the best approximation parameter, and thus, $\tilde V_{\theta}(x)=\phi(x)^\top v_\theta^*$ be the best linear approximation of $ V_{\theta}(x)$.
The goal of our critric is to find a good approximation of $V_\theta$ from simulated trajectories of the MDP. In order to have a well-defined approximation scheme, we first impose the following standard assumption~\citep{BT96}.
%
\begin{assumption}
The mapping $\Phi$ has full column rank.
\end{assumption}
%
For a function $y:\mathcal X\rightarrow\mathbb R$, we define its weighted (by $d$) $\ell_2$-norm as $\|y\|_d=\sqrt{\sum_{x'}d(x^\prime|x)y(x^\prime)^2}$, where $d$ is a distribution over $\mathcal X$. Using this, we define $\Pi:\mathcal X\rightarrow\mathcal{V}$, the orthogonal projection from $\mathbb R$ to $\mathcal V$, w.r.t.~a norm weighted by the stationary distribution of the policy, $d_\theta(x'|x)$.

Note that the TD methods approximate the value function $V_\theta$ with the fixed-point of the joint operator $\Pi T_\theta$, i.e.,~$\tilde V_\theta(x)=v_\theta^{*\top}\phi(x)$, such that
%
\begin{equation}\label{eq:projected_fixed_point}
\forall x\in\mathcal X, \quad\quad\quad \tilde V_\theta(x)=\Pi T_{\theta} [\tilde V_\theta](x).
\end{equation}
%
From Eq.~\ref{eq:representation-result} that has been derived from Theorem~\ref{thm:rep} for dynamic risks, it is easy to see that the risk-sensitive Bellman equation~\eqref{eq:T_supp} is a robust Bellman equation~\citep{nilim_robust_2005} with uncertainty set $\mathcal U(x,P_\theta(\cdot|x))$. Thus, we may use the TD approximation of the robust Bellman equation proposed by~\citet{tamar2014robust} to find an approximation of $V_\theta$.
%In this section we mainly summarize the value function approximation results of robust projected Bellman iteration from \citet{tamar2014robust}.
%
We will need the following assumption analogous to Assumption~2 in~\citet{tamar2014robust}.
%
\begin{assumption}\label{assume:risk}
There exists $\kappa \in (0, 1)$ such that $\xi(x') \leq \kappa/\gamma$, for all $\xi(\cdot) P_\theta(\cdot|x) \in \mathcal U(x, P_\theta(\cdot|x))$ and all $x,x' \in\mathcal X$.
\end{assumption}
%
Given Assumption~\ref{assume:risk}, Proposition~3 in~\citet{tamar2014robust} guarantees that the projected risk-sensitive Bellman operator $\Pi T_{\theta}$ is a contraction w.r.t.~$d_{\theta}$-norm. Therefore, Eq.~\ref{eq:projected_fixed_point} has a unique fixed-point solution $\tilde V_\theta(x)=v_\theta^{*\top}\phi(x)$. This means that $v_\theta^*\in\mathbb R^{\kappa_2}$ satisfies $v_\theta^*\in\arg\min_{v}\|T_{\theta}[\Phi v]-\Phi v\|_{d_\theta}^2$. By the projection theorem on Hilbert spaces, the orthogonality condition for $v_\theta^*$ becomes
%
\begin{align*}
%\begin{split}
\sum_{x\in\mathcal X}&d_\theta(x|x_0)\phi(x)\phi(x)^\top v_\theta^*= \sum_{x\in\mathcal X} d_\theta(x|x_0)\phi(x) C_\theta(x)\\
&+\gamma \sum_{x\in\mathcal X}d_\theta(x|x_0)\phi(x)\max_{\xi\,:\,\xi P_\theta(\cdot |x) \in \U(x,P_\theta(\cdot |x))} \mathbb E_{\xi}[\Phi v_\theta^* ].
%\end{split}
\end{align*}
%
As a result, given a long enough trajectory $x_0$, $a_0$, $x_1$, $a_1$, $\ldots$, $x_{N-1}$, $a_{N-1}$ generated by policy $\theta$, we may estimate the fixed-point solution $v^*_\theta$ using the projected risk sensitive value iteration (PRSVI) algorithm with the update rule
%
\begin{align}
v_{k+1}&=\left(\frac{1}{N}\sum_{t=0}^{N-1} \phi(x_t)\phi(x_t)^{\top}\right)^{-1}\bigg[\frac{1}{N}\sum_{t=0}^{N-1}\phi(x_t) C_{\theta}(x_t)\nonumber\\
&+\gamma\frac{1}{N}\sum_{t=0}^{N-1} \phi(x_t)\max_{\xi P_\theta(\cdot |x_t)\in \U(x_t,P_\theta(\cdot |x_t))} \mathbb E_{\xi}[\Phi v_k ]\bigg].
\label{eq:PRSVI}
\end{align}
%
Note that using the law of large numbers, as both $N$ and $k$ tend to infinity, $v_k$ converges w.p.~1 to $v_\theta^*$, the unique solution of the fixed point equation $\Pi T_{\theta} [\Phi v]=\Phi v$.

%\subsection{SAA Formulation of $\max_{\xi P_\theta(\cdot |x)\in \U(x,P_\theta(\cdot |x))} \mathbb E_{\xi}[\Phi v]$}\label{sec:val_fn_rpprox_SAA}

In order to implement the iterative algorithm~\eqref{eq:PRSVI}, one must repeatedly solve the inner optimization problem $\max_{\xi P_\theta(\cdot |x)\in \U(x,P_\theta(\cdot |x))} \mathbb E_{\xi}[\Phi v ]$. When the state space $\mathcal X$ is large, solving this optimization problem is often computationally expensive or even intractable. Similar to Section~3.4 of~\citet{tamar2014robust}, we propose the following SAA approach to solve this problem. For the trajectory, $x_0$, $a_0$, $x_1$, $a_1$, $\ldots$, $x_{N-1}$, $a_{N-1}$, we define the empirical transition probability $P_N(x'|x,a) \doteq \frac{\sum_{t=0}^{N-1}\mathbf{1}\{x_t=x,\;a_t=a,\;x_{t+1}=x'\}}{\sum_{t=0}^{N-1}\mathbf{1}\{x_t=x,\;a_t=a\}}$\footnote{In the case when the sizes of state and action spaces are huge or when these spaces are continuous, the empirical transition probability can be found by kernel density estimation.} and $\pemp(x'|x)=\sum_{a\in\mathcal A}P_N(x'|x,a)\mu_\theta(a|x)$. Consider the following $\ell_2$-regularized empirical robust optimization problem\footnote{In the SAA approach, we only sum over the elements for which $\pemp(x'|x)>0$, thus, the sum has at most $N$ elements.}

\vspace{-0.2in}
\begin{small}
\begin{align}
\rho_N(\Phi v)&=\max_{\xi: \xi \pemp \in \U(x,\pemp)} \sum_{x'\in\mathcal X} \pemp(x'|x) \xi(x')\phi^\top(x')v \nonumber \\
&+ \frac{1}{2N}\big[\pemp(x'|x) \xi(x')\big]^2.
\label{eq:empirical-PRSVI}
\end{align}
\end{small}
\vspace{-0.2in}

As in~\citet{meng2006regularized}, the $\ell_2$-regularization term in this optimization problem guarantees convergence of optimizers $\xi^*$ and the corresponding KKT multipliers, when $N\rightarrow\infty$. Convergence of these parameters is crucial for the policy gradient analysis in the next sections. We denote by $\xi^*_{\theta,x;N}$, the solution of the above empirical optimization problem, and by $\lambda^{*,\mathcal P}_{\theta,x;N}$, $\lambda^{*,\mathcal E}_{\theta,x;N}$, $\lambda^{*,\mathcal I}_{\theta,x;N}$, the corresponding KKT multipliers.

We obtain the empirical PRSVI algorithm by replacing the inner optimization $\max_{\xi P_\theta(\cdot |x_t)\in \U(x_t,P_\theta(\cdot |x_t))} \mathbb E_{\xi}[\Phi v_\theta^* ]$ in Eq.~\ref{eq:PRSVI} with $\rho_N(\Phi v)$ from Eq.~\ref{eq:empirical-PRSVI}. Similarly, as both $N$ and $k$ tend to infinity, $v_k$ converges w.p.~1 to $v_\theta^*$. More details can be found in the supplementary material.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Actor-Critic algorithm
\subsection{Gradient Estimation}
\label{sec:pol_grad}

%\subsection{Policy gradient of $V_\theta(x)$}\label{sec:pol_grad}
In Section~\ref{sec:val_fn_rpprox}, we showed that we may effectively approximate the value function of a fixed policy $\theta$ using the (empirical) PRSVI algorithm in Eq.~\ref{eq:PRSVI}. In this section, we first derive a formula for the gradient of the Markov-coherent dynamic risk measure $\rho_\infty(\mathcal M)$, and then propose a SAA algorithm for estimating this gradient, in which we use the SAA approximation of value function from Section~\ref{sec:val_fn_rpprox}. As described in Section~\ref{subsec:Risk-Bellman}, $\rho_\infty(\mathcal M)=V_\theta(x_0)$, and thus, we shall first derive a formula for $\nabla_\theta V_\theta(x_0)$.

Let $(\xi^*_{\theta,x},\lambda^{*,\mathcal P}_{\theta,x},\lambda^{*,\mathcal E}_{\theta,x},\lambda^{*,\mathcal I}_{\theta,x})$ be the saddle point of~\eqref{eq:Lagrangian} corresponding to the state $x\in\mathcal X$. In many common coherent risk measures such as CVaR and mean semi-deviation, there are closed-form formulas for $\xi^*_{\theta,x}$ and KKT multipliers $(\lambda^{*,\mathcal P}_{\theta,x},\lambda^{*,\mathcal E}_{\theta,x},\lambda^{*,\mathcal I}_{\theta,x})$. We will briefly discuss the case when the saddle point does not have an explicit solution later in this section. Before analyzing the gradient estimation, we have the following standard assumption in analogous to Assumption \ref{ass:LR_well_behaved} of the static case.
\begin{assumption}
The likelihood ratio $\nabla_\theta\log\mu_\theta(a|x)$ is well-defined and bounded for all $x\in\mathcal X$ and $a\in\mathcal A$.
\end{assumption}

As in Theorem~\ref{thm:static_gradient} for the static case, we may use the envelope theorem and the risk-sensitive Bellman equation, $V_\theta(x)=C_\theta(x) + \gamma\max_{\xi P_\theta(\cdot |x)\in \U(x,P_\theta(\cdot |x))}\mathbb E_{\xi}[V_\theta]$, to derive a formula for $\nabla_\theta V_\theta(x)$. We report this result in Theorem~\ref{thm:dynamic_risk_supp}, which is analogous to the risk-neutral policy gradient theorem~\cite{sutton_policy_2000,konda2000actor,bhatnagar_natural_2009}. The proof is in the supplementary material.
%
\begin{theorem}\label{thm:dynamic_risk_supp}
Under Assumptions~\ref{assume:risk_envelope}, we have
%
\begin{equation*}
\nabla V_\theta(x) \!=\! \mathbb E_{\xi^*_\theta} \! \! \left[\sum_{t=0}^{\infty}\gamma^t\nabla_\theta\log\mu_\theta(\!a_t|x_t\!)h_\theta(\!x_t,\!a_t\!)\!\mid\! x_0\!\!=\!\!x\right],
\end{equation*}
%
where $\mathbb E_{\xi^*_\theta}[\cdot]$ denotes the expectation w.r.t.~trajectories generated by a Markov chain with transition probabilities $P_\theta(\cdot|x)\xi_{\theta,x}^*(\cdot)$, and the stage-wise cost function $h_\theta(x,a)$ is defined as

\vspace{-0.2in}
\begin{small}
\begin{align}
h_\theta(x,a) &= C(x,a) + \sum_{x'  \in \mathcal X} P(x'|x,a)\xi^*_{\theta,x}(x')\Big[\gamma V_\theta(x')-\!{\lambda}^{*,\mathcal P}_{\theta,x} \nonumber \\
&-\sum_{i\in\mathcal I} {\lambda}^{*,\mathcal I}_{\theta,x}(i)\frac{ d f_i(\xi^*_{\theta,x},p)}{d p(x')} - \sum_{e\in\mathcal E}\!{\lambda}^{*,\mathcal E}_{\theta,x}(e) \frac{ d g_e(\xi^*_{\theta,x},p)}{d p(x')}\Big].
\label{eq:h}
\end{align}
\end{small}
\vspace{-0.2in}
\end{theorem}

Theorem \ref{thm:dynamic_risk_supp} indicates that the policy gradient of the Markov-coherent dynamic risk measure $\rho_\infty(\mathcal M)$, i.e.,~$\nabla_\theta\rho_\infty(\mathcal M)=\nabla_\theta V_\theta$, is equivalent to the risk-neutral value function of policy $\theta$ in a MDP with the stage-wise cost function $\nabla_\theta\log\mu_\theta(a|x)h_\theta(x,a)$ (which is well-defined and bounded), and transition probability $P_\theta(\cdot|x)\xi_{\theta,x}^*(\cdot)$. Thus, when the saddle points are known and the state space $\mathcal X$ is not too large, we can compute $\nabla_\theta V_\theta$ using a policy evaluation algorithm. However, when the state space is large, exact calculation of $\nabla V_\theta$ by policy evaluation becomes impossible, and our goal would be to derive a sampling method to estimate $\nabla V_\theta$. Unfortunately, since the risk envelop depends on the policy parameter $\theta$, unlike the risk-neutral case, the risk sensitive (or robust) Bellman equation $T_\theta[V_\theta](x)$ in \eqref{eq:T_supp} is nonlinear in the stationary Markov policy $\mu_\theta$. Therefore $h_\theta$ cannot be considered using the action-value function ($Q$-function) of the robust MDP. Therefore, even if the exact formulation of the value function $V_\theta$ is known, it is computationally intractable to enumerate the summation over $x'$ to compute $h_\theta(x,a)$. On top of that in many applications the value function $V_\theta$ is not known in advance, which further complicates gradient estimation. To estimate the policy gradient when the value function is unknown, we approximate it by the projected risk sensitive value function $\Phi v_\theta^*$. To address the sampling issues, we propose the following \emph{two-phase sampling procedure} for estimating $\nabla V_\theta$.

{\bf (1)} Generate $N$ trajectories $\{x^{(j)}_0,a^{(j)}_0,x^{(j)}_1,a^{(j)}_1,\ldots\}_{j=1}^N$ from the Markov chain induced by policy $\theta$ and transition probabilities $P^\xi_{\theta}(\cdot|x):=\xi_{\theta,x}^*(\cdot)P_\theta(\cdot|x)$.

{\bf (2)} For each state-action pair $(x^{(j)}_t,a^{(j)}_t)=(x,a)$, generate $N$ samples $\{y^{(k)}\}_{k=1}^N$ using the transition probability $P(\cdot|x,a)$ and calculate the following empirical average estimate of $h_\theta(x,a)$

\vspace{-0.25in}
\begin{small}
\begin{align*}
h_{\theta,N}(x,&a):=C(x,a)+\frac{1}{N}\sum_{k=1}^N\xi^*_{\theta,x}(y^{(k)})\Bigg[\gamma {v_\theta^*}^\top\phi(y^{(k)})-{\lambda}^{*,\mathcal P}_{\theta,x} \\
&-\sum_{i\in\mathcal I}{\lambda}^{*,\mathcal I}_{\theta,x}(i)\frac{d f_i(\xi^*_{\theta,x},p)}{d p(y^{(k)})}-\sum_{e\in\mathcal E}{\lambda}^{*,\mathcal E}_{\theta,x}(e) \frac{d g_e(\xi^*_{\theta,x},p)}{d p(y^{(k)})}\Bigg]
\end{align*}
\end{small}
\vspace{-0.2in}

{\bf (3)} Calculate an estimate of $\nabla V_\theta$ using the following average over all the samples: $\frac{1}{N}\sum_{j=1}^N\sum_{t=0}^\infty\gamma^t\nabla_\theta\log\mu_\theta(a^{(j)}_t|x^{(j)}_t)h_{\theta,N}(x^{(j)}_t,a^{(j)}_t)$.

Indeed, by the definition of empirical transition probability $P_N(x'|x,a)$, $h_{\theta,N}(x,a)$ can be re-written as in the same structure of ${h}_{\theta}(x,a)$, except by replacing the transition probability $P(x'|x,a)$ with $P_N(x'|x,a)$.

Furthermore, in the case that the saddle points $(\xi^*_{\theta,x},\lambda^{*,\mathcal P}_{\theta,x},\lambda^{*,\mathcal E}_{\theta,x},\lambda^{*,\mathcal I}_{\theta,x}) $ do not have a closed-form solution, we may follow the SAA procedure of Section~\ref{sec:val_fn_rpprox} and replace them and the transition probabilities $P(x'|x,a)$ with their sample estimates $(\xi^*_{\theta,x;N},\lambda^{*,\mathcal P}_{\theta,x;N},\lambda^{*,\mathcal E}_{\theta,x;N},\lambda^{*,\mathcal I}_{\theta,x;N})$ and $P_N(x'|x,a)$ respectively.

At the end, we show the convergence of the above two-phase sampling procedure.
%\subsection{SAA Formulation for Policy Gradient}\label{sec:pol_approx}
Let $d_{{P}^\xi_{\theta}}(x|x_0)$ and $\pi_{{P}^\xi_{\theta}}(x,a|x_0)$ be the state and state-action occupancy measure induced by the transition probability function $P^\xi_{\theta}(\cdot|x)$, respectively. Similarly, let $d_{\pemp^\xi}(x|x_0)$ and $\pi_{\pemp^\xi}(x,a|x_0)$ be the state and state-action occupancy measure induced by the estimated transition probability function $\pemp^\xi(\cdot|x):=\xi_{\theta,x;N}^*(\cdot)P_{\theta;N}(\cdot|x)$. From the two-phase sampling procedure for policy gradient estimation and by the strong law of large numbers, when $N\rightarrow\infty$, with probability~1, we have that $\frac{1}{N}\sum_{j=1}^N\sum_{t=0}^\infty\gamma^t\mathbf 1\{x^{(j)}_t=x,a^{(j)}_t=a\}=\pi_{\pemp^\xi}(x,a|x_0)$. Based on the strongly convex property of the $\ell_2$-regularized objective function in the inner robust optimization problem $\rho_N(\Phi v)$, we can show that both the state-action occupancy measure $\pi_{\pemp^\xi}(x,a|x_0)$ and the stage-wise cost ${h}_{\theta;N}(x,a)$ converge to the their true values within a value function approximation error bound $\Delta=\|\Phi v^*_\theta-V_\theta\|_\infty$. We refer the readers to the supplementary materials for these technical results. These results together with Theorem~\ref{thm:dynamic_risk_supp} imply the consistency of the policy gradient estimation.
%
\begin{theorem}\label{them:consistency_dyn}
For any $x_0\in\mathcal X$, the following expression holds with probability~1:

\vspace{-0.2in}
\begin{small}
\begin{align*}
\bigg|\lim_{N\rightarrow\infty}\frac{1}{N}&\sum_{j=1}^N\sum_{t=0}^\infty\gamma^t\;\nabla\log\mu_\theta(a^{(j)}_t|x^{(j)}_t)\;h_{\theta,N}(x^{(j)}_t,a^{(j)}_t)\\
&-\nabla V_\theta(x_0)\bigg |=O(\Delta).
\end{align*}
\end{small}
\vspace{-0.2in}

\end{theorem}
\vspace{-5pt}
Thm.~\ref{them:consistency_dyn} guarantees that as the value function approximation error decreases and the number of samples increases, the sampled gradient converges to the true gradient.
%Therefore, when $\|\Phi v^*_\theta-V_\theta\|_\infty\leq\epsilon$ for arbitrarily small $\epsilon>0$, the sampled gradient  converges to true gradient $\nabla_\theta V_\theta$.

%To conclude this section, we have seen that by combining our techniques for static risk measures (Section \ref{sec:static}) with risk sensitive value-function approximation and a new policy gradient formula, we were able to extend the actor-critic method to evaluate $\dt \rho_\infty(\mdp)$. This enables us to perform effective policy search for problem \eqref{eq:DRP_problem}.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Convergence Analysis of Empirical PRSVI}\label{sec:SAA_VA}

\begin{lemma}[Technical Lemma]\label{lem:tech}
Let $P(\cdot|\cdot)$ and $\widetilde P(\cdot|\cdot)$ be two arbitrary transition probability matrices. At state $x\in\mathcal X$, for any $\xi\,:\,\xi P(\cdot|x)\in\mathcal{U}(x,P(\cdot|x))$, there exists a ${M}_\xi>0$ such that for some $\tilde{\xi}\,:\,\tilde{\xi}\widetilde P(\cdot|x)\in\mathcal{U}(x,\widetilde P(\cdot|x))$,
\[
\sum_{x^\prime\in \mathcal X}|\xi(x^\prime)-\tilde{\xi}(x^\prime)|\leq M_{\xi}\sum_{x^\prime\in \mathcal X}\left|P(x^\prime|x)-\widetilde P(x^\prime|x)\right|.
\]
\end{lemma}
\begin{proof}
From Theorem \ref{thm:rep}, we know that $\mathcal{U}(x,P(\cdot|x))$ is a closed, bounded,  convex set of probability distribution functions. Since any conditional probability mass function $P$ is in the interior of $\text{dom}(\mathcal{U})$ and the graph of $\mathcal{U}(x,P(\cdot|x))$ is closed, by Theorem 2.7 in \citet{rockafellar1998variational}, $\mathcal{U}(x,P(\cdot|x))$ is a Lipschitz set-valued mapping with respect to the Hausdorff distance.
Thus, for any $\xi\,:\,\xi P(\cdot|x)\in\mathcal{U}(x,P(\cdot|x))$, the following expression holds for some ${M}_\xi>0$:
\begin{equation*}
\inf_{\hat{\xi}\in\mathcal{U}(x,\widetilde P(\cdot|x))}\sum_{x^\prime\in \mathcal X}|\xi(x^\prime)-\hat{\xi}(x^\prime)|\leq {M}_\xi\sum_{x^\prime\in \mathcal X}\left| P(x^\prime|x)-\widetilde P(x^\prime|x)\right|.
\end{equation*}
Next, we want to show that the infimum of the left side is attained. Since the objective function is convex, and $\mathcal{U}(x,\widetilde P(\cdot|x))$ is a convex compact set, there exists $\tilde{\xi}\,:\,\tilde{\xi}\widetilde P(\cdot|x)\in\mathcal{U}(x,\widetilde P(\cdot|x))$ such that infimum is attained.
\end{proof}

\begin{lemma}[Strong Law of Large Number]\label{lem:SLLN_V}
Consider the sampling based PRSVI algorithm with update sequence $\{\widehat{v}_k\}$. Then as both $N$ and $k$ tend to $\infty$, $\widehat{v}_k$ converges with probability 1 to $v_\theta^*$, the
unique solution of projected risk sensitive fixed point equation $\Pi T_{\mu} [\Phi v]=\Phi v$.
\end{lemma}
\begin{proof}
By the strong law of large number of Markov process, the empirical visiting distribution and transition probability asymptotically converges to their statistical limits with probability 1, i.e.,
\[
\frac{\sum_{t=0}^{N-1}\mathbf{1}\{x_t=x\}}{N}\rightarrow d_\theta(x|x_0),\,\text{and}\,\,\widehat{P}(x'|x,a)\rightarrow P(x'|x,a),\,\forall x,x'\in\mathcal X, \,a\in\mathcal A.
\]
Therefore with probability $1$,
\[
\begin{split}
&\frac{1}{N}\sum_{t=0}^{N-1} \phi(x_t)\phi(x_t)^{\top}\rightarrow \sum_{x}d_\theta(x|x_0)\cdot\phi(x)\phi^\top(x),\\
&\frac{1}{N}\sum_{t=0}^{N-1}\phi(x_t) C_\theta(x_t)\rightarrow \sum_{x}d_\theta(x|x_0)\cdot\phi(x)C_\theta(x).
\end{split}
\]

Now we show that following expression holds with probability $1$:
\begin{equation}\label{eq:claim}
\begin{split}
&\max_{\xi\,:\,\xi\pemp(\cdot|x_t)\in\mathcal U(x_t,\pemp(\cdot|x_t))}\sum_{x'\in\mathcal X}\xi(x') \pemp(x'|x_t)v^\top\phi\left(x'\right)+\frac{1}{2N}(\xi(x')\pemp(x'|x_t))^2\\
\rightarrow&\max_{\xi\,:\,\xi P_\theta(\cdot|x_t)\in\mathcal U(x_t,P_\theta(\cdot|x_t))}\sum_{x'\in\mathcal X}\xi(x') P_\theta(x'|x_t)v^\top\phi\left(x'\right).
\end{split}
\end{equation}
Notice that for $\{\xi^*_{\theta,x_t;N}(x')\}_{x'\in\mathcal X}\in\arg\max_{\xi\,:\,\xi\pemp(\cdot|x_t)\in\mathcal U(x_t,\pemp(\cdot|x_t))}\sum_{x'\in\mathcal X}\xi(x') \pemp(x'|x_t)v^\top\phi\left(x'\right)$,  Lemma \ref{lem:tech} implies
\[
\begin{split}
&\max_{\xi\,:\,\xi\pemp(\cdot|x_t)\in\mathcal U(x_t,\pemp(\cdot|x_t))}\sum_{x'\in\mathcal X}\xi(x') \pemp(x'|x_t)v^\top\phi\left(x'\right)+\frac{1}{2N}(\xi(x')\pemp(x'|x_t))^2\\
&-\max_{\xi\,:\,\xi P_\theta(\cdot|x_t)\in\mathcal U(x_t,P_\theta(\cdot|x_t))}\sum_{x'\in\mathcal X}\xi(x') P_\theta(x'|x_t)v^\top\phi\left(x'\right)\\
\leq & \|\Phi v\|_\infty\left({M}_{\xi^*_{\theta,x_t;N}}+\max_{x\in\mathcal X}|\xi^*_{\theta,x_t;N}(x)|\right)\sum_{x^\prime\in \mathcal X}\left| P_\theta(x^\prime|x_t)-\pemp(x^\prime|x_t)\right|+\frac{1}{2N}.
\end{split}
\]
The quantity $\max_{x\in\mathcal X}|\xi^*_{\theta,x_t;N}(x)|$ is bounded because $\mathcal U(x_t,\pemp(\cdot|x_t))$ is a closed and bounded convex set from the definition of coherent risk measures. By repeating the above analysis by interchanging $P_\theta$ and $\pemp$ and combining previous arguments, one obtains
\[
\begin{split}
& \left|\max_{\xi\,:\,\xi\pemp(\cdot|x_t)\in\mathcal U(x_t,\pemp(\cdot|x_t))}\sum_{x'\in\mathcal X}\xi(x')\pemp(x'|x_t)v^\top\phi\left(x'\right)+\frac{1}{2N}(\xi(x')\pemp(x'|x_t))^2\right.\\
&\left.-\max_{\xi\,:\,\xi P_\theta(\cdot|x_t)\in\mathcal U(x_t,P_\theta(\cdot|x_t))}\sum_{x'\in\mathcal X}\xi(x')P_\theta(x'|x_t)v^\top\phi\left(x'\right)\right|\\
\leq & \|\Phi v\|_\infty\max\left\{\left({M}_{\xi^*}+\max_{x\in\mathcal X}|\xi^*(x)|\right),\left({M}_{\xi^*_{\theta,x_t;N}}+\max_{x\in\mathcal X}|\xi^*_{\theta,x_t;N}(x)|\right)\right\}\sum_{x^\prime\in \mathcal X}\left| P_\theta(x^\prime|x_t)-\pemp(x^\prime|x_t)\right|+\frac{1}{2N}.
\end{split}
\]
Therefore, the claim in expression \eqref{eq:claim} holds when $N\rightarrow\infty$ and $\sum_{x^\prime\in \mathcal X}\left| P_\theta(x^\prime|x_t)- \pemp(x^\prime|x_t)\right|\rightarrow 0$. On the other hand, the strong law of large numbers also implies that with probability $1$,
\[
\frac{1}{N}\sum_{t=0}^{N-1} \phi(x_t)\rho(\Phi v_t)\rightarrow d_\theta(x|x_0)\phi(x)\max_{\xi\,:\,\xi P_\theta(\cdot|x)\in\mathcal U(x,P_\theta(\cdot|x))}\sum_{x'\in\mathcal X}\xi(x') P_\theta(x'|x){v^*_\theta}^\top\phi\left(x'\right).
\]

Combining the above arguments implies
\[
\frac{1}{N}\sum_{t=0}^{N-1} \phi(x_t){\rho}_N(\Phi v_t)\rightarrow d_\theta(x|x_0)\phi(x)\max_{\xi\,:\,\xi P_\theta(\cdot|x)\in\mathcal U(x,P_\theta(\cdot|x))}\sum_{x'\in\mathcal X}\xi(x') P_\theta(x'|x){v_\theta^*}^\top\phi\left(x'\right) .
\]
As $N\rightarrow\infty$, the above arguments imply that $v_k-\widehat{v}_k\rightarrow 0$. On the other hand, Proposition 1  in \citet{tamar2014robust} implies that the projected risk sensitive Bellman operator $\Pi T_\theta[V]$ is a contraction, it follows that from the analysis in Section 6.3 in \citet{Ber2012DynamicProgramming} that the sequence $\{\Phi \widehat{v}_k\}$ generated by projected value iteration converges to the unique fixed point $\Phi v_\theta^*$. This in turns implies that the sequence $\{\Phi v_k\}$ converges to $\Phi v_\theta^*$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Technical Results }\label{sec:SAA_PGA}

Since by convention $\xi^*_{\theta,x;N}(x')=0$ whenever $\pemp(x'|x) =0$. In this section, we simplify the analysis by letting $\pemp(x'|x) >0$ for any $x'\in\mathcal X$ without loss of generality.
Consider the following empirical robust optimization problem:
\begin{equation}\label{eq:mid_opt}
\max_{\xi\,:\,\xi \pemp(\cdot |x)\in \U(x,\pemp(\cdot |x))} \sum_{x'\in\mathcal X} \pemp(x'|x) \xi(x')V_\theta(x'),
\end{equation}
where the solution of the above empirical problem is $\bar\xi^*_{\theta,x;N}$ and the corresponding KKT multipliers are $(\bar\lambda^{*,\mathcal P}_{\theta,x;N},\bar\lambda^{*,\mathcal E}_{\theta,x;N},\bar\lambda^{*,\mathcal I}_{\theta,x;N})$. Comparing to the optimization problem for $\rho_N(\Phi v)$, i.e.,
\begin{equation}\label{eq:rho}
\rho_N(\Phi v)=\max_{\xi\,:\,\xi \pemp(\cdot |x)\in \U(x,\pemp(\cdot |x))} \sum_{x'\in\mathcal X} \pemp(x'|x) \xi(x')\phi^\top(x')v+\frac{1}{2N}(\xi(x')\pemp(x'|x))^2,
\end{equation}
where the solution of the above empirical problem is $\xi^*_{\theta,x;N}$ and the corresponding KKT multipliers are $(\lambda^{*,\mathcal P}_{\theta,x;N},\lambda^{*,\mathcal E}_{\theta,x;N},\lambda^{*,\mathcal I}_{\theta,x;N})$,
the optimization problem in \eqref{eq:mid_opt} can be viewed as having a skewed objective function of the problem in \eqref{eq:rho}, within the deviation of magnitude $\Delta+1/2N$ where $\Delta=\|\Phi v^*_\theta-V_\theta\|_\infty$. Before getting into the main analysis, we have the following observations.
\begin{description}
\item[(i)] Without loss of generality, we can also assume $(\xi^*_{\theta,x;N},(\lambda^{*,\mathcal P}_{\theta,x;N},\lambda^{*,\mathcal E}_{\theta,x;N},\lambda^{*,\mathcal I}_{\theta,x;N}))$ follows the strict complementary slackness condition\footnote{The existence of strict complementary slackness solution follows from the KKT theorem and one can easily construct a strictly complementary pair using i.e. the Balinski-Tucker tableau with the linearized objective function and constraints, in finite time.}.
\item[(ii)] Recall from Assumption \ref{assume:risk_envelope} that the functions $f_i({\xi},p)$ and $g_e(\xi,p)$ are twice differentiable in $\xi$ at $p=P_{\theta,N}(\cdot|x)$ for any $x\in\mathcal X$.
\item[(iii)] The Slater's condition in Assumption \ref{assume:risk_envelope} implies the linear independence constraint qualification (LICQ).
\item[(iv)] Since optimization problem \eqref{eq:rho} has a convex objective function and convex/affine constraints in $\xi\in\reals^{|\mathcal X|}$, equipped with the Slater's condition we have that the first order KKT condition holds at $\xi^*_{\theta,x;N}$ with the corresponding KKT multipliers are $(\lambda^{*,\mathcal P}_{\theta,x;N},\lambda^{*,\mathcal E}_{\theta,x;N},\lambda^{*,\mathcal I}_{\theta,x;N})$. Furthermore, define the Lagrangian function
\begin{equation*}
\begin{split}
\widehat{L}_{\theta;N}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})\doteq&\sum_{x'\in\mathcal X} \pemp(x'|x) \xi(x')\phi^\top(x')v+\frac{1}{2N}(\pemp(x'|x) \xi(x'))^2\!\\&-\!\lambda^{\mathcal P}\left(\sum_{x' \in \mathcal X}\xi(x')P_{\theta;N}(x'|x)\!-\!1\!\right)\\
&-\sum_{e\in\mathcal E}\lambda^{\mathcal E}(e) f_e(\xi,P_{\theta;N}(\cdot|x))-\sum_{i\in\mathcal I}\lambda^{\mathcal I}(i) f_i(\xi,P_{\theta;N}(\cdot|x)).
\end{split}
\end{equation*}
One can easily conclude that $\nabla^2 \widehat{L}_{\theta;N}(\xi,\lambda^{\mathcal P},\lambda^{\mathcal E},\lambda^{\mathcal I})=-\pemp(\cdot|x)^\top\pemp(\cdot|x)/N-\sum_{i\in\mathcal I}\lambda^{\mathcal I}(i) \nabla^2_\xi f_i(\xi,P_{\theta;N}(\cdot|x))$ such that for any vector $\nu\neq 0$,
\[
\nu^\top \nabla^2\widehat{L}_{\theta;N}(\xi^*_{\theta,x;N},\lambda^{*,\mathcal P}_{\theta,x;N},\lambda^{*,\mathcal E}_{\theta,x;N},\lambda^{*,\mathcal I}_{\theta,x;N}) \nu< 0,
\]
which further implies that the second order sufficient condition (SOSC) holds at $(\xi^*_{\theta,x;N},\lambda^{*,\mathcal P}_{\theta,x;N},\lambda^{*,\mathcal E}_{\theta,x;N},\lambda^{*,\mathcal I}_{\theta,x;N})$.
\end{description}
Based on all the above analysis, we have the following sensitivity result from Corollary 3.2.4 in \cite{fiacco1983introduction}, derived based on Implicit Function Theorem.
\begin{proposition}[Basic Sensitivity Theorem]\label{prop:tech}
Under the Assumption \ref{assume:risk_envelope}, for any $x\in\mathcal X$ there exists a bounded non-singular matrix $K_{\theta,x}$ and a bounded vector $L_{\theta,x}$, such that the difference between the optimizers and KKT multipliers of optimization problem \eqref{eq:mid_opt} and \eqref{eq:rho} are bounded as follows:
\[
\begin{bmatrix}
\bar{\xi}^*_{\theta,x;N}\\
\bar{\lambda}^{*,\mathcal I}_{\theta,x;N}\\
\bar{\lambda}^{*,\mathcal P}_{\theta,x;N}\\
\bar{\lambda}^{*,\mathcal E}_{\theta,x;N}\\
\end{bmatrix}= \begin{bmatrix}
\xi^*_{\theta,x;N}\\
{\lambda}^{*,\mathcal I}_{\theta,x;N}\\
{\lambda}^{*,\mathcal P}_{\theta,x;N}\\
{\lambda}^{*,\mathcal E}_{\theta,x;N}\\
\end{bmatrix}+ \Phi_{\theta,x}^{-1}\Psi_{\theta,x}\left(\Delta+\frac{1}{2N}\right)+ o\left(\Delta+\frac{1}{2N}\right).
\]
\end{proposition}
On the other hand, we know from Proposition \ref{prop:consistent} that $\bar\xi^*_{\theta,x;N}\rightarrow \xi^*_{\theta,x}$ and $(\bar\lambda^{*,\mathcal P}_{\theta,x;N},\bar\lambda^{*,\mathcal E}_{\theta,x;N},\bar\lambda^{*,\mathcal I}_{\theta,x;N})\rightarrow(\lambda^{*,\mathcal P}_{\theta,x},\lambda^{*,\mathcal E}_{\theta,x},\lambda^{*,\mathcal I}_{\theta,x})$ with probability $1$ as $N\rightarrow\infty$.  Also recall from the law of large numbers that the sampled approximation error $\max_{x\in\mathcal X,a\in\mathcal A}\|P(\cdot|x,a)-P_N(\cdot|x,a)\|_1\rightarrow 0$ almost surely as $N\rightarrow \infty$. Then we have the following error bound in the stage-wise cost approximation $\widehat{h}_{\theta;N}(x,a)$ and $\gamma-$visiting distribution $\pi_N(x,a)$.
\begin{lemma}\label{lem:tech_2}
There exists a constant $M_h>0$ such that
$
\max_{x\in\mathcal X,a\in\mathcal A}|{h}_\theta(x,a)-\lim_{N\rightarrow\infty}\widehat{h}_{\theta;N}(x,a)|\leq M_h\Delta.
$
\end{lemma}
\begin{proof}
First we can easily see that for any state $x\in\mathcal X$ and action $a\in\mathcal A$,
\[
\begin{split}
|\widehat{h}_{\theta;N}(x,a)-{h}_\theta(x,a)|\leq &M\sum_{i\in\mathcal I}\left|\lambda^{*,\mathcal I}_{\theta,x;N}(i)-{\lambda}^{*,\mathcal I}_{\theta,x}(i)\right|+M\sum_{e\in\mathcal E}\left|\lambda^{*,\mathcal E}_{\theta,x;N}(e)-{\lambda}^{*,\mathcal E}_{\theta,x}(e)\right|+\left|\lambda^{*,\mathcal P}_{\theta,x;N}-{\lambda}^{*,\mathcal P}_{\theta,x}\right|\\
&+\gamma\|V_\theta\|_\infty\|\xi^*_{\theta,x;N}-{\xi}^*_{\theta,x}\|_1+\gamma\|V_\theta-\Phi v_\theta^*\|_\infty\\
&+\gamma\|V_\theta\|_\infty\max\{\|\xi^*_{\theta,x;N}\|_\infty,\|{\xi}^*_{\theta,x}\|_\infty\}\|P(\cdot|x,a)-P_N(\cdot|x,a)\|_1.
\end{split}
\]
Note that at $N\rightarrow \infty$, $\|P(\cdot|x,a)-P_N(\cdot|x,a)\|_1\rightarrow 0$ with probability $1$. Both $\|\xi^*_{\theta;N}\|_\infty$ and $\|{\xi}^*_{\theta,x}\|_\infty$ are finite valued because $\mathcal U(P_\theta)$ and $\mathcal U(\pemp)$ are convex compact sets of real vectors.
Therefore, by noting that $\|V_\theta\|_\infty\leq C_{\max}/(1-\gamma)$ and applying Proposition \ref{prop:consistent} and \ref{prop:tech}, the proof of this Lemma is completed by letting $N\rightarrow \infty$ and defining
\[
\begin{split}
M_h(x)=&\max\{1,M,\frac{\gamma C_{\max}}{1-\gamma}\}\left\|\begin{bmatrix}
\xi^*_{\theta,x;N}-\bar{\xi}^*_{\theta,x;N}\\
\lambda^{*,\mathcal I}_{\theta,x;N}-\bar{\lambda}^{*,\mathcal I}_{\theta,x;N}\\
\lambda^{*,\mathcal P}_{\theta,x;N}-\bar{\lambda}^{*,\mathcal P}_{\theta,x;N}\\
\lambda^{*,\mathcal E}_{\theta,x;N}-\bar{\lambda}^{*,\mathcal E}_{\theta,x;N}\\
\end{bmatrix}+\begin{bmatrix}
\bar\xi^*_{\theta,x;N}-{\xi}^*_{\theta,x}\\
\bar\lambda^{*,\mathcal I}_{\theta,x;N}-{\lambda}^{*,\mathcal I}_{\theta,x}\\
\bar\lambda^{*,\mathcal P}_{\theta,x;N}-{\lambda}^{*,\mathcal P}_{\theta,x}\\
\bar\lambda^{*,\mathcal E}_{\theta,x;N}-{\lambda}^{*,\mathcal E}_{\theta,x}\\
\end{bmatrix}\right\|_1+\gamma\Delta\\
\leq&\left(\max\{1,M,\frac{\gamma C_{\max}}{1-\gamma}\}\|\Phi_{\theta,x}^{-1}\Psi_{\theta,x}\|_1+\gamma\right)\Delta.
\end{split}
\]
\end{proof}
\begin{lemma}\label{lem:tech_3}
There exists a constant $M_\pi>0$ such that
$
\|\pi-\lim_{N\rightarrow\infty}\pi_{N}\|_1\leq M_\pi\Delta.
$
\end{lemma}
\begin{proof}
First, recall that the $\gamma-$visiting distribution satisfies the following identity:
\begin{equation}\label{eq:feas_1}
\gamma\sum_{x^\prime\in \mathcal{X}}d_{{P}^\xi_{\theta}}(x'|x)
P^\xi_{\theta}(x|x^\prime)
=d_{{P}^\xi_{\theta}}(x)-(1-\gamma)\mathbf 1\{x_0=x\},
\end{equation}
From here one easily notice this expression can be rewritten as follows:
\[
\left(I-\gamma  P^\xi_\theta\right)^\top d_{{P}^\xi_{\theta}}(\cdot|x)=\mathbf 1\{x_0=x\},\,\,\forall {x\in\mathcal X}.
\]
On the other hand, by repeating the analysis with $P_{\theta;N}(\cdot|x)$, we can also write
\[
\left(I-\gamma P^\xi_{\theta;N}\right)^\top d_{{P}^\xi_{\theta;N}}=\{\mathbf 1\{x_0=z\}\}_{z\in\mathcal X}.
\]
Combining the above expressions implies for any $x\in\mathcal X$,
\[
d_{{P}^\xi_{\theta}}-d_{{P}^\xi_{\theta;N}}-\gamma \left(\left(P^\xi_\theta\right)^\top d_{{P}^\xi_{\theta}}-(P^\xi_{\theta;N})^\top d_{{P}^\xi_{\theta;N}}\right)=0,
\]
which further implies
\[
\begin{split}
& \left(I-\gamma  P^\xi_\theta\right)^\top\left(d_{{P}^\xi_{\theta}}-d_{{P}^\xi_{\theta;N}}\right)=\gamma \left(P^\xi_\theta -P^\xi_{\theta;N} \right)^\top d_{{P}^\xi_{\theta;N}}\\
\iff & \left(d_{{P}^\xi_{\theta}}-d_{{P}^\xi_{\theta;N}}\right)=\left(I-\gamma P^\xi_\theta\right)^{-\top}\gamma \left(P^\xi_\theta -P^\xi_{\theta;N} \right)^\top d_{{P}^\xi_{\theta;N}}.
\end{split}
\]
Notice that with transition probability matrix $  P^\xi_\theta(\cdot|x)$, we have $(I-\gamma P^\xi_\theta)^{-1}=\sum_{t=0}^\infty\left(\gamma  P^\xi_\theta\right)^k<\infty$. The series is summable because by Perron-Frobenius theorem, the maximum eigenvalue of $P^\xi_\theta$ is less than or equal to $1$ and $I-\gamma P^\xi_\theta$ is invertible. On the other hand, for every given $x_0\in\mathcal X$,
\[
\begin{split}
\left\{\left(P^\xi_\theta -P^\xi_{\theta;N} \right)^\top d_{{P}^\xi_{\theta;N}}\right\}(z')=&\sum_{x\in\mathcal X}\sum_{k=0}^\infty \gamma^k(1-\gamma)\mathbb P_{{P}^\xi_{\theta;N}}(x_k=x|x_0)\left( P^\xi_\theta(z'|x)-P^\xi_{\theta;N}(z'|x)\right),\,\forall z'\in\mathcal X\\
=&\mathbb E_{{P}^\xi_{\theta;N}}\left(\sum_{k=0}^\infty \gamma^k(1-\gamma)\left( P^\xi_\theta(z'|x_k)-P^\xi_{\theta;N}(z'|x_k)\right)|x_0\right),\,\forall z'\in\mathcal X\\
\leq&\mathbb E_{{P}^\xi_{\theta;N}}\left(\sum_{k=0}^\infty \gamma^k(1-\gamma)\left| P^\xi_\theta(z'|x_k)-P^\xi_{\theta;N}(z'|x_k)\right| |x_0\right),\,\forall z'\in\mathcal X\\
\doteq& \mathcal Q(z'),\,\forall z'\in\mathcal X.
\end{split}
\]
Note that every element in matrix $(I-\gamma P^\xi_\theta)^{-1}=\sum_{t=0}^\infty\left(\gamma  P^\xi_\theta\right)^k$ is non-negative. This implies for any $z\in\mathcal X$,
\[
\begin{split}
\left|\left\{d_{{P}^\xi_{\theta}}-d_{{P}^\xi_{\theta;N}}\right\}(z)\right|=&\left|\left\{\left(I-\gamma P^\xi_\theta\right)^{-\top}\gamma \left(P^\xi_\theta -P^\xi_{\theta;N} \right)^\top d_{{P}^\xi_{\theta;N}}\right\}(z)\right|,\\
\leq &\left|\left\{\left(I-\gamma P^\xi_\theta\right)^{-\top}\gamma \mathcal Q\right\}(z)\right|=\left\{\left(I-\gamma P^\xi_\theta\right)^{-\top}\gamma \mathcal Q\right\}(z).
\end{split}
\]
The last equality is due to the fact that every element in vector $ \mathcal Q$ is non-negative. Combining the above results with Proposition \ref{prop:consistent} and \ref{prop:tech}, and noting that
\[
(I-\gamma P^\xi_\theta)^{-1}e=\sum_{t=0}^\infty\left(\gamma  P^\xi_\theta\right)^ke=\frac{1}{1-\gamma}e,
\]
we further have that
\[
\begin{split}
\|\pi-\pi_N\|_1=&\|d_{{P}^\xi_{\theta}}-d_{{P}^\xi_{\theta;N}}\|_1\\
\leq& e^\top\left(I-\gamma P^\xi_\theta\right)^{-\top}\gamma \mathcal Q\\
=&\frac{\gamma}{1-\gamma}e^\top \mathcal Q\\
\leq&\frac{\gamma}{1-\gamma}\max_{x\in\mathcal X}\left\| P^\xi_\theta(\cdot|x)-P^\xi_{\theta;N}(\cdot|x)\right\| _1\\
\leq & \frac{\gamma}{1-\gamma}\max_{x\in\mathcal X}\left(\|\xi^*_{\theta,x}(\cdot)-\xi^*_{\theta,x;N}(\cdot)\|_1\|P_\theta(\cdot|x)\|_\infty+\max\{\|\xi^*_{\theta,x;N}\|_\infty,\|{\xi}^*_{\theta,x}\|_\infty\}\|P(\cdot|x,a)-P_N(\cdot|x,a)\|_1\right),
\end{split}
\]
As in previous arguments, when $N\rightarrow \infty$, one obtains $\|P(\cdot|x,a)-P_N(\cdot|x,a)\|_1\rightarrow 0$ with probability $1$ and $\|\xi^*_{\theta,x}(\cdot)-\xi^*_{\theta,x;N}(\cdot)\|_1\rightarrow 0$. We thus set the constant $M_\pi$ as $ \gamma\|\Phi_{\theta,x}^{-1}\Psi_{\theta,x}\|_1/(1-\gamma)$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Experiments}
%% American options
%
%We empirically evaluate our algorithms on the American put option domain: a standard test-bed for risk-sensitive RL \cite{li_learning_2009,tamar2014robust,chow2014cvar}. In our setting, the state is continuous and represents the price of some stock. It evolves according to a geometric Brownian motion (GBM), i.e., $x_{t+1} / x_t \sim \ln \mathcal{N} \left( \mu_t - \sigma_t^2 / 2, \sigma_t^2\right)$, where $\ln \mathcal{N}$ is the log-normal distribution and $\mu_t$ and $\sigma_t$ are parameters. The action at each time $t$ is binary. An execution action generates reward $\max\{ 0, K - x_t\}$, where $K$ is fixed and known as the \emph{strike price}, and terminates the episode; A hold action generates zero reward, and the price transitions to $x_{t+1}$ as described above. Unless an execution occurred, the episode ends after $T$ steps, with reward $\max\{ 0, K - x_T\}$. For the expected return, the optimal policy is a time dependent threshold policy that holds if $x_t > \theta_t$ \cite{john2006options}, where $\theta_t$ is the threshold, and executes otherwise. Accordingly, we search in the space of soft-threshold policies of the form $\mu_\theta(hold|x_t)=\frac{1}{1+\exp(-\beta(x_t - \theta_t))}$, for some softness parameter $\beta>0$.
%
%We consider a case where the option is `deep in the money', that is, $x_0 < K$. For such a case, the decision maker may execute immediately, and earn reward $K - x_0$, but may also wait for a better price, with the risk of never getting it on time.
%
%We trained policies to optimize the static CVaR($\alpha=0.3$), CVaR($\alpha=0.6$), $0.9 \cdot Expectation + 0.1 \cdot CVaR(\alpha=0.3)$, and $Expectation - 0.1 \cdot Semideviation$, using stochastic gradient descent. The gradient for the expectation was calculated using standard (episodic) policy gradient \cite{MarTsi98}, the gradient for CVaR was calculated according to the GCVaR algorithm of \citet{tamar2015optimizing}, and the gradient for mean-semideviation was calculated using our \texttt{GMSD} algorithm of Section \ref{sec:MSD_supp}. In Figure \ref{fig:static_results} we plot the histograms of the payoff of the different policies. The risk-averse nature of the policies trained with a risk-sensitive objective may be observed. In our experiments we set $K=1$ and $x_0=0.5$. The CVaR($\alpha=0.3$) was very conservative, and chose to execute immediately, and receive a reward of $0.5$. The CVaR($\alpha=0.6$) was less conservative, but still had lower return variability than the standard expectation objective policy.
%
%We also trained policies to optimize the dynamic CVaR($\alpha=0.6$) risk, dynamic $0.95\cdot Expectation + 0.05 \cdot CVaR(\alpha=0.3)$ risk, and dynamic $0.98 \cdot Expectation + 0.02 \cdot CVaR(\alpha=0.3)$ risk using stochastic gradient descent, with the gradient calculated according to the algorithm of Section \ref{sec:dynamic}. We used RBF features for estimating the value function.
%%also code?
%In Figure \ref{fig:dynamic_results} we plot the histograms of the payoff of the different policies. The dynamic CVaR($\alpha=0.6$) was very conservative, and chose to execute immediately. This also occurred with higher values of $\alpha$, such as $\alpha=0.8$. One way to reduce conservatism in dynamic risk, while still maintaining control of reward variability, is to use the combination of expectation and CVaR. As Figure \ref{fig:dynamic_results} demonstrates, this is indeed a practical approach.
%\begin{figure*}
%\begin{center}
%\includegraphics[scale=0.5]{static_histogram1}
%\includegraphics[scale=0.5]{static_histogram2}
%\caption{\label{fig:static_results}Reward histogram for various \emph{static} risk-sensitive policies.}
%\end{center}
%\vskip -0.2in
%\end{figure*}
%
%\begin{figure*}
%\begin{center}
%\includegraphics[scale=0.5]{dynamic_histogram}
%\caption{\label{fig:dynamic_results}Reward histogram for various \emph{dynamic} risk-sensitive policies.}
%\end{center}
%\vskip -0.2in
%\end{figure*}


