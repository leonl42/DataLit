@article{DBLP:journals/corr/abs-1906-02589,
  author    = {Elliot Creager and
               David Madras and
               J{\"{o}}rn{-}Henrik Jacobsen and
               Marissa A. Weis and
               Kevin Swersky and
               Toniann Pitassi and
               Richard S. Zemel},
  title     = {Flexibly Fair Representation Learning by Disentanglement},
  journal   = {CoRR},
  volume    = {abs/1906.02589},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.02589},
  eprinttype = {arXiv},
  eprint    = {1906.02589},
  timestamp = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-02589.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{10.1145/3457607,
    author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
    title = {A Survey on Bias and Fairness in Machine Learning},
    year = {2021},
    issue_date = {July 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {54},
    number = {6},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3457607},
    doi = {10.1145/3457607},
    abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
    journal = {ACM Comput. Surv.},
    month = {jul},
    articleno = {115},
    numpages = {35},
    keywords = {natural language processing, deep learning, representation learning, machine learning, Fairness and bias in artificial intelligence}
}
@inproceedings{10.1145/2090236.2090255,
    author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
    title = {Fairness through Awareness},
    year = {2012},
    isbn = {9781450311151},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2090236.2090255},
    doi = {10.1145/2090236.2090255},
    abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
    booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
    pages = {214â€“226},
    numpages = {13},
    location = {Cambridge, Massachusetts},
    series = {ITCS '12}
}
@InProceedings{pmlr-v28-zemel13,
  title = 	 {Learning Fair Representations},
  author = 	 {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {325--333},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/zemel13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/zemel13.html},
  abstract = 	 {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}


@inproceedings{cer-etal-2018-universal,
    title = "Universal Sentence Encoder for {E}nglish",
    author = "Cer, Daniel  and
      Yang, Yinfei  and
      Kong, Sheng-yi  and
      Hua, Nan  and
      Limtiaco, Nicole  and
      St. John, Rhomni  and
      Constant, Noah  and
      Guajardo-Cespedes, Mario  and
      Yuan, Steve  and
      Tar, Chris  and
      Strope, Brian  and
      Kurzweil, Ray",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2029",
    doi = "10.18653/v1/D18-2029",
    pages = "169--174",
    abstract = "We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.",
}
@article{DBLP:journals/corr/abs-2103-00498,
  author    = {He Zhao and
               Dinh Phung and
               Viet Huynh and
               Yuan Jin and
               Lan Du and
               Wray L. Buntine},
  title     = {Topic Modelling Meets Deep Neural Networks: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2103.00498},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00498},
  eprinttype = {arXiv},
  eprint    = {2103.00498},
  timestamp = {Tue, 23 Nov 2021 15:18:13 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00498.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@article{journals/corr/KingmaB14,
  added-at = {2017-05-26T17:33:20.000+0200},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/23b0328784dbfce338ba0dd2618a7a059/leonie.reichert},
  ee = {http://arxiv.org/abs/1412.6980},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {3b0328784dbfce338ba0dd2618a7a059},
  journal = {CoRR},
  keywords = {final thema:draw},
  timestamp = {2017-06-22T12:45:55.000+0200},
  title = {Adam: A Method for Stochastic Optimization.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14},
  volume = {abs/1412.6980},
  year = 2014
}
@misc{shafahi2019adversarial,
      title={Adversarial Training for Free!}, 
      author={Ali Shafahi and Mahyar Najibi and Amin Ghiasi and Zheng Xu and John Dickerson and Christoph Studer and Larry S. Davis and Gavin Taylor and Tom Goldstein},
      year={2019},
      eprint={1904.12843},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NIPS2014_5ca3e9b1,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}

@inproceedings{edwards2016censoring,
    title = "Censoring Representations with an Adversary",
    abstract = "In practice, there are often explicit constraints on what representations or decisions are acceptable in an application of machine learning. For example it may be a legal requirement that a decision must not favour a particular group. Alternatively it can be that that representation of data must not have identifying information.We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic. This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable. We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing private information from images. We formulate the adversarial model as a minimax problem, and optimize that mini max objective using a stochastic gradient alternate min-max optimizer. We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically significant improvement across most cases. The flexibility of this method is shown via a novel problem: removing annotations from images, from separate training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model.",
    author = "Harrison Edwards and Amos Storkey",
    year = "2016",
    month = may,
    day = "4",
    language = "English",
    pages = "1--14",
    booktitle = "International Conference in Learning Representations (ICLR2016)",
    note = "4th International Conference on Learning Representations, ICLR 2016 ; Conference date: 02-05-2016 Through 04-05-2016",
    url = "https://iclr.cc/archive/www/doku.php%3Fid=iclr2016:main.html",
}


@inproceedings{
    yadav2018stabilizing,
    title={Stabilizing Adversarial Nets with Prediction Methods},
    author={Abhay Yadav and Sohil Shah and Zheng Xu and David Jacobs and Tom Goldstein},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=Skj8Kag0Z},
}
@InProceedings{pmlr-v70-arjovsky17a,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author =       {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {214--223},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/arjovsky17a.html},
  abstract = 	 {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.}
}
@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}
@InProceedings{pmlr-v80-madras18a,
  title = 	 {Learning Adversarially Fair and Transferable Representations},
  author =       {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3384--3393},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/madras18a/madras18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/madras18a.html},
  abstract = 	 {In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.}
}
@misc{kenna2021using,
      title={Using Adversarial Debiasing to Remove Bias from Word Embeddings}, 
      author={Dana Kenna},
      year={2021},
      eprint={2107.10251},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}
@inproceedings{DBLP:conf/fat/SweeneyN20,
  author    = {Chris Sweeney and
               Maryam Najafian},
  editor    = {Mireille Hildebrandt and
               Carlos Castillo and
               L. Elisa Celis and
               Salvatore Ruggieri and
               Linnet Taylor and
               Gabriela Zanfir{-}Fortuna},
  title     = {Reducing sentiment polarity for demographic attributes in word embeddings
               using adversarial learning},
  booktitle = {FAT* '20: Conference on Fairness, Accountability, and Transparency,
               Barcelona, Spain, January 27-30, 2020},
  pages     = {359--368},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3351095.3372837},
  doi       = {10.1145/3351095.3372837},
  timestamp = {Thu, 26 Aug 2021 22:19:24 +0200},
  biburl    = {https://dblp.org/rec/conf/fat/SweeneyN20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1801-07593,
  author    = {Brian Hu Zhang and
               Blake Lemoine and
               Margaret Mitchell},
  title     = {Mitigating Unwanted Biases with Adversarial Learning},
  journal   = {CoRR},
  volume    = {abs/1801.07593},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.07593},
  eprinttype = {arXiv},
  eprint    = {1801.07593},
  timestamp = {Mon, 13 Aug 2018 16:46:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-07593.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Feng2019LearningFR,
  title={Learning Fair Representations via an Adversarial Framework},
  author={Rui Feng and Yang Yang and Yuehan Lyu and Chenhao Tan and Yizhou Sun and Chunping Wang},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.13341}
}

@inproceedings{DBLP:journals/corr/LouizosSLWZ15,
  author    = {Christos Louizos and
               Kevin Swersky and
               Yujia Li and
               Max Welling and
               Richard S. Zemel},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {The Variational Fair Autoencoder},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.00830},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LouizosSLWZ15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{kenfack2021adversarial,
      title={Adversarial Stacked Auto-Encoders for Fair Representation Learning}, 
      author={Patrik Joslin Kenfack and Adil Mehmood Khan and Rasheed Hussain and S. M. Ahsan Kazmi},
      year={2021},
      eprint={2107.12826},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}