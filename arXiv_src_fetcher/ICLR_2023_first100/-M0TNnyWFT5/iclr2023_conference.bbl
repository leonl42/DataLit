\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arani et~al.(2022)Arani, Sarfraz, and Zonooz]{arani2022learning}
Elahe Arani, Fahad Sarfraz, and Bahram Zonooz.
\newblock Learning fast, learning slow: A general continual learning method
  based on complementary learning system.
\newblock \emph{arXiv preprint arXiv:2201.12604}, 2022.

\bibitem[Baars(1994)]{baars1994global}
Bernard~J Baars.
\newblock A global workspace theory of conscious experience.
\newblock \emph{Consciousness in philosophy and cognitive neuroscience}, pp.\
  149--171, 1994.

\bibitem[Baars(2005)]{baars2005global}
Bernard~J Baars.
\newblock Global workspace theory of consciousness: toward a cognitive
  neuroscience of human experience.
\newblock \emph{Progress in brain research}, 150:\penalty0 45--53, 2005.

\bibitem[Baars et~al.(2021)Baars, Geld, and Kozma]{baars2021global}
Bernard~J Baars, Natalie Geld, and Robert Kozma.
\newblock Global workspace theory (gwt) and prefrontal cortex: Recent
  developments.
\newblock \emph{Frontiers in Psychology}, pp.\  5163, 2021.

\bibitem[Baldock et~al.(2021)Baldock, Maennel, and Neyshabur]{baldock2021deep}
Robert Baldock, Hartmut Maennel, and Behnam Neyshabur.
\newblock Deep learning through the lens of example difficulty.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 10876--10889, 2021.

\bibitem[Bengio(2017)]{bengio2017consciousness}
Yoshua Bengio.
\newblock The consciousness prior.
\newblock \emph{arXiv preprint arXiv:1709.08568}, 2017.

\bibitem[Benjamin et~al.(2018)Benjamin, Rolnick, and
  Kording]{benjamin2018measuring}
Ari Benjamin, David Rolnick, and Konrad Kording.
\newblock Measuring and regularizing networks in function space.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Bhat et~al.(2022{\natexlab{a}})Bhat, Zonooz, and
  Arani]{bhat2022consistency}
Prashant~Shivaram Bhat, Bahram Zonooz, and Elahe Arani.
\newblock Consistency is the key to further mitigating catastrophic forgetting
  in continual learning.
\newblock In Sarath Chandar, Razvan Pascanu, and Doina Precup (eds.),
  \emph{Proceedings of The 1st Conference on Lifelong Learning Agents}, volume
  199 of \emph{Proceedings of Machine Learning Research}, pp.\  1195--1212.
  PMLR, 22--24 Aug 2022{\natexlab{a}}.

\bibitem[Bhat et~al.(2022{\natexlab{b}})Bhat, Zonooz, and
  Arani]{pmlr-v199-bhat22a}
Prashant~Shivaram Bhat, Bahram Zonooz, and Elahe Arani.
\newblock Task agnostic representation consolidation: a self-supervised based
  continual learning approach.
\newblock In Sarath Chandar, Razvan Pascanu, and Doina Precup (eds.),
  \emph{Proceedings of The 1st Conference on Lifelong Learning Agents}, volume
  199 of \emph{Proceedings of Machine Learning Research}, pp.\  390--405. PMLR,
  22--24 Aug 2022{\natexlab{b}}.

\bibitem[Bremner et~al.(2012)Bremner, Lewkowicz, and
  Spence]{bremner2012multisensory}
Andrew~J Bremner, David~J Lewkowicz, and Charles Spence.
\newblock \emph{Multisensory development}.
\newblock Oxford University Press, 2012.

\bibitem[Buzzega et~al.(2020)Buzzega, Boschini, Porrello, Abati, and
  Calderara]{buzzega2020dark}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone
  Calderara.
\newblock Dark experience for general continual learning: a strong, simple
  baseline.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 15920--15930, 2020.

\bibitem[Caccia et~al.(2021{\natexlab{a}})Caccia, Aljundi, Asadi, Tuytelaars,
  Pineau, and Belilovsky]{caccia2021new}
Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and
  Eugene Belilovsky.
\newblock New insights on reducing abrupt representation change in online
  continual learning.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Caccia et~al.(2021{\natexlab{b}})Caccia, Aljundi, Asadi, Tuytelaars,
  Pineau, and Belilovsky]{caccia2021reducing}
Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and
  Eugene Belilovsky.
\newblock Reducing representation drift in online continual learning.
\newblock \emph{arXiv preprint arXiv:2104.05025}, 2021{\natexlab{b}}.

\bibitem[Cha et~al.(2021)Cha, Lee, and Shin]{cha2021co2l}
Hyuntak Cha, Jaeho Lee, and Jinwoo Shin.
\newblock Co2l: Contrastive continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  9516--9525, 2021.

\bibitem[Dehaene et~al.(1998)Dehaene, Kerszberg, and
  Changeux]{dehaene1998neuronal}
Stanislas Dehaene, Michel Kerszberg, and Jean-Pierre Changeux.
\newblock A neuronal model of a global workspace in effortful cognitive tasks.
\newblock \emph{Proceedings of the national Academy of Sciences}, 95\penalty0
  (24):\penalty0 14529--14534, 1998.

\bibitem[Dehaene et~al.(2003)Dehaene, Sergent, and
  Changeux]{dehaene2003neuronal}
Stanislas Dehaene, Claire Sergent, and Jean-Pierre Changeux.
\newblock A neuronal network model linking subjective reports and objective
  physiological data during conscious perception.
\newblock \emph{Proceedings of the National Academy of Sciences}, 100\penalty0
  (14):\penalty0 8520--8525, 2003.

\bibitem[French(1999)]{french1999catastrophic}
Robert~M French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock \emph{Trends in cognitive sciences}, 3\penalty0 (4):\penalty0
  128--135, 1999.

\bibitem[Golkar et~al.(2019)Golkar, Kagan, and Cho]{golkar2019continual}
Siavash Golkar, Michael Kagan, and Kyunghyun Cho.
\newblock Continual learning via neural pruning.
\newblock \emph{arXiv preprint arXiv:1903.04476}, 2019.

\bibitem[Goyal \& Bengio(2020)Goyal and Bengio]{goyal2020inductive}
Anirudh Goyal and Yoshua Bengio.
\newblock Inductive biases for deep learning of higher-level cognition.
\newblock \emph{arXiv preprint arXiv:2011.15091}, 2020.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1321--1330. PMLR, 2017.

\bibitem[Gurbuz \& Dovrolis(2022)Gurbuz and Dovrolis]{gurbuz2022nispa}
Mustafa~B Gurbuz and Constantine Dovrolis.
\newblock Nispa: Neuro-inspired stability-plasticity adaptation for continual
  learning in sparse networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8157--8174. PMLR, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hinton \& Salakhutdinov(2006)Hinton and
  Salakhutdinov]{hinton2006reducing}
Geoffrey~E Hinton and Ruslan~R Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock \emph{science}, 313\penalty0 (5786):\penalty0 504--507, 2006.

\bibitem[Hou et~al.(2019)Hou, Pan, Loy, Wang, and Lin]{hou2019learning}
Saihui Hou, Xinyu Pan, Chen~Change Loy, Zilei Wang, and Dahua Lin.
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  831--839, 2019.

\bibitem[Hung et~al.(2019{\natexlab{a}})Hung, Tu, Wu, Chen, Chan, and
  Chen]{hung2019compacting}
Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and
  Chu-Song Chen.
\newblock Compacting, picking and growing for unforgetting continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{a}}.

\bibitem[Hung et~al.(2019{\natexlab{b}})Hung, Lee, Wan, Chen, Chan, and
  Chen]{hung2019increasingly}
Steven~CY Hung, Jia-Hong Lee, Timmy~ST Wan, Chein-Hung Chen, Yi-Ming Chan, and
  Chu-Song Chen.
\newblock Increasingly packing multiple facial-informatics modules in a unified
  deep-learning model via lifelong learning.
\newblock In \emph{Proceedings of the 2019 on International Conference on
  Multimedia Retrieval}, pp.\  339--343, 2019{\natexlab{b}}.

\bibitem[Juliani et~al.(2022)Juliani, Arulkumaran, Sasai, and
  Kanai]{juliani2022link}
Arthur Juliani, Kai Arulkumaran, Shuntaro Sasai, and Ryota Kanai.
\newblock On the link between conscious function and general intelligence in
  humans and machines.
\newblock \emph{arXiv preprint arXiv:2204.05133}, 2022.

\bibitem[Krishnan et~al.(2019)Krishnan, Tadros, Ramyaa, and
  Bazhenov]{krishnan2019biologically}
Giri~P Krishnan, Timothy Tadros, Ramyaa Ramyaa, and Maxim Bazhenov.
\newblock Biologically inspired sleep algorithm for artificial neural networks.
\newblock \emph{arXiv preprint arXiv:1908.02240}, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kudithipudi et~al.(2022)Kudithipudi, Aguilar-Simon, Babb, Bazhenov,
  Blackiston, Bongard, Brna, Chakravarthi~Raja, Cheney, Clune,
  et~al.]{kudithipudi2022biological}
Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov,
  Douglas Blackiston, Josh Bongard, Andrew~P Brna, Suraj Chakravarthi~Raja,
  Nick Cheney, Jeff Clune, et~al.
\newblock Biological underpinnings for lifelong learning machines.
\newblock \emph{Nature Machine Intelligence}, 4\penalty0 (3):\penalty0
  196--210, 2022.

\bibitem[Kuppers et~al.(2020)Kuppers, Kronenberger, Shantia, and
  Haselhoff]{kuppers2020multivariate}
Fabian Kuppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff.
\newblock Multivariate confidence calibration for object detection.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pp.\  326--327, 2020.

\bibitem[Le \& Yang(2015)Le and Yang]{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[Li \& Hoiem(2017)Li and Hoiem]{li2017learning}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (12):\penalty0 2935--2947, 2017.

\bibitem[Luo et~al.(2018)Luo, Zhan, Xue, Wang, Ren, and Yang]{luo2018cosine}
Chunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang, Rui Ren, and Qiang Yang.
\newblock Cosine normalization: Using cosine similarity instead of dot product
  in neural networks.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  382--391. Springer, 2018.

\bibitem[Mallya \& Lazebnik(2018)Mallya and Lazebnik]{mallya2018packnet}
Arun Mallya and Svetlana Lazebnik.
\newblock Packnet: Adding multiple tasks to a single network by iterative
  pruning.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pp.\  7765--7773, 2018.

\bibitem[Maltoni \& Lomonaco(2019)Maltoni and Lomonaco]{maltoni2019continuous}
Davide Maltoni and Vincenzo Lomonaco.
\newblock Continuous learning in single-incremental-task scenarios.
\newblock \emph{Neural Networks}, 116:\penalty0 56--73, 2019.

\bibitem[Mante et~al.(2013)Mante, Sussillo, Shenoy, and
  Newsome]{mante2013context}
Valerio Mante, David Sussillo, Krishna~V Shenoy, and William~T Newsome.
\newblock Context-dependent computation by recurrent dynamics in prefrontal
  cortex.
\newblock \emph{nature}, 503\penalty0 (7474):\penalty0 78--84, 2013.

\bibitem[Mashour et~al.(2020)Mashour, Roelfsema, Changeux, and
  Dehaene]{MASHOUR2020776}
George~A. Mashour, Pieter Roelfsema, Jean-Pierre Changeux, and Stanislas
  Dehaene.
\newblock Conscious processing and the global neuronal workspace hypothesis.
\newblock \emph{Neuron}, 105\penalty0 (5):\penalty0 776--798, 2020.
\newblock ISSN 0896-6273.

\bibitem[Mendez \& Eaton(2020)Mendez and Eaton]{mendez2020lifelong}
Jorge~A Mendez and Eric Eaton.
\newblock Lifelong learning of compositional structures.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Mermillod et~al.(2013)Mermillod, Bugaiska, and
  Bonin]{mermillod2013stability}
Martial Mermillod, Aur{\'e}lia Bugaiska, and Patrick Bonin.
\newblock The stability-plasticity dilemma: Investigating the continuum from
  catastrophic forgetting to age-limited learning effects, 2013.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter]{parisi2019continual}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural Networks}, 113:\penalty0 54--71, 2019.

\bibitem[Peng et~al.(2021)Peng, Tang, Jiang, Li, Lei, Lin, and
  Li]{peng2021overcoming}
Jian Peng, Bo~Tang, Hao Jiang, Zhuo Li, Yinjie Lei, Tao Lin, and Haifeng Li.
\newblock Overcoming long-term catastrophic forgetting through adversarial
  neural pruning and synaptic consolidation.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2021.

\bibitem[Ratcliff(1990)]{ratcliff1990connectionist}
Roger Ratcliff.
\newblock Connectionist models of recognition memory: constraints imposed by
  learning and forgetting functions.
\newblock \emph{Psychological review}, 97\penalty0 (2):\penalty0 285, 1990.

\bibitem[Robins(1995)]{robins1995catastrophic}
Anthony Robins.
\newblock Catastrophic forgetting, rehearsal and pseudorehearsal.
\newblock \emph{Connection Science}, 7\penalty0 (2):\penalty0 123--146, 1995.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Singh et~al.(2020)Singh, Verma, Mazumder, Carin, and
  Rai]{NEURIPS2020_b3b43aee}
Pravendra Singh, Vinay~Kumar Verma, Pratik Mazumder, Lawrence Carin, and Piyush
  Rai.
\newblock Calibrating cnns for lifelong learning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  15579--15590. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf}.

\bibitem[Stephenson et~al.(2020)Stephenson, Ganesh, Hui, Tang, Chung,
  et~al.]{stephenson2020geometry}
Cory Stephenson, Abhinav Ganesh, Yue Hui, Hanlin Tang, SueYeon Chung, et~al.
\newblock On the geometry of generalization and memorization in deep neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Veniat et~al.(2020)Veniat, Denoyer, and Ranzato]{veniat2020efficient}
Tom Veniat, Ludovic Denoyer, and MarcAurelio Ranzato.
\newblock Efficient continual learning with modular networks and task-driven
  priors.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Vitter(1985)]{vitter1985random}
Jeffrey~S Vitter.
\newblock Random sampling with a reservoir.
\newblock \emph{ACM Transactions on Mathematical Software (TOMS)}, 11\penalty0
  (1):\penalty0 37--57, 1985.

\bibitem[Wang et~al.(2022)Wang, Liu, Duan, and Tao]{wang2022continual}
Zhen Wang, Liu Liu, Yiqun Duan, and Dacheng Tao.
\newblock Continual learning through retrieval and imagination.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, volume~8, 2022.

\bibitem[Yoon et~al.(2018)Yoon, Yang, Lee, and Hwang]{yoon2018lifelong}
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Shen, Huang, and Deng]{zhang2020self}
Song Zhang, Gehui Shen, Jinsong Huang, and Zhi-Hong Deng.
\newblock Self-supervised learning aided class-incremental lifelong learning.
\newblock \emph{arXiv preprint arXiv:2006.05882}, 2020.

\end{thebibliography}
