\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdelfattah et~al.(2021)Abdelfattah, Mehrotra, Dudziak, and
  Lane]{zeroproxy}
Mohamed~S Abdelfattah, Abhinav Mehrotra, {\L}ukasz Dudziak, and Nicholas~D
  Lane.
\newblock Zero-cost proxies for lightweight nas.
\newblock \emph{arXiv preprint arXiv:2101.08134}, 2021.

\bibitem[Alizadeh et~al.(2022)Alizadeh, Tailor, Zintgraf, van Amersfoort,
  Farquhar, Lane, and Gal]{alizadeh2022prospect}
Milad Alizadeh, Shyam~A Tailor, Luisa~M Zintgraf, Joost van Amersfoort,
  Sebastian Farquhar, Nicholas~Donald Lane, and Yarin Gal.
\newblock Prospect pruning: Finding trainable weights at initialization using
  meta-gradients.
\newblock \emph{arXiv preprint arXiv:2202.08132}, 2022.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Bellec et~al.(2017)Bellec, Kappel, Maass, and
  Legenstein]{bellec2017deep}
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock \emph{arXiv preprint arXiv:1711.05136}, 2017.

\bibitem[Chen et~al.(2021)Chen, Gong, and Wang]{tenas}
Wuyang Chen, Xinyu Gong, and Zhangyang Wang.
\newblock Neural architecture search on imagenet in four gpu hours: A
  theoretically inspired perspective.
\newblock \emph{arXiv preprint arXiv:2102.11535}, 2021.

\bibitem[Dai et~al.(2018)Dai, Zhu, Guo, and Wipf]{dai2018compressing}
Bin Dai, Chen Zhu, Baining Guo, and David Wipf.
\newblock Compressing neural networks using the variational information
  bottleneck.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1135--1144. PMLR, 2018.

\bibitem[de~Jorge et~al.(2020{\natexlab{a}})de~Jorge, Sanyal, Behl, Torr,
  Rogez, and Dokania]{de2020progressive}
Pau de~Jorge, Amartya Sanyal, Harkirat~S Behl, Philip~HS Torr, Gregory Rogez,
  and Puneet~K Dokania.
\newblock Progressive skeletonization: Trimming more fat from a network at
  initialization.
\newblock \emph{arXiv preprint arXiv:2006.09081}, 2020{\natexlab{a}}.

\bibitem[de~Jorge et~al.(2020{\natexlab{b}})de~Jorge, Sanyal, Behl, Torr,
  Rogez, and Dokania]{force}
Pau de~Jorge, Amartya Sanyal, Harkirat~S Behl, Philip~HS Torr, Gregory Rogez,
  and Puneet~K Dokania.
\newblock Progressive skeletonization: Trimming more fat from a network at
  initialization.
\newblock \emph{arXiv preprint arXiv:2006.09081}, 2020{\natexlab{b}}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dettmers \& Zettlemoyer(2019)Dettmers and
  Zettlemoyer]{dettmers2019sparse}
Tim Dettmers and Luke Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock \emph{arXiv preprint arXiv:1907.04840}, 2019.

\bibitem[Ding et~al.(2022)Ding, Li, and Sun]{ding2022suboptimal}
Tian Ding, Dawei Li, and Ruoyu Sun.
\newblock Suboptimal local minima exist for wide neural networks with smooth
  activations.
\newblock \emph{Mathematics of Operations Research}, 47\penalty0 (4):\penalty0
  2784--2814, 2022.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017learning}
Xin Dong, Shangyu Chen, and Sinno Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Evci et~al.(2020)Evci, Gale, Menick, Castro, and
  Elsen]{evci2020rigging}
Utku Evci, Trevor Gale, Jacob Menick, Pablo~Samuel Castro, and Erich Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2943--2952. PMLR, 2020.

\bibitem[Evci et~al.(2022)Evci, Ioannou, Keskin, and Dauphin]{evci2022gradient}
Utku Evci, Yani Ioannou, Cem Keskin, and Yann Dauphin.
\newblock Gradient flow in sparse neural networks and how lottery tickets win.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  6577--6586, 2022.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{LTH}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Frankle et~al.(2020{\natexlab{a}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3259--3269. PMLR, 2020{\natexlab{a}}.

\bibitem[Frankle et~al.(2020{\natexlab{b}})Frankle, Dziugaite, Roy, and
  Carbin]{franklemissing}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock \emph{arXiv preprint arXiv:2009.08576}, 2020{\natexlab{b}}.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Yiwen Guo, Anbang Yao, and Yurong Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Hassibi \& Stork(1992)Hassibi and Stork]{OBS}
Babak Hassibi and David Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock \emph{Advances in neural information processing systems}, 5, 1992.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, X.~Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2016.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018ntk}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Janowsky(1989)]{janowsky1989pruning}
Steven~A Janowsky.
\newblock Pruning versus clipping in neural networks.
\newblock \emph{Physical Review A}, 39\penalty0 (12):\penalty0 6600, 1989.

\bibitem[Kang \& Han(2020)Kang and Han]{kang2020operation}
Minsoo Kang and Bohyung Han.
\newblock Operation-aware soft channel pruning using differentiable masks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5122--5131. PMLR, 2020.

\bibitem[Kindratenko et~al.(2020)Kindratenko, Mu, Zhan, Maloney, Hashemi, Rabe,
  Xu, Campbell, Peng, and Gropp]{kindratenko2020hal}
Volodymyr Kindratenko, Dawei Mu, Yan Zhan, John Maloney, Sayed~Hadi Hashemi,
  Benjamin Rabe, Ke~Xu, Roy Campbell, Jian Peng, and William Gropp.
\newblock Hal: Computer system for scalable deep learning.
\newblock In \emph{Practice and experience in advanced research computing},
  pp.\  41--48. 2020.

\bibitem[Kopitkov \& Indelman(2020)Kopitkov and Indelman]{kopitkov2020neural}
Dmitry Kopitkov and Vadim Indelman.
\newblock Neural spectrum alignment: Empirical study.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  168--179. Springer, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar10}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kusupati et~al.(2020)Kusupati, Ramanujan, Somani, Wortsman, Jain,
  Kakade, and Farhadi]{kusupati2020soft}
Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek
  Jain, Sham Kakade, and Ali Farhadi.
\newblock Soft threshold weight reparameterization for learnable sparsity.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5544--5555. PMLR, 2020.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{OBD}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock \emph{Advances in neural information processing systems}, 2, 1989.

\bibitem[Lee et~al.(2019{\natexlab{a}})Lee, Xiao, Schoenholz, Bahri, Novak,
  Sohl-Dickstein, and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Advances in neural information processing systems}, 32,
  2019{\natexlab{a}}.

\bibitem[Lee et~al.(2018)Lee, Ajanthan, and Torr]{snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock \emph{arXiv preprint arXiv:1810.02340}, 2018.

\bibitem[Lee et~al.(2019{\natexlab{b}})Lee, Ajanthan, Gould, and
  Torr]{lee2019signal}
Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip~HS Torr.
\newblock A signal propagation perspective for pruning neural networks at
  initialization.
\newblock \emph{arXiv preprint arXiv:1906.06307}, 2019{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Ding, and Sun]{li2022benefit}
Dawei Li, Tian Ding, and Ruoyu Sun.
\newblock On the benefit of width for neural networks: Disappearance of basins.
\newblock \emph{SIAM Journal on Optimization}, 32\penalty0 (3):\penalty0
  1728--1758, 2022.

\bibitem[Liang et~al.(2018{\natexlab{a}})Liang, Sun, Lee, and
  Srikant]{liang2018adding}
Shiyu Liang, Ruoyu Sun, Jason~D Lee, and R~Srikant.
\newblock Adding one neuron can eliminate all bad local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4355--4365, 2018{\natexlab{a}}.

\bibitem[Liang et~al.(2018{\natexlab{b}})Liang, Sun, Li, and
  Srikant]{liang2018understanding}
Shiyu Liang, Ruoyu Sun, Yixuan Li, and Rayadurgam Srikant.
\newblock Understanding the loss surface of neural networks for binary
  classification.
\newblock In Jennifer Dy and Andreas Krause (eds.), \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2835--2843. PMLR,
  10--15 Jul 2018{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v80/liang18a.html}.

\bibitem[Liang et~al.(2019)Liang, Sun, and Srikant]{liang2019revisiting}
Shiyu Liang, Ruoyu Sun, and R.~Srikant.
\newblock Revisiting landscape analysis in deep neural networks: Eliminating
  decreasing paths to infinity, 2019.

\bibitem[Liang et~al.(2021)Liang, Sun, and Srikant]{liang2021ReLU}
Shiyu Liang, Ruoyu Sun, and R.~Srikant.
\newblock Achieving small test error in mildly overparameterized neural
  networks.
\newblock \emph{CoRR}, abs/2104.11895, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.11895}.

\bibitem[Lin et~al.(2021{\natexlab{a}})Lin, Sun, and Zhang]{lin2021faster}
Dachao Lin, Ruoyu Sun, and Zhihua Zhang.
\newblock Faster directional convergence of linear neural networks under
  spherically symmetric data.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Lin et~al.(2021{\natexlab{b}})Lin, Sun, and Zhang]{lin2021landscape}
Dachao Lin, Ruoyu Sun, and Zhihua Zhang.
\newblock On the landscape of one-hidden-layer sparse networks and beyond,
  2021{\natexlab{b}}.

\bibitem[Liu et~al.(2020)Liu, Xu, Shi, Cheung, and So]{liu2020dynamic}
Junjie Liu, Zhe Xu, Runbin Shi, Ray~CC Cheung, and Hayden~KH So.
\newblock Dynamic sparse training: Find efficient sparse network from scratch
  with trainable masked layers.
\newblock \emph{arXiv preprint arXiv:2005.06870}, 2020.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Chen, Chen, Atashgahi, Yin, Kou,
  Shen, Pechenizkiy, Wang, and Mocanu]{liu2021sparse}
Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu~Yin, Huanyu Kou,
  Li~Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal~Constantin Mocanu.
\newblock Sparse training via boosting pruning plasticity with
  neuroregeneration.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Mocanu, Pei, and
  Pechenizkiy]{liu2021selfish}
Shiwei Liu, Decebal~Constantin Mocanu, Yulong Pei, and Mykola Pechenizkiy.
\newblock Selfish sparse rnn training.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6893--6904. PMLR, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2021{\natexlab{c}})Liu, Yin, Mocanu, and
  Pechenizkiy]{liu2021we}
Shiwei Liu, Lu~Yin, Decebal~Constantin Mocanu, and Mykola Pechenizkiy.
\newblock Do we actually need dense over-parameterization? in-time
  over-parameterization in sparse training.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6989--7000. PMLR, 2021{\natexlab{c}}.

\bibitem[Liu \& Zenke(2020)Liu and Zenke]{NTT}
Tianlin Liu and Friedemann Zenke.
\newblock Finding trainable sparse networks through neural tangent transfer.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6336--6347. PMLR, 2020.

\bibitem[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock \emph{arXiv preprint arXiv:1810.05270}, 2018.

\bibitem[Louizos et~al.(2017)Louizos, Welling, and Kingma]{louizos2017learning}
Christos Louizos, Max Welling, and Diederik~P Kingma.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock \emph{arXiv preprint arXiv:1712.01312}, 2017.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{mocanu2018scalable}
Decebal~Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong~H Nguyen,
  Madeleine Gibescu, and Antonio Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature communications}, 9\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Molchanov et~al.(2019)Molchanov, Mallya, Tyree, Frosio, and
  Kautz]{molchanov2019importance}
Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz.
\newblock Importance estimation for neural network pruning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  11264--11272, 2019.

\bibitem[Mostafa \& Wang(2019)Mostafa and Wang]{mostafa2019parameter}
Hesham Mostafa and Xin Wang.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4646--4655. PMLR, 2019.

\bibitem[Mozer \& Smolensky(1989)Mozer and Smolensky]{mozer1989using}
Michael~C Mozer and Paul Smolensky.
\newblock Using relevance to reduce network size automatically.
\newblock \emph{Connection Science}, 1\penalty0 (1):\penalty0 3--16, 1989.

\bibitem[Pellegrini \& Biroli(2022)Pellegrini and Biroli]{pellegrini2022neural}
Franco Pellegrini and Giulio Biroli.
\newblock Neural network pruning denoises the features and makes local
  connectivity emerge in visual tasks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  17601--17626. PMLR, 2022.

\bibitem[Pennington et~al.(2017)Pennington, Schoenholz, and
  Ganguli]{pennington2017resurrecting}
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli.
\newblock Resurrecting the sigmoid in deep learning through dynamical isometry:
  theory and practice.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Pennington et~al.(2018)Pennington, Schoenholz, and
  Ganguli]{pennington2018emergence}
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli.
\newblock The emergence of spectral universality in deep networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1924--1932. PMLR, 2018.

\bibitem[Renda et~al.(2020)Renda, Frankle, and Carbin]{LTR}
Alex Renda, Jonathan Frankle, and Michael Carbin.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock \emph{arXiv preprint arXiv:2003.02389}, 2020.

\bibitem[Safran \& Shamir(2017)Safran and Shamir]{safran2017spurious}
Itay Safran and Ohad Shamir.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock \emph{arXiv preprint arXiv:1712.08968}, 2017.

\bibitem[Savarese et~al.(2020)Savarese, Silva, and Maire]{savarese2020winning}
Pedro Savarese, Hugo Silva, and Michael Maire.
\newblock Winning the lottery with continuous sparsification.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11380--11390, 2020.

\bibitem[Shu et~al.(2021)Shu, Cai, Dai, Ooi, and Low]{nasi}
Yao Shu, Shaofeng Cai, Zhongxiang Dai, Beng~Chin Ooi, and Bryan Kian~Hsiang
  Low.
\newblock Nasi: Label-and data-agnostic neural architecture search at
  initialization.
\newblock \emph{arXiv preprint arXiv:2109.00817}, 2021.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Soltanolkotabi et~al.(2019)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2019theoretical}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (2):\penalty0 742--769, 2019.

\bibitem[Srinivas et~al.(2017)Srinivas, Subramanya, and
  Venkatesh~Babu]{srinivas2017training}
Suraj Srinivas, Akshayvarun Subramanya, and R~Venkatesh~Babu.
\newblock Training sparse neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition workshops}, pp.\  138--145, 2017.

\bibitem[Su \& Yang(2019)Su and Yang]{su2019learning}
Lili Su and Pengkun Yang.
\newblock On learning over-parameterized neural networks: A functional
  approximation perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Sun(2020)]{sun2020optimization}
Ruo-Yu Sun.
\newblock Optimization for deep learning: An overview.
\newblock \emph{Journal of the Operations Research Society of China}, pp.\
  1--46, 2020.

\bibitem[Sun et~al.(2020{\natexlab{a}})Sun, Fang, and Schwing]{sun2020GAN}
Ruoyu Sun, Tiantian Fang, and Alexander Schwing.
\newblock Towards a better global loss landscape of gans.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  10186--10198. Curran Associates, Inc., 2020{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/738a6457be8432bab553e21b4235dd97-Paper.pdf}.

\bibitem[Sun et~al.(2020{\natexlab{b}})Sun, Li, Liang, Ding, and
  Srikant]{sun2020global}
Ruoyu Sun, Dawei Li, Shiyu Liang, Tian Ding, and Rayadurgam Srikant.
\newblock The global landscape of neural networks: An overview.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (5):\penalty0
  95--108, 2020{\natexlab{b}}.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and Ganguli]{synflow}
Hidenori Tanaka, Daniel Kunin, Daniel~L Yamins, and Surya Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6377--6389, 2020.

\bibitem[Venturi et~al.(2018)Venturi, Bandeira, and Bruna]{venturi2018spurious}
Luca Venturi, Afonso Bandeira, and Joan Bruna.
\newblock Spurious valleys in two-layer neural network optimization landscapes.
\newblock \emph{arXiv preprint arXiv:1802.06384}, 2018.

\bibitem[Verma \& Pesquet(2021)Verma and Pesquet]{verma2021sparsifying}
Sagar Verma and Jean-Christophe Pesquet.
\newblock Sparsifying networks via subdifferential inclusion.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10542--10552. PMLR, 2021.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Grosse]{grasp}
Chaoqi Wang, Guodong Zhang, and Roger Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock \emph{arXiv preprint arXiv:2002.07376}, 2020.

\bibitem[Wang et~al.(2022)Wang, Wang, Sun, and Li]{MetaNTK-NAS}
Haoxiang Wang, Yite Wang, Ruoyu Sun, and Bo~Li.
\newblock Global convergence of maml and theory-inspired neural architecture
  search for few-shot learning.
\newblock In \emph{CVPR}, 2022.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Xiao et~al.(2020)Xiao, Pennington, and
  Schoenholz]{xiao2020disentangling}
Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz.
\newblock Disentangling trainability and generalization in deep neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10462--10472. PMLR, 2020.

\bibitem[Xiao et~al.(2019)Xiao, Wang, and Rajasekaran]{xiao2019autoprune}
Xia Xiao, Zigeng Wang, and Sanguthevar Rajasekaran.
\newblock Autoprune: Automatic network pruning by regularizing auxiliary
  parameters.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[You et~al.(2019)You, Li, Xu, Fu, Wang, Chen, Baraniuk, Wang, and
  Lin]{you2019drawing}
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen,
  Richard~G Baraniuk, Zhangyang Wang, and Yingyan Lin.
\newblock Drawing early-bird tickets: Towards more efficient training of deep
  networks.
\newblock \emph{arXiv preprint arXiv:1909.11957}, 2019.

\bibitem[Yu et~al.(2018)Yu, Li, Chen, Lai, Morariu, Han, Gao, Lin, and
  Davis]{yu2018nisp}
Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad~I Morariu, Xintong Han,
  Mingfei Gao, Ching-Yung Lin, and Larry~S Davis.
\newblock Nisp: Pruning networks using neuron importance score propagation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  9194--9203, 2018.

\bibitem[Zhang et~al.(2021)Zhang, Zhang, Hong, Sun, and
  Luo]{zhang2021expressivity}
Jiawei Zhang, Yushun Zhang, Mingyi Hong, Ruoyu Sun, and Zhi-Quan Luo.
\newblock When expressivity meets trainability: Fewer than $ n $ neurons can
  work.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Xu, and Zhang]{zhou2021effective}
Xiao Zhou, Weizhong Zhang, Hang Xu, and Tong Zhang.
\newblock Effective sparsification of neural networks with global sparsity
  constraint.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  3599--3608, 2021.

\end{thebibliography}
