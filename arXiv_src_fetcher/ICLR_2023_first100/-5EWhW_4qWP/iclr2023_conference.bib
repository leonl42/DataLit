@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@article{franklemissing,
  title={Pruning neural networks at initialization: Why are we missing the mark?},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:2009.08576},
  year={2020}
}

@article{synflow,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6377--6389},
  year={2020}
}

@article{snip,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv preprint arXiv:1810.02340},
  year={2018}
}

@article{grasp,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  journal={arXiv preprint arXiv:2002.07376},
  year={2020}
}

@article{force,
  title={Progressive skeletonization: Trimming more fat from a network at initialization},
  author={de Jorge, Pau and Sanyal, Amartya and Behl, Harkirat S and Torr, Philip HS and Rogez, Gregory and Dokania, Puneet K},
  journal={arXiv preprint arXiv:2006.09081},
  year={2020}
}

@article{randomticket,
  title={Sanity-checking pruning methods: Random tickets can win the jackpot},
  author={Su, Jingtong and Chen, Yihang and Cai, Tianle and Wu, Tianhao and Gao, Ruiqi and Wang, Liwei and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20390--20401},
  year={2020}
}

@article{LTH,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{OBD,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{han2015,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{janowsky1989pruning,
  title={Pruning versus clipping in neural networks},
  author={Janowsky, Steven A},
  journal={Physical Review A},
  volume={39},
  number={12},
  pages={6600},
  year={1989},
  publisher={APS}
}

@article{mozer1989using,
  title={Using relevance to reduce network size automatically},
  author={Mozer, Michael C and Smolensky, Paul},
  journal={Connection Science},
  volume={1},
  number={1},
  pages={3--16},
  year={1989},
  publisher={Taylor \& Francis}
}

@article{OBS,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}

@inproceedings{dai2018compressing,
  title={Compressing neural networks using the variational information bottleneck},
  author={Dai, Bin and Zhu, Chen and Guo, Baining and Wipf, David},
  booktitle={International Conference on Machine Learning},
  pages={1135--1144},
  year={2018},
  organization={PMLR}
}

@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11264--11272},
  year={2019}
}

@article{guo2016dynamic,
  title={Dynamic network surgery for efficient dnns},
  author={Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{yu2018nisp,
  title={Nisp: Pruning networks using neuron importance score propagation},
  author={Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9194--9203},
  year={2018}
}

@article{narang2017exploring,
  title={Exploring sparsity in recurrent neural networks},
  author={Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  journal={arXiv preprint arXiv:1704.05119},
  year={2017}
}

@article{louizos2017learning,
  title={Learning sparse neural networks through $ L\_0 $ regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1712.01312},
  year={2017}
}

@article{wen2016learning,
  title={Learning structured sparsity in deep neural networks},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{liu2020dynamic,
  title={Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers},
  author={Liu, Junjie and Xu, Zhe and Shi, Runbin and Cheung, Ray CC and So, Hayden KH},
  journal={arXiv preprint arXiv:2005.06870},
  year={2020}
}

@article{savarese2020winning,
  title={Winning the lottery with continuous sparsification},
  author={Savarese, Pedro and Silva, Hugo and Maire, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11380--11390},
  year={2020}
}

@inproceedings{kang2020operation,
  title={Operation-aware soft channel pruning using differentiable masks},
  author={Kang, Minsoo and Han, Bohyung},
  booktitle={International Conference on Machine Learning},
  pages={5122--5131},
  year={2020},
  organization={PMLR}
}

@inproceedings{kusupati2020soft,
  title={Soft threshold weight reparameterization for learnable sparsity},
  author={Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  booktitle={International Conference on Machine Learning},
  pages={5544--5555},
  year={2020},
  organization={PMLR}
}

@inproceedings{zhou2021effective,
  title={Effective sparsification of neural networks with global sparsity constraint},
  author={Zhou, Xiao and Zhang, Weizhong and Xu, Hang and Zhang, Tong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3599--3608},
  year={2021}
}

@inproceedings{srinivas2017training,
  title={Training sparse neural networks},
  author={Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={138--145},
  year={2017}
}

@article{xiao2019autoprune,
  title={Autoprune: Automatic network pruning by regularizing auxiliary parameters},
  author={Xiao, Xia and Wang, Zigeng and Rajasekaran, Sanguthevar},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{bellec2017deep,
  title={Deep rewiring: Training very sparse deep networks},
  author={Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
  journal={arXiv preprint arXiv:1711.05136},
  year={2017}
}

@inproceedings{mostafa2019parameter,
  title={Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author={Mostafa, Hesham and Wang, Xin},
  booktitle={International Conference on Machine Learning},
  pages={4646--4655},
  year={2019},
  organization={PMLR}
}

@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@inproceedings{liu2021selfish,
  title={Selfish sparse rnn training},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Pei, Yulong and Pechenizkiy, Mykola},
  booktitle={International Conference on Machine Learning},
  pages={6893--6904},
  year={2021},
  organization={PMLR}
}

@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  pages={2943--2952},
  year={2020},
  organization={PMLR}
}

@inproceedings{liu2021we,
  title={Do we actually need dense over-parameterization? in-time over-parameterization in sparse training},
  author={Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  booktitle={International Conference on Machine Learning},
  pages={6989--7000},
  year={2021},
  organization={PMLR}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@article{de2020progressive,
  title={Progressive skeletonization: Trimming more fat from a network at initialization},
  author={de Jorge, Pau and Sanyal, Amartya and Behl, Harkirat S and Torr, Philip HS and Rogez, Gregory and Dokania, Puneet K},
  journal={arXiv preprint arXiv:2006.09081},
  year={2020}
}

@article{alizadeh2022prospect,
  title={Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients},
  author={Alizadeh, Milad and Tailor, Shyam A and Zintgraf, Luisa M and van Amersfoort, Joost and Farquhar, Sebastian and Lane, Nicholas Donald and Gal, Yarin},
  journal={arXiv preprint arXiv:2202.08132},
  year={2022}
}

@article{resnet,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}

@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@article{baker2016designing,
  title={Designing neural network architectures using reinforcement learning},
  author={Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
  journal={arXiv preprint arXiv:1611.02167},
  year={2016}
}

@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@inproceedings{real2019regularized,
  title={Regularized evolution for image classifier architecture search},
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={33},
  number={01},
  pages={4780--4789},
  year={2019}
}

@article{darts,
  title={Darts: Differentiable architecture search},
  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  journal={arXiv preprint arXiv:1806.09055},
  year={2018}
}

@inproceedings{chu2020fair,
  title={Fair darts: Eliminating unfair advantages in differentiable architecture search},
  author={Chu, Xiangxiang and Zhou, Tianbao and Zhang, Bo and Li, Jixiang},
  booktitle={European conference on computer vision},
  pages={465--480},
  year={2020},
  organization={Springer}
}

@article{chu2020darts,
  title={Darts-: robustly stepping out of performance collapse without indicators},
  author={Chu, Xiangxiang and Wang, Xiaoxing and Zhang, Bo and Lu, Shun and Wei, Xiaolin and Yan, Junchi},
  journal={arXiv preprint arXiv:2009.01027},
  year={2020}
}

@inproceedings{pham2018efficient, 
title={Efficient neural architecture search via parameters sharing}, 
author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff}, 
booktitle={International conference on machine learning}, 
pages={4095--4104}, 
year={2018}, 
organization={PMLR} }

@article{nasi,
  title={NASI: Label-and Data-agnostic Neural Architecture Search at Initialization},
  author={Shu, Yao and Cai, Shaofeng and Dai, Zhongxiang and Ooi, Beng Chin and Low, Bryan Kian Hsiang},
  journal={arXiv preprint arXiv:2109.00817},
  year={2021}
}

@article{tenas,
  title={Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective},
  author={Chen, Wuyang and Gong, Xinyu and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2102.11535},
  year={2021}
}

@inproceedings{KNAS,
  title={KNAS: green neural architecture search},
  author={Xu, Jingjing and Zhao, Liang and Lin, Junyang and Gao, Rundong and Sun, Xu and Yang, Hongxia},
  booktitle={International Conference on Machine Learning},
  pages={11613--11625},
  year={2021},
  organization={PMLR}
}

@inproceedings{mellor2021neural,
  title={Neural architecture search without training},
  author={Mellor, Joe and Turner, Jack and Storkey, Amos and Crowley, Elliot J},
  booktitle={International Conference on Machine Learning},
  pages={7588--7598},
  year={2021},
  organization={PMLR}
}

@article{zeroproxy,
  title={Zero-cost proxies for lightweight NAS},
  author={Abdelfattah, Mohamed S and Mehrotra, Abhinav and Dudziak, {\L}ukasz and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2101.08134},
  year={2021}
}

@inproceedings{MetaNTK-NAS,
  title={Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning},
  author={Wang, Haoxiang and Wang, Yite and Sun, Ruoyu and Li, Bo},
  booktitle={CVPR},
  year={2022}
}

@article{neyshabur2015path,
  title={Path-sgd: Path-normalized optimization in deep neural networks},
  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{zennas,
  title={Zen-nas: A zero-shot nas for high-performance deep image recognition},
  author={Lin, Ming and Wang, Pichao and Sun, Zhenhong and Chen, Hesen and Sun, Xiuyu and Qian, Qi and Li, Hao and Jin, Rong},
  journal={arXiv preprint arXiv:2102.01063},
  year={2021}
}

@article{lee2019signal,
  title={A signal propagation perspective for pruning neural networks at initialization},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Gould, Stephen and Torr, Philip HS},
  journal={arXiv preprint arXiv:1906.06307},
  year={2019}
}

@article{jacot2018ntk,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{xiao2020disentangling,
  title={Disentangling trainability and generalization in deep neural networks},
  author={Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel},
  booktitle={International Conference on Machine Learning},
  pages={10462--10472},
  year={2020},
  organization={PMLR}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}

@article{mok2022demystifying,
  title={Demystifying the Neural Tangent Kernel from a Practical Perspective: Can it be trusted for Neural Architecture Search without training?},
  author={Mok, Jisoo and Na, Byunggook and Kim, Ji-Hoon and Han, Dongyoon and Yoon, Sungroh},
  journal={arXiv preprint arXiv:2203.14577},
  year={2022}
}

@article{LTR,
  title={Comparing rewinding and fine-tuning in neural network pruning},
  author={Renda, Alex and Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:2003.02389},
  year={2020}
}

@article{fort2020deep,
  title={Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel},
  author={Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5850--5861},
  year={2020}
}

@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2736--2744},
  year={2017}
}

@inproceedings{huang2018data,
  title={Data-driven sparse structure selection for deep neural networks},
  author={Huang, Zehao and Wang, Naiyan},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={304--320},
  year={2018}
}

@inproceedings{luo2017thinet,
  title={Thinet: A filter level pruning method for deep neural network compression},
  author={Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5058--5066},
  year={2017}
}

@article{you2019drawing,
  title={Drawing early-bird tickets: Towards more efficient training of deep networks},
  author={You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G and Wang, Zhangyang and Lin, Yingyan},
  journal={arXiv preprint arXiv:1909.11957},
  year={2019}
}

@article{liu2021sparse,
  title={Sparse training via boosting pruning plasticity with neuroregeneration},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{cifar10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{NTT,
  title={Finding trainable sparse networks through Neural Tangent Transfer},
  author={Liu, Tianlin and Zenke, Friedemann},
  booktitle={International Conference on Machine Learning},
  pages={6336--6347},
  year={2020},
  organization={PMLR}
}

@inproceedings{verma2021sparsifying,
  title={Sparsifying networks via subdifferential inclusion},
  author={Verma, Sagar and Pesquet, Jean-Christophe},
  booktitle={International Conference on Machine Learning},
  pages={10542--10552},
  year={2021},
  organization={PMLR}
}

@inproceedings{kopitkov2020neural,
  title={Neural spectrum alignment: Empirical study},
  author={Kopitkov, Dmitry and Indelman, Vadim},
  booktitle={International Conference on Artificial Neural Networks},
  pages={168--179},
  year={2020},
  organization={Springer}
}

@article{su2019learning,
  title={On learning over-parameterized neural networks: A functional approximation perspective},
  author={Su, Lili and Yang, Pengkun},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{pennington2017resurrecting,
  title={Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice},
  author={Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{pennington2018emergence,
  title={The emergence of spectral universality in deep networks},
  author={Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1924--1932},
  year={2018},
  organization={PMLR}
}

@inproceedings{pellegrini2022neural,
  title={Neural Network Pruning Denoises the Features and Makes Local Connectivity Emerge in Visual Tasks},
  author={Pellegrini, Franco and Biroli, Giulio},
  booktitle={International Conference on Machine Learning},
  pages={17601--17626},
  year={2022},
  organization={PMLR}
}

@inproceedings{evci2022gradient, title={Gradient flow in sparse neural networks and how lottery tickets win}, author={Evci, Utku and Ioannou, Yani and Keskin, Cem and Dauphin, Yann}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={36}, number={6}, pages={6577--6586}, year={2022} }

@article{vgg,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@incollection{kindratenko2020hal,
  title={Hal: Computer system for scalable deep learning},
  author={Kindratenko, Volodymyr and Mu, Dawei and Zhan, Yan and Maloney, John and Hashemi, Sayed Hadi and Rabe, Benjamin and Xu, Ke and Campbell, Roy and Peng, Jian and Gropp, William},
  booktitle={Practice and experience in advanced research computing},
  pages={41--48},
  year={2020}
}

%--------------
% With Dawei and Tian



@misc{li2018over,
      title={On the Benefit of Width for Neural Networks: Disappearance of Bad Basins}, 
      author={Dawei Li and Tian Ding and Ruoyu Sun},
      year={2021},
      eprint={1812.11039},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}




@article{ding2019spurious,
  title={Sub-Optimal Local Minima Exist for Neural Networks with Almost All Non-Linear Activations},
  author={Ding, Tian and Li, Dawei and Sun, Ruoyu},
  journal={arXiv preprint arXiv:1911.01413},
  year={2019}
}


%--------------
% With Shiyu 

@InProceedings{liang2018understanding,
  title =   {Understanding the Loss Surface of Neural Networks for Binary Classification},
  author =       {Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle =   {Proceedings of the 35th International Conference on Machine Learning},
  pages =   {2835--2843},
  year =   {2018},
  editor =   {Dy, Jennifer and Krause, Andreas},
  volume =   {80},
  series =   {Proceedings of Machine Learning Research},
  month =   {10--15 Jul},
  publisher =    {PMLR},
  pdf =   {http://proceedings.mlr.press/v80/liang18a/liang18a.pdf},
  url =   {https://proceedings.mlr.press/v80/liang18a.html},
}



@inproceedings{liang2018adding,
 title={Adding one neuron can eliminate all bad local minima},
 author={Liang, Shiyu and Sun, Ruoyu and Lee, Jason D and Srikant, R},
 booktitle={Advances in Neural Information Processing Systems},
 pages={4355--4365},
 year={2018}
}


@misc{liang2019revisiting,
      title={Revisiting Landscape Analysis in Deep Neural Networks: Eliminating Decreasing Paths to Infinity}, 
      author={Shiyu Liang and Ruoyu Sun and R. Srikant},
      year={2019},
      eprint={1912.13472},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{liang2021ReLU,
  author    = {Shiyu Liang and
               Ruoyu Sun and
               R. Srikant},
  title     = {Achieving Small Test Error in Mildly Overparameterized Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2104.11895},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.11895},
  eprinttype = {arXiv},
  eprint    = {2104.11895},
}




%--------------
% With Yushun 



@article{zhang2021expressivity,
  title={When Expressivity Meets Trainability: Fewer than $ n $ Neurons Can Work},
  author={Zhang, Jiawei and Zhang, Yushun and Hong, Mingyi and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

%--------------
% With Dachao 

@misc{lin2021landscape,
      title={On the Landscape of One-hidden-layer Sparse Networks and Beyond}, 
      author={Dachao Lin and Ruoyu Sun and Zhihua Zhang},
      year={2021},
      eprint={2009.07439},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{lin2021faster,
  title={Faster Directional Convergence of Linear Neural Networks under Spherically Symmetric Data},
  author={Lin, Dachao and Sun, Ruoyu and Zhang, Zhihua},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


%--------------
% GAN


@inproceedings{sun2020GAN,
 author = {Sun, Ruoyu and Fang, Tiantian and Schwing, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {10186--10198},
 publisher = {Curran Associates, Inc.},
 title = {Towards a Better Global Loss Landscape of GANs},
 url = {https://proceedings.neurips.cc/paper/2020/file/738a6457be8432bab553e21b4235dd97-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{venturi2018spurious,
 title={Spurious valleys in two-layer neural network optimization landscapes},
 author={Venturi, Luca and Bandeira, Afonso and Bruna, Joan},
 journal={arXiv preprint arXiv:1802.06384},
 year={2018}
}

@article{safran2017spurious,
 title={Spurious local minima are common in two-layer relu neural networks},
 author={Safran, Itay and Shamir, Ohad},
 journal={arXiv preprint arXiv:1712.08968},
 year={2017}
}

@article{soltanolkotabi2019theoretical,
 title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
 author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
 journal={IEEE Transactions on Information Theory},
 volume={65},
 number={2},
 pages={742--769},
 year={2019},
 publisher={IEEE}
}

@article{sun2020global,
  title={The Global Landscape of Neural Networks: An Overview},
  author={Sun, Ruoyu and Li, Dawei and Liang, Shiyu and Ding, Tian and Srikant, Rayadurgam},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={95--108},
  year={2020},
  publisher={IEEE}
}


@article{sun2020optimization,
 title={Optimization for Deep Learning: An Overview},
 author={Sun, Ruo-Yu},
 journal={Journal of the Operations Research Society of China},
 pages={1--46},
 year={2020},
 publisher={Springer}
}

@article{li2022benefit,
  title={On the Benefit of Width for Neural Networks: Disappearance of Basins},
  author={Li, Dawei and Ding, Tian and Sun, Ruoyu},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={3},
  pages={1728--1758},
  year={2022},
  publisher={SIAM}
}

@article{ding2022suboptimal,
  title={Suboptimal local minima exist for wide neural networks with smooth activations},
  author={Ding, Tian and Li, Dawei and Sun, Ruoyu},
  journal={Mathematics of Operations Research},
  volume={47},
  number={4},
  pages={2784--2814},
  year={2022},
  publisher={INFORMS}
}