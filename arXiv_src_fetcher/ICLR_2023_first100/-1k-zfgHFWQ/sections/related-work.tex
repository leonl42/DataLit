\section{Related Work}
\label{sec:recent-work}

Traditional methods \cite{Carhart:1985ap,Nilakantan:1987tt,Rogers:2010fp} represent molecular structures with fingerprints. Some prior studies \cite{Svetnik:2004ab,Meyer:2019ld,Wu:2018dv} employ tree-based machine leaning models such as random forests \cite{Breiman:2001rf} and XGBoost \cite{Chen:2016ga} on fingerprints to predict the properties of molecules.
With the development of deep learning, neural approaches have been dominating the field given their strong representation ability.
One line of work \cite{Wang:2019hp,Chithrananda:2020eo} leverages language modeling techniques such as BERT \cite{Devlin:2019uk} to learn molecular representations based on SMILES strings \cite{Weininger:1988sm}.
However, some argue that sequence-based representations cannot fully capture substructure information and propose to leverage Graph Neural Networks (GNNs), which model molecules as graphs with atoms as nodes and bonds as edges \cite{Gilmer:2017tl,Liu:2019uy,Ying:2021ug}.
Despite the prosperous progress, they only model 2D topological structures of molecules, without considering the 3D coordinates of atoms that are known to determine certain chemical and physical functionalities of molecules.
To address this deficiency, recent work further explicitly considers such 3D geometry and designs equivariant networks to obtain the representations \cite{Schutt:2017wh,Klicpera:2020vw,Satorras:2021tz,Fuchs:2020wj,Schutt:2021vm,Du:2021ci,Liu:2021hq,Gasteiger:2021uf,Batzner:2021to,Brandstetter:2022wl,Xu:2021uj}.

Even though molecular representation learning techniques have been extensively investigated, there are very few labeled datasets available for studying the molecular properties of interest (e.g., drug-likeness or quantum properties).
On the other hand, there are abundant unannotated molecules available, which motivates researchers to study pretraining techniques that learn the model weights in a self-supervised manner and transfer the knowledge to downstream datasets with limited annotations via fine-tuning.
A series of pretraining frameworks on 2D molecular graph representations have been developed so far \cite{Rong:2020vk,Hu:2020uz,Zhang:2021wj,Wang:2022gr,Li:2020fo,Xia:2022jw}.
Recent work GEM \cite{Fang:2022et} studies large-scale pretraining for 3D geometry representations.
Additionally, researchers also study to supplement 2D-graph-based pretraining with 3D conformation information \cite{Yang:2021wg,Liu:2022vr,Stark:2021ug}.

A succinct comparison of our work with other representative methods is provided in \cref{tab:comparison-baseline}.
Compared to the above studies, our proposed \themodel is the only model that can \emph{adaptively} leverage multiple featurizations for both pretraining and fine-tuning stages.

\begin{table}
	\centering
	\rowcolors{2}{white}{lightgray!10}
	\caption{Comparing \themodel with representative self-supervised methods on molecular pretraining.}
	\begin{tabular}{l*{8}{c}}
	\toprule
	& \multicolumn{4}{c}{Pretraining} & \multicolumn{4}{c}{Fine-tuning} \\
	\cmidrule(lr){2-5} \cmidrule(lr){6-9}
	\rowcolor{white} \multirow{-2.5}{*}{Method} & 2D & 3D & Fingerprint & SMILES & 2D & 3D & Fingerprint & SMILES \\
	\midrule
	SMILES-BERT \cite{Wang:2019hp} & & & & \cmark & & & & \cmark \\
	ChemBERTa \cite{Chithrananda:2020eo} & & & & \cmark & & & & \cmark \\
%	GraphSAGE \cite{Hamilton:2017tp} & \cmark & & & & \cmark & & & \\
	AttrMask, ContexPred \cite{Hu:2020uz} & \cmark & & & & \cmark & & & \\
%	GPT-GNN \cite{Hu:2020vh} & \cmark & & & & \cmark & & & \\
%	InfoGraph \cite{Sun:2020vi} & \cmark & & & & \cmark & & & \\
	GraphCL \cite{You:2020ut} & \cmark & & & & \cmark & & & \\
%	JOAO \cite{You:2021wl} & \cmark & & & & \cmark & & &  \\
	GraphLoG \cite{Xu:2021tv} & \cmark & & & & \cmark & & & \\
	GROVER \cite{Rong:2020vk} & \cmark & & & & \cmark & & & \\
	GEM \cite{Fang:2022et} & & \cmark & & & & \cmark & & \\
	3D Infomax \cite{Stark:2021ug} & \cmark & \cmark & & & \cmark & & &  \\
	GraphMVP \cite{Liu:2022vr} & \cmark & \cmark & & & \cmark & & &  \\
	\themodel (Ours) & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
	\bottomrule
	\end{tabular}
	\label{tab:comparison-baseline}
\end{table}

