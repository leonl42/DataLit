\section{Conclusions and Discussions}
\label{sec:conclusion}

This paper examines different featurizations for molecular data and highlights the importance of incorporating multiple featurizations during both pretraining and fine-tuning.
Then, we develop a novel pretraining framework \themodel with complementary featurizations for molecular data, which is able to adaptively distill information from each featurization and allows interpretability from the learned model weights.
Extensive experiments on a wide range of property prediction benchmarks show that \themodel consistently outperforms existing baselines without negative transfer.

The study of featurization techniques for molecular machine learning in general remains widely open.
We would like to acknowledge that the relative utility of various featurizations for different molecular predictive tasks could be usefully explored in further work.
Moreover, more future research should be undertaken to specifically analyze the relationship between several featurizations, the representation ability of corresponding neural architectures, as well as the task-featurization correlation.
