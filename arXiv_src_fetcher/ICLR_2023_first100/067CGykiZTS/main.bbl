\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adel et~al.(2015)Adel, Balduzzi, and Ghodsi]{adel2015learning}
Tameem Adel, David Balduzzi, and Ali Ghodsi.
\newblock Learning the structure of sum-product networks via an svd-based
  algorithm.
\newblock In \emph{UAI}, pp.\  32--41, 2015.

\bibitem[Chiu \& Rush(2020)Chiu and Rush]{chiu-rush-2020-scaling}
Justin Chiu and Alexander Rush.
\newblock Scaling hidden {M}arkov language models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  1341--1349, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.103}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.103}.

\bibitem[Choi et~al.(2021)Choi, Dang, and Van~den Broeck]{choi2021group}
YooJung Choi, Meihua Dang, and Guy Van~den Broeck.
\newblock Group fairness by probabilistic modeling with latent fair decisions.
\newblock In \emph{Proceedings of the 35th AAAI Conference on Artificial
  Intelligence}, Feb 2021.

\bibitem[Correia et~al.(2022)Correia, Gala, Quaeghebeur, de~Campos, and
  Peharz]{correia2022continuous}
Alvaro~HC Correia, Gennaro Gala, Erik Quaeghebeur, Cassio de~Campos, and Robert
  Peharz.
\newblock Continuous mixtures of tractable probabilistic models.
\newblock \emph{arXiv preprint arXiv:2209.10584}, 2022.

\bibitem[Dang et~al.(2021)Dang, Khosravi, Liang, Vergari, and Van~den
  Broeck]{dang2021juice}
Meihua Dang, Pasha Khosravi, Yitao Liang, Antonio Vergari, and Guy Van~den
  Broeck.
\newblock Juice: A julia package for logic and probabilistic circuits.
\newblock In \emph{Proceedings of the 35th AAAI Conference on Artificial
  Intelligence (Demo Track)}, Feb 2021.

\bibitem[Dang et~al.(2022)Dang, Liu, and Van~den Broeck]{dang2022sparse}
Meihua Dang, Anji Liu, and Guy Van~den Broeck.
\newblock Sparse probabilistic circuits via pruning and growing.
\newblock In \emph{The 5th Workshop on Tractable Probabilistic Modeling}, 2022.

\bibitem[Darwiche(2003)]{darwiche2003differential}
Adnan Darwiche.
\newblock A differential approach to inference in bayesian networks.
\newblock \emph{Journal of the ACM (JACM)}, 50\penalty0 (3):\penalty0 280--305,
  2003.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{{NAACL-HLT} {(1)}}. Association for Computational
  Linguistics, 2019.

\bibitem[Di~Mauro et~al.(2021)Di~Mauro, Gala, Iannotta, and
  Basile]{di2021random}
Nicola Di~Mauro, Gennaro Gala, Marco Iannotta, and Teresa~MA Basile.
\newblock Random probabilistic circuits.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1682--1691.
  PMLR, 2021.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Gens \& Pedro(2013)Gens and Pedro]{gens2013learning}
Robert Gens and Domingos Pedro.
\newblock Learning the structure of sum-product networks.
\newblock In \emph{International conference on machine learning}, pp.\
  873--880. PMLR, 2013.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  16000--16009, 2022.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Diederik~P Kingma and Prafulla Dhariwal.
\newblock Glow: generative flow with invertible 1$\times$ 1 convolutions.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  10236--10245, 2018.

\bibitem[Kisa et~al.(2014)Kisa, Van~den Broeck, Choi, and
  Darwiche]{kisa2014probabilistic}
Doga Kisa, Guy Van~den Broeck, Arthur Choi, and Adnan Darwiche.
\newblock Probabilistic sentential decision diagrams.
\newblock In \emph{Fourteenth International Conference on the Principles of
  Knowledge Representation and Reasoning}, 2014.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Liu \& Van~den Broeck(2021)Liu and Van~den Broeck]{liu2021tractable}
Anji Liu and Guy Van~den Broeck.
\newblock Tractable regularization of probabilistic circuits.
\newblock In \emph{Advances in Neural Information Processing Systems 35
  (NeurIPS)}, dec 2021.

\bibitem[Liu et~al.(2022)Liu, Mandt, and Van~den Broeck]{liu2022lossless}
Anji Liu, Stephan Mandt, and Guy Van~den Broeck.
\newblock Lossless compression with probabilistic circuits.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, apr 2022.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  10012--10022, 2021.

\bibitem[Lloyd(1982)]{lloyd1982least}
Stuart Lloyd.
\newblock Least squares quantization in pcm.
\newblock \emph{IEEE transactions on information theory}, 28\penalty0
  (2):\penalty0 129--137, 1982.

\bibitem[Maal{\o}e et~al.(2019)Maal{\o}e, Fraccaro, Li{\'e}vin, and
  Winther]{maaloe2019biva}
Lars Maal{\o}e, Marco Fraccaro, Valentin Li{\'e}vin, and Ole Winther.
\newblock Biva: a very deep hierarchy of latent variables for generative
  modeling.
\newblock In \emph{Proceedings of the 33rd International Conference on Neural
  Information Processing Systems}, pp.\  6551--6562, 2019.

\bibitem[Marinescu \& Dechter(2005)Marinescu and Dechter]{marinescu2005and}
Radu Marinescu and Rina Dechter.
\newblock And/or branch-and-bound for graphical models.
\newblock In \emph{IJCAI}, pp.\  224--229. Citeseer, 2005.

\bibitem[Meila \& Jordan(2000)Meila and Jordan]{meila2000learning}
Marina Meila and Michael~I Jordan.
\newblock Learning with mixtures of trees.
\newblock \emph{Journal of Machine Learning Research}, 1\penalty0
  (Oct):\penalty0 1--48, 2000.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Molina et~al.(2019)Molina, Vergari, Stelzner, Peharz, Subramani,
  Di~Mauro, Poupart, and Kersting]{molina2019spflow}
Alejandro Molina, Antonio Vergari, Karl Stelzner, Robert Peharz, Pranav
  Subramani, Nicola Di~Mauro, Pascal Poupart, and Kristian Kersting.
\newblock Spflow: An easy and extensible library for deep probabilistic
  learning using sum-product networks.
\newblock \emph{arXiv preprint arXiv:1901.03704}, 2019.

\bibitem[Peharz et~al.(2016)Peharz, Gens, Pernkopf, and
  Domingos]{peharz2016latent}
Robert Peharz, Robert Gens, Franz Pernkopf, and Pedro Domingos.
\newblock On the latent variable interpretation in sum-product networks.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 39\penalty0 (10):\penalty0 2030--2044, 2016.

\bibitem[Peharz et~al.(2020{\natexlab{a}})Peharz, Lang, Vergari, Stelzner,
  Molina, Trapp, Van~den Broeck, Kersting, and Ghahramani]{peharz2020einsum}
Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina,
  Martin Trapp, Guy Van~den Broeck, Kristian Kersting, and Zoubin Ghahramani.
\newblock Einsum networks: Fast and scalable learning of tractable
  probabilistic circuits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7563--7574. PMLR, 2020{\natexlab{a}}.

\bibitem[Peharz et~al.(2020{\natexlab{b}})Peharz, Vergari, Stelzner, Molina,
  Shao, Trapp, Kersting, and Ghahramani]{peharz2020random}
Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao,
  Martin Trapp, Kristian Kersting, and Zoubin Ghahramani.
\newblock Random sum-product networks: A simple and effective approach to
  probabilistic deep learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  334--344.
  PMLR, 2020{\natexlab{b}}.

\bibitem[Poon \& Domingos(2011)Poon and Domingos]{poon2011sum}
Hoifung Poon and Pedro Domingos.
\newblock Sum-product networks: A new deep architecture.
\newblock In \emph{2011 IEEE International Conference on Computer Vision
  Workshops (ICCV Workshops)}, pp.\  689--690. IEEE, 2011.

\bibitem[Rabiner \& Juang(1986)Rabiner and Juang]{rabiner1986introduction}
Lawrence Rabiner and Biinghwang Juang.
\newblock An introduction to hidden markov models.
\newblock \emph{ieee assp magazine}, 3\penalty0 (1):\penalty0 4--16, 1986.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Rahman et~al.(2014)Rahman, Kothalkar, and Gogate]{rahman2014cutset}
Tahrima Rahman, Prasanna Kothalkar, and Vibhav Gogate.
\newblock Cutset networks: A simple, tractable, and scalable approach for
  improving the accuracy of chow-liu trees.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pp.\  630--645. Springer, 2014.

\bibitem[Shao et~al.(2022)Shao, Molina, Vergari, Stelzner, Peharz, Liebig, and
  Kersting]{shao2022conditional}
Xiaoting Shao, Alejandro Molina, Antonio Vergari, Karl Stelzner, Robert Peharz,
  Thomas Liebig, and Kristian Kersting.
\newblock Conditional sum-product networks: Modular probabilistic circuits via
  gate functions.
\newblock \emph{International Journal of Approximate Reasoning}, 140:\penalty0
  298--313, 2022.

\bibitem[Shih \& Ermon(2020)Shih and Ermon]{shih2020probabilistic}
Andy Shih and Stefano Ermon.
\newblock Probabilistic circuits for variational inference in discrete
  graphical models.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, pp.\  4635--4646, 2020.

\bibitem[Shih et~al.(2021)Shih, Sadigh, and Ermon]{shih2021hyperspns}
Andy Shih, Dorsa Sadigh, and Stefano Ermon.
\newblock Hyperspns: compact and expressive probabilistic circuits.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 8571--8582, 2021.

\end{thebibliography}
