% \newpage
\section{Related Work}

\textbf{Action Recognition.} Over the last decade, researchers have proposed various methods to address video recognition tasks~\cite{2017_quovadis, 2019_slowfast, 2016_tsn, 2018_trn, 2019_actiontransnet}. Video alignment, as a self-supervised pretext task for action recognition, enables the extraction of video representations without requiring labeled data. Previous work has leveraged traditional computer vision techniques to derive these representations. For instance, optical flow~\cite{1994_opticalflow} computes motion between two frames based on brightness constancy, and the CONDENSATION algorithm~\cite{1998_condensation} employs a probabilistic approach informed by prior motion data. Similarly, Space-Time Interest Points (STIPs)~\cite{2005_stips} detect salient points in video sequences by adapting image-based point detectors to analyze both spatial and temporal patterns.

\noindent \textbf{Self-Supervised Learning.} With the rise of deep learning, the field has shifted towards using Self-Supervised Learning (SSL) methods to learn video representations. Prior works \cite{2017_tcn, 2019_tcc, 2021_lav, 2021_gta, 2022_carl} have addressed the problem of video alignment using self-supervised methods. Time Contrastive Network (TCN) \cite{2017_tcn} utilizes contrastive learning to distinguish frames from different segments while grouping those within the same segment. Temporal Cycle-Consistency (TCC) \cite{2019_tcc} introduces cycle-consistency loss to identify and match recurring action sequences within or across videos. Learning by Aligning Videos (LAV) \cite{2021_lav} uses Soft-DTW \cite{2017_softdtw} and Inverse Difference Moment (IDM) regularization to optimize alignment and ensure balanced frame distribution. Global Temporal Alignment (GTA) \cite{2021_gta} implements a modified differentiable DTW with global consistency loss for consistent temporal alignment. Contrastive Action Representation Learning (CARL) \cite{2022_carl} introduces Sequence Contrastive Loss (SCL), which minimizes the KL-divergence between augmented views based on timestamp distance. Learning Representation by position PROPagation (LRProp) \cite{2024_lrprop} utilizes a Soft-DTW \cite{2017_softdtw} alignment path and a novel pair-wise position propagation technique. We will use these state-of-the-art methods to evaluate our model's performance on action recognition tasks.