\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amizadeh et~al.(2018)Amizadeh, Matusevych, and
  Weimer]{amizadeh2018learning}
Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer.
\newblock Learning to solve circuit-sat: An unsupervised differentiable
  approach.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Andrade et~al.(2012)Andrade, Resende, and Werneck]{andrade2012fast}
Diogo~V Andrade, Mauricio~GC Resende, and Renato~F Werneck.
\newblock Fast local search for the maximum independent set problem.
\newblock \emph{Journal of Heuristics}, 18\penalty0 (4):\penalty0 525--547,
  2012.

\bibitem[Angelini \& Ricci-Tersenghi(2019)Angelini and
  Ricci-Tersenghi]{angelini2019monte}
Maria~Chiara Angelini and Federico Ricci-Tersenghi.
\newblock Monte carlo algorithms are very effective in finding the largest
  independent set in sparse random graphs.
\newblock \emph{Physical Review E}, 100\penalty0 (1):\penalty0 013302, 2019.

\bibitem[Angelini \& Ricci-Tersenghi(2022)Angelini and
  Ricci-Tersenghi]{angelini2022cracking}
Maria~Chiara Angelini and Federico Ricci-Tersenghi.
\newblock Cracking nuts with a sledgehammer: when modern graph neural networks
  do worse than classical greedy algorithms.
\newblock \emph{arXiv preprint arXiv:2206.13211}, 2022.

\bibitem[Bello et~al.(2016)Bello, Pham, Le, Norouzi, and
  Bengio]{bello2016neural}
Irwan Bello, Hieu Pham, Quoc~V Le, Mohammad Norouzi, and Samy Bengio.
\newblock Neural combinatorial optimization with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.09940}, 2016.

\bibitem[Chen \& Tian(2019)Chen and Tian]{chen2019learning}
Xinyun Chen and Yuandong Tian.
\newblock Learning to perform local rewriting for combinatorial optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Conklin et~al.(2021)Conklin, Wang, Smith, and Titov]{conklin2021meta}
Henry Conklin, Bailin Wang, Kenny Smith, and Ivan Titov.
\newblock Meta-learning to compositionally generalize.
\newblock In \emph{ACL/IJCNLP (1)}, 2021.

\bibitem[Crama(1997)]{crama1997combinatorial}
Yves Crama.
\newblock Combinatorial optimization models for production scheduling in
  automated manufacturing systems.
\newblock \emph{European Journal of Operational Research}, 99\penalty0
  (1):\penalty0 136--153, 1997.

\bibitem[Delarue et~al.(2020)Delarue, Anderson, and
  Tjandraatmadja]{delarue2020reinforcement}
Arthur Delarue, Ross Anderson, and Christian Tjandraatmadja.
\newblock Reinforcement learning with combinatorial actions: An application to
  vehicle routing.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Fey \& Lenssen(2019)Fey and Lenssen]{Fey/Lenssen/2019}
Matthias Fey and Jan~E. Lenssen.
\newblock Fast graph representation learning with {PyTorch Geometric}.
\newblock In \emph{ICLR Workshop on Representation Learning on Graphs and
  Manifolds}, 2019.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1126--1135. PMLR, 2017.

\bibitem[Gasse et~al.(2019)Gasse, Ch{\'e}telat, Ferroni, Charlin, and
  Lodi]{gasse2019exact}
Maxime Gasse, Didier Ch{\'e}telat, Nicola Ferroni, Laurent Charlin, and Andrea
  Lodi.
\newblock Exact combinatorial optimization with graph convolutional neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Gurobi~Optimization(2022)]{Gurobi}
LLC Gurobi~Optimization.
\newblock Gurobi optimizer reference manual, 2022.

\bibitem[Hopfield \& Tank(1985)Hopfield and Tank]{hopfield1985neural}
John~J Hopfield and David~W Tank.
\newblock “neural” computation of decisions in optimization problems.
\newblock \emph{Biological cybernetics}, 52\penalty0 (3):\penalty0 141--152,
  1985.

\bibitem[Hsu et~al.(2018)Hsu, Levine, and Finn]{hsu2018unsupervised}
Kyle Hsu, Sergey Levine, and Chelsea Finn.
\newblock Unsupervised learning via meta-learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hudson et~al.(2021)Hudson, Li, Malencia, and Prorok]{hudson2021graph}
Benjamin Hudson, Qingbiao Li, Matthew Malencia, and Amanda Prorok.
\newblock Graph neural network guided local search for the traveling
  salesperson problem.
\newblock \emph{arXiv preprint arXiv:2110.05291}, 2021.

\bibitem[Jeong \& Kim(2020)Jeong and Kim]{jeong2020ood}
Taewon Jeong and Heeyoung Kim.
\newblock Ood-maml: Meta-learning for few-shot out-of-distribution detection
  and classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3907--3916, 2020.

\bibitem[Joshi et~al.(2019)Joshi, Laurent, and Bresson]{joshi2019efficient}
Chaitanya~K Joshi, Thomas Laurent, and Xavier Bresson.
\newblock An efficient graph convolutional network technique for the travelling
  salesman problem.
\newblock \emph{arXiv preprint arXiv:1906.01227}, 2019.

\bibitem[Karalias \& Loukas(2020)Karalias and Loukas]{karalias2020erdos}
Nikolaos Karalias and Andreas Loukas.
\newblock Erdos goes neural: an unsupervised learning framework for
  combinatorial optimization on graphs.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6659--6672, 2020.

\bibitem[Khalil et~al.(2016)Khalil, Le~Bodic, Song, Nemhauser, and
  Dilkina]{khalil2016learning}
Elias Khalil, Pierre Le~Bodic, Le~Song, George Nemhauser, and Bistra Dilkina.
\newblock Learning to branch in mixed integer programming.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Khalil et~al.(2017)Khalil, Dai, Zhang, Dilkina, and
  Song]{khalil2017learning}
Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le~Song.
\newblock Learning combinatorial optimization algorithms over graphs.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Kwon et~al.(2020)Kwon, Choo, Kim, Yoon, Gwon, and Min]{kwon2020pomo}
Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and
  Seungjai Min.
\newblock Pomo: Policy optimization with multiple optima for reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Kwon et~al.(2021)Kwon, Choo, Yoon, Park, Park, and
  Gwon]{kwon2021matrix}
Yeong-Dae Kwon, Jinho Choo, Iljoo Yoon, Minah Park, Duwon Park, and Youngjune
  Gwon.
\newblock Matrix encoding networks for neural combinatorial optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Leskovec \& Krevl(2014)Leskovec and Krevl]{leskovec2014snap}
Jure Leskovec and Andrej Krevl.
\newblock Snap datasets: Stanford large network dataset collection, 2014.

\bibitem[Mazyavkina et~al.(2021)Mazyavkina, Sviridov, Ivanov, and
  Burnaev]{mazyavkina2021reinforcement}
Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev.
\newblock Reinforcement learning for combinatorial optimization: A survey.
\newblock \emph{Computers \& Operations Research}, 134:\penalty0 105400, 2021.

\bibitem[Mezard \& Montanari(2009)Mezard and Montanari]{mezard2009information}
Marc Mezard and Andrea Montanari.
\newblock \emph{Information, physics, and computation}.
\newblock Oxford University Press, 2009.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
  and Grohe]{morris2019weisfeiler}
Christopher Morris, Martin Ritzert, Matthias Fey, William~L Hamilton, Jan~Eric
  Lenssen, Gaurav Rattan, and Martin Grohe.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~33, pp.\  4602--4609, 2019.

\bibitem[Nandwani et~al.(2021)Nandwani, Jindal, Singla,
  et~al.]{nandwani2021neural}
Yatin Nandwani, Deepanshu Jindal, Parag Singla, et~al.
\newblock Neural learning of one-of-many solutions for combinatorial problems
  in structured output spaces.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018first}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[O'rourke(1987)]{o1987art}
Joseph O'rourke.
\newblock \emph{Art gallery theorems and algorithms}, volume~57.
\newblock Oxford New York, 1987.

\bibitem[Papadimitriou \& Steiglitz(1998)Papadimitriou and
  Steiglitz]{papadimitriou1998combinatorial}
Christos~H Papadimitriou and Kenneth Steiglitz.
\newblock \emph{Combinatorial optimization: algorithms and complexity}.
\newblock Courier Corporation, 1998.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural
  Information Processing Systems 32}. 2019.

\bibitem[Rahman \& Virag(2017)Rahman and Virag]{rahman2017local}
Mustazee Rahman and Balint Virag.
\newblock Local algorithms for independent sets are half-optimal.
\newblock \emph{The Annals of Probability}, 45\penalty0 (3):\penalty0
  1543--1577, 2017.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019meta}
Aravind Rajeswaran, Chelsea Finn, Sham~M Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Schuetz et~al.(2022)Schuetz, Brubaker, and
  Katzgraber]{schuetz2022combinatorial}
Martin~JA Schuetz, J~Kyle Brubaker, and Helmut~G Katzgraber.
\newblock Combinatorial optimization with physics-inspired graph neural
  networks.
\newblock \emph{Nature Machine Intelligence}, 4\penalty0 (4):\penalty0
  367--377, 2022.

\bibitem[Selsam et~al.(2018)Selsam, Lamm, B{\"u}nz, Liang, de~Moura, and
  Dill]{selsam2018learning}
Daniel Selsam, Matthew Lamm, Benedikt B{\"u}nz, Percy Liang, Leonardo de~Moura,
  and David~L Dill.
\newblock Learning a sat solver from single-bit supervision.
\newblock \emph{arXiv preprint arXiv:1802.03685}, 2018.

\bibitem[Smith(1999)]{smith1999neural}
Kate~A Smith.
\newblock Neural networks for combinatorial optimization: a review of more than
  a decade of research.
\newblock \emph{Informs journal on Computing}, 11\penalty0 (1):\penalty0
  15--34, 1999.

\bibitem[Toenshoff et~al.(2019)Toenshoff, Ritzert, Wolf, and
  Grohe]{toenshoff2019run}
Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe.
\newblock Run-csp: unsupervised learning of message passing networks for binary
  constraint satisfaction problems.
\newblock 2019.

\bibitem[Toenshoff et~al.(2021)Toenshoff, Ritzert, Wolf, and
  Grohe]{toenshoff2021graph}
Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe.
\newblock Graph neural networks for maximum constraint satisfaction.
\newblock \emph{Frontiers in artificial intelligence}, 3:\penalty0 580607,
  2021.

\bibitem[Vinyals et~al.(2015)Vinyals, Fortunato, and
  Jaitly]{vinyals2015pointer}
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
\newblock Pointer networks.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Wang et~al.(2022)Wang, Wu, Yang, Hao, and Li]{wang2022unsupervised}
Haoyu Wang, Nan Wu, Hang Yang, Cong Hao, and Pan Li.
\newblock Unsupervised learning for combinatorial optimization with principled
  objective relaxation.
\newblock \emph{Advances in neural information processing systems}, 35, 2022.

\bibitem[Xu(2007)]{xu2007benchmarks}
K~BHOSLIB Xu.
\newblock Benchmarks with hidden optimum solutions for graph problems.
\newblock \emph{URL http://www. nlsde. buaa. edu.
  cn/kexu/benchmarks/graph-benchmarks. htm}, 2007.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Yabuta \& Kitazawa(2008)Yabuta and Kitazawa]{yabuta2008optimum}
Kenichi Yabuta and Hitoshi Kitazawa.
\newblock Optimum camera placement considering camera specification for
  security monitoring.
\newblock In \emph{2008 IEEE International Symposium on Circuits and Systems
  (ISCAS)}, pp.\  2114--2117. IEEE, 2008.

\bibitem[Yanardag \& Vishwanathan(2015)Yanardag and
  Vishwanathan]{yanardag2015deep}
Pinar Yanardag and SVN Vishwanathan.
\newblock Deep graph kernels.
\newblock In \emph{Proceedings of the 21th ACM SIGKDD international conference
  on knowledge discovery and data mining}, pp.\  1365--1374, 2015.

\bibitem[Yao et~al.(2019)Yao, Bandeira, and Villar]{yao2019experimental}
Weichi Yao, Afonso~S Bandeira, and Soledad Villar.
\newblock Experimental performance of graph neural networks on random instances
  of max-cut.
\newblock In \emph{Wavelets and Sparsity XVIII}, volume 11138, pp.\  242--251.
  SPIE, 2019.

\bibitem[Yehuda et~al.(2020)Yehuda, Gabel, and Schuster]{yehuda2020s}
Gal Yehuda, Moshe Gabel, and Assaf Schuster.
\newblock It’s not what machines can learn, it’s what we cannot teach.
\newblock In \emph{International conference on machine learning}, pp.\
  10831--10841. PMLR, 2020.

\bibitem[Yolcu \& P{\'o}czos(2019)Yolcu and P{\'o}czos]{yolcu2019learning}
Emre Yolcu and Barnab{\'a}s P{\'o}czos.
\newblock Learning local search heuristics for boolean satisfiability.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\end{thebibliography}
