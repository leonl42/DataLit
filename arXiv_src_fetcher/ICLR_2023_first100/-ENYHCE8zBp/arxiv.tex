
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}
%\usepackage{/arxiv/iclr2023_conference}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{soul}
\newcommand{\pan}[1]{\textcolor{blue}{P:#1}}
\usepackage{amssymb}
% table
\usepackage{booktabs}
% wraptable
\usepackage{wrapfig,lipsum}
% image
\usepackage{graphicx}
% pseudo code
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
% subfigure
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}

\usepackage{xspace}
\usepackage{booktabs}
\usepackage{xcolor}         % colors
\usepackage{comment}
% \usepackage{cite}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{array,multirow,graphicx}

% defination and theorem
%\usepacakge{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newcommand{\proj}{Meta-EGN\xspace}
%\renewcommand{\stcomp}[1]{\overline{#1}}
\newcommand{\hy}[1]{{\sethlcolor{pink}\hl{#1}}}

\title{Unsupervised Learning for Combinatorial Optimization Needs Meta Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Haoyu Wang$^{1}$, Pan Li$^{1,2}$ \\
1. Department of Electrical and Computer Engineering, Georgia Institute of Technology \\
2. Department of Computer Science, Purdue University\\
\texttt{hwang3028@gatech.edu, panli@gatech.edu, panli@purdue.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle




\begin{abstract}%\vspace{-1mm}
%\pan{Carefully check the reference~\cite{manchanda2022generalization}. We can call it a concurrent work.See if there is any argument we can use.}
%\vspace{200}
%A common motivation of applying machine learning to combinatorial optimization (CO) is that heuristics learned from solving historical problems may be helpful in solving a new but similar problem. Recently, unsupervised learning for CO has attracted much attention. The adopted framework uses a neural network (NN) to parameterize problem solutions and train the NN directly by optimizing the CO objectives of historical problems. Although having already shown advantages over traditional CO solvers, the current framework ignores the fact that practical CO needs a good performance for every future encountered problem rather than an averaged good performance over a hypothetical distribution of future problems. With this observation, we propose a new meta-learning-based formulation of unsupervised learning for CO and propose a MAML-based training pipeline. The obtained model shows significant performance improvement over baselines \pan{(real datasets, benchmarks, generalization over different datasets, generalization over different scales of the problem)} outperform the state-of-the-art unsupervised solvers over   
%Compared with traditional CO solvers, such a learning-based method expects the NN to extract heuristics from historical problems, and  solve newly encountered problems in a fast and smart way. However, the via a neural network (NN)

A general framework of unsupervised learning for combinatorial optimization (CO) is to train a neural network (NN) whose output gives a problem solution by directly optimizing the CO objective. Albeit with some advantages over traditional solvers, the current framework optimizes an averaged performance over the distribution of historical problem instances, which misaligns with the actual goal of CO that looks for a good solution to every future encountered instance. With this observation, we propose a new objective of unsupervised learning for CO where the goal of learning is to search for good initialization for future problem instances rather than give direct solutions. We propose a meta-learning-based training pipeline for this new objective. Our method achieves good empirical performance. We observe that even the initial solution given by our model before fine-tuning can significantly outperform the baselines under various evaluation settings including evaluation across multiple datasets, and the case with big shifts in the problem scale. The reason we conjecture is that meta-learning-based training lets the model be loosely tied to each local optimum for a training instance while being more adaptive to the changes of optimization landscapes across instances. \footnote{Our code is available at: \url{https://github.com/Graph-COM/Meta_CO}}
%help with finding valleys in the optimization landscape with good local optima  for the CO problems that often contain a lot of bad local optima. %which covers the benchmark problems including max clique, minimum vectex cover, max independent set 
%\pan{(real datasets, benchmarks, generalization over different datasets, generalization over different scales of the problem)} outperform the state-of-the-art unsupervised solvers over   
\end{abstract}
\input{arxiv/001-Introduction}

\input{arxiv/002-Related_work}

\input{arxiv/003-Preliminaries}

\input{arxiv/004-Methods}

\input{arxiv/005-Experiments}

\input{arxiv/006-Conclusion}

\input{arxiv/008-Acknowledgement.tex}



%\newpage

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\appendix
\input{arxiv/007-Appendix}

\end{document}
