
\section{Conclusion}
This work proposes an unsupervised learning framework \proj with the goal of optimizing NNs towards instance-wise good solutions to CO problems. \proj leverages MAML to achieve the goal. \proj views each training instance as a separate task and learns a good initialization for all these tasks. \proj significantly improves the performance of its baseline and has shown good generalization when the data used for training and testing has different scales or distributions.  In addition, \proj can learn to improve the greedy heuristics while paying almost no extra time cost in the problem of maximum independent set on large-scale random regular graphs. 


%To respond to the recent concern on the effectiveness of learning-based methods, \proj can learn to improve greedy heuristics without much extra time cost complexity in the MIS on large scale RRGs. 

%By taking a graph instance as the input, \porh

%By learning its meta-learning objective, \proj learns from historical graphs for better initialization as well as do fine-tuning to further approach the optimality of each instance. The one-step fine-tuning enables \proj an even wider performance guarantee compared with the previous methods. \proj significantly improves the performance of its baseline even before the fine-tuning step and also has better generalization across scale or data distribution changes. In addition, \proj improves the greedy heuristics as well as maintain the time complexity in the MIS on large scale RRGs. 



%In the future, we aim to further modify \proj in the problems that require global assignment  and broaden \proj to fine-tune more heuristics or approximation methods.


%fill in the gap between the actual goal of CO problems (to achieve optimality for every single instance) and the optimization objective adopted by previous unsupervised learning for CO (the optimality in the averaged sense), and proposes \proj, a meta-learning based unsupervised learning for CO framework. \proj aims to learn better heuristics from the historical graphs for better initialization and do further fine-tuning on the testing instances to move closer towards the optimality for the single instance. The one-step fine-tuning enables \proj an even wider performance guarantee compared with the previous methods.
%\proj significantly improves the performance of its baselines in the max clique problem and the minimum vertex covering problems on both real and synthetic datasets, even before the fine-tuning step. Experiments show that \proj has better generalization across scale and data distribution. In addition, the work utilizes \proj to learn to further improve the performance of heuristics as well as maintain the time complexity in the max independent set problem on large scale random-regular graphs, as a response to ~\cite{angelini2022cracking}. In the future, we aim to further improve \proj in global assignment problems and broaden \proj to fine-tune more heuristics or approximation methods.

\iffalse
\subsection{Limitations}
\proj on all the MC baselines and boost their performance? Because SA or PT requires carefully tuned. leave it for further study.
\fi