\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock Preprint arXiv:1907.02893, 2019.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018recognition}
Sara Beery, Grant Van~Horn, and Pietro Perona.
\newblock Recognition in terra incognita.
\newblock In \emph{European Conference on Computer Vision}, 2018.

\bibitem[Ben-David et~al.(2007)Ben-David, Blitzer, Crammer, Pereira,
  et~al.]{ben2007analysis}
Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et~al.
\newblock Analysis of representations for domain adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, 2007.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 79\penalty0 (1):\penalty0 151--175, 2010.

\bibitem[Ben-Tal et~al.(2013)Ben-Tal, Den~Hertog, De~Waegenaere, Melenberg, and
  Rennen]{ben2013robust}
Aharon Ben-Tal, Dick Den~Hertog, Anja De~Waegenaere, Bertrand Melenberg, and
  Gijs Rennen.
\newblock Robust solutions of optimization problems affected by uncertain
  probabilities.
\newblock \emph{Management Science}, 59\penalty0 (2):\penalty0 341--357, 2013.

\bibitem[Bernhard \& Rapaport(1995)Bernhard and Rapaport]{bernhard1995theorem}
Pierre Bernhard and Alain Rapaport.
\newblock On a theorem of danskin with an application to a theorem of von
  neumann-sion.
\newblock \emph{Nonlinear Analysis: Theory, Methods \& Applications},
  24\penalty0 (8):\penalty0 1163--1181, 1995.

\bibitem[Borkan et~al.(2019)Borkan, Dixon, Sorensen, Thain, and
  Vasserman]{borkan2019nuanced}
Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.
\newblock Nuanced metrics for measuring unintended bias with real data for text
  classification.
\newblock In \emph{World Wide Web conference}, 2019.

\bibitem[Creager et~al.(2021)Creager, Jacobsen, and
  Zemel]{creager2021environment}
Elliot Creager, J{\"o}rn-Henrik Jacobsen, and Richard Zemel.
\newblock Environment inference for invariant learning.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, 2009.

\bibitem[Devansh~Arpit(2019)]{arpit2019predicting}
Richard~Socher Devansh~Arpit, Caiming~Xiong.
\newblock Predicting with high correlation features.
\newblock Preprint arXiv:1910.00164, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Conference of the North American Chapter of the Association
  for Computational Linguistics}, 2019.

\bibitem[Epasto et~al.(2020)Epasto, Mahdian, Mirrokni, and
  Zampetakis]{epasto2020optimal}
Alessandro Epasto, Mohammad Mahdian, Vahab Mirrokni, and Emmanouil Zampetakis.
\newblock Optimal approximation-smoothness tradeoffs for soft-max functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
  Larochelle, Fran{\c{c}}ois Laviolette, Mario Marchand, and Victor Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2096--2030, 2016.

\bibitem[Geirhos et~al.(2018)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and
  Brendel]{geirhos2018imagenet}
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix~A
  Wichmann, and Wieland Brendel.
\newblock Imagenet-trained cnns are biased towards texture; increasing shape
  bias improves accuracy and robustness.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Gulrajani \& Lopez-Paz(2020)Gulrajani and
  Lopez-Paz]{gulrajani2020search}
Ishaan Gulrajani and David Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Gururangan et~al.(2018)Gururangan, Swayamdipta, Levy, Schwartz,
  Bowman, and Smith]{gururangan2018annotation}
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman,
  and Noah~A Smith.
\newblock Annotation artifacts in natural language inference data.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2016.

\bibitem[He et~al.(2021)He, Shen, and Cui]{he2021towards}
Yue He, Zheyan Shen, and Peng Cui.
\newblock Towards non-iid image classification: A dataset and baselines.
\newblock \emph{Pattern Recognition}, 110:\penalty0 107383, 2021.

\bibitem[Heinze-Deml \& Meinshausen(2021)Heinze-Deml and
  Meinshausen]{heinze2021conditional}
Christina Heinze-Deml and Nicolai Meinshausen.
\newblock Conditional variance penalties and domain shift robustness.
\newblock \emph{Machine Learning}, 110\penalty0 (2):\penalty0 303--348, 2021.

\bibitem[Hendrycks \& Dietterich(2018)Hendrycks and
  Dietterich]{hendrycks2018benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hu et~al.(2020)Hu, Zhang, Chen, and Chan]{hu2020domain}
Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan.
\newblock Domain generalization via multidomain discriminant analysis.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2020.

\bibitem[Idrissi et~al.(2021)Idrissi, Arjovsky, Pezeshki, and
  Lopez-Paz]{idrissi2021simple}
Badr~Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz.
\newblock Simple data balancing achieves competitive worst-group-accuracy.
\newblock Preprint, 2021.

\bibitem[Kpotufe \& Martinet(2021)Kpotufe and Martinet]{kpotufe2021marginal}
Samory Kpotufe and Guillaume Martinet.
\newblock Marginal singularity and the benefits of labels in covariate-shift.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (6):\penalty0
  3299--3323, 2021.

\bibitem[Krueger et~al.(2021)Krueger, Caballero, Jacobsen, Zhang, Binas, Zhang,
  Le~Priol, and Courville]{krueger2021out}
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
  Binas, Dinghuai Zhang, Remi Le~Priol, and Aaron Courville.
\newblock Out-of-distribution generalization via risk extrapolation (rex).
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Levy et~al.(2020)Levy, Carmon, Duchi, and Sidford]{levy2020large}
Daniel Levy, Yair Carmon, John~C Duchi, and Aaron Sidford.
\newblock Large-scale methods for distributionally robust optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Li et~al.(2017)Li, Yang, Song, and Hospedales]{li2017deeper}
Da~Li, Yongxin Yang, Yi-Zhe Song, and Timothy~M Hospedales.
\newblock Deeper, broader and artier domain generalization.
\newblock In \emph{IEEE International Conference on Computer Vision}, 2017.

\bibitem[Li et~al.(2018)Li, Tian, Gong, Liu, Liu, Zhang, and Tao]{li2018deep}
Ya~Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and
  Dacheng Tao.
\newblock Deep domain generalization via conditional invariant adversarial
  networks.
\newblock In \emph{European Conference on Computer Vision}, 2018.

\bibitem[Lin et~al.(2020)Lin, Jin, and Jordan]{lin2020gradient}
Tianyi Lin, Chi Jin, and Michael Jordan.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Haghgoo, Chen, Raghunathan, Koh,
  Sagawa, Liang, and Finn]{liu2021just}
Evan~Z Liu, Behzad Haghgoo, Annie~S Chen, Aditi Raghunathan, Pang~Wei Koh,
  Shiori Sagawa, Percy Liang, and Chelsea Finn.
\newblock Just train twice: Improving group robustness without training group
  information.
\newblock In \emph{International Conference on Machine Learning},
  2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Hu, Cui, Li, and
  Shen]{liu2021heterogeneous}
Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo~Li, and Zheyan Shen.
\newblock Heterogeneous risk minimization.
\newblock In \emph{International Conference on Machine Learning},
  2021{\natexlab{b}}.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{IEEE International Conference on Computer Vision}, 2015.

\bibitem[Lohn(2020)]{lohn2020estimating}
Andrew~J Lohn.
\newblock Estimating the brittleness of ai: safety integrity levels and the
  need for testing out-of-distribution performance.
\newblock Preprint arXiv:2009.00802, 2020.

\bibitem[Long et~al.(2018)Long, Cao, Wang, and Jordan]{long2018conditional}
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael~I Jordan.
\newblock Conditional adversarial domain adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Loshchilov \& Hutter(2018)Loshchilov and
  Hutter]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mahajan et~al.(2021)Mahajan, Tople, and Sharma]{mahajan2021domain}
Divyat Mahajan, Shruti Tople, and Amit Sharma.
\newblock Domain generalization using causal matching.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Makar \& D'Amour(2022)Makar and D'Amour]{makar2022fairness}
Maggie Makar and Alexander D'Amour.
\newblock Fairness and robustness in anti-causal prediction.
\newblock Preprint, 2022.

\bibitem[Meinshausen \& B{\"u}hlmann(2015)Meinshausen and
  B{\"u}hlmann]{meinshausen2015maximin}
Nicolai Meinshausen and Peter B{\"u}hlmann.
\newblock Maximin effects in inhomogeneous large-scale data.
\newblock \emph{The Annals of Statistics}, 43\penalty0 (4):\penalty0
  1801--1830, 2015.

\bibitem[Muandet et~al.(2013)Muandet, Balduzzi, and
  Sch{\"o}lkopf]{muandet2013domain}
Krikamol Muandet, David Balduzzi, and Bernhard Sch{\"o}lkopf.
\newblock Domain generalization via invariant feature representation.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock 2019.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Vemprala, Madry, and
  Kapoor]{salman2020unadversarial}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, Aleksander Madry, and
  Ashish Kapoor.
\newblock Unadversarial examples: designing objects for robust vision.
\newblock Preprint arXiv:2012.12235, 2020.

\bibitem[Schneider et~al.(2020)Schneider, Rusak, Eck, Bringmann, Brendel, and
  Bethge]{schneider2020improving}
Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel,
  and Matthias Bethge.
\newblock Improving robustness against common corruptions by covariate shift
  adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Seo et~al.(2022)Seo, Lee, and Han]{seo2022information}
Seonguk Seo, Joon-Young Lee, and Bohyung Han.
\newblock Information-theoretic bias reduction via causal view of spurious
  correlation.
\newblock Preprint arXiv:2201.03121, 2022.

\bibitem[Shiryaev(2016)]{shiryaev2016probability}
Albert~N Shiryaev.
\newblock \emph{Probability-1}, volume~95.
\newblock Springer, 2016.

\bibitem[Sinha et~al.(2018)Sinha, Namkoong, and Duchi]{sinha2018certifying}
Aman Sinha, Hongseok Namkoong, and John Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Sohoni et~al.(2020)Sohoni, Dunnmon, Angus, Gu, and
  R{\'e}]{sohoni2020no}
Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R{\'e}.
\newblock No subclass left behind: Fine-grained robustness in coarse-grained
  classification problems.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Steinke \& Zakynthinou(2020)Steinke and
  Zakynthinou]{steinke2020reasoning}
Thomas Steinke and Lydia Zakynthinou.
\newblock Reasoning about generalization via conditional mutual information.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Tu et~al.(2020)Tu, Lalwani, Gella, and He]{tu2020empirical}
Lifu Tu, Garima Lalwani, Spandana Gella, and He~He.
\newblock An empirical study on robustness to spurious correlations using
  pre-trained language models.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 621--633, 2020.

\bibitem[van~der Vaart \& Wellner(2000)van~der Vaart and Wellner]{van2000weak}
Aad~W. van~der Vaart and Jon~A. Wellner.
\newblock \emph{Weak convergence and empirical processes}.
\newblock Springer series in statistics. Springer, 2000.

\bibitem[Vapnik(1999)]{vapnik1999nature}
Vladimir Vapnik.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer science \& business media, 1999.

\bibitem[Veitch et~al.(2021)Veitch, D'Amour, Yadlowsky, and
  Eisenstein]{veitch2021counterfactual}
Victor Veitch, Alexander D'Amour, Steve Yadlowsky, and Jacob Eisenstein.
\newblock Counterfactual invariance to spurious correlations: Why and how to
  pass stress tests.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Vershynin(2018)]{vershynin2018}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2018.

\bibitem[Volpi et~al.(2018)Volpi, Namkoong, Sener, Duchi, Murino, and
  Savarese]{volpi2018generalizing}
Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio Murino, and
  Silvio Savarese.
\newblock Generalizing to unseen domains via adversarial data augmentation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and
  Belongie]{wah2011caltech}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge
  Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock Technical report, 2011.

\bibitem[Wainwright(2019)]{wainwright2019}
Martin~J. Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2019.

\bibitem[Wald et~al.(2021)Wald, Feder, Greenfeld, and
  Shalit]{wald2021calibration}
Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit.
\newblock On calibration and out-of-domain generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Wang et~al.(2022)Wang, Yi, Chen, and Zhu]{wang2022out}
Ruoyu Wang, Mingyang Yi, Zhitang Chen, and Shengyu Zhu.
\newblock Out-of-distribution generalization with causal invariant
  transformations.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2022.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2018broad}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Conference of the North American Chapter of the Association
  for Computational Linguistics}, 2018.

\bibitem[Xie et~al.(2020)Xie, Kumar, Jones, Khani, Ma, and Liang]{xie2020in}
Sang~Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and
  Percy Liang.
\newblock In-n-out: Pre-training and self-training using auxiliary information
  for out-of-distribution robustness.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Xu \& Raginsky(2017)Xu and Raginsky]{xu2017information}
Aolin Xu and Maxim Raginsky.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Ye et~al.(2021)Ye, Xie, Cai, Li, Li, and Wang]{ye2021towards}
Haotian Ye, Chuanlong Xie, Tianle Cai, Ruichen Li, Zhenguo Li, and Liwei Wang.
\newblock Towards a theoretical framework of out-of-distribution
  generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Ye et~al.(2022)Ye, Li, Hong, Bai, Chen, Zhou, and Li]{ye2021ood}
Nanyang Ye, Kaican Li, Lanqing Hong, Haoyue Bai, Yiting Chen, Fengwei Zhou, and
  Zhenguo Li.
\newblock Ood-bench: Benchmarking and understanding out-of-distribution
  generalization datasets and algorithms.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2022.

\bibitem[Yi et~al.(2021{\natexlab{a}})Yi, Hou, Shang, Jiang, Liu, and
  Ma]{yi2021reweighting}
Mingyang Yi, Lu~Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Zhi-Ming Ma.
\newblock Reweighting augmented samples by minimizing the maximal expected
  loss.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Yi et~al.(2021{\natexlab{b}})Yi, Hou, Sun, Shang, Jiang, Liu, and
  Ma]{yi2021improved}
Mingyang Yi, Lu~Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu, and
  Zhi-Ming Ma.
\newblock Improved ood generalization via adversarial training and
  pre-training.
\newblock In \emph{International Conference on Machine Learning},
  2021{\natexlab{b}}.

\bibitem[Zhou et~al.(2017)Zhou, Lapedriza, Khosla, Oliva, and
  Torralba]{zhou2017places}
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Places: A 10 million image database for scene recognition.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 40\penalty0 (6):\penalty0 1452--1464, 2017.

\bibitem[Zhou et~al.(2022)Zhou, Lin, Pi, Zhang, Xu, Cui, and
  Zhang]{zhou2022model}
Xiao Zhou, Yong Lin, Renjie Pi, Weizhong Zhang, Renzhe Xu, Peng Cui, and Tong
  Zhang.
\newblock Model agnostic sample reweighting for out-of-distribution learning.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Zhu et~al.(2017)Zhu, Park, Isola, and Efros]{zhu2017unpaired}
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei~A Efros.
\newblock Unpaired image-to-image translation using cycle-consistent
  adversarial networks.
\newblock In \emph{IEEE International Conference on Computer Vision}, 2017.

\end{thebibliography}
