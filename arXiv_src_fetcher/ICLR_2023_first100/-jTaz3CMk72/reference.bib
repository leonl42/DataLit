@article{javanmard2014confidence,
	title={Confidence intervals and hypothesis testing for high-dimensional regression},
	author={Javanmard, Adel and Montanari, Andrea},
	journal={Journal of Machine Learning Research},
	volume={15},
	number={1},
	pages={2869--2909},
	year={2014},
	publisher={JMLR. org}
}

@article{ben2007analysis,
	title={Analysis of representations for domain adaptation},
	author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando and others},
	journal={Advances in Neural Information Processing Systems},
	year={2007}
}

@article{ben2010theory,
	title={A theory of learning from different domains},
	author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
	journal={Machine learning},
	volume={79},
	number={1},
	pages={151--175},
	year={2010},
	publisher={Springer}
}

@inproceedings{muandet2013domain,
	title={Domain generalization via invariant feature representation},
	author={Muandet, Krikamol and Balduzzi, David and Sch{\"o}lkopf, Bernhard},
	booktitle={International Conference on Machine Learning},
	year={2013}
}

@article{ganin2016domain,
	title={Domain-adversarial training of neural networks},
	author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
	journal={Journal of Machine Learning Research},
	volume={17},
	number={1},
	pages={2096--2030},
	year={2016},
	publisher={JMLR}
}

@article{javanmard2018debiasing,
	title={Debiasing the lasso: Optimal sample size for gaussian designs},
	author={Javanmard, Adel and Montanari, Andrea and others},
	journal={The Annals of Statistics},
	volume={46},
	number={6A},
	pages={2593--2622},
	year={2018},
	publisher={Institute of Mathematical Statistics}
}

@inproceedings{jung2020anew,
	title={A New Analysis of Differential Privacy's Generalization Guarantees},
	author={Jung, Christopher and Katrina Ligett},
	booktitle={Innovations in Theoretical Computer Science},
	year={2020}
}

@article{bu2020tightening,
	title={Tightening Mutual Information-Based Bounds on Generalization Error},
	author={Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V},
	journal={IEEE Journal on Selected Areas in Information Theory},
	volume={1},
	number={1},
	pages={121--130},
	year={2020},
	publisher={IEEE}
}

@inproceedings{steinke2020reasoning,
	title={Reasoning about generalization via conditional mutual information},
	author={Steinke, Thomas and Zakynthinou, Lydia},
	booktitle={Conference on Learning Theory},
	year={2020}
}

@inproceedings{xu2017information,
	title={Information-theoretic analysis of generalization capability of learning algorithms},
	author={Xu, Aolin and Raginsky, Maxim},
	booktitle={Advances in Neural Information Processing Systems},
	year={2017}
}

@inproceedings{dwork2015preserving,
	title={A New Analysis of Differential Privacy's Generalization Guarantees},
	author={Cynthia, Dwork and Vitaly, Feldman and Moritz, Hardt and Toniann, Pitassi and Omer, Reingold and Aaron, Roth},
	booktitle={ACM Symposium on Theory of Computing},
	year={2015}
}

@inproceedings{gonen2017fast,
	title={Fast rates for empirical risk minimization of strict saddle problems},
	author={Gonen, Alon and Shalev-Shwartz, Shai},
	booktitle={Conference on Learning Theory},
	year={2017}
}

@article{bickel2009simultaneous,
	title={Simultaneous analysis of Lasso and Dantzig selector},
	author={Bickel, Peter J and Ritov, Yaâ€™acov and Tsybakov, Alexandre B and others},
	journal={The Annals of Statistics},
	volume={37},
	number={4},
	pages={1705--1732},
	year={2009},
	publisher={Institute of Mathematical Statistics}
}

@techreport{bartlett2021deep,
	title={Deep learning: a statistical viewpoint},
	author={Bartlett, Peter L and Montanari, Andrea and Rakhlin, Alexander},
	number={arXiv:2103.09177},
	type={Preprint},
	year={2021}
}

@article{fan2017estimation,
	title={Estimation of high dimensional mean regression in the absence of symmetry and light tail assumptions},
	author={Fan, Jianqing and Li, Quefeng and Wang, Yuyan},
	journal={Journal of the Royal Statistical Society. Series B, Statistical methodology},
	volume={79},
	number={1},
	pages={247},
	year={2017},
	publisher={NIH Public Access}
}

@article{zhang2010nearly,
	title={Nearly unbiased variable selection under minimax concave penalty},
	author={Zhang, Cun-Hui},
	journal={The Annals of Statistics},
	volume={38},
	number={2},
	pages={894--942},
	year={2010},
	publisher={Institute of Mathematical Statistics}
}

@inproceedings{johnson2013accelerating,
	title={Accelerating stochastic gradient descent using predictive variance reduction},
	author={Johnson, Rie and Zhang, Tong},
	booktitle={Advances in Neural Information Processing Systems},
	year={2013}
}

@inproceedings{neyshabur2015path,
	title={Path-sgd: Path-normalized optimization in deep neural networks},
	author={Neyshabur, Behnam and Salakhutdinov, Ruslan R and Srebro, Nati},
	booktitle={Advances in Neural Information Processing Systems},
	year={2015}
}

@inproceedings{neyshabur2017exploring,
	title={Exploring generalization in deep learning},
	author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
	booktitle={Advances in Neural Information Processing Systems},
	year={2017}
}

@inproceedings{hardt2016train,
	title={Train faster, generalize better: Stability of stochastic gradient descent},
	author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
	booktitle={International Conference on Machine Learning},
	year={2016}
}

@inproceedings{jin2017escape,
	title={How to Escape Saddle Points Efficiently},
	author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M. and Jordan, Michael I.},
	booktitle={International Conference on Machine Learning},
	year={2017}
}

@inproceedings{keskar2016large,
	title={On large-batch training for deep learning: Generalization gap and sharp minima},
	author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	booktitle={International Conference on Learning Representations},
	year={2016}
}

@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Conference on Computer Vision and Pattern Recognition},
	year={2016}
}

@inproceedings{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
	booktitle={Advances in Neural Information Processing Systems},
	year={2017}
}

@inproceedings{arora2018stronger,
	title={Stronger generalization bounds for deep nets via a compression approach},
	author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
	booktitle={International Conference on Machine Learning},
	year={2018}
}

@inproceedings{arora2018theoretical,
	title={Theoretical analysis of auto rate-tuning by batch normalization},
	author={Arora, Sanjeev and Li, Zhiyuan and Lyu, Kaifeng},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@inproceedings{kingma2014adam,
	author    = {Diederik P. Kingma and Jimmy Ba},
	title     = {Adam: A method for stochastic optimization},
	booktitle = {International Conference on Learning Representations},
	year      = {2015}
}

@article{krizhevsky2009learning,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey},
	year={2009},
	publisher={Citeseer}
}

@article{mei2018landscape,
	title={The landscape of empirical risk for nonconvex losses},
	author={Mei, Song and Bai, Yu and Montanari, Andrea},
	journal={The Annals of Statistics},
	volume={46},
	number={6A},
	pages={2747--2774},
	year={2018},
	publisher={Institute of Mathematical Statistics}
}

@book{vershynin2018, 
	place={Cambridge}, 
	author={Vershynin, Roman}, 
	year={2018},
	series={Cambridge Series in Statistical and Probabilistic Mathematics},
	title={High-dimensional probability: An introduction with applications in data science}, 
	publisher={Cambridge University Press},  
	collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@book{wainwright2019,
	author={Wainwright, Martin J.},  
	year= {2019}, 
	series={Cambridge Series in Statistical and Probabilistic Mathematics},
	title={High-dimensional statistics: A non-asymptotic viewpoint},
	publisher={Cambridge University Press}
}

@techreport{bubeck2014convex,
	title={Convex optimization: Algorithms and complexity},
	author={Bubeck, S{\'e}bastien},
	number={arXiv:1405.4980},
	type={Preprint},
	year={2014}
}

@article{ghadimi2013stochastic,
	title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
	author={Ghadimi, Saeed and Lan, Guanghui},
	journal={SIAM Journal on Optimization},
	volume={23},
	number={4},
	pages={2341--2368},
	year={2013},
	publisher={SIAM}
}

@inproceedings{yuan2019stagewise,
	title={Stagewise training accelerates convergence of testing error over SGD},
	author={Yuan, Zhuoning and Yan, Yan and Jin, Rong and Yang, Tianbao},
	booktitle={Advances in Neural Information Processing Systems},
	year={2019}
}

@inproceedings{charles2018stability,
	title={Stability and generalization of learning algorithms that converge to global optima},
	author={Charles, Zachary and Papailiopoulos, Dimitris},
	booktitle={International Conference on Machine Learning},
	year={2018}
}

@article{zhang2013communication,
	title={Communication-efficient algorithms for statistical optimization},
	author={Zhang, Yuchen and Duchi, John C. and Wainwright, Martin J.},
	journal={The Journal of Machine Learning Research},
	volume={14},
	number={1},
	pages={3321--3363},
	year={2013},
	publisher={JMLR. org}
}

@inproceedings{shamir2013stochastic,
	title={Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes},
	author={Shamir, Ohad and Zhang, Tong},
	booktitle={International Conference on Machine Learning},
	year={2013}
}

@inproceedings{fang2019sharp,
	title={Sharp Analysis for Nonconvex SGD Escaping from Saddle Points},
	author={Fang, Cong and Lin, Zhouchen and Zhang, Tong},
	booktitle={Conference on Learning Theory},
	year={2019}
}

@inproceedings{daneshmand2018escaping,
	title={Escaping Saddles with Stochastic Gradients},
	author={Daneshmand, Hadi and Kohler, Jonas and Lucchi, Aurelien and Hofmann, Thomas},
	booktitle={International Conference on Machine Learning},
	year={2018}
}

@inproceedings{xu2018first,
	title={First-order stochastic algorithms for escaping from saddle points in almost linear time},
	author={Xu, Yi and Jin, Rong and Yang, Tianbao},
	booktitle={Advances in Neural Information Processing Systems},
	year={2018}
}

@techreport{jin2019stochastic,
	title={Stochastic gradient descent escapes saddle points efficiently},
	author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M. and Jordan, Michael I.},
	number={arXiv:1902.04811},
	type={Preprint},
	year={2019}
}

@inproceedings{ge2015escaping,
	title={Escaping from saddle pointsâ€”online stochastic gradient for tensor decomposition},
	author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
	booktitle={Conference on Learning Theory},
	year={2015}
}

@inproceedings{mokhtari2018escaping,
	title={Escaping saddle points in constrained optimization},
	author={Mokhtari, Aryan and Ozdaglar, Asuman and Jadbabaie, Ali},
	booktitle={Advances in Neural Information Processing Systems},
	year={2018}
}

@book{nocedal2006numerical,
	title={Numerical optimization},
	author={Nocedal, Jorge and Wright, Stephen},
	year={2006},
	publisher={Springer Science \& Business Media}
}

@inproceedings{nesterov1983method,
	title={A method for solving the convex programming problem with convergence rate ${O}(1/k^{2}$)},
	author={Nesterov, Yurii E},
	booktitle={Dokl. Akad. nauk SSSR},
	volume={269},
	pages={543--547},
	year={1983}
}

@article{schmidt2017minimizing,
	title={Minimizing finite sums with the stochastic average gradient},
	author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
	journal={Mathematical Programming},
	volume={162},
	number={1-2},
	pages={83--112},
	year={2017},
	publisher={Springer}
}

@inproceedings{fang2018spider,
	title={SPIDER: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
	author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
	booktitle={Advances in Neural Information Processing Systems},
	year={2018}
}

@inproceedings{nguyen2017sarah,
	title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
	author={Nguyen, Lam M. and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
	booktitle={International Conference on Machine Learning},
	year={2017}
}

@article{bartlett2002rademacher,
	title={Rademacher and Gaussian complexities: Risk bounds and structural results},
	author={Bartlett, Peter L. and Mendelson, Shahar},
	journal={Journal of Machine Learning Research},
	volume={3},
	number={11},
	pages={463--482},
	year={2002}
}

@article{blumer1989learnability,
	title={Learnability and the Vapnik-Chervonenkis dimension},
	author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K.},
	journal={Journal of the ACM},
	volume={36},
	number={4},
	pages={929--965},
	year={1989},
	publisher={ACM New York, NY, USA}
}

@article{bousquet2002stability,
	title={Stability and generalization},
	author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
	journal={Journal of Machine Learning Research},
	volume={2},
	number={3},
	pages={499--526},
	year={2002}
}

@techreport{chen2018stability,
	title={Stability and convergence trade-off of iterative optimization algorithms},
	author={Chen, Yuansi and Jin, Chi and Yu, Bin},
	number={arXiv:1804.01619},
	type={Preprint},
	year={2018}
}

@inproceedings{meng2017generalization,
	title={Generalization error bounds for optimization algorithms via stability},
	author={Meng, Qi and Wang, Yue and Chen, Wei and Wang, Taifeng and Ma, Zhi-Ming and Liu, Tie-Yan},
	booktitle={Association for the Advancement of Artificial Intelligence},
	year={2017}
}

@inproceedings{bousquet2020sharper,
	title={Sharper bounds for uniformly stable algorithms},
	author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
	booktitle={Conference on Learning Theory},
	year={2020}
}

@article{bartlett2020benign,
	title={Benign overfitting in linear regression},
	author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
	journal={Proceedings of the National Academy of Sciences},
	volume={117},
	number={48},
	pages={30063--30070},
	year={2020},
	publisher={National Acad Sciences}
}

@inproceedings{shalev2009stochastic,
	title={Stochastic Convex Optimization.},
	author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
	booktitle={Conference on Learning Theory},
	year={2009}
}

@article{bottou2018optimization,
	title={Optimization methods for large-scale machine learning},
	author={Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
	journal={SIAM Review},
	volume={60},
	number={2},
	pages={223--311},
	year={2018},
	publisher={SIAM}
}

@inproceedings{tripuraneni2018stochastic,
	title={Stochastic cubic regularization for fast nonconvex optimization},
	author={Tripuraneni, Nilesh and Stern, Mitchell and Jin, Chi and Regier, Jeffrey and Jordan, Michael I.},
	booktitle={Advances in Neural Information Processing Systems},
	year={2018}
}

@article{cherkassky1999model,
	title={Model complexity control for regression using {VC} generalization bounds},
	author={Cherkassky, Vladimir and Shao, Xuhui and Mulier, Filip M. and Vapnik, Vladimir N.},
	journal={IEEE Transactions on Neural Networks},
	volume={10},
	number={5},
	pages={1075--1089},
	year={1999},
	publisher={IEEE}
}

@article{opper1994learning,
	title={Learning and generalization in a two-layer neural network: The role of the Vapnik-Chervonvenkis dimension},
	author={Opper, Manfred},
	journal={Physical Review Letters},
	volume={72},
	number={13},
	pages={2113},
	year={1994},
	publisher={APS}
}

@inproceedings{guyon1993automatic,
	title={Automatic capacity tuning of very large VC-dimension classifiers},
	author={Guyon, Isabelle and Boser, Bernhard E. and Vapnik, Vladimir},
	booktitle={Advances in Neural Information Processing Systems},
	year={1993}
}

@inproceedings{mohri2009rademacher,
	title={Rademacher complexity bounds for non-iid processes},
	author={Mohri, Mehryar and Rostamizadeh, Afshin},
	booktitle={Advances in Neural Information Processing Systems},
	year={2009}
}

@inproceedings{neyshabur2018role,
	title={The role of over-parametrization in generalization of neural networks},
	author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@article{williamson2001generalization,
	title={Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators},
	author={Williamson, Robert C. and Smola, Alexander J. and Scholkopf, Bernhard},
	journal={IEEE Transactions on Information Theory},
	volume={47},
	number={6},
	pages={2516--2532},
	year={2001},
	publisher={IEEE}
}

@article{zhang2002covering,
	title={Covering number bounds of certain regularized linear function classes},
	author={Zhang, Tong},
	journal={Journal of Machine Learning Research},
	volume={2},
	number={3},
	pages={527--550},
	year={2002}
}

@inproceedings{shawe1999generalization,
	title={Generalization performance of classifiers in terms of observed covering numbers},
	author={Shawe-Taylor, John and Williamson, Robert C.},
	booktitle={European Conference on Computational Learning Theory},
	year={1999}
}


@techreport{neyshabur2018towards,
	title={Towards understanding the role of over-parametrization in generalization of neural networks},
	author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	number={arXiv:1805.12076},
	type={Preprint},
	year={2018}
}

@inproceedings{roux2012stochastic,
	title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
	author={Roux, Nicolas Le and Schmidt, Mark and Bach, Francis},
	booktitle={Advances in Neural Information Processing Systems},
	year={2012}
}

@techreport{nguyen2017stochastic,
	title={Stochastic recursive gradient algorithm for nonconvex optimization},
	author={Nguyen, Lam M. and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
	number={arXiv:1705.07261},
	type={Preprint},
	year={2017}
}

@inproceedings{chen2018convergence,
	title={On the convergence of a class of adam-type algorithms for non-convex optimization},
	author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@inproceedings{2017hitting,
	title={A hitting time analysis of stochastic gradient langevin dynamics},
	author={Zhang, Yuchen and Liang, Percy and Charikar, Moses},
	booktitle={Conference on Learning Theory},
	year={2017}
}

@inproceedings{jin2018local,
	title={On the local minima of the empirical risk},
	author={Jin, Chi and Liu, Lydia T. and Ge, Rong and Jordan, Michael I.},
	booktitle={Advances in Neural Information Processing Systems},
	year={2018}
}

@article{bian2015complexity,
	title={Complexity analysis of interior point algorithms for non-Lipschitz and nonconvex minimization},
	author={Bian, Wei and Chen, Xiaojun and Ye, Yinyu},
	journal={Mathematical Programming},
	volume={149},
	number={1-2},
	pages={301--327},
	year={2015},
	publisher={Springer}
}

@article{cartis2018second,
	title={Second-order optimality and beyond: Characterization and evaluation complexity in convexly constrained nonlinear optimization},
	author={Cartis, Coralia and Gould, Nick IM. and Toint, Philippe L.},
	journal={Foundations of Computational Mathematics},
	volume={18},
	number={5},
	pages={1073--1107},
	year={2018},
	publisher={Springer}
}

@inproceedings{arora2019fine,
	title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
	author={Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	booktitle={International Conference on Machine Learning},
	year={2019}
}

@inproceedings{cao2019generalization,
	title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
	author={Cao, Yuan and Gu, Quanquan},
	booktitle={Advances in Neural Information Processing Systems},
	year={2019}
}

@inproceedings{allen2019learning,
	title={Learning and generalization in overparameterized neural networks, going beyond two layers},
	author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
	booktitle={Advances in Neural Information Processing Systems},
	year={2019}
}

@inproceedings{feldman2019high,
	title={High probability generalization bounds for uniformly stable algorithms with nearly optimal rate},
	author={Feldman, Vitaly and Vondrak, Jan},
	booktitle={Conference on Learning Theory},
	year={2019}
}

@article{bengio2015rmsprop,
	title={Rmsprop and equilibrated adaptive learning rates for nonconvex optimization},
	author={Bengio, Yoshua and CA, MONTREAL},
	journal={Corr abs/1502.04390},
	year={2015}
}

@article{robbins1951stochastic,
	title={A stochastic approximation method},
	author={Robbins, Herbert and Monro, Sutton},
	journal={The Annals of Mathematical Statistics},
	pages={400--407},
	year={1951},
	publisher={JSTOR}
}

@article{tieleman2012lecture,
	title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
	author={Tieleman, Tijmen and Hinton, Geoffrey},
	journal={COURSERA: Neural networks for machine learning},
	volume={4},
	number={2},
	pages={26--31},
	year={2012}
}

@article{scikit-learn,
	title={Scikit-learn: Machine Learning in {P}ython},
	author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}

@inproceedings{ge2016matrix,
	title={Matrix completion has no spurious local minimum},
	author={Ge, Rong and Lee, Jason D. and Ma, Tengyu},
	booktitle={Advances in Neural Information Processing Systems},
	year={2016}
}

@inproceedings{allen2019convergence,
	title={A convergence theory for deep learning via over-parameterization},
	author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	booktitle={International Conference on Machine Learning},
	year={2019},
}

@inproceedings{du2019gradient,
	title={Gradient descent finds global minima of deep neural networks},
	author={Du, Simon and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	booktitle={International Conference on Machine Learning},
	year={2019}
}

@inproceedings{zhang2017empirical,
	title={Empirical risk minimization for stochastic convex optimization: ${O}(1/n)$-and ${O}(1 / n^{2})$-type of risk bounds},
	author={Zhang, Lijun and Yang, Tianbao and Jin, Rong},
	booktitle={Conference on Learning Theory},
	year={2017}
}

@inproceedings{zhang2017hitting,
	title={A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics},
	author={Zhang, Yuchen and Liang, Percy and Charikar, Moses},
	booktitle={Conference on Learning Theory},
	year={2017}
}

@inproceedings{feldman2016generalization,
	title={Generalization of erm in stochastic convex optimization: The dimension strikes back},
	author={Feldman, Vitaly},
	booktitle={Advances in Neural Information Processing Systems},
	year={2016}
}

@inproceedings{hoffer2017train,
	title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	booktitle={Advances in Neural Information Processing Systems},
	year={2017}
}

@inproceedings{bassily2021stability,
	title={Stability of stochastic gradient descent on nonsmooth convex losses},
	author={Bassily, Raef and Feldman, Vitaly and Guzm{\'a}n, Crist{\'o}bal and Talwar, Kunal},
	booktitle={Advances in Neural Information Processing Systems},
	year={2021}
}

@inproceedings{karimi2016linear,
	title={Linear convergence of gradient and proximal-gradient methods under the Polyak-{\L}ojasiewicz condition},
	author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
	booktitle={European Conference on Machine Learning and Knowledge Discovery in Databases},
	year={2016}
}

@inproceedings{kawaguchi2016deep,
	title={Deep learning without poor local minima},
	author={Kawaguchi, Kenji},
	booktitle={Advances in Neural Information Processing Systems},
	year={2016}
}

@techreport{deng2020toward,
	title={Toward better generalization bounds with locally elastic stability},
	author={Deng, Zhun and He, Hangfeng and Su, Weijie},
	number={arXiv:2010.13988},
	type={Preprint},
	year={2020}
}

@article{lecun1998gradient,
	title={Gradient-based learning applied to document recognition},
	author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	journal={Proceedings of the IEEE},
	volume={86},
	number={11},
	pages={2278--2324},
	year={1998},
	publisher={IEEE}
}

@article{soudry2018implicit,
	title={The implicit bias of gradient descent on separable data},
	author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
	journal={The Journal of Machine Learning Research},
	volume={19},
	number={1},
	pages={2822--2878},
	year={2018},
	publisher={JMLR. org}
}

@techreport{jin2019short,
	title={A short note on concentration inequalities for random vectors with subgaussian norm},
	author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
	type={Preprint},
	number={arXiv:1902.03736},
	year={2019}
}

@inproceedings{ye2021ood,
	title={OoD-Bench: Benchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms},
	author={Ye, Nanyang and Li, Kaican and Hong, Lanqing and Bai, Haoyue and Chen, Yiting and Zhou, Fengwei and Li, Zhenguo},
	booktitle={Conference on Computer Vision and Pattern Recognition},
	year={2022}
}

@article{meinshausen2015maximin,
	title={Maximin effects in inhomogeneous large-scale data},
	author={Meinshausen, Nicolai and B{\"u}hlmann, Peter},
	journal={The Annals of Statistics},
	volume={43},
	number={4},
	pages={1801--1830},
	year={2015},
	publisher={Institute of Mathematical Statistics}
}

@article{rothenhausler2021anchor,
	title={Anchor regression: Heterogeneous data meet causality},
	author={Rothenh{\"a}usler, Dominik and Meinshausen, Nicolai and B{\"u}hlmann, Peter and Peters, Jonas},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={83},
	number={2},
	pages={215--246},
	year={2021},
	publisher={Wiley Online Library}
}

@article{rojas2018invariant,
	title={Invariant models for causal transfer learning},
	author={Rojas-Carulla, Mateo and Sch{\"o}lkopf, Bernhard and Turner, Richard and Peters, Jonas},
	journal={The Journal of Machine Learning Research},
	volume={19},
	number={1},
	pages={1309--1342},
	year={2018},
}

@techreport{arjovsky2019invariant,
	title={Invariant risk minimization},
	author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
	type={Preprint},
	number={arXiv:1907.02893},
	year={2019}
}

@inproceedings{li2017deeper,
	title={Deeper, broader and artier domain generalization},
	author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M},
	booktitle={IEEE International Conference on Computer Vision},
	year={2017}
}

@inproceedings{nouiehed2019solving,
	title={Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods},
	author={Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D and Razaviyayn, Meisam},
	journal={Advances in Neural Information Processing Systems},
	year={2019}
}

@inproceedings{yi2021reweighting,
	title={Reweighting Augmented Samples by Minimizing the Maximal Expected Loss},
	author={Yi, Mingyang and Hou, Lu and Shang, Lifeng and Jiang, Xin and Liu, Qun and Ma, Zhi-Ming},
	booktitle={International Conference on Learning Representations},
	year={2021}
}

@article{epasto2020optimal,
	title={Optimal Approximation-Smoothness Tradeoffs for Soft-Max Functions},
	author={Epasto, Alessandro and Mahdian, Mohammad and Mirrokni, Vahab and Zampetakis, Emmanouil},
	journal={Advances in Neural Information Processing Systems},
	year={2020}
}

@inproceedings{lin2020gradient,
	title={On gradient descent ascent for nonconvex-concave minimax problems},
	author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
	booktitle={International Conference on Machine Learning},
	year={2020}
}

@techreport{arpit2019predicting,
	title={Predicting with high correlation features},
	author={Devansh Arpit, Caiming Xiong, Richard Socher},
	number={arXiv:1910.00164},
	type={Preprint},
	year={2019}
}

@inproceedings{hendrycks2018benchmarking,
	title={Benchmarking neural network robustness to common corruptions and perturbations},
	author={Hendrycks, Dan and Dietterich, Thomas},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@inproceedings{recht2019imagenet,
	title={Do ImageNet classifiers generalize to imageNet?},
	author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	booktitle={International Conference on Machine Learning},
	year={2019}
}

@inproceedings{schneider2020improving,
	title={Improving robustness against common corruptions by covariate shift adaptation},
	author={Schneider, Steffen and Rusak, Evgenia and Eck, Luisa and Bringmann, Oliver and Brendel, Wieland and Bethge, Matthias},
	booktitle={Advances in Neural Information Processing Systems},
	year={2020}
}

@techreport{salman2020unadversarial,
	title={Unadversarial examples: designing objects for robust vision},
	author={Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Vemprala, Sai and Madry, Aleksander and Kapoor, Ashish},
	number={arXiv:2012.12235},
	type={Preprint},
	year={2020}
}

@article{tu2020empirical,
	title={An empirical study on robustness to spurious correlations using pre-trained language models},
	author={Tu, Lifu and Lalwani, Garima and Gella, Spandana and He, He},
	journal={Transactions of the Association for Computational Linguistics},
	volume={8},
	pages={621--633},
	year={2020},
	publisher={MIT Press}
}

@techreport{lohn2020estimating,
	title={Estimating the brittleness of AI: safety integrity levels and the need for testing out-of-distribution performance},
	author={Lohn, Andrew J},
	number={arXiv:2009.00802},
	type={Preprint},
	year={2020}
}

@inproceedings{mahajan2021domain,
	title={Domain generalization using causal matching},
	author={Mahajan, Divyat and Tople, Shruti and Sharma, Amit},
	booktitle={International Conference on Machine Learning},
	year={2021}
}

@inproceedings{hu2020domain,
	title={Domain generalization via multidomain discriminant analysis},
	author={Hu, Shoubo and Zhang, Kun and Chen, Zhitang and Chan, Laiwan},
	booktitle={Uncertainty in Artificial Intelligence},
	year={2020},
}

@inproceedings{li2018deep,
	title={Deep domain generalization via conditional invariant adversarial networks},
	author={Li, Ya and Tian, Xinmei and Gong, Mingming and Liu, Yajing and Liu, Tongliang and Zhang, Kun and Tao, Dacheng},
	booktitle={European Conference on Computer Vision},
	year={2018}
}


@inproceedings{sinha2018certifying,
	title={Certifying some distributional robustness with principled adversarial training},
	author={Sinha, Aman and Namkoong, Hongseok and Duchi, John},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@inproceedings{volpi2018generalizing,
	title={Generalizing to unseen domains via adversarial data augmentation},
	author={Volpi, Riccardo and Namkoong, Hongseok and Sener, Ozan and Duchi, John and Murino, Vittorio and Savarese, Silvio},
	booktitle={Advances in Neural Information Processing Systems},
	year={2018}
}

@inproceedings{yi2021improved,
	title={Improved OOD Generalization via Adversarial Training and Pre-training},
	author={Yi, Mingyang and Hou, Lu and Sun, Jiacheng and Shang, Lifeng and Jiang, Xin and Liu, Qun and Ma, Zhi-Ming},
	booktitle={International Conference on Machine Learning},
	year={2021}
}

@article{he2021towards,
	title={Towards non-iid image classification: A dataset and baselines},
	author={He, Yue and Shen, Zheyan and Cui, Peng},
	journal={Pattern Recognition},
	volume={110},
	pages={107383},
	year={2021},
	publisher={Elsevier}
}

@inproceedings{liu2021heterogeneous,
	title={Heterogeneous Risk Minimization},
	author={Liu, Jiashuo and Hu, Zheyuan and Cui, Peng and Li, Bo and Shen, Zheyan},
	booktitle={International Conference on Machine Learning},
	year={2021},
}

@inproceedings{chang2020invariant,
	title={Invariant rationalization},
	author={Chang, Shiyu and Zhang, Yang and Yu, Mo and Jaakkola, Tommi},
	booktitle={International Conference on Machine Learning},
	year={2020},
}

@techreport{gimenez2021identifying,
	title={Identifying invariant factors across multiple environments with kl regression},
	author={Gimenez, Jaime Roquero and Zou, James},
	type={Preprint},
	number={arXiv:2002.08341},
	year={2021}
}

@book{van2000weak,
	author={Aad W. van der Vaart and Jon A. Wellner},  
	year= {2000}, 
	series={Springer series in statistics},
	title={Weak convergence and empirical processes},
	publisher={Springer}
}

@article{bernhard1995theorem,
	title={On a theorem of Danskin with an application to a theorem of Von Neumann-Sion},
	author={Bernhard, Pierre and Rapaport, Alain},
	journal={Nonlinear Analysis: Theory, Methods \& Applications},
	volume={24},
	number={8},
	pages={1163--1181},
	year={1995},
	publisher={Pergamon}
}

@inproceedings{gulrajani2020search,
	title={In Search of Lost Domain Generalization},
	author={Gulrajani, Ishaan and Lopez-Paz, David},
	booktitle={International Conference on Learning Representations},
	year={2020}
}

@inproceedings{geirhos2018imagenet,
	title={ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
	author={Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@inproceedings{beery2018recognition,
	title={Recognition in terra incognita},
	author={Beery, Sara and Van Horn, Grant and Perona, Pietro},
	booktitle={European Conference on Computer Vision},
	year={2018}
}

@article{sagawa2019distributionally,
	title={Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization},
	author={Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B and Liang, Percy},
	booktitle={International Conference on Learning Representations},
	year={2019}
}

@inproceedings{li2018adversarial,
	title={Domain generalization with adversarial feature learning},
	author={Li, Haoliang and Pan, Sinno Jialin and Wang, Shiqi and Kot, Alex C},
	booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
	year={2018}
}

@inproceedings{fang2013unbiased,
	title={Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias},
	author={Fang, Chen and Xu, Ye and Rockmore, Daniel N},
	booktitle={IEEE International Conference on Computer Vision},
	year={2013}
}

@inproceedings{zhu2017unpaired,
	title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
	author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
	booktitle={IEEE International Conference on Computer Vision},
	year={2017}
}

@inproceedings{azizzadenesheli2018regularized,
	title={Regularized Learning for Domain Adaptation under Label Shifts},
	author={Azizzadenesheli, Kamyar and Liu, Anqi and Yang, Fanny and Anandkumar, Animashree},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@article{kpotufe2021marginal,
	title={Marginal singularity and the benefits of labels in covariate-shift},
	author={Kpotufe, Samory and Martinet, Guillaume},
	journal={The Annals of Statistics},
	volume={49},
	number={6},
	pages={3299--3323},
	year={2021},
	publisher={Institute of Mathematical Statistics}
}

@inproceedings{liu2015deep,
	title={Deep learning face attributes in the wild},
	author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
	booktitle={IEEE International Conference on Computer Vision},
	year={2015}
}

@techreport{wah2011caltech,
	title={The caltech-ucsd birds-200-2011 dataset},
	author={Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
	year={2011},
	publisher={California Institute of Technology}
}

@article{zhou2017places,
	title={Places: A 10 million image database for scene recognition},
	author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={40},
	number={6},
	pages={1452--1464},
	year={2017},
	publisher={IEEE}
}

@inproceedings{deng2009imagenet,
	title={Imagenet: A large-scale hierarchical image database},
	author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
	year={2009}
}

@book{vapnik1999nature,
	title={The nature of statistical learning theory},
	author={Vapnik, Vladimir},
	year={1999},
	publisher={Springer science \& business media}
}

@techreport{seo2022information,
	title={Information-Theoretic Bias Reduction via Causal View of Spurious Correlation},
	author={Seo, Seonguk and Lee, Joon-Young and Han, Bohyung},
	type={Preprint},
	number={arXiv:2201.03121},
	year={2022}
}

@inproceedings{williams2018broad,
	title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
	author={Williams, Adina and Nangia, Nikita and Bowman, Samuel},
	booktitle={Conference of the North American Chapter of the Association for Computational Linguistics},
	year={2018}
}

@inproceedings{borkan2019nuanced,
	title={Nuanced metrics for measuring unintended bias with real data for text classification},
	author={Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
	booktitle={World Wide Web conference},
	year={2019}
}

@inproceedings{xie2020in,
	title={In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness},
	author={Xie, Sang Michael and Kumar, Ananya and Jones, Robbie and Khani, Fereshte and Ma, Tengyu and Liang, Percy},
	booktitle={International Conference on Learning Representations},
	year={2020}
}

@article{wald2021calibration,
	title={On calibration and out-of-domain generalization},
	author={Wald, Yoav and Feder, Amir and Greenfeld, Daniel and Shalit, Uri},
	journal={Advances in Neural Information Processing Systems},
	year={2021}
}

@article{heinze2021conditional,
	title={Conditional variance penalties and domain shift robustness},
	author={Heinze-Deml, Christina and Meinshausen, Nicolai},
	journal={Machine Learning},
	volume={110},
	number={2},
	pages={303--348},
	year={2021},
	publisher={Springer}
}

@inproceedings{krueger2021out,
	title={Out-of-distribution generalization via risk extrapolation (rex)},
	author={Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Le Priol, Remi and Courville, Aaron},
	booktitle={International Conference on Machine Learning},
	year={2021}
}

@article{levy2020large,
	title={Large-scale methods for distributionally robust optimization},
	author={Levy, Daniel and Carmon, Yair and Duchi, John C and Sidford, Aaron},
	journal={Advances in Neural Information Processing Systems},
	year={2020}
}

@article{ben2013robust,
	title={Robust solutions of optimization problems affected by uncertain probabilities},
	author={Ben-Tal, Aharon and Den Hertog, Dick and De Waegenaere, Anja and Melenberg, Bertrand and Rennen, Gijs},
	journal={Management Science},
	volume={59},
	number={2},
	pages={341--357},
	year={2013},
	publisher={INFORMS}
}

@article{shapiro2017distributionally,
	title={Distributionally robust stochastic programming},
	author={Shapiro, Alexander},
	journal={SIAM Journal on Optimization},
	volume={27},
	number={4},
	pages={2258--2275},
	year={2017},
	publisher={SIAM}
}

@inproceedings{liu2021just,
	title={Just train twice: Improving group robustness without training group information},
	author={Liu, Evan Z and Haghgoo, Behzad and Chen, Annie S and Raghunathan, Aditi and Koh, Pang Wei and Sagawa, Shiori and Liang, Percy and Finn, Chelsea},
	booktitle={International Conference on Machine Learning},
	year={2021}
}

@techreport{idrissi2021simple,
	title={Simple data balancing achieves competitive worst-group-accuracy},
	author={Idrissi, Badr Youbi and Arjovsky, Martin and Pezeshki, Mohammad and Lopez-Paz, David},
	type={Preprint},
	journal={arXiv:2110.14503},
	year={2021}
}

@inproceedings{wang2022out,
	title={Out-of-distribution Generalization with Causal Invariant Transformations},
	author={Wang, Ruoyu and Yi, Mingyang and Chen, Zhitang and Zhu, Shengyu},
	booktitle={Conference on Computer Vision and Pattern Recognition},
	year={2022}
}

@inproceedings{gururangan2018annotation,
	title={Annotation Artifacts in Natural Language Inference Data},
	author={Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A},
	booktitle={North American Chapter of the Association for Computational Linguistics},
	year={2018}
}

@inproceedings{devlin2019bert,
	title={BERT: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle={Conference of the North American Chapter of the Association for Computational Linguistics},
	year={2019}
}

@inproceedings{loshchilov2018decoupled,
	title={Decoupled Weight Decay Regularization},
	author={Loshchilov, Ilya and Hutter, Frank},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@article{ye2021towards,
	title={Towards a theoretical framework of out-of-distribution generalization},
	author={Ye, Haotian and Xie, Chuanlong and Cai, Tianle and Li, Ruichen and Li, Zhenguo and Wang, Liwei},
	journal={Advances in Neural Information Processing Systems},
	year={2021}
}

@inproceedings{zhou2022model,
	title={Model Agnostic Sample Reweighting for Out-of-Distribution Learning},
	author={Zhou, Xiao and Lin, Yong and Pi, Renjie and Zhang, Weizhong and Xu, Renzhe and Cui, Peng and Zhang, Tong},
	booktitle={International Conference on Machine Learning},
	year={2022}
}

@inproceedings{veitch2021counterfactual,
	title={Counterfactual invariance to spurious correlations: Why and how to pass stress tests},
	author={Veitch, Victor and D'Amour, Alexander and Yadlowsky, Steve and Eisenstein, Jacob},
	booktitle={Advances in Neural Information Processing Systems},
	year={2021}
}

@book{shiryaev2016probability,
	title={Probability-1},
	author={Shiryaev, Albert N},
	volume={95},
	year={2016},
	publisher={Springer}
}

@article{long2018conditional,
	title={Conditional adversarial domain adaptation},
	author={Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Jordan, Michael I},
	journal={Advances in Neural Information Processing Systems},
	year={2018}
}

@article{sohoni2020no,
	title={No subclass left behind: Fine-grained robustness in coarse-grained classification problems},
	author={Sohoni, Nimit and Dunnmon, Jared and Angus, Geoffrey and Gu, Albert and R{\'e}, Christopher},
	journal={Advances in Neural Information Processing Systems},
	year={2020}
}

@inproceedings{creager2021environment,
	title={Environment inference for invariant learning},
	author={Creager, Elliot and Jacobsen, J{\"o}rn-Henrik and Zemel, Richard},
	booktitle={International Conference on Machine Learning},
	year={2021}
}

@inproceedings{makar2022causally,
	title={Causally motivated shortcut removal using auxiliary labels},
	author={Makar, Maggie and Packer, Ben and Moldovan, Dan and Blalock, Davis and Halpern, Yoni and Dâ€™Amour, Alexander},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	year={2022}
}

@techreport{makar2022fairness,
	title={Fairness and robustness in anti-causal prediction},
	author={Makar, Maggie and D'Amour, Alexander},
	journal={arXiv:2209.09423},
	type={Preprint}, 
	year={2022}
}