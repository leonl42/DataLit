% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath,bm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{enumitem}
\usepackage{url}
\usepackage{ntheorem}
\usepackage{wrapfig}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% \usepackage[skip=4pt]{caption}
\usepackage[font=small,skip=3pt]{caption}
\setlength{\textfloatsep}{9.5pt plus 2.0pt minus 2.0pt}
\setlength{\floatsep}{9.5pt plus 2.0pt minus 2.0pt}
\setlength{\intextsep}{9.5pt plus 2.0pt minus 2.0pt}
\linespread{0.985}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{1269} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}
\graphicspath{{./figures/}}

\newtoggle{independent_supp}        % Camera-ready version
\togglefalse{independent_supp}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\Large Structured Local Radiance Fields for Human Avatar Modeling}

\author{
	Zerong Zheng\textsuperscript{1},
	Han Huang\textsuperscript{2},
	Tao Yu\textsuperscript{1},
	Hongwen Zhang\textsuperscript{1},
	Yandong Guo\textsuperscript{2},
	Yebin Liu\textsuperscript{1}
	\\ \\
	\textsuperscript{1}Department of Automation, Tsinghua University
	\quad
	\textsuperscript{2}OPPO Research Institute 
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}

% We propose a new representation for clothed human characters in this paper. Our representation is built upon neural radiance fields, or NeRF in short, for its excellent performance in learning the appearance of static scenes. To extend NeRF for dynamic character modeling, we break a global NeRF into a set of structured local radiance fields, which are uniformly attached to the surface of SMPL model. Each of the local radiance fields is responsible for representing the shape and appearance in the local space around its center. The local radiance fields can be driven by the body skeleton, while having their own residual movements to represent the middle-frequency non-rigid deformation of garments. Furthermore, each local radiance field is conditioned on a dynamic detail embedding, which encodes the high-frequency dynamic details that cannot be modeled via node translation. In this way, we factorize the cloth deformations in a coarse-to-fine manner: the coarsest level is the skeleton motion, the middle level is the residual movements of the local radiance fields, and the finest level is the time-varying details inside each local radiance field.


  
It is extremely challenging to create an animatable clothed human avatar from RGB videos, especially for loose clothes due to the difficulties in motion modeling. To address this problem, we introduce a novel representation on the basis of recent neural scene  rendering techniques. The core of our representation is a set of structured local radiance fields, which are anchored to the pre-defined nodes sampled on a statistical human body template. These local radiance fields not only leverage the flexibility of implicit representation in shape and appearance modeling, but also factorize cloth deformations into skeleton motions, node residual translations and the dynamic detail variations inside each individual radiance field. To learn our representation from RGB data and facilitate pose generalization, we propose to learn the node translations and the detail variations in a conditional generative latent space. Overall, our method enables automatic construction of animatable human avatars for various types of clothes without the need for scanning subject-specific templates, and can generate realistic images with dynamic details for novel poses. Experiment show that our method outperforms state-of-the-art methods both qualitatively and quantitatively. 


\end{abstract}

\input{1_intro}
\input{2_related_work}
\input{3_overview}
\input{4_method}
\input{5_results}
\input{6_conclusion}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\renewcommand\thesection{\Alph{section}}
\renewcommand\thefigure{\Alph{figure}}
\renewcommand\thetable{\Alph{table}}
\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\noindent\textbf{\Large Supplemental Document} 
\input{9_supp_context}


\end{document}
