\begin{table} [htbp]
\caption{ReLU scaling in stage 2 of ``DeepReDuce''}
\label{tab:ReluScaling}
\centering 
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{ p {2.5cm} |c } 
 \toprule
 {\bf Scaling methods} &  $Net \text{(DR)}$  \\ \toprule
 Channel scaling by $\alpha$ &   $Net (\alpha\hat{\phi}, \alpha^2\Psi, \alpha^2\Gamma, \hat{l_{r}}, \hat{b_{r}}, \hat{s_{r}}, \hat{l_{nr}}, \hat{b_{nr}}, \hat{s_{nr}})$ \\ \midrule
 Fmap scaling by $\rho$   &   $Net (\rho^2\hat{\phi}, \Psi, \rho^2\Gamma, \hat{l_{r}}, \hat{b_{r}}, \hat{s_{r}}, \hat{l_{nr}}, \hat{b_{nr}}, \hat{s_{nr}})$\\ \midrule
 Layer-wise ReLU dropout (2:1)  &   $Net (\frac{\hat{\phi}}{2}, \Psi,  \Gamma, \frac{\hat{l_{r}}}{2}, \hat{b_{r}}, \hat{s_{r}}, L-\frac{\hat{l_{r}}}{2}, \hat{b_{nr}}, \hat{s_{nr}})$\\ \midrule
 Scaling \#Blocks in ReLU-stages (2:1) & $Net (\frac{\hat{\phi}}{2}, < \Psi,  < \Gamma, \frac{\hat{l_{r}}}{2}, \frac{\hat{b_{r}}}{2}, \hat{s_{r}}, L-\hat{l_{r}}, B-\hat{b_{r}}, \hat{s_{nr}})$ \\ 
 \bottomrule
\end{tabular} }

\end{table}
