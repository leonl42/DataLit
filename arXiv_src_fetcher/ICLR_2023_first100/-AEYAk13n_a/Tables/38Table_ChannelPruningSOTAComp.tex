\begin{table} [t] \centering
\caption{Performance comparison of channel pruning \cite{he2020learning} and DeepReDuce for FLOPs and ReLU saving, and accuracy drop on ResNet56 with CIFAR-10 (C10) and CIFAR-100 (C100) datasets. DeepReDuce models save significantly higher \#ReLUs at similar FLOPs saving and accuracy drop.}
\label{tab:PruningSOTAComp} 
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{ccp{1.2cm}p{1cm}p{0.8cm}cc}\toprule
& Method & Baseline Acc.(\%) & Pruned Acc.(\%) & Acc. $\downarrow$(\%) & FLOPs & ReLUs \\ \toprule
\multirow{3}{*}{ \rotatebox[origin=c]{90}{C10} } & Ch. pruning  & 93.59 & 93.34 & -0.25 & 59.1M & 311.7K \\ \cline{2-7}
& \multirow{2}{*}{DeepReDuce} & \multirow{2}{*}{93.48} & 94.07 & +0.59 & 87.7M & 221.2K \\
& & & 93.16 & -0.32 & 66.5M & 147.5K  \\ \midrule
\multirow{3}{*}{ \rotatebox[origin=c]{90}{C100} } & Ch. pruning  & 71.41 & 70.83 & -0.58 & 60.8M & 311.7K  \\ \cline{2-7}
& \multirow{2}{*}{DeepReDuce} & \multirow{2}{*}{70.93} & 73.66 & +2.57 & 87.7M & 221.2K \\
& & & 71.68 & +0.59 & 66.5M & 147.5K  \\
\bottomrule
\end{tabular}}
\vspace{-2em}
\end{table}



