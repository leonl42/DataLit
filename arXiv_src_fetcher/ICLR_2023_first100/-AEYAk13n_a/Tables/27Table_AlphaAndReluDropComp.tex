\begin{table} [t]
\caption{
Performance comparison of channel scaling ($\alpha$=0.5) and dropping ReLUs 
from alternate layers (S$_k^{RT}$ where $k$ is the network stage) using ResNet18 on CIFAR-100. 
Both methods reduce ReLUs by a factor of 2$\times$; 
however, alternate ReLU dropping results in more accurate iso-ReLU networks.}
\label{tab:AlphaVsReluDrop}
\centering 
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{ccccc} 
 \toprule
 Network & \#Conv & \#ReLUs & W/o KD(\%) & W/ KD(\%) \\ \toprule
%\multirow{1}{*}{\bf Network } & \multirow{1}{*}{\bf \#Conv } & \multirow{1}{*}{\bf \#ReLU } & \multicolumn{2}{c}{\bf CIFAR-100 } \\
%& & & {\bf w/o kD} & {\bf w/ KD} \\
 
$S_{2}$+$S_{3}$+$S_{4}$  & 17 & 229K & 73.14 & 76.22 \\
$S_{2}^{RT}$+$S_{3}^{RT}$+$S_{4}^{RT}$ & 17 & 115K & 72.97 & 74.72 \\
$S_{2}$+$S_{3}$+$S_{4}$, $\alpha$=0.5 & 17 & 115K & 71.59 & 73.78 \\ \midrule
%$Arch2$ $\rightarrow$ Res18(FR) - relu[conv1 + S1] & 11 & 115K & 71.96 &	74.57 \\ 
$S_{2}$+$S_{3}$  & 17 & 197K & 72.77 & 75.51 \\
$S_{2}^{RT}$+$S_{3}^{RT}$ & 17 & 98K & 70.97 & 71.95 \\
$S_{2}$+$S_{3}$, $\alpha$=0.5 & 17 & 98K & 69.54 & 71.16 \\ \midrule
%$Arch2$ $\rightarrow$ Res18(FR) - relu[conv1 + S1 +S4] & 13 & 98K & 70.15 &	72.17 \\ \midrule
$S_{3}$+$S_{4}$ & 17 & 98K & 68.4 & 73.16 \\
$S_{3}^{RT}$+$S_{4}^{RT}$ & 17 &49K & 69.62 & 71.06 \\
$S_{3}$+$S_{4}$, $\alpha$=0.5 & 17 & 49K & 66.43 & 70.29 \\ 
%$Arch2$ $\rightarrow$ Res18(FR) - relu[conv1 + S1 + S2] & 13 & 49K & 66.68 &	69.79 \\
\bottomrule
\end{tabular}} 
\vspace{-1.5em}
\end{table}
