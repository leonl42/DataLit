\section{Generality of DeepReDuce} \label{sec:Generality}
In this section we show DeepReDuce generalizes beyond ResNets and does not depend on residual connections.
Note that VGG16 was shown in Section~\ref{sec:ExperimentalResults},
here we provide additional analysis for completeness.

{\bf Effect of residual connections:} \textcolor{red}{
We first examine the impact of ResNets' residual connections 
%(present in ResNet \cite{he2016deep}) 
on the efficacy of DeepReDuce. 
We removed all the residual connections in ResNet18 (student network) and evaluate the stage-wise ReLU criticality, the first step in DeepReDuce. 
We then perform KD using both a residual teacher (Full-ReLU ResNet18) and a non-residual teacher network for a complete evaluation of impact of residual connections on ReLUs' criticality. 
Results are shown in Table \ref{tab:CriticalityNonRes}.} 
\textcolor{red}{
The accuracy degradation is substantial and varies with the position of ReLUs in the networks. 
When KD is performed with non-residual teacher, the recovered accuracy is less than KD with a residual teacher. 
Nonetheless, the (relative) criticality order of non-residual ResNet18 is same as residual ResNet18  (Table \ref{tab:ReluHeteroR18}). } 


\input{Tables/40Table_ReLUCriticalityNonRes}



{\bf DeepReDuce on MobileNets:} 
\textcolor{red}{Next, we examine the effectiveness of DeepReDuce on MobileNetV1 \cite{howard2017mobilenets}.
We chose to study MobileNet as it is very differenct from ResNet---the convolution layers
do not use residuals and the Depthwise architecture is FLOP optimized, which we believe is a poor match for DeepReDuce's ReLU minimization objective.
%types (a non-residual network with FLOPs-optimized Depthwise separable convolution) are significantly different than that of ResNet models.
We first evaluate the ReLUs' criticality (see Table \ref{tab:CriticalityInMobileNets} in Appendix \ref{SecAppendix:ParetoPointsMV1}) and then compare the performance of DeepReDuce models with conventionally (channel/fmap-resolution) scaled MobileNetV1. 
Results are shown in Figure \ref{fig:ParetoFrontierMV1} and the optimization steps for all DeepReDuce models are listed in Table \ref{tab:ParetoPointsMV1} in Appendix \ref{SecAppendix:ParetoPointsMV1}. 
The substantial gain (reduction) in accuracy (ReLU count) at iso-ReLU (iso-accuracy) shows the effectiveness of DeepReDuce ReLU optimization on MobileNetV1 \fixme{add numbers here? sounds generic.}.
 }

\textcolor{red}{
Thus, while residual connections benefit DeepReDuce, they are not
a necessity as DeepReDuce also performs well when residual connections are eliminated from the network and the non-residual networks such as MobileNets and VGG16 (Figure \ref{fig:ParetoFrontierVGG16}).}



%First, we evaluate the criticality of ReLUs in MobileNets (V1 \cite{howard2017mobilenets} and V2 \cite{sandler2018mobilenetv2}) on the CIFAR-100 and computed the stage-wise criticality metric $C_k$ (Eq. \ref{eqn:CriticalityMetric}). The results are shown in Table \ref{tab:CriticalityInMobileNets}.  We observed the similar trend as ResNet18 and ResNet34 on CIFAR-100/TinyImageNet (shown in Tables \ref{tab:ReluHeteroR18} and \ref{tab:ResNetBlockDropout}), accuracy differs significantly across stages and $S_1$ ($S_4$) ReLUs are least (most) critical.

%\textcolor{red}{
%Further, to examine the efficacy of DeepReDuce on MobileNetV1, we compare the performance of DeepReDuce models with the conventionally (channel/fmap-resolution) scaled MobileNetV1. Results are shown in Figure \ref{fig:ParetoFrontierMV1} and the optimization steps for all the DeepReDuce models are listed in Table \ref{tab:ParetoPointsMV1} in Appendix \ref{SecAppendix:ParetoPointsMV1}. The substantial gain (reduction) in accuracy (ReLU count) at iso-ReLU (iso-accuracy) shows the effectiveness of DeepReDuce ReLU optimization on MobileNetV1  whose architecture and convolution types (a non-residual network with FLOPs-optimized Depthwise separable convolution) are significantly different than that of ResNet models. }






%{\bf DeepReDuce vs. conventional scaling in MobileNets}