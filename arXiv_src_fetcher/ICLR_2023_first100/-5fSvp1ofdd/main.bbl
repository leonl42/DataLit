\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Andrychowicz \bgroup \em et al.\egroup
  }{2017}]{andrychowicz2017hindsight}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, OpenAI~Pieter Abbeel, and Wojciech
  Zaremba.
\newblock Hindsight experience replay.
\newblock In {\em NeurIPS}, 2017.

\bibitem[\protect\citeauthoryear{Brockman \bgroup \em et al.\egroup
  }{2016}]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[\protect\citeauthoryear{Fan \bgroup \em et al.\egroup
  }{2018}]{fan2018learning}
Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu.
\newblock Learning to teach.
\newblock In {\em ICLR}, 2018.

\bibitem[\protect\citeauthoryear{Hessel \bgroup \em et al.\egroup
  }{2018}]{hessel2018rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In {\em AAAI}, 2018.

\bibitem[\protect\citeauthoryear{Isele and Cosgun}{2018}]{isele2018selective}
David Isele and Akansel Cosgun.
\newblock Selective experience replay for lifelong learning.
\newblock In {\em AAAI}, 2018.

\bibitem[\protect\citeauthoryear{Lillicrap \bgroup \em et al.\egroup
  }{2016}]{lillicrap2016continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In {\em ICLR}, 2016.

\bibitem[\protect\citeauthoryear{Lin}{1992}]{lin1992self}
Long-Ji Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock {\em Machine learning}, 8(3-4):293--321, 1992.

\bibitem[\protect\citeauthoryear{Lin}{1993}]{lin1993reinforcement}
Long-Ji Lin.
\newblock Reinforcement learning for robots using neural networks.
\newblock Technical report, Carnegie-Mellon Univ Pittsburgh PA School of
  Computer Science, 1993.

\bibitem[\protect\citeauthoryear{Liu and Zou}{2017}]{liu2017effects}
Ruishan Liu and James Zou.
\newblock The effects of memory replay in reinforcement learning.
\newblock {\em arXiv preprint arXiv:1710.06574}, 2017.

\bibitem[\protect\citeauthoryear{Mattar and Daw}{2018}]{mattar2018prioritized}
Marcelo~Gomes Mattar and Nathaniel~D Daw.
\newblock Prioritized memory access explains planning and hippocampal replay.
\newblock {\em bioRxiv}, page 225664, 2018.

\bibitem[\protect\citeauthoryear{Mnih \bgroup \em et al.\egroup
  }{2013}]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock In {\em NIPS Deep Learning Workshop}, 2013.

\bibitem[\protect\citeauthoryear{Mnih \bgroup \em et al.\egroup
  }{2015}]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529, 2015.

\bibitem[\protect\citeauthoryear{Novati and
  Koumoutsakos}{2018}]{novati2018remember}
Guido Novati and Petros Koumoutsakos.
\newblock Remember and forget for experience replay.
\newblock {\em arXiv preprint arXiv:1807.05827}, 2018.

\bibitem[\protect\citeauthoryear{Pan \bgroup \em et al.\egroup
  }{2018}]{pan2018organizing}
Yangchen Pan, Muhammad Zaheer, Adam White, Andrew Patterson, and Martha White.
\newblock Organizing experience: a deeper look at replay mechanisms for
  sample-based planning in continuous state domains.
\newblock In {\em IJCAI}, 2018.

\bibitem[\protect\citeauthoryear{Schaul \bgroup \em et al.\egroup
  }{2016}]{schaul2016prioritized}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock In {\em ICML}, 2016.

\bibitem[\protect\citeauthoryear{Shohamy and
  Daw}{2015}]{shohamy2015integrating}
Daphna Shohamy and Nathaniel~D Daw.
\newblock Integrating memories to guide decisions.
\newblock {\em Current Opinion in Behavioral Sciences}, 5:85--90, 2015.

\bibitem[\protect\citeauthoryear{Silver \bgroup \em et al.\egroup
  }{2014}]{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In {\em ICML}, 2014.

\bibitem[\protect\citeauthoryear{Sutton and
  Barto}{2018}]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[\protect\citeauthoryear{Todorov \bgroup \em et al.\egroup
  }{2012}]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em IROS}, 2012.

\bibitem[\protect\citeauthoryear{Van~Hasselt \bgroup \em et al.\egroup
  }{2016}]{van2016deep}
Hado Van~Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In {\em AAAI}, 2016.

\bibitem[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup
  }{2017}]{wang2016sample}
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray
  Kavukcuoglu, and Nando de~Freitas.
\newblock Sample efficient actor-critic with experience replay.
\newblock In {\em ICLR}, 2017.

\bibitem[\protect\citeauthoryear{Williams}{1992}]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3-4):229--256, 1992.

\bibitem[\protect\citeauthoryear{Wu \bgroup \em et al.\egroup
  }{2018}]{wu2018learning}
Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Lai Jian-Huang, and Tie-Yan
  Liu.
\newblock Learning to teach with dynamic loss functions.
\newblock In {\em NeurIPS}, 2018.

\bibitem[\protect\citeauthoryear{Xu \bgroup \em et al.\egroup
  }{2018a}]{xu2018learning}
Tianbing Xu, Qiang Liu, Liang Zhao, Wei Xu, and Jian Peng.
\newblock Learning to explore with meta-policy gradient.
\newblock In {\em ICML}, 2018.

\bibitem[\protect\citeauthoryear{Xu \bgroup \em et al.\egroup
  }{2018b}]{xu2018meta}
Zhongwen Xu, Hado van Hasselt, and David Silver.
\newblock Meta-gradient reinforcement learning.
\newblock In {\em NeurIPS}, 2018.

\bibitem[\protect\citeauthoryear{Yin and Pan}{2017}]{yin2017knowledge}
Haiyan Yin and Sinno~Jialin Pan.
\newblock Knowledge transfer for deep reinforcement learning with hierarchical
  experience replay.
\newblock In {\em AAAI}, 2017.

\bibitem[\protect\citeauthoryear{Zhang and Sutton}{2017}]{zhang2017deeper}
Shangtong Zhang and Richard~S Sutton.
\newblock A deeper look at experience replay.
\newblock {\em NIPS Deep Reinforcement Learning Symposium}, 2017.

\bibitem[\protect\citeauthoryear{Zheng \bgroup \em et al.\egroup
  }{2018}]{zheng2018learning}
Zeyu Zheng, Junhyuk Oh, and Satinder Singh.
\newblock On learning intrinsic rewards for policy gradient methods.
\newblock In {\em NeurIPS}, 2018.

\end{thebibliography}
