<?xml version="1.0" encoding="UTF-8"?>
<?latexml searchpaths="/home/miri/Documents/DataLit/arXiv_src_fetcher/ICLR_2023_first100/-4HJSA3Y2vg"?>
<?latexml class="article"?>
<?latexml package="geometry" options="margin=1in"?>
<?latexml package="iclr2023_conference,times"?>
<?latexml package="amsmath,amsfonts,bm"?>
<?latexml package="hyperref"?>
<?latexml package="url"?>
<?latexml package="booktabs"?>
<?latexml package="amsfonts"?>
<?latexml package="nicefrac"?>
<?latexml package="microtype"?>
<?latexml package="xcolor"?>
<?latexml package="enumitem"?>
<?latexml package="bbold"?>
<?latexml package="graphicx"?>
<?latexml package="booktabs"?>
<?latexml package="todonotes"?>
<?latexml package="comment"?>
<?latexml package="caption"?>
<?latexml package="subcaption"?>
<?latexml package="cleveref"?>
<?latexml package="amssymb"?>
<?latexml package="amsmath"?>
<?latexml package="pifont"?>
<?latexml package="wrapfig"?>
<?latexml RelaxNGSchema="LaTeXML"?>
<document xmlns="http://dlmf.nist.gov/LaTeXML" class="ltx_authors_1line">
  <resource src="LaTeXML.css" type="text/css"/>
  <resource src="ltx-article.css" type="text/css"/>
  <para xml:id="p1">
    <ERROR class="undefined">\iclrfinalcopy</ERROR>
  </para>
  <title>BARTSmiles: Generative Masked Language <break/>Models for Molecular Representations</title>
  <creator role="author">
    <personname>Gayane Chilingaryan<sup>1</sup><note mark="1" role="footnotemark" xml:id="footnotex1"><tags>
          <tag>1</tag>
          <tag role="autoref">footnote 1</tag>
          <tag role="refnum">1</tag>
          <tag role="typerefnum">footnote 1</tag>
        </tags></note> , Hovhannes Tamoyan<sup>1</sup><note mark="1" role="footnotemark" xml:id="footnotex2"><tags>
          <tag>1</tag>
          <tag role="autoref">footnote 1</tag>
          <tag role="refnum">1</tag>
          <tag role="typerefnum">footnote 1</tag>
        </tags></note> , Ani Tevosyan<sup>1</sup> , Nelly Babayan<sup><text font="italic">2,3</text></sup>, 
&amp;Lusine Khondkaryan<sup><text font="italic">2,3</text></sup>, Karen Hambardzumyan<sup>1</sup>, Zaven Navoyan<sup>3</sup>, Hrant Khachatrian<sup><text font="italic">1,4</text></sup>, 
&amp;Armen Aghajanyan<sup>5</sup> <break/><break/><sup>1</sup> YerevaNN, Yerevan, Armenia <sup>2</sup> Institute of Molecular Biology, NAS RA, Yerevan, Armenia <break/><sup>3</sup> Toxometris.ai, Yerevan, Armenia <sup>4</sup> Yerevan State University, Yerevan, Armenia <break/><sup>5</sup> Meta AI Research, Seattle, Washington, USA</personname>
    <contact role="thanks">Joint First Authors. Correspondence: <text font="typewriter">armenag@meta.com</text></contact>
  </creator>
  <abstract name="Abstract">
    <p>We discover a robust self-supervised strategy tailored towards molecular representations for generative masked language models through a series of tailored, in-depth ablations. Using this pre-training strategy, we train <text font="typewriter">BARTSmiles</text>, a BART-like model with an order of magnitude more compute than previous self-supervised molecular representations. In-depth evaluations show that <text font="typewriter">BARTSmiles</text> consistently outperforms other self-supervised representations across classification, regression, and generation tasks setting a new state-of-the-art on 11 tasks. We then quantitatively show that when applied to the molecular domain, the BART objective learns representations that implicitly encode our downstream tasks of interest. For example, by selecting seven neurons from a frozen <text font="typewriter">BARTSmiles</text>, we can obtain a model having performance within two percentage points of the full fine-tuned model on task Clintox. Lastly, we show that standard attribution interpretability methods, when applied to <text font="typewriter">BARTSmiles</text>, highlight certain substructures that chemists use to explain specific properties of molecules. The code and the pretrained model are publicly available.</p>
  </abstract>
  <figure inlist="lof" placement="h!" xml:id="S0.F1">
    <tags>
      <tag><text fontsize="90%">Figure 1</text></tag>
      <tag role="autoref">Figure 1</tag>
      <tag role="refnum">1</tag>
      <tag role="typerefnum">Figure 1</tag>
    </tags>
    <graphics candidates="figures/cycle_.pdf" graphic="figures/cycle_.pdf" options="width=377.24727pt,keepaspectratio=true" xml:id="S0.F1.g1"/>
    <toccaption><tag close=" ">1</tag>Performance of <text font="typewriter">BARTSmiles</text> on a variety of tasks compared to the current state-of-the-art.</toccaption>
    <caption><tag close=": "><text fontsize="90%">Figure 1</text></tag><text fontsize="90%">Performance of <text font="typewriter">BARTSmiles</text> on a variety of tasks compared to the current state-of-the-art.</text></caption>
  </figure>
  <section inlist="toc" xml:id="S1">
    <tags>
      <tag>1</tag>
      <tag role="autoref">section 1</tag>
      <tag role="refnum">1</tag>
      <tag role="typerefnum">§1</tag>
    </tags>
    <title><tag close=" ">1</tag>Introduction</title>
    <para xml:id="S1.p1">
      <p>Recent advancements in large-scale representation learning have significantly improved downstream performance on virtually every modality, from images <cite class="ltx_citemacro_citet"><bibref bibrefs="CM3,BEIT,MAE" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> to text <cite class="ltx_citemacro_citet"><bibref bibrefs="ROBERTA,MARGE,HTLM1" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> to speech <cite class="ltx_citemacro_citet"><bibref bibrefs="XLSR,WHISPER" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>. Meanwhile, the domain of molecular representations has only seen a fraction of the scale of these other domains. In this paper, we show that pre-training with masked language modeling objective on a large collection of molecules allows us to learn generalizable representations that perform well on a wide range of computational chemistry benchmarks.</p>
    </para>
    <para xml:id="S1.p2">
      <p>We train <text font="typewriter">BARTSmiles</text>, a general purpose, pre-trained generative masked language model for molecules trained with an order of magnitude more compute than previous pre-trained molecular representations over all available molecules from the ZINC20 dataset (over 1.7 billion molecules <cite class="ltx_citemacro_citep">(<bibref bibrefs="ZINC20" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>).</p>
    </para>
    <para xml:id="S1.p3">
      <p>To validate the efficacy of our representations, we do in-depth evaluations across a large amount of classification, regression, and generation tasks achieving a new state of the art on 11 tasks. We then explore the representations from a domain-expert perspective by contrasting the output of an attribution interpretability method with structural alerts, a rule-based system developed by chemists to explain particular molecular properties. In particular, for a model trained to predict the molecular toxicity, the Integrated Gradients method usually highlights atoms known to be part of the substructures responsible for toxicity <cite class="ltx_citemacro_citep">(<bibref bibrefs="integrated-gradients" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>.</p>
    </para>
    <para xml:id="S1.p4">
      <p>Furthermore, our pre-trained model, in a completely unsupervised fashion, learned representations for specific tasks, i.e., certain neurons correlated highly with downstream tasks. For specific tasks, we see that a linear combination of frozen 7 to 15 neurons is enough to reach more than 90% of the full fine-tuning performance.</p>
    </para>
    <para xml:id="S1.p5">
      <p>To summarize our contributions are the following:</p>
      <itemize xml:id="S1.I1">
        <item xml:id="S1.I1.i1">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">1st item</tag>
          </tags>
          <para xml:id="S1.I1.i1.p1">
            <p>A robust pre-training strategy tailored towards molecular representations discovered through a series of in-depth ablations for generative masked language models.</p>
          </para>
        </item>
        <item xml:id="S1.I1.i2">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">2nd item</tag>
          </tags>
          <para xml:id="S1.I1.i2.p1">
            <p>We present <text font="typewriter">BARTSmiles</text> pre-trained model for molecules trained with an order of magnitude more compute than previous pre-trained molecular representations. The model is publicly released at <ref class="ltx_url" font="typewriter" href="https://github.com/YerevaNN/BARTSmiles/">https://github.com/YerevaNN/BARTSmiles/</ref>.</p>
          </para>
        </item>
        <item xml:id="S1.I1.i3">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">3rd item</tag>
          </tags>
          <para xml:id="S1.I1.i3.p1">
            <p>We have fine-tuned <text font="typewriter">BARTSmiles</text> on multiple chemical property prediction, chemical reaction prediction and retrosynthesis tasks, and set state-of-the-art results on 11 of them.</p>
          </para>
        </item>
        <item xml:id="S1.I1.i4">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">4th item</tag>
          </tags>
          <para xml:id="S1.I1.i4.p1">
            <p>We present a quantitative analysis of molecular properties learned during the pre-training process by analyzing individual neurons in the representations that correlate highly with downstream tasks.</p>
          </para>
        </item>
        <item xml:id="S1.I1.i5">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">5th item</tag>
          </tags>
          <para xml:id="S1.I1.i5.p1">
            <p>A qualitative analysis using attribution interpretability methods finds that the model internally learns patterns known to in-domain experts to correlate with certain chemical properties.</p>
          </para>
        </item>
      </itemize>
    </para>
  </section>
  <section inlist="toc" xml:id="S2">
    <tags>
      <tag>2</tag>
      <tag role="autoref">section 2</tag>
      <tag role="refnum">2</tag>
      <tag role="typerefnum">§2</tag>
    </tags>
    <title><tag close=" ">2</tag>Related work</title>
    <paragraph inlist="toc" xml:id="S2.SS0.SSS0.Px1">
      <title>Chemical Property Prediction:</title>
      <para xml:id="S2.SS0.SSS0.Px1.p1">
        <p>Machine learning has been applied to chemical property prediction since the 1960s <cite class="ltx_citemacro_citep">(<bibref bibrefs="qsar-history" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. Initially, molecules were represented by fingerprints such as Morgan fingerprints <cite class="ltx_citemacro_citep">(<bibref bibrefs="morgan-fingerprints" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, circular fingerprints <cite class="ltx_citemacro_citep">(<bibref bibrefs="circular-fingerprints" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> or extended-connectivity circular fingerprints (ECFP) <cite class="ltx_citemacro_citep">(<bibref bibrefs="ECFP-fingerprints" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, which were treated as inputs to various machine learning models. Deep learning has been applied to fingerprints since at least 2014 <cite class="ltx_citemacro_citep">(<bibref bibrefs="NN-QSAR-2014" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. <cite class="ltx_citemacro_citet"><bibref bibrefs="CNN-on-molecules" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> was the first to apply convolutional networks directly on the graphs by generalizing the computation of circular fingerprints. In contrast, <cite class="ltx_citemacro_citet"><bibref bibrefs="GCN-on-molecules" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> introduced graph convolutions on molecular graphs.</p>
      </para>
      <para xml:id="S2.SS0.SSS0.Px1.p2">
        <p><cite class="ltx_citemacro_citet"><bibref bibrefs="moleculenet" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> introduced the MoleculeNet benchmark suite, which enabled direct comparisons among machine learning algorithms for several classification and regression tasks. It became the standard evaluation system for most of the subsequent work. On the generative modeling front <cite class="ltx_citemacro_citet"><bibref bibrefs="lowe-USPTO-2012" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> extracted a large set of chemical reactions from US patents, becoming a standard benchmark for evaluating the generation abilities of machine learning methods.</p>
      </para>
    </paragraph>
    <paragraph inlist="toc" xml:id="S2.SS0.SSS0.Px2">
      <title>Self-Supervised Learning for Molecular Representations:</title>
      <para xml:id="S2.SS0.SSS0.Px2.p1">
        <p><cite class="ltx_citemacro_citet"><bibref bibrefs="mol2vec" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> introduced Mol2vec, an adaptation of word2vec for molecules. Within the realm of self-supervised research, <cite class="ltx_citemacro_citet"><bibref bibrefs="SMILES_BERT" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> was the first to apply BERT-like models on SMILES strings but unfortunately did not evaluate on MoleculeNet benchmarks. <cite class="ltx_citemacro_citet"><bibref bibrefs="chemberta" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> further scaled up BERT-like models for molecules showing consistent improvement with scale. On the other hand, MolCLR <cite class="ltx_citemacro_citet"><bibref bibrefs="MolCLR" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> and iMolCLR <cite class="ltx_citemacro_citet"><bibref bibrefs="iMolCLR" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> use graph neural networks coupled with contrastive learning on 10 million unlabeled molecules to learn fine-tunable representations. Recently, <cite class="ltx_citemacro_citet"><bibref bibrefs="reaction-aware-molRL" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> improved molecular representations by adding additional inductive biases through constraints on the sums of the representations of reactants.</p>
      </para>
      <para xml:id="S2.SS0.SSS0.Px2.p2">
        <p>The work most related to ours is ChemFormer <cite class="ltx_citemacro_citet"><bibref bibrefs="chemformer" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, which also trains a BART-like model but, unlike our paper, does so with a fraction of the compute and scale. Additionally, through ablations, our paper explores the tweaks needed to the generative masked language model objective to make it optimal in the domain of molecular representations. We also present a unified fine-tuning recipe that removes the need for complex hyper-parameter tuning while consistently outperforming previous state-of-the-art models.</p>
      </para>
    </paragraph>
  </section>
  <section inlist="toc" xml:id="S3">
    <tags>
      <tag>3</tag>
      <tag role="autoref">section 3</tag>
      <tag role="refnum">3</tag>
      <tag role="typerefnum">§3</tag>
    </tags>
    <title><tag close=" ">3</tag>Pretraining</title>
    <para xml:id="S3.p1">
      <p>Below we present the pre-training setting used for training <text font="typewriter">BARTSmiles</text>.</p>
    </para>
    <paragraph inlist="toc" xml:id="S3.SS0.SSS0.Px1">
      <title>Dataset:</title>
      <para xml:id="S3.SS0.SSS0.Px1.p1">
        <p>We use the Zinc20 dataset from <cite class="ltx_citemacro_citet"><bibref bibrefs="ZINC20" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> for all of our pre-training ablations and experiments. We deduplicate the data based on the hashes of canonicalized SMILES using RDKit <cite class="ltx_citemacro_citep">(<bibref bibrefs="RDKIT" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, leaving us with a total of slightly north of <Math mode="inline" tex="1.7" text="1.7" xml:id="S3.SS0.SSS0.Px1.p1.m1">
            <XMath>
              <XMTok meaning="1.7" role="NUMBER">1.7</XMTok>
            </XMath>
          </Math> billion samples from which we reserve 10000 samples as our validation set.</p>
      </para>
    </paragraph>
    <paragraph inlist="toc" labels="LABEL:sec:model_implementation" xml:id="S3.SS0.SSS0.Px2">
      <title>Implementation:</title>
      <para xml:id="S3.SS0.SSS0.Px2.p1">
        <p>In all of our experiments, we parameterize our masked language model with the standard pre-layer norm Transformer architecture, precisely the BART-Large model proposed in <cite class="ltx_citemacro_citet"><bibref bibrefs="BART" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>. We have a maximum sequence length per sample of 128 tokens for all models. We train using the FairSeq framework <cite class="ltx_citemacro_citet"><bibref bibrefs="fairseq" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> with PyTorch <cite class="ltx_citemacro_citet"><bibref bibrefs="pytorch" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> as the underlying framework. For our larger models, we use the fully sharded data-parallel implementation available in FairScale <cite class="ltx_citemacro_citep">(<bibref bibrefs="fairscale" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. All experiments use automatic mixed-precision available in FairSeq. We use the Aim ecosystem <cite class="ltx_citemacro_citep">(<bibref bibrefs="AIM" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> for experiment tracking.</p>
      </para>
    </paragraph>
    <paragraph inlist="toc" xml:id="S3.SS0.SSS0.Px3">
      <title>Choice Of Objective:</title>
      <para xml:id="S3.SS0.SSS0.Px3.p1">
        <p>When choosing an objective, there are a certain number of pre-conditions to consider depending on the end goal of the model. <cite class="ltx_citemacro_citet"><bibref bibrefs="lm_objective_architecture" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> argues that for zero-shot or k-shot prompting, causal language models with uni-directional attention is optimal, while bidirectionality (in both context and attention masks) is the primary driver of success in the fine-tuning setting.</p>
      </para>
      <para xml:id="S3.SS0.SSS0.Px3.p2">
        <p>Within the fine-tuning setting, there are different objectives conditioned on model type. For encoder-based models, the masked language modeling objective as initially proposed in <cite class="ltx_citemacro_citet"><bibref bibrefs="bert" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> and further refined in <cite class="ltx_citemacro_citet"><bibref bibrefs="ROBERTA" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>. The downside of encoder models is the inability to do generative fine-tuning, which led to the introduction of the denoising model for the encoder-decoder models <cite class="ltx_citemacro_citet"><bibref bibrefs="BART" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>. Decoder causal models are problematic because they are not bidirectional, although recently proposed objectives such as causal masking in <cite class="ltx_citemacro_citet"><bibref bibrefs="CM3" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> are bidirectional in context but not in attention.</p>
      </para>
      <para xml:id="S3.SS0.SSS0.Px3.p3">
        <p>For <text font="typewriter">BARTSmiles</text> we select both the denoising objective and architecture from <cite class="ltx_citemacro_citet"><bibref bibrefs="BART" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> while previous works have focused on encoder-only <cite class="ltx_citemacro_citep">(<bibref bibrefs="SMILES_BERT,chemberta" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.</p>
      </para>
    </paragraph>
    <subsection inlist="toc" xml:id="S3.SS1">
      <tags>
        <tag>3.1</tag>
        <tag role="autoref">subsection 3.1</tag>
        <tag role="refnum">3.1</tag>
        <tag role="typerefnum">§3.1</tag>
      </tags>
      <title><tag close=" ">3.1</tag>Ablation</title>
      <para xml:id="S3.SS1.p1">
        <p>Given the novelty of the molecular representation domain for pre-training, we aim to do an in-depth ablation analysis on how to successfully pre-train within this domain.</p>
      </para>
      <para xml:id="S3.SS1.p2">
        <p>We measure the quality of learned representations by fine-tuning with a fixed set of hyper-parameters on three datasets, HIV from <cite class="ltx_citemacro_citet"><bibref bibrefs="moleculenet" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, BBBP from <cite class="ltx_citemacro_citet"><bibref bibrefs="bbbp" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, and ClinTox from <cite class="ltx_citemacro_citet"><bibref bibrefs="moleculenet" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, reporting the average of AUC-ROC scores. Note that we have selected these datasets before our final fine-tuning experiments. For the exact fixed hyper-parameters please refer to Table <ref labelref="LABEL:tab:hyperparams-pretrain"/> in Appendix <ref labelref="LABEL:sec:hp"/>. We provide an overarching summary of our ablation in Figure <ref labelref="LABEL:fig:ablation_summary"/>.</p>
      </para>
      <figure float="right" inlist="lof" labels="LABEL:fig:ablation_summary" xml:id="S3.F2">
        <tags>
          <tag><text fontsize="90%">Figure 2</text></tag>
          <tag role="autoref">Figure 2</tag>
          <tag role="refnum">2</tag>
          <tag role="typerefnum">Figure 2</tag>
        </tags>
<!--  %**** arxiv˙version.tex Line 125 **** -->        <graphics candidates="figures/chem_ablation.pdf" class="ltx_centering" graphic="figures/chem_ablation.pdf" options="width=260.17464pt,keepaspectratio=true" xml:id="S3.F2.g1"/>
        <toccaption class="ltx_centering"><tag close=" ">2</tag>A summary of decision made when pre-training <text font="typewriter">BARTSmiles</text>, as well as their absolute gain on our ablation benchmark. All numbers are AUC-ROC on the three datasets.</toccaption>
        <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 2</text></tag><text fontsize="90%">A summary of decision made when pre-training <text font="typewriter">BARTSmiles</text>, as well as their absolute gain on our ablation benchmark. All numbers are AUC-ROC on the three datasets.</text></caption>
      </figure>
      <paragraph inlist="toc" xml:id="S3.SS1.SSS0.Px1">
        <title>Choice of Tokenization:</title>
        <para xml:id="S3.SS1.SSS0.Px1.p1">
          <p>Previous work, such as <cite class="ltx_citemacro_citet"><bibref bibrefs="SMILES_BERT" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase>(</bibrefphrase>
                <bibrefphrase>)</bibrefphrase>
              </bibref></cite> uses pre-defined tokenization rules on SMILES usually separating individual logical elements such as atoms (i.e., <text font="italic">C</text>, <text font="italic">N</text>, <text font="italic">H</text>) and chemical bond symbols like (i.e., <text font="italic">#</text>, <text font="italic">-</text>, <text font="italic">=</text>). These fundamental units for molecules are analogous to using characters in natural language. Recent advancements in natural language pre-training have ubiquitously used sub-word tokenization as a compromise between the full character coverage of character level tokenization and the need for large vocabularies for word-based tokenization. We train a unigram tokenizer on a random sample of 10 Million samples from our training set with a 1021 vocabulary size using SentencePiece <cite class="ltx_citemacro_citep">(<bibref bibrefs="sentencepiece" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>.</p>
        </para>
        <para xml:id="S3.SS1.SSS0.Px1.p2">
          <p>To ablate the benefit of learned tokenization vs. hand-crafted tokenization, we train two equivalent models (§<ref labelref="LABEL:sec:model_implementation"/>), using the <text font="typewriter">BARTSmiles</text>-Base architecture. We fix the amount of compute for each run to the amount needed for one epoch, using learned tokenization with <text font="typewriter">BARTSmiles</text>-Base on 100M samples. Since our hand-crafted tokenization produces more tokens, given a fixed compute budget, the model trained with this tokenization sees roughly 92M samples. Each model was trained on 128 A100 GPUs. As a baseline, we also fine-tune a randomly initialized model of the same architecture and size as <text font="typewriter">BARTSmiles</text>-Base. We present our results in Table <ref labelref="LABEL:table:tokenization_ablation"/>.</p>
        </para>
        <table inlist="lot" labels="LABEL:table:tokenization_ablation" placement="h" xml:id="S3.T1">
          <tags>
            <tag><text fontsize="90%">Table 1</text></tag>
            <tag role="autoref">Table 1</tag>
            <tag role="refnum">1</tag>
            <tag role="typerefnum">Table 1</tag>
          </tags>
          <tabular class="ltx_centering" vattach="middle">
            <tbody>
              <tr>
                <td border="tt"/>
                <td align="left" border="tt">Custom Tokenization (Random)</td>
                <td align="left" border="tt">Custom Tokenization</td>
                <td align="left" border="tt" class="ltx_nopad_r">Learned Tokenization</td>
              </tr>
              <tr>
                <td align="left" border="bb t">Performance</td>
                <td align="left" border="bb t">0.628</td>
                <td align="left" border="bb t">0.779</td>
                <td align="left" border="bb t" class="ltx_nopad_r"><text font="bold">0.801</text></td>
              </tr>
            </tbody>
          </tabular>
          <toccaption class="ltx_centering"><tag close=" ">1</tag>Performance (average AUC-ROC over three datasets) on ablation benchmark dataset over a randomly initialized network, a network pre-trained with hand-crafted tokenization rules, and a network pre-trained with SentencePiece tokenization.</toccaption>
          <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Table 1</text></tag><text fontsize="90%">Performance (average AUC-ROC over three datasets) on ablation benchmark dataset over a randomly initialized network, a network pre-trained with hand-crafted tokenization rules, and a network pre-trained with SentencePiece tokenization.</text></caption>
        </table>
        <para xml:id="S3.SS1.SSS0.Px1.p3">
          <p>We significantly improve pre-training representations by using a learned tokenization method, and given that learned tokenization is more efficient (due to smaller context lengths) and pre-trains better, we select using a learned tokenizer for the rest of our experiments.</p>
        </para>
      </paragraph>
      <paragraph inlist="toc" xml:id="S3.SS1.SSS0.Px2">
        <title>Masking Strategy in Objective:</title>
        <para xml:id="S3.SS1.SSS0.Px2.p1">
          <p>We selected the denoising objective using encoder-decoder models from <cite class="ltx_citemacro_citep">(<bibref bibrefs="BART" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite> to learn fine-tunable representations for classification, regression, and generative tasks. Fundamentally the denoising objective has the following hyper-parameters <text font="typewriter">random_mask</text> (percent of tokens flipped to <text font="typewriter">&lt;mask&gt;</text>), <Math mode="inline" tex="\lambda" text="lambda" xml:id="S3.SS1.SSS0.Px2.p1.m1">
              <XMath>
                <XMTok font="italic" name="lambda" role="UNKNOWN">λ</XMTok>
              </XMath>
            </Math> (Poisson distribution for length of masked span), <text font="typewriter">randomize_tokens</text> (whether or not to randomize specific tokens, if true, 50% of <text font="typewriter">random_mask</text> is used for this). In conjunction with a <text font="typewriter">mask_token_budget</text> (the percent of tokens per sample allowed to be changed or masked), these three parameters cover the surface area of our ablation.</p>
        </para>
        <para xml:id="S3.SS1.SSS0.Px2.p2">
          <p>Given the computational cost of individual pre-training runs, even on the scale of <text font="typewriter">BARTSmiles</text>-Base, we are unable to do a proper grid search; therefore, we aimed first to find an optimal <text font="typewriter">mask_token_budget</text> (Table <ref labelref="LABEL:table:ablation_mask_token"/>), then for that given budget, find optimal <text font="typewriter">random_mask</text> and <Math mode="inline" tex="\lambda" text="lambda" xml:id="S3.SS1.SSS0.Px2.p2.m1">
              <XMath>
                <XMTok font="italic" name="lambda" role="UNKNOWN">λ</XMTok>
              </XMath>
            </Math> (Table <ref labelref="LABEL:table:ablation_lambda"/>), and finally given all the previous parameters we ablate whether or not to include to randomize specific tokens (Table <ref labelref="LABEL:table:ablation_randomize"/>).</p>
        </para>
        <table inlist="lot" labels="LABEL:tab:ablation-all" placement="!htb" xml:id="S3.T2">
          <tags>
            <tag><text fontsize="90%">Table 2</text></tag>
            <tag role="autoref">Table 2</tag>
            <tag role="refnum">2</tag>
            <tag role="typerefnum">Table 2</tag>
          </tags>
          <table class="ltx_figure_panel" inlist="lot" labels="LABEL:table:ablation_mask_token" xml:id="S3.T2.st1">
            <tags>
              <tag><text fontsize="90%">(a)</text></tag>
              <tag role="autoref">2(a)</tag>
              <tag role="refnum">2(a)</tag>
            </tags>
            <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
              <tbody>
                <tr>
                  <td align="left" border="tt" thead="row"><text font="typewriter" fontsize="80%">mask_token</text></td>
                  <td align="right" border="tt" class="ltx_nopad_r"><text fontsize="80%">AUC</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text font="typewriter" fontsize="80%">_budget</text></td>
                  <td class="ltx_nopad_r"/>
                </tr>
                <tr>
                  <td align="left" border="t" thead="row"><text fontsize="80%">0.10</text></td>
                  <td align="right" border="t" class="ltx_nopad_r"><text fontsize="80%">0.753</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="80%">0.15</text></td>
                  <td align="right" class="ltx_nopad_r"><text fontsize="80%">0.812</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="80%">0.20</text></td>
                  <td align="right" class="ltx_nopad_r"><text font="bold" fontsize="80%">0.821</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="80%">0.25</text></td>
                  <td align="right" class="ltx_nopad_r"><text fontsize="80%">0.808</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="80%">0.30</text></td>
                  <td align="right" class="ltx_nopad_r"><text fontsize="80%">0.801</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="80%">0.35</text></td>
                  <td align="right" class="ltx_nopad_r"><text fontsize="80%">0.760</text></td>
                </tr>
                <tr>
                  <td align="left" border="bb" thead="row"><text fontsize="80%">0.40</text></td>
                  <td align="right" border="bb" class="ltx_nopad_r"><text fontsize="80%">0.701</text></td>
                </tr>
              </tbody>
            </tabular>
            <toccaption class="ltx_centering"><tag close=" "><text fontsize="80%">(a)</text></tag></toccaption>
            <caption class="ltx_centering" fontsize="80%"><tag close=" "><text fontsize="113%">(a)</text></tag></caption>
          </table>
          <table class="ltx_figure_panel" xml:id="S3.T2.tab1"/>
          <table class="ltx_figure_panel" inlist="lot" labels="LABEL:table:ablation_lambda" xml:id="S3.T2.st2">
            <tags>
              <tag><text fontsize="90%">(b)</text></tag>
              <tag role="autoref">2(b)</tag>
              <tag role="refnum">2(b)</tag>
            </tags>
<!--  %**** pretrain-ablation-tables.tex Line 25 **** -->            <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
              <thead>
                <tr>
                  <td align="left" border="tt" thead="column"><text font="typewriter" fontsize="80%">random</text></td>
                  <td align="left" border="tt" thead="column"><Math mode="inline" tex="\lambda" text="lambda" xml:id="S3.T2.st2.m1">
                      <XMath>
                        <XMTok font="italic" fontsize="80%" name="lambda" role="UNKNOWN">λ</XMTok>
                      </XMath>
                    </Math></td>
                  <td align="left" border="tt" class="ltx_nopad_r" thead="column"><text fontsize="80%">AUC</text></td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" thead="column"><text font="typewriter" fontsize="80%">_mask</text></td>
                  <td thead="column"/>
                  <td class="ltx_nopad_r"/>
                </tr>
                <tr>
                  <td align="left" border="t"><text fontsize="80%">0.05</text></td>
                  <td align="left" border="t"><text fontsize="80%">1.5</text></td>
                  <td align="left" border="t" class="ltx_nopad_r"><text fontsize="80%">0.814</text></td>
                </tr>
                <tr>
                  <td align="left"><text fontsize="80%">0.05</text></td>
                  <td align="left"><text fontsize="80%">2.5</text></td>
                  <td align="left" class="ltx_nopad_r"><text fontsize="80%">0.818</text></td>
                </tr>
                <tr>
                  <td align="left"><text fontsize="80%">0.05</text></td>
                  <td align="left"><text fontsize="80%">3.5</text></td>
                  <td align="left" class="ltx_nopad_r"><text fontsize="80%">0.813</text></td>
                </tr>
                <tr>
                  <td align="left"><text fontsize="80%">0.10</text></td>
                  <td align="left"><text fontsize="80%">1.5</text></td>
                  <td align="left" class="ltx_nopad_r"><text fontsize="80%">0.820</text></td>
                </tr>
                <tr>
                  <td align="left"><text fontsize="80%">0.10</text></td>
                  <td align="left"><text fontsize="80%">2.5</text></td>
                  <td align="left" class="ltx_nopad_r"><text fontsize="80%">0.821</text></td>
                </tr>
                <tr>
                  <td align="left" border="bb"><text fontsize="80%">0.10</text></td>
                  <td align="left" border="bb"><text fontsize="80%">3.5</text></td>
                  <td align="left" border="bb" class="ltx_nopad_r"><text font="bold" fontsize="80%">0.821</text></td>
                </tr>
              </tbody>
            </tabular>
            <toccaption class="ltx_centering"><tag close=" "><text fontsize="80%">(b)</text></tag></toccaption>
            <caption class="ltx_centering" fontsize="80%"><tag close=" "><text fontsize="113%">(b)</text></tag></caption>
          </table>
          <break class="ltx_break"/>
          <table class="ltx_figure_panel" inlist="lot" labels="LABEL:table:ablation_randomize" xml:id="S3.T2.st3">
            <tags>
              <tag><text fontsize="90%">(c)</text></tag>
              <tag role="autoref">2(c)</tag>
              <tag role="refnum">2(c)</tag>
            </tags>
            <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
              <tbody>
                <tr>
                  <td align="left" border="tt" thead="row"><text font="typewriter" fontsize="80%">randomize</text></td>
                  <td align="right" border="tt" class="ltx_nopad_r"><text fontsize="80%">AUC</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text font="typewriter" fontsize="80%">_tokens</text></td>
                  <td class="ltx_nopad_r"/>
                </tr>
                <tr>
                  <td align="left" border="t" thead="row"><text fontsize="80%">✓</text></td>
                  <td align="right" border="t" class="ltx_nopad_r"><text fontsize="80%">0.821</text></td>
                </tr>
                <tr>
                  <td align="left" border="bb" thead="row"><text fontsize="80%">✗</text></td>
                  <td align="right" border="bb" class="ltx_nopad_r"><text font="bold" fontsize="80%">0.835</text></td>
                </tr>
              </tbody>
            </tabular>
<!--  %**** pretrain-ablation-tables.tex Line 50 **** -->            <toccaption class="ltx_centering"><tag close=" "><text fontsize="80%">(c)</text></tag></toccaption>
            <caption class="ltx_centering" fontsize="80%"><tag close=" "><text fontsize="113%">(c)</text></tag></caption>
          </table>
          <toccaption><tag close=" "><text fontsize="80%">2</text></tag><text fontsize="80%">All the ablations ran using the </text><text font="typewriter" fontsize="80%">BARTSmiles</text><text fontsize="80%">-Base
model on 100 million samples from ZINC20. In (b) we ablate the impact of the random mask percentage and the poisson </text><Math mode="inline" tex="\lambda" text="lambda" xml:id="S3.T2.m1">
              <XMath>
                <XMTok font="italic" fontsize="80%" name="lambda" role="UNKNOWN">λ</XMTok>
              </XMath>
            </Math><text fontsize="80%"> parameter which controls the length of the individual masks. For this we fix </text><text font="typewriter" fontsize="80%">mask_token_budget</text><text fontsize="80%"> to 0.20, the best forming configuration from (a). In (c) we ablate the impact of randomizing tokens in the input, using the best configurations found in (a) and (b) All numbers are the average of AUC-ROC scores on HIV, BBBP and ClinTox datasets.
</text></toccaption>
          <caption fontsize="80%"><tag close=": "><text fontsize="113%">Table 2</text></tag><text fontsize="113%">All the ablations ran using the <text font="typewriter">BARTSmiles</text>-Base
model on 100 million samples from ZINC20. In (b) we ablate the impact of the random mask percentage and the poisson <Math mode="inline" tex="\lambda" text="lambda" xml:id="S3.T2.m2">
                <XMath>
                  <XMTok font="italic" name="lambda" role="UNKNOWN">λ</XMTok>
                </XMath>
              </Math> parameter which controls the length of the individual masks. For this we fix <text font="typewriter">mask_token_budget</text> to 0.20, the best forming configuration from (a). In (c) we ablate the impact of randomizing tokens in the input, using the best configurations found in (a) and (b) All numbers are the average of AUC-ROC scores on HIV, BBBP and ClinTox datasets.
</text></caption>
        </table>
        <para xml:id="S3.SS1.SSS0.Px2.p3">
          <p>In general, we see that the standard hyper-parameter of 0.3 for the <text font="typewriter">mask_token_budget</text> as proposed in <cite class="ltx_citemacro_citet"><bibref bibrefs="BART" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase>(</bibrefphrase>
                <bibrefphrase>)</bibrefphrase>
              </bibref></cite> is suboptimal by a significant margin, and the use of randomizing tokens is suboptimal as well. The type of masking beyond that seems to not be of importance.</p>
        </para>
      </paragraph>
      <paragraph inlist="toc" xml:id="S3.SS1.SSS0.Px3">
        <title>Importance of Pre-Training Scale</title>
        <para xml:id="S3.SS1.SSS0.Px3.p1">
          <p>As the last step in our ablation, we significantly increased the amount of compute to do a single pass over the full Zinc20 dataset, which required 20 hours on 1024 A100 GPUs. The previous model utilizing the most compute used 3330 V100 hours <cite class="ltx_citemacro_cite"><bibref bibrefs="MolFormer" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase>(</bibrefphrase>
                <bibrefphrase>)</bibrefphrase>
              </bibref></cite>, while our model was trained for 20480 A100 hours. Training on the full dataset improves performance by five absolute points compared to the same setting trained on 100 million samples.</p>
        </para>
      </paragraph>
    </subsection>
  </section>
  <section inlist="toc" xml:id="S4">
    <tags>
      <tag>4</tag>
      <tag role="autoref">section 4</tag>
      <tag role="refnum">4</tag>
      <tag role="typerefnum">§4</tag>
    </tags>
    <title><tag close=" ">4</tag>Fine-tuning</title>
    <subsection inlist="toc" xml:id="S4.SS1">
      <tags>
        <tag>4.1</tag>
        <tag role="autoref">subsection 4.1</tag>
        <tag role="refnum">4.1</tag>
        <tag role="typerefnum">§4.1</tag>
      </tags>
      <title><tag close=" ">4.1</tag>Datasets</title>
      <para xml:id="S4.SS1.p1">
        <p>To evaluate our pre-trained models, we fine-tune on multiple classification and regression datasets, mostly from the MoleculeNet benchmark suite <cite class="ltx_citemacro_citep">(<bibref bibrefs="moleculenet" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.</p>
      </para>
      <para xml:id="S4.SS1.p2">
        <p>MoleculeNet is a collection of 17 commonly used datasets for evaluating chemical representations. MoleculeNet defines splitting methods and evaluation metrics for all datasets. We have run experiments on 10 of them (excluding quantum mechanical datasets and the huge datasets PCBA and MUV). We use the AUC-ROC score to evaluate the classification models. Three of the selected datasets are regression tasks evaluated with RMSE. Four out of seven classification tasks have multiple target labels for each sample. We treat each target variable as a separate task and report the mean of the AUC scores. The remaining three are single-label classification tasks, and scaffolds of the molecules (the two-dimensional structural frameworks of the molecules as defined by <cite class="ltx_citemacro_citet"><bibref bibrefs="scaffold1996" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>) define their training-test splits. This implies some distribution shift in the test set, which introduces additional difficulties in model selection.</p>
      </para>
      <para xml:id="S4.SS1.p3">
        <p>We additionally perform fine-tuning on two datasets developed for toxicology. The first one is called the Ames dataset <cite class="ltx_citemacro_citet"><bibref bibrefs="Ames" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, which holds the results of a bacterial reverse mutation test, commonly known as the Ames test. And the second one holds the results of another genotoxicity test, namely the micronucleus (MN) assay <cite class="ltx_citemacro_citep">(<bibref bibrefs="Micronucleus" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. MN assay is a cytogenetic test to assess the genotoxicity of chemicals and physical factors on a chromosomal level. We use cross-validation for the MN assay dataset to select the best hyperparameters and apply the best model to an external dataset of 65 compounds to obtain the final score. This makes our results comparable to the baselines introduced by <cite class="ltx_citemacro_citep">(<bibref bibrefs="Ames" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.</p>
      </para>
      <table inlist="lot" labels="LABEL:tab:results-classification" xml:id="S4.T3">
        <tags>
          <tag><text fontsize="90%">Table 3</text></tag>
          <tag role="autoref">Table 3</tag>
          <tag role="refnum">3</tag>
          <tag role="typerefnum">Table 3</tag>
        </tags>
<!--  %**** arxiv˙version.tex Line 175 **** -->        <tabular class="ltx_centering" vattach="middle">
          <tbody>
            <tr>
              <td border="tt"/>
              <td align="left" border="tt"><text fontsize="70%">SIDER</text></td>
              <td align="left" border="tt"><text fontsize="70%">ClinTox</text></td>
              <td align="left" border="tt"><text fontsize="70%">Tox21</text></td>
              <td align="left" border="tt"><text fontsize="70%">ToxCast</text></td>
              <td align="left" border="tt"><text fontsize="70%">HIV</text></td>
              <td align="left" border="tt"><text fontsize="70%">BACE</text></td>
              <td align="left" border="tt"><text fontsize="70%">BBBP</text></td>
            </tr>
            <tr>
              <td align="left" border="t"><text fontsize="70%">RF MolCLR</text></td>
              <td align="left" border="t"><text fontsize="70%">0.684</text></td>
              <td align="left" border="t"><text fontsize="70%">0.713</text></td>
              <td align="left" border="t"><text fontsize="70%">0.769</text></td>
              <td align="left" border="t"><text fontsize="70%">-</text></td>
              <td align="left" border="t"><text fontsize="70%">0.781</text></td>
              <td align="left" border="t"><text fontsize="70%">0.867</text></td>
              <td align="left" border="t"><text fontsize="70%">0.714</text></td>
            </tr>
            <tr>
              <td align="left"><text fontsize="70%">SVM MolCLR</text></td>
              <td align="left"><text fontsize="70%">0.682</text></td>
              <td align="left"><text fontsize="70%">0.669</text></td>
              <td align="left"><text fontsize="70%">0.818</text></td>
              <td align="left"><text fontsize="70%">-</text></td>
              <td align="left"><text fontsize="70%">0.792</text></td>
              <td align="left"><text fontsize="70%">0.862</text></td>
              <td align="left"><text fontsize="70%">0.729</text></td>
            </tr>
            <tr>
              <td align="left" border="t"><text fontsize="70%">MoleculeNet GC</text></td>
              <td align="left" border="t"><text fontsize="70%">0.638</text></td>
              <td align="left" border="t"><text fontsize="70%">0.807</text></td>
              <td align="left" border="t"><text fontsize="70%">0.829</text></td>
              <td align="left" border="t"><text fontsize="70%">0.716</text></td>
              <td align="left" border="t"><text fontsize="70%">0.763</text></td>
              <td align="left" border="t"><text fontsize="70%">0.783</text></td>
              <td align="left" border="t"><text fontsize="70%">0.690</text></td>
            </tr>
            <tr>
              <td align="left"><text fontsize="70%">D-MPNN</text></td>
              <td align="left"><text fontsize="70%">0.646</text></td>
              <td align="left"><text fontsize="70%">0.894</text></td>
              <td align="left"><text fontsize="70%">0.845</text></td>
              <td align="left"><text fontsize="70%">0.737</text></td>
              <td align="left"><text fontsize="70%">*0.816</text></td>
              <td align="left"><text fontsize="70%">*0.875</text></td>
              <td align="left"><text fontsize="70%">*0.913</text></td>
            </tr>
            <tr>
              <td align="left"><text fontsize="70%">Attentive FP</text></td>
              <td align="left"><text fontsize="70%">0.637</text></td>
              <td align="left"><text fontsize="70%">0.940</text></td>
              <td align="left"><text font="bold" fontsize="70%">0.858</text></td>
              <td align="left"><text fontsize="70%">0.805</text></td>
              <td align="left"><text fontsize="70%">*0.832</text></td>
              <td align="left"><text fontsize="70%">*0.850</text></td>
              <td align="left"><text fontsize="70%">*0.920</text></td>
            </tr>
            <tr>
              <td align="left"><text fontsize="70%">3D Infomax</text></td>
              <td align="left"><text fontsize="70%">0.534</text></td>
              <td align="left"><text fontsize="70%">0.594</text></td>
              <td align="left"><text fontsize="70%">0.745</text></td>
              <td align="left"><text fontsize="70%">0.644</text></td>
              <td align="left"><text fontsize="70%">0.761</text></td>
              <td align="left"><text fontsize="70%">0.794</text></td>
              <td align="left"><text fontsize="70%">0.691</text></td>
            </tr>
            <tr>
              <td align="left" border="t"><text fontsize="70%">ChemBERTa 10M</text></td>
              <td align="center" border="t"><text fontsize="70%">-</text></td>
              <td align="center" border="t"><text fontsize="70%">-</text></td>
              <td align="center" border="t"><text fontsize="70%">-</text></td>
              <td align="left" border="t"><text fontsize="70%">-</text></td>
              <td align="left" border="t"><text fontsize="70%">0.622</text></td>
              <td align="center" border="t"><text fontsize="70%">-</text></td>
              <td align="left" border="t"><text fontsize="70%">0.643</text></td>
            </tr>
            <tr>
              <td align="left"><text fontsize="70%">MoLFormer-XL</text></td>
              <td align="left"><text fontsize="70%">0.690</text></td>
              <td align="left"><text fontsize="70%">0.948</text></td>
              <td align="left"><text fontsize="70%">0.847</text></td>
              <td align="left"><text fontsize="70%">-</text></td>
              <td align="left"><text fontsize="70%">*0.822</text></td>
              <td align="left"><text fontsize="70%">*0.882</text></td>
              <td align="left"><text fontsize="70%">*0.937</text></td>
            </tr>
            <tr>
              <td align="left"><text fontsize="70%">GIN</text></td>
              <td align="left"><text fontsize="70%">0.627</text></td>
              <td align="left"><text fontsize="70%">0.726</text></td>
              <td align="left"><text fontsize="70%">0.781</text></td>
              <td align="left"><text fontsize="70%">0.657</text></td>
              <td align="left"><text font="bold" fontsize="70%">0.799</text></td>
              <td align="left"><text fontsize="70%">0.845</text></td>
              <td align="left"><text fontsize="70%">0.687</text></td>
            </tr>
            <tr>
              <td align="left"><text fontsize="70%">GROVER large</text></td>
              <td align="left"><text fontsize="70%">0.658</text></td>
              <td align="left"><text fontsize="70%">0.944</text></td>
              <td align="left"><text fontsize="70%">0.831</text></td>
              <td align="left"><text fontsize="70%">0.737</text></td>
              <td align="left"><text fontsize="70%">-</text></td>
              <td align="left"><text fontsize="70%">*0.894</text></td>
              <td align="left"><text fontsize="70%">*0.940</text></td>
            </tr>
            <tr>
              <td align="left"><text fontsize="70%">MolCLR</text></td>
              <td align="left"><text fontsize="70%">0.680</text></td>
              <td align="left"><text fontsize="70%">0.932</text></td>
              <td align="left"><text fontsize="70%">0.798</text></td>
              <td align="left"><text fontsize="70%">-</text></td>
              <td align="left"><text font="bold" fontsize="70%">0.806</text></td>
              <td align="left"><text font="bold" fontsize="70%">0.890</text></td>
              <td align="left"><text fontsize="70%">0.736</text></td>
            </tr>
            <tr>
              <td align="left"><text fontsize="70%">iMolCLR</text></td>
              <td align="left"><text font="bold" fontsize="70%">0.699</text></td>
              <td align="left"><text fontsize="70%">0.954</text></td>
              <td align="left"><text fontsize="70%">0.799</text></td>
              <td align="left"><text fontsize="70%">-</text></td>
              <td align="left"><text font="bold" fontsize="70%">0.808</text></td>
              <td align="left"><text font="bold" fontsize="70%">0.885</text></td>
              <td align="left"><text font="bold" fontsize="70%">0.764</text></td>
            </tr>
            <tr>
              <td align="left" border="bb t"><text font="typewriter" fontsize="70%">BARTSmiles</text></td>
              <td align="left" border="bb t"><text font="bold" fontsize="70%">0.705</text></td>
              <td align="left" border="bb t"><text font="bold" fontsize="70%">0.997</text></td>
              <td align="left" border="bb t"><text font="bold" fontsize="70%">0.851</text></td>
              <td align="left" border="bb t"><text font="bold" fontsize="70%">0.825</text></td>
              <td align="left" border="bb t"><text fontsize="70%">0.745</text></td>
              <td align="left" border="bb t"><text fontsize="70%">0.855</text></td>
              <td align="left" border="bb t"><text fontsize="70%">0.74</text></td>
            </tr>
          </tbody>
        </tabular>
        <toccaption class="ltx_centering"><tag close=" "><text fontsize="70%">3</text></tag><text fontsize="70%">Results on the classification datasets from MoleculeNet. All numbers are AUC-ROC, higher is better. We bold the state-of-the-art numbers as well as all numbers within a 0.01 range. * indicates a non-standard split.</text></toccaption>
        <caption class="ltx_centering" fontsize="70%"><tag close=": "><text fontsize="129%">Table 3</text></tag><text fontsize="129%">Results on the classification datasets from MoleculeNet. All numbers are AUC-ROC, higher is better. We bold the state-of-the-art numbers as well as all numbers within a 0.01 range. * indicates a non-standard split.</text></caption>
      </table>
      <subsubsection inlist="toc" xml:id="S4.SS1.SSS1">
        <tags>
          <tag>4.1.1</tag>
          <tag role="autoref">subsubsection 4.1.1</tag>
          <tag role="refnum">4.1.1</tag>
          <tag role="typerefnum">§4.1.1</tag>
        </tags>
        <title><tag close=" ">4.1.1</tag>Recipe</title>
        <para xml:id="S4.SS1.SSS1.p1">
          <p>We fine-tune our pre-trained model with Fairseq <cite class="ltx_citemacro_citep">(<bibref bibrefs="fairseq" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>, a sequence modeling toolkit based on PyTorch. We use the recommended hyperparameters for the GLUE benchmark with a few modifications <cite class="ltx_citemacro_citep">(<bibref bibrefs="GLUE" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>. In particular, we set maximum input and output lengths to <Math mode="inline" tex="128" text="128" xml:id="S4.SS1.SSS1.p1.m1">
              <XMath>
                <XMTok meaning="128" role="NUMBER">128</XMTok>
              </XMath>
            </Math>, changed dropout values, and set the gradient clipping norm to <Math mode="inline" tex="0.1" text="0.1" xml:id="S4.SS1.SSS1.p1.m2">
              <XMath>
                <XMTok meaning="0.1" role="NUMBER">0.1</XMTok>
              </XMath>
            </Math>. We skip inputs longer than <Math mode="inline" tex="128" text="128" xml:id="S4.SS1.SSS1.p1.m3">
              <XMath>
                <XMTok meaning="128" role="NUMBER">128</XMTok>
              </XMath>
            </Math> and set the initial fp16 scaling to <Math mode="inline" tex="128" text="128" xml:id="S4.SS1.SSS1.p1.m4">
              <XMath>
                <XMTok meaning="128" role="NUMBER">128</XMTok>
              </XMath>
            </Math>. We use batch size <Math mode="inline" tex="16" text="16" xml:id="S4.SS1.SSS1.p1.m5">
              <XMath>
                <XMTok meaning="16" role="NUMBER">16</XMTok>
              </XMath>
            </Math>. We refer to Table <ref labelref="LABEL:tab:hyperparams-finetune"/> in Appendix <ref labelref="LABEL:sec:hp"/> for the rest of the hyperparameters.</p>
        </para>
        <para xml:id="S4.SS1.SSS1.p2">
          <p>We train for ten epochs and use linear warmup, with the peak occurring at 16 percent of the training. After the peak. For each dataset, we do a fixed grid search over dropout and learning rate parameters with the values <Math mode="inline" tex="\left[0.1,0.2,0.3\right]" text="list@(0.1, 0.2, 0.3)" xml:id="S4.SS1.SSS1.p2.m1">
              <XMath>
                <XMDual>
                  <XMApp>
                    <XMTok meaning="list"/>
                    <XMRef idref="S4.SS1.SSS1.p2.m1.1"/>
                    <XMRef idref="S4.SS1.SSS1.p2.m1.2"/>
                    <XMRef idref="S4.SS1.SSS1.p2.m1.3"/>
                  </XMApp>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="true">[</XMTok>
                    <XMTok meaning="0.1" role="NUMBER" xml:id="S4.SS1.SSS1.p2.m1.1">0.1</XMTok>
                    <XMTok role="PUNCT">,</XMTok>
                    <XMTok meaning="0.2" role="NUMBER" xml:id="S4.SS1.SSS1.p2.m1.2">0.2</XMTok>
                    <XMTok role="PUNCT">,</XMTok>
                    <XMTok meaning="0.3" role="NUMBER" xml:id="S4.SS1.SSS1.p2.m1.3">0.3</XMTok>
                    <XMTok role="CLOSE" stretchy="true">]</XMTok>
                  </XMWrap>
                </XMDual>
              </XMath>
            </Math> and <Math mode="inline" tex="\left[5\mathrm{e}{-6},1\mathrm{e}{-5},3\mathrm{e}{-5}\right]" text="list@(5 * e - 6, 1 * e - 5, 3 * e - 5)" xml:id="S4.SS1.SSS1.p2.m2">
              <XMath>
                <XMDual>
                  <XMApp>
                    <XMTok meaning="list"/>
                    <XMRef idref="S4.SS1.SSS1.p2.m2.1"/>
                    <XMRef idref="S4.SS1.SSS1.p2.m2.2"/>
                    <XMRef idref="S4.SS1.SSS1.p2.m2.3"/>
                  </XMApp>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="true">[</XMTok>
                    <XMApp xml:id="S4.SS1.SSS1.p2.m2.1">
                      <XMTok meaning="minus" role="ADDOP">-</XMTok>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok meaning="5" role="NUMBER">5</XMTok>
                        <XMTok role="UNKNOWN">e</XMTok>
                      </XMApp>
                      <XMTok meaning="6" role="NUMBER">6</XMTok>
                    </XMApp>
                    <XMTok role="PUNCT">,</XMTok>
                    <XMApp xml:id="S4.SS1.SSS1.p2.m2.2">
                      <XMTok meaning="minus" role="ADDOP">-</XMTok>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok meaning="1" role="NUMBER">1</XMTok>
                        <XMTok role="UNKNOWN">e</XMTok>
                      </XMApp>
                      <XMTok meaning="5" role="NUMBER">5</XMTok>
                    </XMApp>
                    <XMTok role="PUNCT">,</XMTok>
                    <XMApp xml:id="S4.SS1.SSS1.p2.m2.3">
                      <XMTok meaning="minus" role="ADDOP">-</XMTok>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok meaning="3" role="NUMBER">3</XMTok>
                        <XMTok role="UNKNOWN">e</XMTok>
                      </XMApp>
                      <XMTok meaning="5" role="NUMBER">5</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="true">]</XMTok>
                  </XMWrap>
                </XMDual>
              </XMath>
            </Math>, respectively. Finally, we apply Stochastic Weight Averaging (SWA) on three sets of four checkpoints: around the checkpoint having the best validation loss, the best validation accuracy, and the last checkpoint <cite class="ltx_citemacro_citep">(<bibref bibrefs="SWA" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>. We select the best one according to the performance on the validation set.</p>
        </para>
        <para xml:id="S4.SS1.SSS1.p3">
          <p>For multi-task problems, we train individual models for each task. We perform the grid search only on the first four tasks, pick the best hyperparameters and apply them to all tasks. We also tried to train a single multi-task classification model for each dataset to cover all tasks simultaneously, but the results were significantly worse (e.g. by 0.25 points on Tox21 and by 0.1 points on SIDER).</p>
        </para>
        <table inlist="lot" placement="h" xml:id="S4.T4">
          <tags>
            <tag><text fontsize="90%">Table 4</text></tag>
            <tag role="autoref">Table 4</tag>
            <tag role="refnum">4</tag>
            <tag role="typerefnum">Table 4</tag>
          </tags>
          <figure align="center" class="ltx_figure_panel" inlist="lof" labels="LABEL:tab:results-regression" xml:id="S4.F3.sf1">
            <tags>
              <tag><text fontsize="90%">(a)</text></tag>
              <tag role="autoref">3(a)</tag>
              <tag role="refnum">3(a)</tag>
            </tags>
            <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
              <tbody>
                <tr>
                  <td border="tt" thead="row"/>
                  <td align="center" border="tt"><text fontsize="70%">ESOL</text></td>
                  <td align="center" border="tt"><text fontsize="70%">FreeSolv</text></td>
                  <td align="center" border="tt"><text fontsize="70%">Lipophilicity</text></td>
                  <td align="center" border="tt"><text fontsize="70%">Avg</text></td>
                </tr>
                <tr>
                  <td align="left" border="t" thead="row"><text fontsize="70%">RF MolCLR</text></td>
                  <td align="center" border="t"><text fontsize="70%">1.070</text></td>
                  <td align="center" border="t"><text fontsize="70%">2.030</text></td>
                  <td align="center" border="t"><text fontsize="70%">0.880</text></td>
                  <td align="center" border="t"><text fontsize="70%">1.327</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">SVM MolCLR</text></td>
                  <td align="center"><text fontsize="70%">1.500</text></td>
                  <td align="center"><text fontsize="70%">3.140</text></td>
                  <td align="center"><text fontsize="70%">0.820</text></td>
                  <td align="center"><text fontsize="70%">1.820</text></td>
                </tr>
                <tr>
                  <td align="left" border="t" thead="row"><text fontsize="70%">MoleculeNet GC</text></td>
                  <td align="center" border="t"><text fontsize="70%">0.97</text></td>
                  <td align="center" border="t"><text fontsize="70%">1.40</text></td>
                  <td align="center" border="t"><text fontsize="70%">0.655</text></td>
                  <td align="center" border="t"><text fontsize="70%">1.008</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">D-MPNN</text></td>
                  <td align="center"><text fontsize="70%">0.980</text></td>
                  <td align="center"><text fontsize="70%">2.180</text></td>
                  <td align="center"><text fontsize="70%">0.650</text></td>
                  <td align="center"><text fontsize="70%">1.270</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">Attentive FP</text></td>
                  <td align="center"><text fontsize="70%">0.503</text></td>
                  <td align="center"><text fontsize="70%">0.736</text></td>
                  <td align="center"><text fontsize="70%">0.578</text></td>
                  <td align="center"><text fontsize="70%">0.606</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">3D Infomax</text></td>
                  <td align="center"><text fontsize="70%">0.894</text></td>
                  <td align="center"><text fontsize="70%">2.337</text></td>
                  <td align="center"><text fontsize="70%">0.695</text></td>
                  <td align="center"><text fontsize="70%">1.309</text></td>
                </tr>
                <tr>
                  <td align="left" border="t" thead="row"><text fontsize="70%">Chemformer</text></td>
                  <td align="center" border="t"><text fontsize="70%">0.633</text></td>
                  <td align="center" border="t"><text fontsize="70%">1.230</text></td>
                  <td align="center" border="t"><text fontsize="70%">0.598</text></td>
                  <td align="center" border="t"><text fontsize="70%">0.820</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">MoLFormer-XL</text></td>
                  <td align="center"><text fontsize="70%">0.279</text></td>
                  <td align="center"><text fontsize="70%">0.231</text></td>
                  <td align="center"><text fontsize="70%">0.529</text></td>
                  <td align="center"><text fontsize="70%">0.346</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">GROVER large</text></td>
                  <td align="center"><text fontsize="70%">0.831</text></td>
                  <td align="center"><text fontsize="70%">1.544</text></td>
                  <td align="center"><text fontsize="70%">0.560</text></td>
                  <td align="center"><text fontsize="70%">0.978</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">MolCLR</text></td>
                  <td align="center"><text fontsize="70%">1.110</text></td>
                  <td align="center"><text fontsize="70%">2.200</text></td>
                  <td align="center"><text fontsize="70%">0.650</text></td>
                  <td align="center"><text fontsize="70%">1.320</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">iMolCLR</text></td>
                  <td align="center"><text fontsize="70%">1.130</text></td>
                  <td align="center"><text fontsize="70%">2.090</text></td>
                  <td align="center"><text fontsize="70%">0.640</text></td>
                  <td align="center"><text fontsize="70%">1.287</text></td>
                </tr>
                <tr>
                  <td align="left" border="bb t" thead="row"><text font="typewriter" fontsize="70%">BARTSmiles</text></td>
                  <td align="center" border="bb t"><text font="bold" fontsize="70%">0.095</text></td>
                  <td align="center" border="bb t"><text font="bold" fontsize="70%">0.114</text></td>
                  <td align="center" border="bb t"><text font="bold" fontsize="70%">0.292</text></td>
                  <td align="center" border="bb t"><text font="bold" fontsize="70%">0.167</text></td>
                </tr>
              </tbody>
            </tabular>
            <toccaption class="ltx_centering"><tag close=" "><text fontsize="70%">(a)</text></tag><text fontsize="70%">Regression tasks from MoleculeNet. All numbers are RMSE, lower is better. </text></toccaption>
            <caption class="ltx_centering" fontsize="70%"><tag close=" "><text fontsize="129%">(a)</text></tag><text fontsize="129%">Regression tasks from MoleculeNet. All numbers are RMSE, lower is better. </text></caption>
          </figure>
          <figure align="center" class="ltx_figure_panel" inlist="lof" labels="LABEL:tab:results-tox" xml:id="S4.F3.sf3">
            <tags>
              <tag><text fontsize="90%">(c)</text></tag>
              <tag role="autoref">3(c)</tag>
              <tag role="refnum">3(c)</tag>
            </tags>
            <tabular class="ltx_centering ltx_figure_panel ltx_guessed_headers" vattach="middle">
              <tbody>
                <tr>
                  <td border="tt" thead="row"/>
                  <td align="right" border="tt"><text fontsize="70%">AUC</text></td>
                </tr>
                <tr>
                  <td align="left" border="t" thead="row"><text fontsize="70%">FP_Pubchem_SVM</text></td>
                  <td align="right" border="t"><text fontsize="70%">0.948</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">FP_MACCS_RF</text></td>
                  <td align="right"><text fontsize="70%">0.947</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">Descriptor_SVM</text></td>
                  <td align="right"><text font="bold" fontsize="70%">0.952</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">Descriptor_RF</text></td>
                  <td align="right"><text fontsize="70%">0.933</text></td>
                </tr>
                <tr>
                  <td align="left" border="t" thead="row"><text font="typewriter" fontsize="70%">BARTSmiles</text></td>
                  <td align="right" border="t"><text fontsize="70%">0.914</text></td>
                </tr>
                <tr>
                  <td align="left" border="bb" thead="row"><text font="typewriter" fontsize="70%">BARTSmiles</text><text fontsize="70%"> (Frozen) + ECFP + SVM</text></td>
                  <td align="right" border="bb"><text font="bold" fontsize="70%">0.948</text></td>
                </tr>
              </tbody>
            </tabular>
            <toccaption class="ltx_centering"><tag close=" "><text fontsize="70%">(b)</text></tag><text fontsize="70%">Micronucleus assay dataset</text></toccaption>
            <caption class="ltx_centering" fontsize="70%"><tag close=" "><text fontsize="129%">(b)</text></tag><text fontsize="129%">Micronucleus assay dataset</text></caption>
            <tabular class="ltx_centering ltx_figure_panel ltx_guessed_headers" vattach="middle">
              <tbody>
                <tr>
                  <td border="tt" thead="row"/>
                  <td align="center" border="tt"><text fontsize="70%">AUC</text></td>
                </tr>
                <tr>
                  <td align="left" border="t" thead="row"><text fontsize="70%">SVM</text></td>
                  <td align="center" border="t"><text fontsize="70%">0.86</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">GP</text></td>
                  <td align="center"><text fontsize="70%">0.84</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">RF</text></td>
                  <td align="center"><text fontsize="70%">0.83</text></td>
                </tr>
                <tr>
                  <td align="left" thead="row"><text fontsize="70%">k-NN</text></td>
                  <td align="center"><text fontsize="70%">0.79</text></td>
                </tr>
                <tr>
                  <td align="left" border="t" thead="row"><text font="typewriter" fontsize="70%">BARTSmiles</text></td>
                  <td align="center" border="t"><text fontsize="70%">0.830</text></td>
                </tr>
                <tr>
                  <td align="left" border="bb" thead="row"><text font="typewriter" fontsize="70%">BARTSmiles</text><text fontsize="70%"> (Frozen) + ECFP + SVM</text></td>
                  <td align="center" border="bb"><text font="bold" fontsize="70%">0.869</text></td>
                </tr>
              </tbody>
            </tabular>
            <toccaption class="ltx_centering"><tag close=" "><text fontsize="70%">(c)</text></tag><text fontsize="70%">Ames dataset (cross-validation)</text></toccaption>
            <caption class="ltx_centering" fontsize="70%"><tag close=" "><text fontsize="129%">(c)</text></tag><text fontsize="129%">Ames dataset (cross-validation)</text></caption>
          </figure>
          <toccaption class="ltx_centering"><tag close=" "><text fontsize="70%">4</text></tag><text fontsize="70%">Results on MoleculeNet regression tasks and two toxicology datasets.</text></toccaption>
          <caption class="ltx_centering" fontsize="70%"><tag close=": "><text fontsize="129%">Table 4</text></tag><text fontsize="129%">Results on MoleculeNet regression tasks and two toxicology datasets.</text></caption>
        </table>
      </subsubsection>
    </subsection>
    <subsection inlist="toc" xml:id="S4.SS2">
      <tags>
        <tag>4.2</tag>
        <tag role="autoref">subsection 4.2</tag>
        <tag role="refnum">4.2</tag>
        <tag role="typerefnum">§4.2</tag>
      </tags>
      <title><tag close=" ">4.2</tag>Baselines</title>
      <para xml:id="S4.SS2.p1">
        <p>MoleculeNet paper introduced its own set of baselines using multiple machine learning methods. MoleculeNet did not apply most baselines to all datasets. Among the ones applied to all datasets, the convolutional graph baseline performed the best, and we have included it as our baseline.
The other graph-based supervised baselines include D-MPNN <cite class="ltx_citemacro_citep">(<bibref bibrefs="DMPNN-original" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, Attentive FP <cite class="ltx_citemacro_citep">(<bibref bibrefs="AttentiveFP" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> and the 3D InfoMax method <cite class="ltx_citemacro_citep">(<bibref bibrefs="3dinfomaxGNN" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.</p>
      </para>
      <para xml:id="S4.SS2.p2">
        <p>We also compare with baselines that use self-supervised pretraining. We skip SMILES-BERT, as SMILES-BERT did not test it on MoleculeNet. ChemBERTa <cite class="ltx_citemacro_citep">(<bibref bibrefs="chemberta" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, Chemformer <cite class="ltx_citemacro_citep">(<bibref bibrefs="chemformer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> and MolFormer-XL <cite class="ltx_citemacro_citep">(<bibref bibrefs="MolFormer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> are based on regular text-based transformers. Graph Isomorphism Networks (GIN) with Context Prediction from <cite class="ltx_citemacro_cite"><bibref bibrefs="GIN" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, GROVER-Large from <cite class="ltx_citemacro_citet"><bibref bibrefs="GROVER" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, MolCLR <cite class="ltx_citemacro_citep">(<bibref bibrefs="MolCLR" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> and iMolCLR <cite class="ltx_citemacro_citep">(<bibref bibrefs="iMolCLR" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> are self-supervised methods based on graph neural networks. For Micronucleus Assay and Ames datasets we compare with the baselines introduced in the original papers.</p>
      </para>
      <subsubsection inlist="toc" xml:id="S4.SS2.SSS1">
        <tags>
          <tag>4.2.1</tag>
          <tag role="autoref">subsubsection 4.2.1</tag>
          <tag role="refnum">4.2.1</tag>
          <tag role="typerefnum">§4.2.1</tag>
        </tags>
        <title><tag close=" ">4.2.1</tag>Results</title>
        <para xml:id="S4.SS2.SSS1.p1">
          <p>Our model produced state-of-the-art results on all three regression tasks of the MoleculeNet suite, as seen in Table <ref labelref="LABEL:tab:results-regression"/><!--  %**** arxiv˙version.tex Line 275 **** -->. On classifications tasks the results are mixed (Table <ref labelref="LABEL:tab:results-classification"/>). On Clintox and Toxcast datasets, our model beats all previous models. Toxcast is a huge dataset with 617 tasks, so many papers do not include results. On SIDER and Tox21, the performance of our models are comparable to the state-of-the-art.</p>
        </para>
        <para xml:id="S4.SS2.SSS1.p2">
          <p>The other three classification tasks (HIV, BACE, and BBBP) require specific scaffold splits and are sensitive to the choice of the split. Unfortunately, many papers do not specify whether they used the split recommended by MoleculeNet. <cite class="ltx_citemacro_citet"><bibref bibrefs="GNN-drug-comparison" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase>(</bibrefphrase>
                <bibrefphrase>)</bibrefphrase>
              </bibref></cite> claim D-MPNN and Attentive FP results are based on random splits. MolFormer and GROVER-large use scaffold splits, but the splits are different.
We explored the split’s impact on the BBBP dataset’s results.</p>
        </para>
        <para xml:id="S4.SS2.SSS1.p3">
          <p>Molecule scaffolds induce 1024 clusters of 1940 molecules in the BBBP dataset. The split is organized so that all non-singleton clusters belong to the training set, while the validation and test sets contain only molecules with unique scaffolds. Most of the errors of our model are concentrated on roughly 30% of the singleton clusters in the test set. This implies that even a scaffold split with a different random seed can significantly affect the results. In Table  <ref labelref="LABEL:tab:results-classification"/>, we have marked the results of these three classification datasets with an asterisk if we believe the authors used a different split.</p>
        </para>
        <para xml:id="S4.SS2.SSS1.p4">
          <p>The results for the two toxicology datasets that direct fine-tuning did not beat the baselines based on fingerprints and descriptors. To test whether the fingerprints still contain helpful information not covered by <text font="typewriter">BARTSmiles</text>, we fine-tuned a classifier head that takes 2048-dimensional ECFP4 fingerprints <cite class="ltx_citemacro_citep">(<bibref bibrefs="ECFP-fingerprints" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite> as an input in addition to the [CLS] vector. We also froze <text font="typewriter">BARTSmiles</text>, averaged the last layers’ representations per molecule, concatenated them with ECFP fingerprints, and ran the default SVM implementation from <cite class="ltx_citemacro_citet"><bibref bibrefs="scikit" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase>(</bibrefphrase>
                <bibrefphrase>)</bibrefphrase>
              </bibref></cite>. This helped close the gap with the baselines, as seen in Table <ref labelref="LABEL:tab:results-tox"/>. Fundamentally this implies that fingerprints carry some information that is not trivially recoverable from our pre-trained representation. We do a deeper analysis of our pre-training dataset in § <ref labelref="LABEL:section:dataset_rep_space"/> and hypothesize that the Zinc20 dataset is not representative of the downstream tasks computational chemistry is interested in.</p>
        </para>
      </subsubsection>
    </subsection>
    <subsection inlist="toc" xml:id="S4.SS3">
      <tags>
        <tag>4.3</tag>
        <tag role="autoref">subsection 4.3</tag>
        <tag role="refnum">4.3</tag>
        <tag role="typerefnum">§4.3</tag>
      </tags>
      <title><tag close=" ">4.3</tag>Generative tasks</title>
      <para xml:id="S4.SS3.p1">
        <p>We follow a straightforward recipe for generative fine-tuning for all generative tasks, mainly derived from <cite class="ltx_citemacro_citet"><bibref bibrefs="RXF" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>. Similarly to our results on SIDER, we found that training in full 32-bit precision (fp32) significantly impacted downstream results. We use the R3F method from <cite class="ltx_citemacro_citet"><bibref bibrefs="RXF" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite>, running a sweep over the noise types and <Math mode="inline" tex="\lambda" text="lambda" xml:id="S4.SS3.p1.m1">
            <XMath>
              <XMTok font="italic" name="lambda" role="UNKNOWN">λ</XMTok>
            </XMath>
          </Math> regularization term for each task (See Table <ref labelref="LABEL:tab:hyperparams-generative"/> in Appendix <ref labelref="LABEL:sec:hp"/>). Consistent with our fine-tuning recipe for classification and regression tasks, we utilize SWA and do not do any augmentation on the data. For additional stability, we initialize our fine-tuning optimizer (Adam) with the pre-training moving averages of the gradient and squared gradient from our pre-training. We also use a polynomial decay strategy where the peak learning rate is at <Math mode="inline" tex="0.06*\text{max\_updates}" text="0.06 * [max_updates]" xml:id="S4.SS3.p1.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">∗</XMTok>
                <XMTok meaning="0.06" role="NUMBER">0.06</XMTok>
                <XMText>max_updates</XMText>
              </XMApp>
            </XMath>
          </Math> where the max number of updates is the number of updates needed to do ten epochs for the respective dataset.</p>
      </para>
      <para xml:id="S4.SS3.p2">
        <p>During the evaluation, we implement two different strategies for sequence generation. The first runs a straightforward beam search with a beam size of ten. The second approach samples 128 samples from the natural distribution of tokens from the model (temperature of 1.0) and then re-ranks the list of 128 samples based on the perplexity of the entire predicted SMILES.</p>
      </para>
      <para xml:id="S4.SS3.p3">
        <p>We select two common generative molecular tasks; retrosynthesis and chemical reaction prediction.</p>
      </para>
      <subsubsection inlist="toc" xml:id="S4.SS3.SSS1">
        <tags>
          <tag>4.3.1</tag>
          <tag role="autoref">subsubsection 4.3.1</tag>
          <tag role="refnum">4.3.1</tag>
          <tag role="typerefnum">§4.3.1</tag>
        </tags>
        <title><tag close=" ">4.3.1</tag>Retrosynthesis</title>
        <para xml:id="S4.SS3.SSS1.p1">
          <p>Retrosynthesis is a chemical synthesis technique involving the deconstruction of a target molecule into its starting materials to assess the best synthetic route. Retrosynthesis is a cornerstone of modern organic product synthesis, traditionally the domain of human experts’ knowledge. Due to the algorithmic advances and the availability of large chemical reaction collections, computational approaches have been suggested to solve the problems in retrosynthesis <cite class="ltx_citemacro_citep">(<bibref bibrefs="davey2018retrosynthesis,watson2019retrosynthetic" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>. We apply our generative fine-tuning strategy on the USPTO dataset <cite class="ltx_citemacro_citep">(<bibref bibrefs="lowe-USPTO-2012" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>. Table <ref labelref="LABEL:table:retrosynthesis"/> shows the ratio of molecules for which the correct reactants were among the top K predictions generated by the respective models. Our approach has the best result for the Top-1 metric.</p>
        </para>
        <table inlist="lot" labels="LABEL:table:retrosynthesis" xml:id="S4.T5">
          <tags>
            <tag><text fontsize="90%">Table 5</text></tag>
            <tag role="autoref">Table 5</tag>
            <tag role="refnum">5</tag>
            <tag role="typerefnum">Table 5</tag>
          </tags>
          <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
            <tbody>
              <tr>
                <td border="tt" thead="row"/>
                <td align="left" border="tt">Top-1</td>
                <td align="left" border="tt">Top-5</td>
                <td align="left" border="tt" class="ltx_nopad_r">Top-10</td>
              </tr>
              <tr>
                <td align="left" border="t" thead="row">RetroComposer <cite class="ltx_citemacro_citep">(<bibref bibrefs="retrocomposer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase>, </bibrefphrase>
                    </bibref>)</cite></td>
                <td align="left" border="t">53.3</td>
                <td align="left" border="t">80.9</td>
                <td align="left" border="t" class="ltx_nopad_r">85.0</td>
              </tr>
              <tr>
                <td align="left" thead="row">ATx100 <cite class="ltx_citemacro_citep">(<bibref bibrefs="augmented_transformer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase>, </bibrefphrase>
                    </bibref>)</cite></td>
                <td align="left">53.5</td>
                <td align="left"><text font="bold">81.0</text></td>
                <td align="left" class="ltx_nopad_r"><text font="bold">85.7</text></td>
              </tr>
              <tr>
                <td align="left" thead="row">GraphRetro <cite class="ltx_citemacro_citep">(<bibref bibrefs="graph_retro" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase>, </bibrefphrase>
                    </bibref>)</cite></td>
                <td align="left">53.7</td>
                <td align="left">72.2</td>
                <td align="left" class="ltx_nopad_r">75.5</td>
              </tr>
              <tr>
                <td align="left" thead="row">ChemFormer <cite class="ltx_citemacro_citep">(<bibref bibrefs="chemformer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase>, </bibrefphrase>
                    </bibref>)</cite></td>
                <td align="left">54.3</td>
                <td align="left">62.3</td>
                <td align="left" class="ltx_nopad_r">63.0</td>
              </tr>
              <tr>
                <td align="left" thead="row">Dual-TF <cite class="ltx_citemacro_citep">(<bibref bibrefs="retrosynthesis_energy" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase>, </bibrefphrase>
                    </bibref>)</cite></td>
                <td align="left"><text font="bold">55.3</text></td>
                <td align="left">73.0</td>
                <td align="left" class="ltx_nopad_r">75.0</td>
              </tr>
              <tr>
                <td align="left" border="t" thead="row"><text font="typewriter">BARTSmiles</text> Beam-10</td>
                <td align="left" border="t"><text font="bold">55.2</text></td>
                <td align="left" border="t">68.5</td>
                <td align="left" border="t" class="ltx_nopad_r">73.2</td>
              </tr>
              <tr>
                <td align="left" border="b" thead="row"><text font="typewriter">BARTSmiles</text> Sample-128 + PPL ReRank</td>
                <td align="left" border="b"><text font="bold">55.6</text></td>
                <td align="left" border="b">74.2</td>
                <td align="left" border="b" class="ltx_nopad_r">80.9</td>
              </tr>
            </tbody>
          </tabular>
          <toccaption class="ltx_centering"><tag close=" ">5</tag>We present our experimental results on the USPTO-50k retrosynthesis task across varying sampling strategies. We bold the best performing numbers within a 0.5 range.</toccaption>
          <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Table 5</text></tag><text fontsize="90%">We present our experimental results on the USPTO-50k retrosynthesis task across varying sampling strategies. We bold the best performing numbers within a 0.5 range.</text></caption>
        </table>
      </subsubsection>
      <subsubsection inlist="toc" xml:id="S4.SS3.SSS2">
        <tags>
          <tag>4.3.2</tag>
          <tag role="autoref">subsubsection 4.3.2</tag>
          <tag role="refnum">4.3.2</tag>
          <tag role="typerefnum">§4.3.2</tag>
        </tags>
        <title><tag close=" ">4.3.2</tag>Chemical Reaction Prediction</title>
        <para xml:id="S4.SS3.SSS2.p1">
          <p>The dual problem of retrosynthesis is chemical reaction prediction which is the automatic construction of the target molecule given the set of reactant molecules. This can also be treated as a sequence-to-sequence task, as first shown by <cite class="ltx_citemacro_citet"><bibref bibrefs="reaction-prediction-seq2seq" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase>(</bibrefphrase>
                <bibrefphrase>)</bibrefphrase>
              </bibref></cite>, and can be performed by fine-tuning <text font="typewriter">BARTSmiles</text> on a relevant dataset. We follow the setup presented by <cite class="ltx_citemacro_citet"><bibref bibrefs="molecular-transformer" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase>(</bibrefphrase>
                <bibrefphrase>)</bibrefphrase>
              </bibref></cite> and apply our generative fine-tuning recipe to several subsets of the USPTO dataset. As seen in Table <ref labelref="LABEL:table:chemical_reaction_prediction"/>, <text font="typewriter">BARTSmiles</text> outperforms all baselines across all subsets.</p>
        </para>
        <table inlist="lot" labels="LABEL:table:chemical_reaction_prediction" placement="h" xml:id="S4.T6">
          <tags>
            <tag><text fontsize="90%">Table 6</text></tag>
            <tag role="autoref">Table 6</tag>
            <tag role="refnum">6</tag>
            <tag role="typerefnum">Table 6</tag>
          </tags>
          <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
            <tbody>
              <tr>
                <td align="left" border="tt" thead="row"><text fontsize="70%">USPTO Split</text></td>
                <td align="left" border="tt" thead="row"><text fontsize="70%">MIT (S)</text></td>
                <td align="left" border="tt" thead="row"><text fontsize="70%">MIT (M)</text></td>
                <td align="left" border="tt"><text fontsize="70%">LEF (S)</text></td>
                <td align="left" border="tt"><text fontsize="70%">LEF (M)</text></td>
                <td align="left" border="tt"><text fontsize="70%">STEREO (S)</text></td>
                <td align="left" border="tt" class="ltx_nopad_r"><text fontsize="70%">STEREO (M)</text></td>
              </tr>
              <tr>
                <td align="left" border="t" thead="row"><text fontsize="70%">S2S </text><cite class="ltx_citemacro_citep"><text fontsize="70%">(</text><bibref bibrefs="reaction-prediction-seq2seq" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase><text fontsize="70%">, </text></bibrefphrase>
                    </bibref><text fontsize="70%">)</text></cite></td>
                <td align="left" border="t" thead="row"><text fontsize="70%">80.3</text></td>
                <td border="t" thead="row"/>
                <td border="t"/>
                <td border="t"/>
                <td align="left" border="t"><text fontsize="70%">65.4</text></td>
                <td border="t" class="ltx_nopad_r"/>
              </tr>
              <tr>
                <td align="left" thead="row"><text fontsize="70%">WLDN </text><cite class="ltx_citemacro_citep"><text fontsize="70%">(</text><bibref bibrefs="jin2017predicting" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase><text fontsize="70%">, </text></bibrefphrase>
                    </bibref><text fontsize="70%">)</text></cite></td>
                <td align="left" thead="row"><text fontsize="70%">79.6</text></td>
                <td align="left" thead="row"><text fontsize="70%">74</text></td>
                <td align="left"><text fontsize="70%">84.0</text></td>
                <td/>
                <td/>
                <td class="ltx_nopad_r"/>
              </tr>
              <tr>
                <td align="left" thead="row"><text fontsize="70%">ELECTRO </text><cite class="ltx_citemacro_citep"><text fontsize="70%">(</text><bibref bibrefs="bradshaw2018generative" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase><text fontsize="70%">, </text></bibrefphrase>
                    </bibref><text fontsize="70%">)</text></cite></td>
                <td thead="row"/>
                <td thead="row"/>
                <td align="left"><text fontsize="70%">87.0</text></td>
                <td/>
                <td/>
                <td class="ltx_nopad_r"/>
              </tr>
              <tr>
                <td align="left" thead="row"><text fontsize="70%">GTPN </text><cite class="ltx_citemacro_citep"><text fontsize="70%">(</text><bibref bibrefs="do2019graph" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase><text fontsize="70%">, </text></bibrefphrase>
                    </bibref><text fontsize="70%">)</text></cite></td>
                <td align="left" thead="row"><text fontsize="70%">82.4</text></td>
                <td thead="row"/>
                <td align="left"><text fontsize="70%">87.4</text></td>
                <td/>
                <td/>
                <td class="ltx_nopad_r"/>
              </tr>
              <tr>
                <td align="left" thead="row"><text fontsize="70%">WLDN5 </text><cite class="ltx_citemacro_citep"><text fontsize="70%">(</text><bibref bibrefs="coley2019graph" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase><text fontsize="70%">, </text></bibrefphrase>
                    </bibref><text fontsize="70%">)</text></cite></td>
                <td align="left" thead="row"><text fontsize="70%">85.6</text></td>
                <td thead="row"/>
                <td align="left"><text fontsize="70%">88.3</text></td>
                <td/>
                <td/>
                <td class="ltx_nopad_r"/>
              </tr>
              <tr>
                <td align="left" thead="row"><text fontsize="70%">Transformer </text><cite class="ltx_citemacro_citep"><text fontsize="70%">(</text><bibref bibrefs="molecular-transformer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                      <bibrefphrase><text fontsize="70%">, </text></bibrefphrase>
                    </bibref><text fontsize="70%">)</text></cite></td>
                <td align="left" thead="row"><text fontsize="70%">90.4</text></td>
                <td align="left" thead="row"><text fontsize="70%">88.6</text></td>
                <td align="left"><text font="bold" fontsize="70%">92.0</text></td>
                <td align="left"><text fontsize="70%">90.3</text></td>
                <td align="left"><text fontsize="70%">78.1</text></td>
                <td align="left" class="ltx_nopad_r"><text fontsize="70%">76.2</text></td>
              </tr>
              <tr>
                <td align="left" border="t" thead="row"><text font="typewriter" fontsize="70%">BARTSmiles</text><text fontsize="70%"> Beam-10</text></td>
                <td align="left" border="t" thead="row"><text font="bold" fontsize="70%">91.8</text></td>
                <td align="left" border="t" thead="row"><text font="bold" fontsize="70%">89.1</text></td>
                <td align="left" border="t"><text font="bold" fontsize="70%">92.1</text></td>
                <td align="left" border="t"><text font="bold" fontsize="70%">92.8</text></td>
                <td align="left" border="t"><text font="bold" fontsize="70%">82.5</text></td>
                <td align="left" border="t" class="ltx_nopad_r"><text font="bold" fontsize="70%">82.1</text></td>
              </tr>
              <tr>
                <td align="left" border="bb" thead="row"><text font="typewriter" fontsize="70%">BARTSmiles</text><text fontsize="70%"> Sample + ReRank</text></td>
                <td align="left" border="bb" thead="row"><text fontsize="70%">91.1</text></td>
                <td align="left" border="bb" thead="row"><text font="bold" fontsize="70%">89.1</text></td>
                <td align="left" border="bb"><text fontsize="70%">91.8</text></td>
                <td align="left" border="bb"><text fontsize="70%">90.2</text></td>
                <td align="left" border="bb"><text fontsize="70%">80.4</text></td>
                <td align="left" border="bb" class="ltx_nopad_r"><text fontsize="70%">81.5</text></td>
              </tr>
            </tbody>
          </tabular>
          <toccaption class="ltx_centering"><tag close=" "><text fontsize="70%">6</text></tag><text fontsize="70%">We present our experimental results on the USPTO chemical prediction task across varying sampling strategies and three data subsets (MIT, LEF and STEREO). (S) signifies the “split” and (M) signifies the “mixed” sampling strategy from </text><cite class="ltx_citemacro_citet"><bibref bibrefs="molecular-transformer" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase><text fontsize="70%">(</text></bibrefphrase>
                <bibrefphrase><text fontsize="70%">)</text></bibrefphrase>
              </bibref></cite><text fontsize="70%">). We bold the best performing numbers within a 0.5 range. The baselines scores are taken from </text><cite class="ltx_citemacro_citet"><bibref bibrefs="molecular-transformer" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase><text fontsize="70%">(</text></bibrefphrase>
                <bibrefphrase><text fontsize="70%">)</text></bibrefphrase>
              </bibref></cite><text fontsize="70%">. </text></toccaption>
          <caption class="ltx_centering" fontsize="70%"><tag close=": "><text fontsize="129%">Table 6</text></tag><text fontsize="129%">We present our experimental results on the USPTO chemical prediction task across varying sampling strategies and three data subsets (MIT, LEF and STEREO). (S) signifies the “split” and (M) signifies the “mixed” sampling strategy from <cite class="ltx_citemacro_citet"><bibref bibrefs="molecular-transformer" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                  <bibrefphrase>(</bibrefphrase>
                  <bibrefphrase>)</bibrefphrase>
                </bibref></cite>). We bold the best performing numbers within a 0.5 range. The baselines scores are taken from <cite class="ltx_citemacro_citet"><bibref bibrefs="molecular-transformer" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                  <bibrefphrase>(</bibrefphrase>
                  <bibrefphrase>)</bibrefphrase>
                </bibref></cite>. </text></caption>
        </table>
      </subsubsection>
    </subsection>
  </section>
  <section inlist="toc" xml:id="S5">
    <tags>
      <tag>5</tag>
      <tag role="autoref">section 5</tag>
      <tag role="refnum">5</tag>
      <tag role="typerefnum">§5</tag>
    </tags>
    <title><tag close=" ">5</tag>Interpretability</title>
    <subsection inlist="toc" xml:id="S5.SS1">
      <tags>
        <tag>5.1</tag>
        <tag role="autoref">subsection 5.1</tag>
        <tag role="refnum">5.1</tag>
        <tag role="typerefnum">§5.1</tag>
      </tags>
      <title><tag close=" ">5.1</tag>Uncovering the key features</title>
      <para xml:id="S5.SS1.p1">
        <p><cite class="ltx_citemacro_citet"><bibref bibrefs="radford2017learning" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> showed that a large language model trained to generate text had a particular neuron correlated with the sentence’s sentiment, colloquially dubbed the sentiment neuron. Further work showed that self-supervised representations had a very low intrinsic dimension for downstream tasks, arguing that self-supervised representations were capable of learning complex tasks implicitly <cite class="ltx_citemacro_citep">(<bibref bibrefs="intrinsic_dimensionality_finetuning" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. For each dataset, we extracted the representations of each molecule by taking the average of the last layer representations of all tokens given by the pre-trained model and trained an L1-regularized logistic regression model to predict the label. We indirectly control the number of selected features by changing the regularization strength. The results are shown on Fig. <ref labelref="LABEL:fig:num_features_vs_auc_roc"/>.</p>
      </para>
      <para xml:id="S5.SS1.p2">
        <p>Clintox tasks are among the easiest ones. As seen in Figure <ref labelref="LABEL:fig:num_features_vs_auc_roc"/>, a single unsupervised neuron can predict the label of its first subtask with a <Math mode="inline" tex="0.77" text="0.77" xml:id="S5.SS1.p2.m1">
            <XMath>
              <XMTok meaning="0.77" role="NUMBER">0.77</XMTok>
            </XMath>
          </Math> ROC-AUC score, while with seven neurons, one can achieve <Math mode="inline" tex="0.987" text="0.987" xml:id="S5.SS1.p2.m2">
            <XMath>
              <XMTok meaning="0.987" role="NUMBER">0.987</XMTok>
            </XMath>
          </Math> score. It is interesting to note that for the regularization parameter <Math mode="inline" tex="C\in\{2^{-6},2^{-7},2^{-8}\}" text="C element-of set@(2 ^ (- 6), 2 ^ (- 7), 2 ^ (- 8))" xml:id="S5.SS1.p2.m3">
            <XMath>
              <XMApp>
                <XMTok meaning="element-of" name="in" role="RELOP">∈</XMTok>
                <XMTok font="italic" role="UNKNOWN">C</XMTok>
                <XMDual>
                  <XMApp>
                    <XMTok meaning="set"/>
                    <XMRef idref="S5.SS1.p2.m3.1"/>
                    <XMRef idref="S5.SS1.p2.m3.2"/>
                    <XMRef idref="S5.SS1.p2.m3.3"/>
                  </XMApp>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">{</XMTok>
                    <XMApp xml:id="S5.SS1.p2.m3.1">
                      <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                      <XMTok meaning="2" role="NUMBER">2</XMTok>
                      <XMApp>
                        <XMTok fontsize="70%" meaning="minus" role="ADDOP">-</XMTok>
                        <XMTok fontsize="70%" meaning="6" role="NUMBER">6</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMTok role="PUNCT">,</XMTok>
                    <XMApp xml:id="S5.SS1.p2.m3.2">
                      <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                      <XMTok meaning="2" role="NUMBER">2</XMTok>
                      <XMApp>
                        <XMTok fontsize="70%" meaning="minus" role="ADDOP">-</XMTok>
                        <XMTok fontsize="70%" meaning="7" role="NUMBER">7</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMTok role="PUNCT">,</XMTok>
                    <XMApp xml:id="S5.SS1.p2.m3.3">
                      <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                      <XMTok meaning="2" role="NUMBER">2</XMTok>
                      <XMApp>
                        <XMTok fontsize="70%" meaning="minus" role="ADDOP">-</XMTok>
                        <XMTok fontsize="70%" meaning="8" role="NUMBER">8</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">}</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math>, both subtasks of Clintox select the same set of features.</p>
      </para>
      <para xml:id="S5.SS1.p3">
        <p>This might indicate that there are features learned in a completely unsupervised fashion that are highly correlated with the labels of these datasets, at least on the corresponding validation sets of the datasets.</p>
      </para>
    </subsection>
    <subsection inlist="toc" labels="LABEL:section:dataset_rep_space" xml:id="S5.SS2">
      <tags>
        <tag>5.2</tag>
        <tag role="autoref">subsection 5.2</tag>
        <tag role="refnum">5.2</tag>
        <tag role="typerefnum">§5.2</tag>
      </tags>
      <title><tag close=" ">5.2</tag>Datasets in the Representation Space</title>
      <para xml:id="S5.SS2.p1">
        <p>To explore the relative positions of various datasets in the <text font="typewriter">BARTSmiles</text> space, we compute the <Math mode="inline" tex="1024" text="1024" xml:id="S5.SS2.p1.m1">
            <XMath>
              <XMTok meaning="1024" role="NUMBER">1024</XMTok>
            </XMath>
          </Math>-dimensional representations of all molecules by taking the average of token representations, and compute Frechet distance <cite class="ltx_citemacro_citep">(<bibref bibrefs="FrechetChemnet" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> between the datasets. As the ZINC dataset is vast, we uniformly sample <Math mode="inline" tex="0.05\%" text="0.05percent" xml:id="S5.SS2.p1.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="percent" role="POSTFIX">%</XMTok>
                <XMTok meaning="0.05" role="NUMBER">0.05</XMTok>
              </XMApp>
            </XMath>
          </Math> of the molecules. As seen in Fig. <ref labelref="LABEL:fig:frechet"/>, several datasets like BBBP and SIDER are further from ZINC, which might explain a relatively poor performance of <text font="typewriter">BARTSmiles</text> compared to the models like iMolCLR, which do not use ZINC for pretraining.</p>
      </para>
      <figure inlist="lof" labels="LABEL:fig:frechet LABEL:fig:num_features_vs_auc_roc" placement="h" xml:id="S5.F4">
        <tags>
          <tag><text fontsize="90%">Figure 4</text></tag>
          <tag role="autoref">Figure 4</tag>
          <tag role="refnum">4</tag>
          <tag role="typerefnum">Figure 4</tag>
        </tags>
        <figure align="center" class="ltx_figure_panel ltx_minipage" vattach="top" width="195.1pt" xml:id="S5.F4.fig1">
          <graphics candidates="figures/sentiment.pdf" graphic="figures/sentiment.pdf" options="width=390.25534pt,keepaspectratio=true" xml:id="S5.F4.g1"/>
          <toccaption class="ltx_centering"><tag close=" ">3</tag><text fontsize="90%">The AUC-ROC score plotted against the number of self-supervised features selected from <text font="typewriter">BARTSmiles</text>. Dashed lines indicate the performance achieved on the same dataset with full fine-tuning of the model.</text></toccaption>
          <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 3</text></tag><text fontsize="90%">The AUC-ROC score plotted against the number of self-supervised features selected from <text font="typewriter">BARTSmiles</text>. Dashed lines indicate the performance achieved on the same dataset with full fine-tuning of the model.</text></caption>
        </figure>
        <figure align="center" class="ltx_figure_panel ltx_minipage" vattach="top" width="195.1pt" xml:id="S5.F4.fig2">
          <figure inlist="lof" xml:id="S5.F4.sf1">
            <tags>
              <tag><text fontsize="90%">(a)</text></tag>
              <tag role="autoref">4(a)</tag>
              <tag role="refnum">4(a)</tag>
            </tags>
            <graphics candidates="figures/Frechet.pdf" graphic="figures/Frechet.pdf" options="width=390.25534pt,keepaspectratio=true" xml:id="S5.F4.sf1.g1"/>
            <toccaption><tag close=" ">(a)</tag></toccaption>
            <caption><tag close=" "><text fontsize="90%">(a)</text></tag></caption>
          </figure>
          <toccaption class="ltx_centering"><tag close=" ">4</tag><text fontsize="90%">Frechet distances between datasets in the representation space of <text font="typewriter">BARTSmiles</text>.</text></toccaption>
          <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 4</text></tag><text fontsize="90%">Frechet distances between datasets in the representation space of <text font="typewriter">BARTSmiles</text>.</text></caption>
        </figure>
      </figure>
    </subsection>
    <subsection inlist="toc" labels="LABEL:sec:captum" xml:id="S5.SS3">
      <tags>
        <tag>5.3</tag>
        <tag role="autoref">subsection 5.3</tag>
        <tag role="refnum">5.3</tag>
        <tag role="typerefnum">§5.3</tag>
      </tags>
      <title><tag close=" ">5.3</tag>Interpreting Fine-Tuned Models</title>
      <figure inlist="lof" labels="LABEL:fig:captum-ames-main" placement="h" xml:id="S5.F5">
        <tags>
          <tag><text fontsize="90%">Figure 5</text></tag>
          <tag role="autoref">Figure 5</tag>
          <tag role="refnum">5</tag>
          <tag role="typerefnum">Figure 5</tag>
        </tags>
<!--  %**** arxiv˙version.tex Line 375 **** -->        <figure align="center" class="ltx_figure_panel" placement="b" xml:id="S5.F5.fig1">
          <graphics candidates="figures/ames/ames21.pdf" graphic="figures/ames/ames21.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S5.F5.g1"/>
        </figure>
        <figure align="center" class="ltx_figure_panel" placement="b" xml:id="S5.F5.fig2">
          <graphics candidates="figures/ames/ames38.pdf" graphic="figures/ames/ames38.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S5.F5.g2"/>
        </figure>
        <break class="ltx_break"/>
        <figure align="center" class="ltx_figure_panel" placement="b" xml:id="S5.F5.fig3">
          <graphics candidates="figures/ames/ames28.pdf" graphic="figures/ames/ames28.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S5.F5.g3"/>
        </figure>
        <figure align="center" class="ltx_figure_panel" placement="b" xml:id="S5.F5.fig4">
          <graphics candidates="figures/ames/ames37.pdf" graphic="figures/ames/ames37.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S5.F5.g4"/>
        </figure>
        <toccaption class="ltx_centering"><tag close=" ">5</tag>The contributions of each token and each atom to the prediction of our model fine-tuned on the Ames dataset, according to the Integrated Gradient algorithm.</toccaption>
        <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 5</text></tag><text fontsize="90%">The contributions of each token and each atom to the prediction of our model fine-tuned on the Ames dataset, according to the Integrated Gradient algorithm.</text></caption>
      </figure>
      <paragraph inlist="toc" xml:id="S5.SS3.SSS0.Px1">
        <title>Methodology:</title>
        <para xml:id="S5.SS3.SSS0.Px1.p1">
          <p>In this section, we explore the contribution of each token of the given SMILES to the predictions of the fine-tuned model. We use the Integrated Gradients method <cite class="ltx_citemacro_citep">(<bibref bibrefs="integrated-gradients" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite> implemented in Captum library <cite class="ltx_citemacro_citep">(<bibref bibrefs="captum" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite> to get the contributions of the tokens. Note that due to the tokenization we used in pretraining, there is no one-to-one mapping between tokens and the atoms. Nevertheless, we attempt to visualize the contributions of each atom on the molecular graph. If a token contains multiple symbols, we assume the contributions are additive and split the contribution of the token equally for all symbols in the token. Thus, we get the contribution of each atom and highlight the atom in the graph accordingly. Note that the SMILES string contains many non-atom symbols like brackets. We do not visualize the contributions of the brackets. We also visualize the contribution of a double-bond symbol <Math mode="inline" tex="=" text="=" xml:id="S5.SS3.SSS0.Px1.p1.m1">
              <XMath>
                <XMTok meaning="equals" role="RELOP">=</XMTok>
              </XMath>
            </Math> by highlighting the bond in the graph.</p>
        </para>
      </paragraph>
      <paragraph inlist="toc" xml:id="S5.SS3.SSS0.Px2">
        <title>Ames Dataset:</title>
        <para xml:id="S5.SS3.SSS0.Px2.p1">
          <p>For the Ames dataset, we fine-tuned another model on a random split, and explored the contributions of the atoms of the molecules from the validation set. We compared the highlighted atoms in the graph with the known molecular substructures related to the mutagenic properties of molecules, so-called structural alerts <cite class="ltx_citemacro_citep">(<bibref bibrefs="tox-structural-alerts" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>. Figure <ref labelref="LABEL:fig:captum-ames-main"/> shows a couple of examples. On each of the plots, the left image highlights the structural pattern while the right one shows the contributions of each atom according to the Integrated Gradients method. Many more examples and a more thorough discussion can be found in Appendix <ref labelref="LABEL:sec:app-captum-ames"/>.</p>
        </para>
        <para xml:id="S5.SS3.SSS0.Px2.p2">
          <p>The compound #21 in Fig. <ref labelref="LABEL:fig:captum-ames-main"/><!--  %**** arxiv˙version.tex Line 400 **** --> is a nitroaromatic compound representing organic molecules that consist of at least one nitro group (-NO2) attached to an aromatic ring. The structural alert in this molecule is the nitro group (highlighted on the left part of the figure), whose components have been highlighted as a contribution in most cases of this class of compounds. The compound #38 on the same Figure is an epoxy group-bearing molecule. The epoxy group consists of an oxygen atom joined by single bonds to two adjacent carbon atoms, which form the three-membered epoxide ring  <cite class="ltx_citemacro_citep">(<bibref bibrefs="tox-structural-alerts" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite> and is a well-known structural alert. It is worth noting that in the case of nitroaromatic compounds, the components of the structural alerts have been detected, along with the high probability (<Math mode="inline" tex="&gt;0.8" text="absent &gt; 0.8" xml:id="S5.SS3.SSS0.Px2.p2.m1">
              <XMath>
                <XMApp>
                  <XMTok meaning="greater-than" role="RELOP">&gt;</XMTok>
                  <XMTok meaning="absent"/>
                  <XMTok meaning="0.8" role="NUMBER">0.8</XMTok>
                </XMApp>
              </XMath>
            </Math>) of correct prediction.</p>
        </para>
        <para xml:id="S5.SS3.SSS0.Px2.p3">
          <p>To summarize, the Integrated Gradients method, in most cases, recognizes contributions of non-complex substructures. In the case of more complex substructures, it provides explanations currently not understood through the lens of structural alerts. Additionally, the recognition of known substructures having the contribution to specific labels does not interfere with correct opposite label prediction, evidencing that the model’s learning ability is much deeper than estimating simple correlations between molecular substructures and their effects.</p>
        </para>
      </paragraph>
      <paragraph inlist="toc" xml:id="S5.SS3.SSS0.Px3">
        <title>ESOL dataset:</title>
        <para xml:id="S5.SS3.SSS0.Px3.p1">
          <p>Next, we analyze the attribution scores Integrated Gradients gave for one of the regression tasks, ESOL, which aims to measure the solubility of the molecules in water.
We first noted a strange pattern: the first token of each molecule tended to receive a positive score, regardless of the molecule’s label or structure. We confirmed this by measuring the mean attribution score across the tokens with respect to their positions (Fig. <ref labelref="LABEL:fig:captum-esol-main"/>, top-left). We decided to “normalize” the attribution scores by subtracting those mean attributions from the scores given by the Integrated Gradients method.</p>
        </para>
        <para xml:id="S5.SS3.SSS0.Px3.p2">
          <p>Since water is a polar solvent, polar groups, such as hydroxyl group (-OH) and carbonyl group (C=O), will contribute to the solubility of the molecule <cite class="ltx_citemacro_citep">(<bibref bibrefs="book-review-organic-functional-groups" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>. The Integrated Gradients method highlighted almost all instances of hydroxyl groups, as seen in the bottom row of Fig. <ref labelref="LABEL:fig:captum-esol-main"/>, and most instances of carbonyl groups, especially if the molecule is more soluble (top-right image of Fig. <ref labelref="LABEL:fig:captum-esol-main"/>). It is known that long chains of hydrocarbon compounds are known to decrease the solubility of the molecules. This is captured by the negative attributions of ”C” tokens in such molecules, as seen in the bottom row of Fig. <ref labelref="LABEL:fig:captum-esol-main"/>.
More examples and analysis is presented in Appendix <ref labelref="LABEL:sec:app-captum-esol"/>.</p>
        </para>
        <figure inlist="lof" labels="LABEL:fig:captum-esol-main LABEL:fig:esol-stats LABEL:fig:esol12 LABEL:fig:esol19 LABEL:fig:esol51" placement="h" xml:id="S5.F6">
          <tags>
            <tag><text fontsize="90%">Figure 6</text></tag>
            <tag role="autoref">Figure 6</tag>
            <tag role="refnum">6</tag>
            <tag role="typerefnum">Figure 6</tag>
          </tags>
          <figure align="center" class="ltx_figure_panel" placement="b" xml:id="S5.F6.fig1">
            <graphics candidates="figures/esol-stats.pdf" graphic="figures/esol-stats.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S5.F6.g1"/>
          </figure>
          <figure align="center" class="ltx_figure_panel" placement="b" xml:id="S5.F6.fig2">
            <graphics candidates="figures/esol/esol12.pdf" graphic="figures/esol/esol12.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S5.F6.g2"/>
          </figure>
          <break class="ltx_break"/>
          <figure align="center" class="ltx_figure_panel" placement="b" xml:id="S5.F6.fig3">
<!--  %**** arxiv˙version.tex Line 425 **** -->            <graphics candidates="figures/esol/esol19.pdf" class="ltx_centering" graphic="figures/esol/esol19.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S5.F6.g3"/>
          </figure>
          <figure align="center" class="ltx_figure_panel" placement="b" xml:id="S5.F6.fig4">
            <graphics candidates="figures/esol/esol51.pdf" graphic="figures/esol/esol51.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S5.F6.g4"/>
          </figure>
          <toccaption class="ltx_centering"><tag close=" ">6</tag>Top-left: the average attribution score of a token depending on its position in the tokenized SMILES. There is a significant positive bias on the first token. Others: the contributions of each token and each atom to the prediction of our model fine-tuned on the ESOL dataset, according to the Integrated Gradient algorithm.</toccaption>
          <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 6</text></tag><text fontsize="90%">Top-left: the average attribution score of a token depending on its position in the tokenized SMILES. There is a significant positive bias on the first token. Others: the contributions of each token and each atom to the prediction of our model fine-tuned on the ESOL dataset, according to the Integrated Gradient algorithm.</text></caption>
        </figure>
        <para xml:id="S5.SS3.SSS0.Px3.p3">
          <p>We hope the interpretability methods developed for deep learning, our visualization toolkit, and the strong predictive models made possible by the large pre-trained models will help chemists uncover new correlations between substructures and target variables or even causal links.</p>
        </para>
      </paragraph>
    </subsection>
  </section>
  <section inlist="toc" xml:id="S6">
    <tags>
      <tag>6</tag>
      <tag role="autoref">section 6</tag>
      <tag role="refnum">6</tag>
      <tag role="typerefnum">§6</tag>
    </tags>
    <title><tag close=" ">6</tag>Conclusion</title>
    <para xml:id="S6.p1">
      <p>In this paper, we introduce <text font="typewriter">BARTSmiles</text>, a large pre-trained model for molecular representations which sets state-of-the-art results across many classification, regression, and generation tasks. We showed the power of self-supervised training through a series of quantitative and qualitative analyses of representations showing that interpretability methods applied to these representations mirror that of chemists. There is still room for investigation of the impact of data on pre-training, i.e., ZINC contains more than a billion molecules, but its distribution might be irrelevant for many downstream tasks. Further collaboration between machine learning and chemistry experts might allow for the discovery of new molecular substructures linked to chemical phenomena.</p>
    </para>
  </section>
  <section inlist="toc" xml:id="S7">
    <tags>
      <tag>7</tag>
      <tag role="autoref">section 7</tag>
      <tag role="refnum">7</tag>
      <tag role="typerefnum">§7</tag>
    </tags>
    <title><tag close=" ">7</tag>Acknowledgements</title>
    <para xml:id="S7.p1">
      <p>We would like to thank Luke Zettlemoyer, Lucas Allan, Garik Petrosyan and Garegin Papoian for useful feedback.
This work was partially supported by the RA MES State Committee of Science, in the framework of the research project No. 20TTCG-1F004.</p>
    </para>
  </section>
  <bibliography xml:id="bib">
    <title>References</title>
    <biblist>
      <bibitem key="RXF" xml:id="bib.bib1">
        <tags>
          <tag role="number">1</tag>
          <tag role="year">2020a</tag>
          <tag role="authors">Aghajanyan et al.</tag>
          <tag role="fullauthors">Aghajanyan, Shrivastava, Gupta,
Goyal, Zettlemoyer, and Gupta</tag>
          <tag role="refnum">Aghajanyan et al. (2020a)</tag>
          <tag role="key">RXF</tag>
        </tags>
        <bibblock>
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke
Zettlemoyer, and Sonal Gupta.
</bibblock>
        <bibblock>Better fine-tuning by reducing representational collapse.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2008.03156</emph>, 2020a.
</bibblock>
      </bibitem>
      <bibitem key="intrinsic_dimensionality_finetuning" xml:id="bib.bib2">
        <tags>
          <tag role="number">2</tag>
          <tag role="year">2020b</tag>
          <tag role="authors">Aghajanyan et al.</tag>
          <tag role="fullauthors">Aghajanyan, Zettlemoyer, and
Gupta</tag>
          <tag role="refnum">Aghajanyan et al. (2020b)</tag>
          <tag role="key">intrinsic_dimensionality_finetuning</tag>
        </tags>
        <bibblock>
Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta.
</bibblock>
        <bibblock>Intrinsic dimensionality explains the effectiveness of language model
fine-tuning.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2012.13255</emph>, 2020b.
</bibblock>
      </bibitem>
      <bibitem key="HTLM1" xml:id="bib.bib3">
        <tags>
          <tag role="number">3</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Aghajanyan et al.</tag>
          <tag role="fullauthors">Aghajanyan, Okhonko, Lewis, Joshi, Xu, Ghosh,
and Zettlemoyer</tag>
          <tag role="refnum">Aghajanyan et al. (2021)</tag>
          <tag role="key">HTLM1</tag>
        </tags>
        <bibblock><!--  %**** arxiv˙version.bbl Line 25 **** -->
Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh,
and Luke Zettlemoyer.
</bibblock>
        <bibblock>Htlm: Hyper-text pre-training and prompting of language models.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2107.06955</emph>, 2021.
</bibblock>
      </bibitem>
      <bibitem key="CM3" xml:id="bib.bib4">
        <tags>
          <tag role="number">4</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Aghajanyan et al.</tag>
          <tag role="fullauthors">Aghajanyan, Huang, Ross, Karpukhin, Xu, Goyal,
Okhonko, Joshi, Ghosh, Lewis, et al.</tag>
          <tag role="refnum">Aghajanyan et al. (2022)</tag>
          <tag role="key">CM3</tag>
        </tags>
        <bibblock>
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman
Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al.
</bibblock>
        <bibblock>Cm3: A causal masked multimodal model of the internet.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2201.07520</emph>, 2022.
</bibblock>
      </bibitem>
      <bibitem key="AIM" xml:id="bib.bib5">
        <tags>
          <tag role="number">5</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Arakelyan et al.</tag>
          <tag role="fullauthors">Arakelyan, Soghomonyan, and The Aim team</tag>
          <tag role="refnum">Arakelyan et al. (2020)</tag>
          <tag role="key">AIM</tag>
        </tags>
        <bibblock>
Gor Arakelyan, Gevorg Soghomonyan, and The Aim team.
</bibblock>
        <bibblock>Aim.
</bibblock>
        <bibblock>6 2020.
</bibblock>
        <bibblock>doi: <ref class="ltx_nolink ltx_Url" href="10.5281/zenodo.6536395">10.5281/zenodo.6536395</ref>.
</bibblock>
        <bibblock>URL <ref class="ltx_url" font="typewriter" href="https://github.com/aimhubio/aim">https://github.com/aimhubio/aim</ref>.
</bibblock>
      </bibitem>
      <bibitem key="fairscale" xml:id="bib.bib6">
        <tags>
          <tag role="number">6</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Baines et al.</tag>
          <tag role="fullauthors">Baines, Bhosale, Caggiano, Goyal, Goyal, Ott,
Lefaudeux, Liptchinsky, Rabbat, Sheiffer, Sridhar, and Xu</tag>
          <tag role="refnum">Baines et al. (2021)</tag>
          <tag role="key">fairscale</tag>
        </tags>
        <bibblock>
Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth
Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam
Sheiffer, Anjali Sridhar, and Min Xu.
</bibblock>
        <bibblock>Fairscale: A general purpose modular pytorch library for high
performance and large scale training.
</bibblock>
        <bibblock><!--  %**** arxiv˙version.bbl Line 50 **** --><ref class="ltx_url" font="typewriter" href="https://github.com/facebookresearch/fairscale">https://github.com/facebookresearch/fairscale</ref>, 2021.
</bibblock>
      </bibitem>
      <bibitem key="BEIT" xml:id="bib.bib7">
        <tags>
          <tag role="number">7</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Bao et al.</tag>
          <tag role="fullauthors">Bao, Dong, and Wei</tag>
          <tag role="refnum">Bao et al. (2021)</tag>
          <tag role="key">BEIT</tag>
        </tags>
        <bibblock>
Hangbo Bao, Li Dong, and Furu Wei.
</bibblock>
        <bibblock>Beit: Bert pre-training of image transformers.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2106.08254</emph>, 2021.
</bibblock>
      </bibitem>
      <bibitem key="scaffold1996" xml:id="bib.bib8">
        <tags>
          <tag role="number">8</tag>
          <tag role="year">1996</tag>
          <tag role="authors">Bemis &amp; Murcko</tag>
          <tag role="fullauthors">Bemis and Murcko</tag>
          <tag role="refnum">Bemis &amp; Murcko (1996)</tag>
          <tag role="key">scaffold1996</tag>
        </tags>
        <bibblock>
Guy W Bemis and Mark A Murcko.
</bibblock>
        <bibblock>The properties of known drugs. 1. molecular frameworks.
</bibblock>
        <bibblock><emph font="italic">Journal of medicinal chemistry</emph>, 39(15):2887–2893, 1996.
</bibblock>
      </bibitem>
      <bibitem key="ml-solubility-noise" xml:id="bib.bib9">
        <tags>
          <tag role="number">9</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Boobier et al.</tag>
          <tag role="fullauthors">Boobier, Hose, Blacker, and
Nguyen</tag>
          <tag role="refnum">Boobier et al. (2020)</tag>
          <tag role="key">ml-solubility-noise</tag>
        </tags>
        <bibblock>
Samuel Boobier, David RJ Hose, A John Blacker, and Bao N Nguyen.
</bibblock>
        <bibblock>Machine learning with physicochemical relationships: solubility
prediction in organic solvents and water.
</bibblock>
        <bibblock><emph font="italic">Nature communications</emph>, 11(1):1–10, 2020.
</bibblock>
      </bibitem>
      <bibitem key="bradshaw2018generative" xml:id="bib.bib10">
        <tags>
          <tag role="number">10</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Bradshaw et al.</tag>
          <tag role="fullauthors">Bradshaw, Kusner, Paige, Segler, and
Hernández-Lobato</tag>
          <tag role="refnum">Bradshaw et al. (2018)</tag>
          <tag role="key">bradshaw2018generative</tag>
        </tags>
        <bibblock>
John Bradshaw, Matt J Kusner, Brooks Paige, Marwin HS Segler, and
José Miguel Hernández-Lobato.
</bibblock>
        <bibblock>A generative model for electron paths.
</bibblock>
        <bibblock><!--  %**** arxiv˙version.bbl Line 75 **** --><emph font="italic">arXiv preprint arXiv:1805.10970</emph>, 2018.
</bibblock>
      </bibitem>
      <bibitem key="chemberta" xml:id="bib.bib11">
        <tags>
          <tag role="number">11</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Chithrananda et al.</tag>
          <tag role="fullauthors">Chithrananda, Grand, and
Ramsundar</tag>
          <tag role="refnum">Chithrananda et al. (2020)</tag>
          <tag role="key">chemberta</tag>
        </tags>
        <bibblock>
Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar.
</bibblock>
        <bibblock>Chemberta: Large-scale self-supervised pretraining for molecular
property prediction.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2010.09885</emph>, 2020.
</bibblock>
      </bibitem>
      <bibitem key="coley2019graph" xml:id="bib.bib12">
        <tags>
          <tag role="number">12</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Coley et al.</tag>
          <tag role="fullauthors">Coley, Jin, Rogers, Jamison, Jaakkola, Green,
Barzilay, and Jensen</tag>
          <tag role="refnum">Coley et al. (2019)</tag>
          <tag role="key">coley2019graph</tag>
        </tags>
        <bibblock>
Connor W Coley, Wengong Jin, Luke Rogers, Timothy F Jamison, Tommi S Jaakkola,
William H Green, Regina Barzilay, and Klavs F Jensen.
</bibblock>
        <bibblock>A graph-convolutional neural network model for the prediction of
chemical reactivity.
</bibblock>
        <bibblock><emph font="italic">Chemical science</emph>, 10(2):370–377, 2019.
</bibblock>
      </bibitem>
      <bibitem key="XLSR" xml:id="bib.bib13">
        <tags>
          <tag role="number">13</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Conneau et al.</tag>
          <tag role="fullauthors">Conneau, Baevski, Collobert, Mohamed, and
Auli</tag>
          <tag role="refnum">Conneau et al. (2020)</tag>
          <tag role="key">XLSR</tag>
        </tags>
        <bibblock>
Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and
Michael Auli.
</bibblock>
        <bibblock>Unsupervised cross-lingual representation learning for speech
recognition.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2006.13979</emph><!--  %**** arxiv˙version.bbl Line 100 **** -->, 2020.
</bibblock>
      </bibitem>
      <bibitem key="NN-QSAR-2014" xml:id="bib.bib14">
        <tags>
          <tag role="number">14</tag>
          <tag role="year">2014</tag>
          <tag role="authors">Dahl et al.</tag>
          <tag role="fullauthors">Dahl, Jaitly, and Salakhutdinov</tag>
          <tag role="refnum">Dahl et al. (2014)</tag>
          <tag role="key">NN-QSAR-2014</tag>
        </tags>
        <bibblock>
George E Dahl, Navdeep Jaitly, and Ruslan Salakhutdinov.
</bibblock>
        <bibblock>Multi-task neural networks for qsar predictions.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1406.1231</emph>, 2014.
</bibblock>
      </bibitem>
      <bibitem key="davey2018retrosynthesis" xml:id="bib.bib15">
        <tags>
          <tag role="number">15</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Davey</tag>
          <tag role="refnum">Davey (2018)</tag>
          <tag role="key">davey2018retrosynthesis</tag>
        </tags>
        <bibblock>
Stephen G Davey.
</bibblock>
        <bibblock>Retrosynthesis: Computer says yes.
</bibblock>
        <bibblock><emph font="italic">Nature Reviews Chemistry</emph>, 2(1):1–1, 2018.
</bibblock>
      </bibitem>
      <bibitem key="qsar-history" xml:id="bib.bib16">
        <tags>
          <tag role="number">16</tag>
          <tag role="year">2016</tag>
          <tag role="authors">Dearden</tag>
          <tag role="refnum">Dearden (2016)</tag>
          <tag role="key">qsar-history</tag>
        </tags>
        <bibblock>
John Dearden.
</bibblock>
        <bibblock>The history and development of quantitative structure-activity
relationships (qsars).
</bibblock>
        <bibblock><emph font="italic">IJQSPR</emph>, 1(1):1–44, 2016.
</bibblock>
        <bibblock>doi: <ref class="ltx_nolink ltx_Url" href="10.4018/978-1-5225-0549-5.ch003">10.4018/978-1-5225-0549-5.ch003</ref>.
</bibblock>
      </bibitem>
      <bibitem key="bert" xml:id="bib.bib17">
        <tags>
          <tag role="number">17</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Devlin et al.</tag>
          <tag role="fullauthors">Devlin, Chang, Lee, and Toutanova</tag>
          <tag role="refnum">Devlin et al. (2018)</tag>
          <tag role="key">bert</tag>
        </tags>
        <bibblock>
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</bibblock>
        <bibblock>Bert: Pre-training of deep bidirectional transformers for language
understanding.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1810.04805</emph>, 2018.
</bibblock>
      </bibitem>
      <bibitem key="do2019graph" xml:id="bib.bib18">
        <tags>
          <tag role="number">18</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Do et al.</tag>
          <tag role="fullauthors">Do, Tran, and Venkatesh</tag>
          <tag role="refnum">Do et al. (2019)</tag>
          <tag role="key">do2019graph</tag>
        </tags>
        <bibblock>
Kien Do, Truyen Tran, and Svetha Venkatesh.
</bibblock>
        <bibblock><!--  %**** arxiv˙version.bbl Line 125 **** -->Graph transformation policy network for chemical reaction prediction.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery &amp; Data Mining</emph>, pp.  750–760, 2019.
</bibblock>
      </bibitem>
      <bibitem key="CNN-on-molecules" xml:id="bib.bib19">
        <tags>
          <tag role="number">19</tag>
          <tag role="year">2015</tag>
          <tag role="authors">Duvenaud et al.</tag>
          <tag role="fullauthors">Duvenaud, Maclaurin, Iparraguirre, Bombarell,
Hirzel, Aspuru-Guzik, and Adams</tag>
          <tag role="refnum">Duvenaud et al. (2015)</tag>
          <tag role="key">CNN-on-molecules</tag>
        </tags>
        <bibblock>
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell,
Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams.
</bibblock>
        <bibblock>Convolutional networks on graphs for learning molecular fingerprints.
</bibblock>
        <bibblock><emph font="italic">Advances in neural information processing systems</emph>, 28, 2015.
</bibblock>
      </bibitem>
      <bibitem key="Micronucleus" xml:id="bib.bib20">
        <tags>
          <tag role="number">20</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Fan et al.</tag>
          <tag role="fullauthors">Fan, Yang, Li, Sun, Di, Li, Tang, and
Liu</tag>
          <tag role="refnum">Fan et al. (2018)</tag>
          <tag role="key">Micronucleus</tag>
        </tags>
        <bibblock>
Defang Fan, Hongbin Yang, Fuxing Li, Lixia Sun, Peiwen Di, Weihua Li, Yun Tang,
and Guixia Liu.
</bibblock>
        <bibblock>In silico prediction of chemical genotoxicity using machine learning
methods and structural alerts.
</bibblock>
        <bibblock><emph font="italic">Toxicology research</emph>, 7(2):211–220, 2018.
</bibblock>
      </bibitem>
      <bibitem key="circular-fingerprints" xml:id="bib.bib21">
        <tags>
          <tag role="number">21</tag>
          <tag role="year">2006</tag>
          <tag role="authors">Glen et al.</tag>
          <tag role="fullauthors">Glen, Bender, Arnby, Carlsson, Boyer, and
Smith</tag>
          <tag role="refnum">Glen et al. (2006)</tag>
          <tag role="key">circular-fingerprints</tag>
        </tags>
        <bibblock>
Robert C Glen, Andreas Bender, Catrin H Arnby, Lars Carlsson, Scott Boyer, and
James Smith.
</bibblock>
        <bibblock>Circular fingerprints: flexible molecular descriptors with
applications from physical chemistry to adme.
</bibblock>
        <bibblock><!--  %**** arxiv˙version.bbl Line 150 **** --><emph font="italic">IDrugs</emph>, 9(3):199, 2006.
</bibblock>
      </bibitem>
      <bibitem key="Ames" xml:id="bib.bib22">
        <tags>
          <tag role="number">22</tag>
          <tag role="year">2009</tag>
          <tag role="authors">Hansen et al.</tag>
          <tag role="fullauthors">Hansen, Mika, Schroeter, Sutter, ter Laak,
Steger-Hartmann, Heinrich, and Müller</tag>
          <tag role="refnum">Hansen et al. (2009)</tag>
          <tag role="key">Ames</tag>
        </tags>
        <bibblock>
Katja Hansen, Sebastian Mika, Timon Schroeter, Andreas Sutter, Antonius ter
Laak, Thomas Steger-Hartmann, Nikolaus Heinrich, and Klaus-Robert
Müller.
</bibblock>
        <bibblock>Benchmark data set for in silico prediction of ames mutagenicity.
</bibblock>
        <bibblock><emph font="italic">Journal of Chemical Information and Modeling</emph>, 49(9):2077–2081, 2009.
</bibblock>
        <bibblock>doi: <ref class="ltx_nolink ltx_Url" href="10.1021/ci900161g">10.1021/ci900161g</ref>.
</bibblock>
        <bibblock>URL <ref class="ltx_url" font="typewriter" href="http://dx.doi.org/10.1021/ci900161g">http://dx.doi.org/10.1021/ci900161g</ref>.
</bibblock>
      </bibitem>
      <bibitem key="MAE" xml:id="bib.bib23">
        <tags>
          <tag role="number">23</tag>
          <tag role="year">2022</tag>
          <tag role="authors">He et al.</tag>
          <tag role="fullauthors">He, Chen, Xie, Li, Dollár, and Girshick</tag>
          <tag role="refnum">He et al. (2022)</tag>
          <tag role="key">MAE</tag>
        </tags>
        <bibblock>
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross
Girshick.
</bibblock>
        <bibblock>Masked autoencoders are scalable vision learners.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  16000–16009, 2022.
</bibblock>
      </bibitem>
      <bibitem key="GIN" xml:id="bib.bib24">
        <tags>
          <tag role="number">24</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Hu et al.</tag>
          <tag role="fullauthors">Hu, Liu, Gomes, Zitnik, Liang, Pande, and
Leskovec</tag>
          <tag role="refnum">Hu et al. (2020)</tag>
          <tag role="key">GIN</tag>
        </tags>
        <bibblock>
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,
and Jure Leskovec.
</bibblock>
        <bibblock>Strategies for pre-training graph neural networks.
</bibblock>
        <bibblock><!--  %**** arxiv˙version.bbl Line 175 **** -->In <emph font="italic">International Conference on Learning Representations</emph>, 2020.
</bibblock>
        <bibblock>URL <ref class="ltx_url" font="typewriter" href="https://openreview.net/forum?id=HJlWWJSFDH">https://openreview.net/forum?id=HJlWWJSFDH</ref>.
</bibblock>
      </bibitem>
      <bibitem key="ZINC20" xml:id="bib.bib25">
        <tags>
          <tag role="number">25</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Irwin et al.</tag>
          <tag role="fullauthors">Irwin, Tang, Young, Dandarchuluun, Wong,
Khurelbaatar, Moroz, Mayfield, and Sayle</tag>
          <tag role="refnum">Irwin et al. (2020)</tag>
          <tag role="key">ZINC20</tag>
        </tags>
        <bibblock>
John J Irwin, Khanh G Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R
Wong, Munkhzul Khurelbaatar, Yurii S Moroz, John Mayfield, and Roger A Sayle.
</bibblock>
        <bibblock>Zinc20—a free ultralarge-scale chemical database for ligand
discovery.
</bibblock>
        <bibblock><emph font="italic">Journal of chemical information and modeling</emph>, 60(12):6065–6073, 2020.
</bibblock>
      </bibitem>
      <bibitem key="chemformer" xml:id="bib.bib26">
        <tags>
          <tag role="number">26</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Irwin et al.</tag>
          <tag role="fullauthors">Irwin, Dimitriadis, He, and Bjerrum</tag>
          <tag role="refnum">Irwin et al. (2022)</tag>
          <tag role="key">chemformer</tag>
        </tags>
        <bibblock>
Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum.
</bibblock>
        <bibblock>Chemformer: a pre-trained transformer for computational chemistry.
</bibblock>
        <bibblock><emph font="italic">Machine Learning: Science and Technology</emph>, 3(1):015022, 2022.
</bibblock>
      </bibitem>
      <bibitem key="SWA" xml:id="bib.bib27">
        <tags>
          <tag role="number">27</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Izmailov et al.</tag>
          <tag role="fullauthors">Izmailov, Podoprikhin, Garipov, Vetrov, and
Wilson</tag>
          <tag role="refnum">Izmailov et al. (2018)</tag>
          <tag role="key">SWA</tag>
        </tags>
        <bibblock>
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
Andrew Gordon Wilson.
</bibblock>
        <bibblock>Averaging weights leads to wider optima and better generalization.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1803.05407</emph><!--  %**** arxiv˙version.bbl Line 200 **** -->, 2018.
</bibblock>
      </bibitem>
      <bibitem key="mol2vec" xml:id="bib.bib28">
        <tags>
          <tag role="number">28</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Jaeger et al.</tag>
          <tag role="fullauthors">Jaeger, Fulle, and Turk</tag>
          <tag role="refnum">Jaeger et al. (2018)</tag>
          <tag role="key">mol2vec</tag>
        </tags>
        <bibblock>
Sabrina Jaeger, Simone Fulle, and Samo Turk.
</bibblock>
        <bibblock>Mol2vec: unsupervised machine learning approach with chemical
intuition.
</bibblock>
        <bibblock><emph font="italic">Journal of chemical information and modeling</emph>, 58(1):27–35, 2018.
</bibblock>
      </bibitem>
      <bibitem key="GNN-drug-comparison" xml:id="bib.bib29">
        <tags>
          <tag role="number">29</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Jiang et al.</tag>
          <tag role="fullauthors">Jiang, Wu, Hsieh, Chen, Liao, Wang, Shen, Cao, Wu,
and Hou</tag>
          <tag role="refnum">Jiang et al. (2021)</tag>
          <tag role="key">GNN-drug-comparison</tag>
        </tags>
        <bibblock>
Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang,
Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou.
</bibblock>
        <bibblock>Could graph neural networks learn better molecular representation for
drug discovery? a comparison study of descriptor-based and graph-based
models.
</bibblock>
        <bibblock><emph font="italic">Journal of cheminformatics</emph>, 13(1):1–23,
2021.
</bibblock>
      </bibitem>
      <bibitem key="jin2017predicting" xml:id="bib.bib30">
        <tags>
          <tag role="number">30</tag>
          <tag role="year">2017</tag>
          <tag role="authors">Jin et al.</tag>
          <tag role="fullauthors">Jin, Coley, Barzilay, and Jaakkola</tag>
          <tag role="refnum">Jin et al. (2017)</tag>
          <tag role="key">jin2017predicting</tag>
        </tags>
        <bibblock>
Wengong Jin, Connor Coley, Regina Barzilay, and Tommi Jaakkola.
</bibblock>
        <bibblock>Predicting organic reaction outcomes with weisfeiler-lehman network.
</bibblock>
        <bibblock><emph font="italic">Advances in neural information processing systems</emph>, 30, 2017.
</bibblock>
      </bibitem>
      <bibitem key="GCN-on-molecules" xml:id="bib.bib31">
        <tags>
          <tag role="number">31</tag>
          <tag role="year">2016</tag>
          <tag role="authors">Kearnes et al.</tag>
          <tag role="fullauthors">Kearnes, McCloskey, Berndl, Pande, and
Riley</tag>
          <tag role="refnum">Kearnes et al. (2016)</tag>
          <tag role="key">GCN-on-molecules</tag>
        </tags>
        <bibblock>
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley.
</bibblock>
        <bibblock><!--  %**** arxiv˙version.bbl Line 225 **** -->Molecular graph convolutions: moving beyond fingerprints.
</bibblock>
        <bibblock><emph font="italic">Journal of computer-aided molecular design</emph>, 30(8):595–608, 2016.
</bibblock>
      </bibitem>
      <bibitem key="captum" xml:id="bib.bib32">
        <tags>
          <tag role="number">32</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Kokhlikyan et al.</tag>
          <tag role="fullauthors">Kokhlikyan, Miglani, Martin, Wang, Alsallakh,
Reynolds, Melnikov, Kliushkina, Araya, Yan, and Reblitz-Richardson</tag>
          <tag role="refnum">Kokhlikyan et al. (2020)</tag>
          <tag role="key">captum</tag>
        </tags>
        <bibblock>
Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,
Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi
Yan, and Orion Reblitz-Richardson.
</bibblock>
        <bibblock>Captum: A unified and generic model interpretability library for
pytorch, 2020.
</bibblock>
      </bibitem>
      <bibitem key="scikit" xml:id="bib.bib33">
        <tags>
          <tag role="number">33</tag>
          <tag role="year">2016</tag>
          <tag role="authors">Kramer</tag>
          <tag role="refnum">Kramer (2016)</tag>
          <tag role="key">scikit</tag>
        </tags>
        <bibblock>
Oliver Kramer.
</bibblock>
        <bibblock>Scikit-learn.
</bibblock>
        <bibblock>In <emph font="italic">Machine learning for evolution strategies</emph>, pp.  45–53.
Springer, 2016.
</bibblock>
      </bibitem>
      <bibitem key="sentencepiece" xml:id="bib.bib34">
        <tags>
          <tag role="number">34</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Kudo &amp; Richardson</tag>
          <tag role="fullauthors">Kudo and Richardson</tag>
          <tag role="refnum">Kudo &amp; Richardson (2018)</tag>
          <tag role="key">sentencepiece</tag>
        </tags>
        <bibblock>
Taku Kudo and John Richardson.
</bibblock>
        <bibblock>Sentencepiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1808.06226</emph>, 2018.
</bibblock>
      </bibitem>
      <bibitem key="RDKIT" xml:id="bib.bib35">
        <tags>
          <tag role="number">35</tag>
          <tag role="year">2013</tag>
          <tag role="authors">Landrum et al.</tag>
          <tag role="refnum">Landrum et al. (2013)</tag>
          <tag role="key">RDKIT</tag>
        </tags>
        <bibblock><!--  %**** arxiv˙version.bbl Line 250 **** -->
Greg Landrum et al.
</bibblock>
        <bibblock>Rdkit: A software suite for cheminformatics, computational chemistry,
and predictive modeling, 2013.
</bibblock>
      </bibitem>
      <bibitem key="book-review-organic-functional-groups" xml:id="bib.bib36">
        <tags>
          <tag role="number">36</tag>
          <tag role="year">2003</tag>
          <tag role="authors">Lemke</tag>
          <tag role="refnum">Lemke (2003)</tag>
          <tag role="key">book-review-organic-functional-groups</tag>
        </tags>
        <bibblock>
Thomas L Lemke.
</bibblock>
        <bibblock><emph font="italic">Review of organic functional groups: introduction to medicinal
organic chemistry</emph>.
</bibblock>
        <bibblock>Lippincott Williams &amp; Wilkins, 2003.
</bibblock>
      </bibitem>
      <bibitem key="BART" xml:id="bib.bib37">
        <tags>
          <tag role="number">37</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Lewis et al.</tag>
          <tag role="fullauthors">Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
Stoyanov, and Zettlemoyer</tag>
          <tag role="refnum">Lewis et al. (2019)</tag>
          <tag role="key">BART</tag>
        </tags>
        <bibblock>
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
</bibblock>
        <bibblock>Bart: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1910.13461</emph>, 2019.
</bibblock>
      </bibitem>
      <bibitem key="MARGE" xml:id="bib.bib38">
        <tags>
          <tag role="number">38</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Lewis et al.</tag>
          <tag role="fullauthors">Lewis, Ghazvininejad, Ghosh, Aghajanyan, Wang, and
Zettlemoyer</tag>
          <tag role="refnum">Lewis et al. (2020)</tag>
          <tag role="key">MARGE</tag>
        </tags>
        <bibblock>
Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and
Luke Zettlemoyer.
</bibblock>
        <bibblock>Pre-training via paraphrasing, 2020.
</bibblock>
      </bibitem>
      <bibitem key="ROBERTA" xml:id="bib.bib39">
        <tags>
          <tag role="number">39</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Liu et al.</tag>
          <tag role="fullauthors">Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
Zettlemoyer, and Stoyanov</tag>
          <tag role="refnum">Liu et al. (2019)</tag>
          <tag role="key"><!--  %**** arxiv_version.bbl Line 275 **** -->ROBERTA</tag>
        </tags>
        <bibblock>
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
</bibblock>
        <bibblock>Roberta: A robustly optimized bert pretraining approach.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1907.11692</emph>, 2019.
</bibblock>
      </bibitem>
      <bibitem key="lowe-USPTO-2012" xml:id="bib.bib40">
        <tags>
          <tag role="number">40</tag>
          <tag role="year">2012</tag>
          <tag role="authors">Lowe</tag>
          <tag role="refnum">Lowe (2012)</tag>
          <tag role="key">lowe-USPTO-2012</tag>
        </tags>
        <bibblock>
Daniel Mark Lowe.
</bibblock>
        <bibblock><emph font="italic">Extraction of chemical structures and reactions from the
literature</emph>.
</bibblock>
        <bibblock>PhD thesis, University of Cambridge, 2012.
</bibblock>
      </bibitem>
      <bibitem key="bbbp" xml:id="bib.bib41">
        <tags>
          <tag role="number">41</tag>
          <tag role="year">2012</tag>
          <tag role="authors">Martins et al.</tag>
          <tag role="fullauthors">Martins, Teixeira, Pinheiro, and Falcao</tag>
          <tag role="refnum">Martins et al. (2012)</tag>
          <tag role="key">bbbp</tag>
        </tags>
        <bibblock>
Ines Filipa Martins, Ana L Teixeira, Luis Pinheiro, and Andre O Falcao.
</bibblock>
        <bibblock>A bayesian approach to in silico blood-brain barrier penetration
modeling.
</bibblock>
        <bibblock><emph font="italic">Journal of chemical information and modeling</emph>, 52(6):1686–1697, 2012.
</bibblock>
      </bibitem>
      <bibitem key="morgan-fingerprints" xml:id="bib.bib42">
        <tags>
          <tag role="number">42</tag>
          <tag role="year">1965</tag>
          <tag role="authors">Morgan</tag>
          <tag role="refnum">Morgan (1965)</tag>
          <tag role="key">morgan-fingerprints</tag>
        </tags>
        <bibblock>
Harry L Morgan.
</bibblock>
        <bibblock>The generation of a unique machine description for chemical
structures-a technique developed at chemical abstracts service.
</bibblock>
        <bibblock><emph font="italic">Journal of chemical documentation</emph>, 5(2):107–113, 1965.
</bibblock>
      </bibitem>
      <bibitem key="fairseq" xml:id="bib.bib43">
        <tags>
          <tag role="number">43</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Ott et al.</tag>
          <tag role="fullauthors">Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
Auli</tag>
          <tag role="refnum">Ott et al. (2019)</tag>
          <tag role="key">fairseq</tag>
        </tags>
        <bibblock>
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
David Grangier, and Michael Auli.
</bibblock>
        <bibblock>fairseq: A fast, extensible toolkit for sequence modeling.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1904.01038</emph>, 2019.
</bibblock>
      </bibitem>
      <bibitem key="pytorch" xml:id="bib.bib44">
        <tags>
          <tag role="number">44</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Paszke et al.</tag>
          <tag role="fullauthors">Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
Killeen, Lin, Gimelshein, Antiga, et al.</tag>
          <tag role="refnum">Paszke et al. (2019)</tag>
          <tag role="key">pytorch</tag>
        </tags>
        <bibblock>
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
</bibblock>
        <bibblock>Pytorch: An imperative style, high-performance deep learning library.
</bibblock>
        <bibblock><emph font="italic">Advances in neural information processing systems</emph>,
32:8026–8037, 2019.
</bibblock>
      </bibitem>
      <bibitem key="tox-structural-alerts" xml:id="bib.bib45">
        <tags>
          <tag role="number">45</tag>
          <tag role="year">2016</tag>
          <tag role="authors">Plošnik et al.</tag>
          <tag role="fullauthors">Plošnik, Vračko, and
Sollner Dolenc</tag>
          <tag role="refnum">Plošnik et al. (2016)</tag>
          <tag role="key">tox-structural-alerts</tag>
        </tags>
        <bibblock>
Alja Plošnik, Marjan Vračko, and Marija Sollner Dolenc.
</bibblock>
        <bibblock>Mutagenic and carcinogenic structural alerts and their mechanisms of
action.
</bibblock>
        <bibblock><emph font="italic">Arhiv za higijenu rada i toksikologiju</emph>, 67(3):169–182, 2016.
</bibblock>
      </bibitem>
      <bibitem key="FrechetChemnet" xml:id="bib.bib46">
        <tags>
          <tag role="number">46</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Preuer et al.</tag>
          <tag role="fullauthors">Preuer, Renz, Unterthiner, Hochreiter, and
Klambauer</tag>
          <tag role="refnum">Preuer et al. (2018)</tag>
          <tag role="key"><!--  %**** arxiv_version.bbl Line 325 **** -->FrechetChemnet</tag>
        </tags>
        <bibblock>
Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter
Klambauer.
</bibblock>
        <bibblock>Fréchet chemnet distance: a metric for generative models for
molecules in drug discovery.
</bibblock>
        <bibblock><emph font="italic">Journal of chemical information and modeling</emph>, 58(9):1736–1741, 2018.
</bibblock>
      </bibitem>
      <bibitem key="WHISPER" xml:id="bib.bib47">
        <tags>
          <tag role="number">47</tag>
          <tag role="authors">Radford et al.</tag>
          <tag role="fullauthors">Radford, Kim, Xu, Brockman, McLeavey, and
Sutskever</tag>
          <tag role="refnum">(47)</tag>
          <tag role="key">WHISPER</tag>
        </tags>
        <bibblock>
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever.
</bibblock>
        <bibblock>Robust speech recognition via large-scale weak supervision.
</bibblock>
      </bibitem>
      <bibitem key="radford2017learning" xml:id="bib.bib48">
        <tags>
          <tag role="number">48</tag>
          <tag role="year">2017</tag>
          <tag role="authors">Radford et al.</tag>
          <tag role="fullauthors">Radford, Jozefowicz, and
Sutskever</tag>
          <tag role="refnum">Radford et al. (2017)</tag>
          <tag role="key">radford2017learning</tag>
        </tags>
        <bibblock>
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
</bibblock>
        <bibblock>Learning to generate reviews and discovering sentiment.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1704.01444</emph>, 2017.
</bibblock>
      </bibitem>
      <bibitem key="ECFP-fingerprints" xml:id="bib.bib49">
        <tags>
          <tag role="number">49</tag>
          <tag role="year">2010</tag>
          <tag role="authors">Rogers &amp; Hahn</tag>
          <tag role="fullauthors">Rogers and Hahn</tag>
          <tag role="refnum">Rogers &amp; Hahn (2010)</tag>
          <tag role="key">ECFP-fingerprints</tag>
        </tags>
        <bibblock>
David Rogers and Mathew Hahn.
</bibblock>
        <bibblock>Extended-connectivity fingerprints.
</bibblock>
        <bibblock><emph font="italic">Journal of chemical information and modeling</emph>, 50(5):742–754, 2010.
</bibblock>
      </bibitem>
      <bibitem key="GROVER" xml:id="bib.bib50">
        <tags>
          <tag role="number">50</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Rong et al.</tag>
          <tag role="fullauthors">Rong, Bian, Xu, Xie, Wei, Huang, and Huang</tag>
          <tag role="refnum">Rong et al. (2020)</tag>
          <tag role="key">GROVER</tag>
        </tags>
        <bibblock>
Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and
Junzhou Huang.
</bibblock>
        <bibblock>Self-supervised graph transformer on large-scale molecular data.
</bibblock>
        <bibblock><emph font="italic">Advances in Neural Information Processing Systems</emph>,
33:12559–12571, 2020.
</bibblock>
      </bibitem>
      <bibitem key="MolFormer" xml:id="bib.bib51">
        <tags>
          <tag role="number">51</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Ross et al.</tag>
          <tag role="fullauthors">Ross, Belgodere, Chenthamarakshan, Padhi, Mroueh, and
Das</tag>
          <tag role="refnum">Ross et al. (2021)</tag>
          <tag role="key">MolFormer</tag>
        </tags>
        <bibblock>
Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef
Mroueh, and Payel Das.
</bibblock>
        <bibblock>Do large scale molecular language representations capture important
structural information?
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2106.09553</emph>, 2021.
</bibblock>
      </bibitem>
      <bibitem key="reaction-prediction-seq2seq" xml:id="bib.bib52">
        <tags>
          <tag role="number">52</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Schwaller et al.</tag>
          <tag role="fullauthors">Schwaller, Gaudin, Lanyi, Bekas, and
Laino</tag>
          <tag role="refnum">Schwaller et al. (2018)</tag>
          <tag role="key">reaction-prediction-seq2seq</tag>
        </tags>
        <bibblock>
Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, and Teodoro
Laino.
</bibblock>
        <bibblock>“found in translation”: predicting outcomes of complex organic
chemistry reactions using neural sequence-to-sequence models.
</bibblock>
        <bibblock><emph font="italic">Chemical science</emph>, 9(28):6091–6098, 2018.
</bibblock>
      </bibitem>
      <bibitem key="molecular-transformer" xml:id="bib.bib53">
        <tags>
          <tag role="number">53</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Schwaller et al.</tag>
          <tag role="fullauthors">Schwaller, Laino, Gaudin, Bolgar, Hunter, Bekas,
and Lee</tag>
          <tag role="refnum">Schwaller et al. (2019)</tag>
          <tag role="key"><!--  %**** arxiv_version.bbl Line 375 **** -->molecular-transformer</tag>
        </tags>
        <bibblock>
Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar,
Christopher A Hunter, Costas Bekas, and Alpha A Lee.
</bibblock>
        <bibblock>Molecular transformer: a model for uncertainty-calibrated chemical
reaction prediction.
</bibblock>
        <bibblock><emph font="italic">ACS central science</emph>, 5(9):1572–1583,
2019.
</bibblock>
      </bibitem>
      <bibitem key="graph_retro" xml:id="bib.bib54">
        <tags>
          <tag role="number">54</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Somnath et al.</tag>
          <tag role="fullauthors">Somnath, Bunne, Coley, Krause, and
Barzilay</tag>
          <tag role="refnum">Somnath et al. (2020)</tag>
          <tag role="key">graph_retro</tag>
        </tags>
        <bibblock>
Vignesh Ram Somnath, Charlotte Bunne, Connor W Coley, Andreas Krause, and
Regina Barzilay.
</bibblock>
        <bibblock>Learning graph models for template-free retrosynthesis.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2006.07038</emph>, 2020.
</bibblock>
      </bibitem>
      <bibitem key="3dinfomaxGNN" xml:id="bib.bib55">
        <tags>
          <tag role="number">55</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Stärk et al.</tag>
          <tag role="fullauthors">Stärk, Beaini, Corso, Tossou, Dallago,
Günnemann, and Liò</tag>
          <tag role="refnum">Stärk et al. (2022)</tag>
          <tag role="key">3dinfomaxGNN</tag>
        </tags>
        <bibblock>
Hannes Stärk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian
Dallago, Stephan Günnemann, and Pietro Liò.
</bibblock>
        <bibblock>3d infomax improves gnns for molecular property prediction.
</bibblock>
        <bibblock>In <emph font="italic">International Conference on Machine Learning</emph>, pp. 20479–20502. PMLR, 2022.
</bibblock>
      </bibitem>
      <bibitem key="retrosynthesis_energy" xml:id="bib.bib56">
        <tags>
          <tag role="number">56</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Sun et al.</tag>
          <tag role="fullauthors">Sun, Dai, Li, Kearnes, and Dai</tag>
          <tag role="refnum">Sun et al. (2020)</tag>
          <tag role="key">retrosynthesis_energy</tag>
        </tags>
        <bibblock>
Ruoxi Sun, Hanjun Dai, Li Li, Steven Kearnes, and Bo Dai.
</bibblock>
        <bibblock><!--  %**** arxiv˙version.bbl Line 400 **** -->Energy-based view of retrosynthesis.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2007.13437</emph>, 2020.
</bibblock>
      </bibitem>
      <bibitem key="integrated-gradients" xml:id="bib.bib57">
        <tags>
          <tag role="number">57</tag>
          <tag role="year">2017</tag>
          <tag role="authors">Sundararajan et al.</tag>
          <tag role="fullauthors">Sundararajan, Taly, and
Yan</tag>
          <tag role="refnum">Sundararajan et al. (2017)</tag>
          <tag role="key">integrated-gradients</tag>
        </tags>
        <bibblock>
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
</bibblock>
        <bibblock>Axiomatic attribution for deep networks.
</bibblock>
        <bibblock>In <emph font="italic">International conference on machine learning</emph>, pp. 3319–3328. PMLR, 2017.
</bibblock>
      </bibitem>
      <bibitem key="augmented_transformer" xml:id="bib.bib58">
        <tags>
          <tag role="number">58</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Tetko et al.</tag>
          <tag role="fullauthors">Tetko, Karpov, Van Deursen, and
Godin</tag>
          <tag role="refnum">Tetko et al. (2020)</tag>
          <tag role="key">augmented_transformer</tag>
        </tags>
        <bibblock>
Igor V Tetko, Pavel Karpov, Ruud Van Deursen, and Guillaume Godin.
</bibblock>
        <bibblock>State-of-the-art augmented nlp transformer models for direct and
single-step retrosynthesis.
</bibblock>
        <bibblock><emph font="italic">Nature communications</emph>, 11(1):1–11, 2020.
</bibblock>
      </bibitem>
      <bibitem key="GLUE" xml:id="bib.bib59">
        <tags>
          <tag role="number">59</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Singh, Michael, Hill, Levy, and Bowman</tag>
          <tag role="refnum">Wang et al. (2018)</tag>
          <tag role="key">GLUE</tag>
        </tags>
        <bibblock>
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
Bowman.
</bibblock>
        <bibblock>GLUE: A multi-task benchmark and analysis platform for natural
language understanding.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP</emph>, pp.  353–355,
Brussels, Belgium, November 2018. Association for Computational Linguistics.
</bibblock>
        <bibblock><!--  %**** arxiv˙version.bbl Line 425 **** -->doi: <ref class="ltx_nolink ltx_Url" href="10.18653/v1/W18-5446">10.18653/v1/W18-5446</ref>.
</bibblock>
        <bibblock>URL <ref class="ltx_url" font="typewriter" href="https://www.aclweb.org/anthology/W18-5446">https://www.aclweb.org/anthology/W18-5446</ref>.
</bibblock>
      </bibitem>
      <bibitem key="reaction-aware-molRL" xml:id="bib.bib60">
        <tags>
          <tag role="number">60</tag>
          <tag role="year">2022a</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Li, Jin, Cho, Ji, Han, and
Burke</tag>
          <tag role="refnum">Wang et al. (2022a)</tag>
          <tag role="key">reaction-aware-molRL</tag>
        </tags>
        <bibblock>
Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han,
and Martin Burke.
</bibblock>
        <bibblock>Chemical-reaction-aware molecule representation learning.
</bibblock>
        <bibblock>In <emph font="italic">International Conference on Learning Representations</emph>,
2022a.
</bibblock>
        <bibblock>URL <ref class="ltx_url" font="typewriter" href="https://openreview.net/forum?id=6sh3pIzKS-">https://openreview.net/forum?id=6sh3pIzKS-</ref>.
</bibblock>
      </bibitem>
      <bibitem key="SMILES_BERT" xml:id="bib.bib61">
        <tags>
          <tag role="number">61</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Guo, Wang, Sun, and Huang</tag>
          <tag role="refnum">Wang et al. (2019)</tag>
          <tag role="key">SMILES_BERT</tag>
        </tags>
        <bibblock>
Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang.
</bibblock>
        <bibblock>Smiles-bert: large scale unsupervised pre-training for molecular
property prediction.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the 10th ACM international conference on
bioinformatics, computational biology and health informatics</emph>, pp. 429–436, 2019.
</bibblock>
      </bibitem>
      <bibitem key="lm_objective_architecture" xml:id="bib.bib62">
        <tags>
          <tag role="number">62</tag>
          <tag role="year">2022b</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Roberts, Hesslow, Scao, Chung,
Beltagy, Launay, and Raffel</tag>
          <tag role="refnum">Wang et al. (2022b)</tag>
          <tag role="key">lm_objective_architecture</tag>
        </tags>
        <bibblock>
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung,
Iz Beltagy, Julien Launay, and Colin Raffel.
</bibblock>
        <bibblock><!--  %**** arxiv˙version.bbl Line 450 **** -->What language model architecture and pretraining objective work best
for zero-shot generalization?
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2204.05832</emph>, 2022b.
</bibblock>
      </bibitem>
      <bibitem key="iMolCLR" xml:id="bib.bib63">
        <tags>
          <tag role="number">63</tag>
          <tag role="year">2022c</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Magar, Liang, and
Farimani</tag>
          <tag role="refnum">Wang et al. (2022c)</tag>
          <tag role="key">iMolCLR</tag>
        </tags>
        <bibblock>
Yuyang Wang, Rishikesh Magar, Chen Liang, and Amir Barati Farimani.
</bibblock>
        <bibblock>Improving molecular contrastive learning via faulty negative
mitigation and decomposed fragment contrast.
</bibblock>
        <bibblock><emph font="italic">Journal of Chemical Information and Modeling</emph>, 59(8):3370–3388, 2022c.
</bibblock>
        <bibblock>doi: <ref class="ltx_nolink ltx_Url" href="10.1021/acs.jcim.2c00495">10.1021/acs.jcim.2c00495</ref>.
</bibblock>
      </bibitem>
      <bibitem key="MolCLR" xml:id="bib.bib64">
        <tags>
          <tag role="number">64</tag>
          <tag role="year">2022d</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Wang, Cao, and
Barati Farimani</tag>
          <tag role="refnum">Wang et al. (2022d)</tag>
          <tag role="key">MolCLR</tag>
        </tags>
        <bibblock>
Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani.
</bibblock>
        <bibblock>Molecular contrastive learning of representations via graph neural
networks.
</bibblock>
        <bibblock><emph font="italic">Nature Machine Intelligence</emph>, 4(3):279–287, 2022d.
</bibblock>
      </bibitem>
      <bibitem key="watson2019retrosynthetic" xml:id="bib.bib65">
        <tags>
          <tag role="number">65</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Watson et al.</tag>
          <tag role="fullauthors">Watson, Wang, and
Nicolaou</tag>
          <tag role="refnum">Watson et al. (2019)</tag>
          <tag role="key">watson2019retrosynthetic</tag>
        </tags>
        <bibblock>
Ian A Watson, Jibo Wang, and Christos A Nicolaou.
</bibblock>
        <bibblock>A retrosynthetic analysis algorithm implementation.
</bibblock>
        <bibblock><emph font="italic">Journal of cheminformatics</emph><!--  %**** arxiv˙version.bbl Line 475 **** -->, 11(1):1–12,
2019.
</bibblock>
      </bibitem>
      <bibitem key="moleculenet" xml:id="bib.bib66">
        <tags>
          <tag role="number">66</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Wu et al.</tag>
          <tag role="fullauthors">Wu, Ramsundar, Feinberg, Gomes, Geniesse, Pappu,
Leswing, and Pande</tag>
          <tag role="refnum">Wu et al. (2018)</tag>
          <tag role="key">moleculenet</tag>
        </tags>
        <bibblock>
Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse,
Aneesh S Pappu, Karl Leswing, and Vijay Pande.
</bibblock>
        <bibblock>Moleculenet: a benchmark for molecular machine learning.
</bibblock>
        <bibblock><emph font="italic">Chemical science</emph>, 9(2):513–530, 2018.
</bibblock>
      </bibitem>
      <bibitem key="AttentiveFP" xml:id="bib.bib67">
        <tags>
          <tag role="number">67</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Xiong et al.</tag>
          <tag role="fullauthors">Xiong, Wang, Liu, Zhong, Wan, Li, Li, Luo, Chen,
Jiang, et al.</tag>
          <tag role="refnum">Xiong et al. (2019)</tag>
          <tag role="key">AttentiveFP</tag>
        </tags>
        <bibblock>
Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong
Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al.
</bibblock>
        <bibblock>Pushing the boundaries of molecular representation for drug discovery
with the graph attention mechanism.
</bibblock>
        <bibblock><emph font="italic">Journal of medicinal chemistry</emph>, 63(16):8749–8760, 2019.
</bibblock>
      </bibitem>
      <bibitem key="retrocomposer" xml:id="bib.bib68">
        <tags>
          <tag role="number">68</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Yan et al.</tag>
          <tag role="fullauthors">Yan, Zhao, Lu, Yu, and Huang</tag>
          <tag role="refnum">Yan et al. (2021)</tag>
          <tag role="key">retrocomposer</tag>
        </tags>
        <bibblock>
Chaochao Yan, Peilin Zhao, Chan Lu, Yang Yu, and Junzhou Huang.
</bibblock>
        <bibblock>Retrocomposer: Discovering novel reactions by composing templates for
retrosynthesis prediction.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2112.11225</emph>, 2021.
</bibblock>
      </bibitem>
      <bibitem key="DMPNN-original" xml:id="bib.bib69">
        <tags>
          <tag role="number">69</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Yang et al.</tag>
          <tag role="fullauthors">Yang, Swanson, Jin, Coley, Eiden, Gao, Guzman-Perez,
Hopper, Kelley, Mathea, et al.</tag>
          <tag role="refnum">Yang et al. (2019)</tag>
          <tag role="key"><!--  %**** arxiv_version.bbl Line 500 **** -->DMPNN-original</tag>
        </tags>
        <bibblock>
Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao,
Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al.
</bibblock>
        <bibblock>Analyzing learned molecular representations for property prediction.
</bibblock>
        <bibblock><emph font="italic">Journal of chemical information and modeling</emph>, 59(8):3370–3388, 2019.
</bibblock>
      </bibitem>
    </biblist>
  </bibliography>
  <pagination role="newpage"/>
  <appendix inlist="toc" xml:id="A1">
    <tags>
      <tag>Appendix A</tag>
      <tag role="autoref">Appendix A</tag>
      <tag role="refnum">A</tag>
      <tag role="typerefnum">Appendix A</tag>
    </tags>
    <title><tag close=" ">Appendix A</tag>Dataset Descriptions</title>
    <toctitle><tag close=" ">A</tag>Dataset Descriptions</toctitle>
    <table inlist="lot" placement="h" xml:id="A1.T7">
      <tags>
        <tag><text fontsize="90%">Table 7</text></tag>
        <tag role="autoref">Table 7</tag>
        <tag role="refnum">7</tag>
        <tag role="typerefnum">Table 7</tag>
      </tags>
      <tabular vattach="middle">
        <tbody>
          <tr>
            <td align="left" border="tt">Dataset</td>
            <td align="left" border="tt">Task type</td>
            <td align="left" border="tt"># of tasks</td>
            <td align="left" border="tt"># of compounds</td>
            <td align="left" border="tt">Split type</td>
            <td align="left" border="tt">Metric</td>
          </tr>
          <tr>
            <td align="left" border="t">ESOL</td>
            <td align="left" border="t">Regression</td>
            <td align="left" border="t">1</td>
            <td align="left" border="t">1128</td>
            <td align="left" border="t">Random</td>
            <td align="left" border="t">RMSE</td>
          </tr>
          <tr>
            <td align="left">FreeSolv</td>
            <td align="left">Regression</td>
            <td align="left">1</td>
            <td align="left">642</td>
            <td align="left">Random</td>
            <td align="left">RMSE</td>
          </tr>
          <tr>
            <td align="left">Lipophilicity</td>
            <td align="left">Regression</td>
            <td align="left">1</td>
            <td align="left">4200</td>
            <td align="left">Random</td>
            <td align="left">RMSE</td>
          </tr>
          <tr>
            <td align="left" border="t">HIV</td>
            <td align="left" border="t">Classification</td>
            <td align="left" border="t">1</td>
            <td align="left" border="t">41127</td>
            <td align="left" border="t">Scaffold</td>
            <td align="left" border="t">ROC-AUC</td>
          </tr>
          <tr>
            <td align="left">BACE</td>
            <td align="left">Classification</td>
            <td align="left">1</td>
            <td align="left">1513</td>
            <td align="left">Scaffold</td>
            <td align="left">ROC-AUC</td>
          </tr>
          <tr>
            <td align="left">BBBP</td>
            <td align="left">Classification</td>
            <td align="left">1</td>
            <td align="left">2039</td>
            <td align="left">Scaffold</td>
            <td align="left">ROC-AUC</td>
          </tr>
          <tr>
            <td align="left" border="t">Tox21</td>
            <td align="left" border="t">Classification</td>
            <td align="left" border="t">12</td>
            <td align="left" border="t">7831</td>
            <td align="left" border="t">Random</td>
            <td align="left" border="t">ROC-AUC</td>
          </tr>
          <tr>
            <td align="left">ToxCast</td>
            <td align="left">Classification</td>
            <td align="left">617</td>
            <td align="left">8575</td>
            <td align="left">Random</td>
            <td align="left">ROC-AUC</td>
          </tr>
          <tr>
            <td align="left">SIDER</td>
            <td align="left">Classification</td>
            <td align="left">27</td>
            <td align="left">1427</td>
            <td align="left">Random</td>
            <td align="left">ROC-AUC</td>
          </tr>
          <tr>
            <td align="left">ClinTox</td>
            <td align="left">Classification</td>
            <td align="left">2</td>
            <td align="left">1478</td>
            <td align="left">Random</td>
            <td align="left">ROC-AUC</td>
          </tr>
          <tr>
            <td align="left" border="t">Ames</td>
            <td align="left" border="t">Classification</td>
            <td align="left" border="t">1</td>
            <td align="left" border="t">6512</td>
            <td align="left" border="t">Random</td>
            <td align="left" border="t">ROC-AUC</td>
          </tr>
          <tr>
            <td align="left" border="bb">Micronucleus Assay</td>
            <td align="left" border="bb">Classification</td>
            <td align="left" border="bb">1</td>
            <td align="left" border="bb">641</td>
            <td align="left" border="bb">Random</td>
            <td align="left" border="bb">ROC-AUC</td>
          </tr>
        </tbody>
      </tabular>
      <toccaption><tag close=" ">7</tag>Dataset statistics for our fine-tuning evaluations.</toccaption>
      <caption><tag close=": "><text fontsize="90%">Table 7</text></tag><text fontsize="90%">Dataset statistics for our fine-tuning evaluations.</text></caption>
    </table>
  </appendix>
  <appendix inlist="toc" labels="LABEL:sec:app-captum-ames" xml:id="A2">
    <tags>
      <tag>Appendix B</tag>
      <tag role="autoref">Appendix B</tag>
      <tag role="refnum">B</tag>
      <tag role="typerefnum">Appendix B</tag>
    </tags>
    <title><tag close=" ">Appendix B</tag>Ames Interpretability: Detailed Analysis</title>
    <toctitle><tag close=" ">B</tag>Ames Interpretability: Detailed Analysis</toctitle>
    <figure inlist="lof" labels="LABEL:fig:captum-ames-1" xml:id="A2.F7">
      <tags>
        <tag><text fontsize="90%">Figure 7</text></tag>
        <tag role="autoref">Figure 7</tag>
        <tag role="refnum">7</tag>
        <tag role="typerefnum">Figure 7</tag>
      </tags>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig1">
        <graphics candidates="figures/ames/ames0.pdf" graphic="figures/ames/ames0.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g1"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig2">
        <graphics candidates="figures/ames/ames5.pdf" graphic="figures/ames/ames5.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g2"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig3">
        <graphics candidates="figures/ames/ames7.pdf" graphic="figures/ames/ames7.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g3"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig4">
        <graphics candidates="figures/ames/ames8.pdf" graphic="figures/ames/ames8.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g4"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig5">
        <graphics candidates="figures/ames/ames9.pdf" graphic="figures/ames/ames9.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g5"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig6">
        <graphics candidates="figures/ames/ames10.pdf" graphic="figures/ames/ames10.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g6"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig7">
<!--  %**** appendix-ames.tex Line 25 **** -->        <graphics candidates="figures/ames/ames11.pdf" class="ltx_centering" graphic="figures/ames/ames11.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g7"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig8">
        <graphics candidates="figures/ames/ames13.pdf" graphic="figures/ames/ames13.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g8"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig9">
        <graphics candidates="figures/ames/ames14.pdf" graphic="figures/ames/ames14.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g9"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig10">
        <graphics candidates="figures/ames/ames16.pdf" graphic="figures/ames/ames16.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g10"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig11">
        <graphics candidates="figures/ames/ames18.pdf" graphic="figures/ames/ames18.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g11"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig12">
        <graphics candidates="figures/ames/ames19.pdf" graphic="figures/ames/ames19.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g12"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig13">
        <graphics candidates="figures/ames/ames20.pdf" graphic="figures/ames/ames20.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g13"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig14">
        <graphics candidates="figures/ames/ames21.pdf" graphic="figures/ames/ames21.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g14"/>
<!--  %**** appendix-ames.tex Line 50 **** -->      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig15">
        <graphics candidates="figures/ames/ames22.pdf" graphic="figures/ames/ames22.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g15"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig16">
        <graphics candidates="figures/ames/ames23.pdf" graphic="figures/ames/ames23.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g16"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig17">
        <graphics candidates="figures/ames/ames24.pdf" graphic="figures/ames/ames24.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g17"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig18">
        <graphics candidates="figures/ames/ames25.pdf" graphic="figures/ames/ames25.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g18"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig19">
        <graphics candidates="figures/ames/ames26.pdf" graphic="figures/ames/ames26.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g19"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig20">
        <graphics candidates="figures/ames/ames27.pdf" graphic="figures/ames/ames27.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g20"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F7.fig21">
        <graphics candidates="figures/ames/ames28.pdf" graphic="figures/ames/ames28.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F7.g21"/>
      </figure>
      <toccaption class="ltx_centering"><tag close=" "><!--  %**** appendix-ames.tex Line 75 **** -->7<!--  %**** appendix-ames.tex Line 75 **** 
     %**** appendix-ames.tex Line 75 **** 
     %**** appendix-ames.tex Line 75 **** 
     %**** appendix-ames.tex Line 75 **** 
     %**** appendix-ames.tex Line 75 **** --></tag><!--  %**** appendix-ames.tex Line 75 **** -->Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 1/5</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 7</text></tag><!--  %**** appendix-ames.tex Line 75 **** 
     %**** appendix-ames.tex Line 75 **** 
     %**** appendix-ames.tex Line 75 **** 
     %**** appendix-ames.tex Line 75 **** --><text fontsize="90%">Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 1/5</text></caption>
    </figure>
    <figure inlist="lof" labels="LABEL:fig:captum-ames-2" xml:id="A2.F8">
      <tags>
        <tag><text fontsize="90%">Figure 8</text></tag>
        <tag role="autoref">Figure 8</tag>
        <tag role="refnum">8</tag>
        <tag role="typerefnum">Figure 8</tag>
      </tags>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig1">
        <graphics candidates="figures/ames/ames30.pdf" graphic="figures/ames/ames30.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g1"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig2">
        <graphics candidates="figures/ames/ames31.pdf" graphic="figures/ames/ames31.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g2"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig3">
        <graphics candidates="figures/ames/ames32.pdf" graphic="figures/ames/ames32.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g3"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig4">
        <graphics candidates="figures/ames/ames33.pdf" graphic="figures/ames/ames33.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g4"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig5">
        <graphics candidates="figures/ames/ames35.pdf" graphic="figures/ames/ames35.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g5"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig6">
        <graphics candidates="figures/ames/ames36.pdf" graphic="figures/ames/ames36.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g6"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig7">
        <graphics candidates="figures/ames/ames37.pdf" graphic="figures/ames/ames37.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g7"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig8">
        <graphics candidates="figures/ames/ames38.pdf" graphic="figures/ames/ames38.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g8"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig9">
        <graphics candidates="figures/ames/ames40.pdf" graphic="figures/ames/ames40.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g9"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig10">
        <graphics candidates="figures/ames/ames41.pdf" graphic="figures/ames/ames41.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g10"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig11">
        <graphics candidates="figures/ames/ames42.pdf" graphic="figures/ames/ames42.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g11"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig12">
        <graphics candidates="figures/ames/ames43.pdf" graphic="figures/ames/ames43.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g12"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig13">
        <graphics candidates="figures/ames/ames44.pdf" graphic="figures/ames/ames44.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g13"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig14">
        <graphics candidates="figures/ames/ames45.pdf" graphic="figures/ames/ames45.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g14"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig15">
        <graphics candidates="figures/ames/ames46.pdf" graphic="figures/ames/ames46.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g15"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig16">
        <graphics candidates="figures/ames/ames48.pdf" graphic="figures/ames/ames48.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g16"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig17">
        <graphics candidates="figures/ames/ames49.pdf" graphic="figures/ames/ames49.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g17"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig18">
        <graphics candidates="figures/ames/ames52.pdf" graphic="figures/ames/ames52.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g18"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig19">
        <graphics candidates="figures/ames/ames53.pdf" graphic="figures/ames/ames53.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g19"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig20">
        <graphics candidates="figures/ames/ames54.pdf" graphic="figures/ames/ames54.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g20"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F8.fig21">
        <graphics candidates="figures/ames/ames56.pdf" graphic="figures/ames/ames56.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F8.g21"/>
      </figure>
<!--  %**** appendix-ames.tex Line 150 **** -->      <toccaption class="ltx_centering"><tag close=" ">8</tag>Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 2/5</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 8</text></tag><text fontsize="90%">Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 2/5</text></caption>
    </figure>
    <figure inlist="lof" labels="LABEL:fig:captum-ames-3" xml:id="A2.F9">
      <tags>
        <tag><text fontsize="90%">Figure 9</text></tag>
        <tag role="autoref">Figure 9</tag>
        <tag role="refnum">9</tag>
        <tag role="typerefnum">Figure 9</tag>
      </tags>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig1">
        <graphics candidates="figures/ames/ames57.pdf" graphic="figures/ames/ames57.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g1"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig2">
        <graphics candidates="figures/ames/ames58.pdf" graphic="figures/ames/ames58.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g2"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig3">
        <graphics candidates="figures/ames/ames59.pdf" graphic="figures/ames/ames59.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g3"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig4">
        <graphics candidates="figures/ames/ames60.pdf" graphic="figures/ames/ames60.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g4"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig5">
        <graphics candidates="figures/ames/ames61.pdf" graphic="figures/ames/ames61.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g5"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig6">
        <graphics candidates="figures/ames/ames62.pdf" graphic="figures/ames/ames62.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g6"/>
<!--  %**** appendix-ames.tex Line 175 **** -->      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig7">
        <graphics candidates="figures/ames/ames63.pdf" graphic="figures/ames/ames63.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g7"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig8">
        <graphics candidates="figures/ames/ames66.pdf" graphic="figures/ames/ames66.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g8"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig9">
        <graphics candidates="figures/ames/ames69.pdf" graphic="figures/ames/ames69.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g9"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig10">
        <graphics candidates="figures/ames/ames71.pdf" graphic="figures/ames/ames71.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g10"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig11">
        <graphics candidates="figures/ames/ames73.pdf" graphic="figures/ames/ames73.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g11"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig12">
        <graphics candidates="figures/ames/ames75.pdf" graphic="figures/ames/ames75.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g12"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig13">
        <graphics candidates="figures/ames/ames76.pdf" graphic="figures/ames/ames76.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g13"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig14">
<!--  %**** appendix-ames.tex Line 200 **** -->        <graphics candidates="figures/ames/ames77.pdf" class="ltx_centering" graphic="figures/ames/ames77.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g14"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig15">
        <graphics candidates="figures/ames/ames78.pdf" graphic="figures/ames/ames78.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g15"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig16">
        <graphics candidates="figures/ames/ames82.pdf" graphic="figures/ames/ames82.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g16"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig17">
        <graphics candidates="figures/ames/ames83.pdf" graphic="figures/ames/ames83.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g17"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig18">
        <graphics candidates="figures/ames/ames84.pdf" graphic="figures/ames/ames84.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g18"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig19">
        <graphics candidates="figures/ames/ames86.pdf" graphic="figures/ames/ames86.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g19"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig20">
        <graphics candidates="figures/ames/ames88.pdf" graphic="figures/ames/ames88.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g20"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F9.fig21">
        <graphics candidates="figures/ames/ames89.pdf" graphic="figures/ames/ames89.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F9.g21"/>
<!--  %**** appendix-ames.tex Line 225 **** -->      </figure>
      <toccaption class="ltx_centering"><tag close=" ">9</tag>Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 3/5</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 9</text></tag><text fontsize="90%">Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 3/5</text></caption>
    </figure>
    <figure inlist="lof" labels="LABEL:fig:captum-ames-4" xml:id="A2.F10">
      <tags>
        <tag><text fontsize="90%">Figure 10</text></tag>
        <tag role="autoref">Figure 10</tag>
        <tag role="refnum">10</tag>
        <tag role="typerefnum">Figure 10</tag>
      </tags>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig1">
        <graphics candidates="figures/ames/ames90.pdf" graphic="figures/ames/ames90.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g1"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig2">
        <graphics candidates="figures/ames/ames91.pdf" graphic="figures/ames/ames91.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g2"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig3">
        <graphics candidates="figures/ames/ames92.pdf" graphic="figures/ames/ames92.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g3"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig4">
        <graphics candidates="figures/ames/ames93.pdf" graphic="figures/ames/ames93.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g4"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig5">
        <graphics candidates="figures/ames/ames94.pdf" graphic="figures/ames/ames94.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g5"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig6">
        <graphics candidates="figures/ames/ames95.pdf" graphic="figures/ames/ames95.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g6"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig7">
        <graphics candidates="figures/ames/ames96.pdf" graphic="figures/ames/ames96.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g7"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig8">
        <graphics candidates="figures/ames/ames98.pdf" graphic="figures/ames/ames98.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g8"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig9">
        <graphics candidates="figures/ames/ames99.pdf" graphic="figures/ames/ames99.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g9"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig10">
        <graphics candidates="figures/ames/ames100.pdf" graphic="figures/ames/ames100.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g10"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig11">
        <graphics candidates="figures/ames/ames101.pdf" graphic="figures/ames/ames101.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g11"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig12">
        <graphics candidates="figures/ames/ames102.pdf" graphic="figures/ames/ames102.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g12"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig13">
        <graphics candidates="figures/ames/ames104.pdf" graphic="figures/ames/ames104.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g13"/>
<!--  %**** appendix-ames.tex Line 275 **** -->      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig14">
        <graphics candidates="figures/ames/ames107.pdf" graphic="figures/ames/ames107.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g14"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig15">
        <graphics candidates="figures/ames/ames108.pdf" graphic="figures/ames/ames108.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g15"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig16">
        <graphics candidates="figures/ames/ames109.pdf" graphic="figures/ames/ames109.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g16"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig17">
        <graphics candidates="figures/ames/ames110.pdf" graphic="figures/ames/ames110.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g17"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig18">
        <graphics candidates="figures/ames/ames113.pdf" graphic="figures/ames/ames113.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g18"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig19">
        <graphics candidates="figures/ames/ames115.pdf" graphic="figures/ames/ames115.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g19"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig20">
        <graphics candidates="figures/ames/ames116.pdf" graphic="figures/ames/ames116.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g20"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F10.fig21">
        <graphics candidates="figures/ames/ames117.pdf" graphic="figures/ames/ames117.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F10.g21"/>
      </figure>
      <toccaption class="ltx_centering"><tag close=" ">10</tag>Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 4/5</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 10</text></tag><text fontsize="90%">Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 4/5</text></caption>
    </figure>
    <figure inlist="lof" labels="LABEL:fig:captum-ames-5" xml:id="A2.F11">
      <tags>
        <tag><text fontsize="90%">Figure 11</text></tag>
        <tag role="autoref">Figure 11</tag>
        <tag role="refnum">11</tag>
        <tag role="typerefnum">Figure 11</tag>
      </tags>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig1">
        <graphics candidates="figures/ames/ames118.pdf" graphic="figures/ames/ames118.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g1"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig2">
        <graphics candidates="figures/ames/ames120.pdf" graphic="figures/ames/ames120.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g2"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig3">
        <graphics candidates="figures/ames/ames121.pdf" graphic="figures/ames/ames121.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g3"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig4">
        <graphics candidates="figures/ames/ames122.pdf" graphic="figures/ames/ames122.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g4"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig5">
        <graphics candidates="figures/ames/ames123.pdf" graphic="figures/ames/ames123.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g5"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig6">
<!--  %**** appendix-ames.tex Line 325 **** -->        <graphics candidates="figures/ames/ames124.pdf" class="ltx_centering" graphic="figures/ames/ames124.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g6"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig7">
        <graphics candidates="figures/ames/ames125.pdf" graphic="figures/ames/ames125.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g7"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig8">
        <graphics candidates="figures/ames/ames126.pdf" graphic="figures/ames/ames126.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g8"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig9">
        <graphics candidates="figures/ames/ames127.pdf" graphic="figures/ames/ames127.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g9"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig10">
        <graphics candidates="figures/ames/ames128.pdf" graphic="figures/ames/ames128.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g10"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig11">
        <graphics candidates="figures/ames/ames129.pdf" graphic="figures/ames/ames129.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g11"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig12">
        <graphics candidates="figures/ames/ames130.pdf" graphic="figures/ames/ames130.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g12"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig13">
        <graphics candidates="figures/ames/ames131.pdf" graphic="figures/ames/ames131.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g13"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig14">
        <graphics candidates="figures/ames/ames133.pdf" graphic="figures/ames/ames133.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g14"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig15">
        <graphics candidates="figures/ames/ames135.pdf" graphic="figures/ames/ames135.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g15"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A2.F11.fig16">
        <graphics candidates="figures/ames/ames136.pdf" graphic="figures/ames/ames136.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A2.F11.g16"/>
      </figure>
      <toccaption class="ltx_centering"><tag close=" ">11</tag>Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 5/5</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 11</text></tag><text fontsize="90%">Explaining predictions of the fine-tuned model on Ames dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 5/5</text></caption>
    </figure>
    <para xml:id="A2.p1">
      <p>In Figures <ref labelref="LABEL:fig:captum-ames-1"/> to <ref labelref="LABEL:fig:captum-ames-5"/> we show the structural alerts of more than a hundred molecules from the test set of Ames dataset. Chemicals under the numbers 10, 20, 21, 37, 40, 61, 71, 90, 93, 110, 117, and 127 are nitroaromatic compounds representing organic molecules that consist of at least one nitro group (-NO2) attached to an aromatic ring. These compounds and their residuals are well known for their carcinogenic and mutagenic potential. The nitro group in these molecules is characterized as structural alerts, i.e., molecular substructures related to the chemical’s carcinogenic and mutagenic properties.</p>
    </para>
    <para xml:id="A2.p2">
      <p>In most cases, Integrated Gradients recognized two components of the structural alert: the positively charged nitrogen atom and a double bonded oxygen atom. In some cases, only a positive charge was highlighted along with the oxygen atom. It is essential to mention that a positively charged nitrogen atom plays a crucial role in the toxic potential of this structural group since this positive charge determines the electrophilic nature of the molecule. An electrophile is a chemical species that form bonds with nucleophiles by accepting an electron pair. Whereas DNA is abundantly equipped with nucleophilic sites, reaction with electrophiles results in diverse chemical-DNA interactions.</p>
    </para>
    <para xml:id="A2.p3">
      <p>However, in all cases, the most highlighted atom was a double-bonded oxygen atom. Two explanations may arise from this observation. First is that Integrated Gradients “detects” it as a nitroso group (N=O), e.g., as in the compounds under the numbers 42, 59, and 69, where they represent a separate structural alert group. Second, the activation of most nitroaromatic compounds’ mutagenic properties is linked to the nitroreduction. As a result of nitroreduction, nitroso and superoxide species are produced.In both cases, double-bonded oxygen atoms play a vital role in forming toxic intermediates. So, in this case, it can be assumed that the algorithm recognizes target sites, such as positively charged nitrogen or oxygen atoms, responsible for producing DNA-damaging compounds.</p>
    </para>
    <para xml:id="A2.p4">
      <p><!--  %**** appendix-ames.tex Line 375 **** -->The same pattern is observed in the case of compounds under the numbers 18, 38, 57, 60, 124, and 126. These compounds are characterized by the bearing of at least one epoxy group consisting of an oxygen atom joined by single bonds to two adjacent carbon atoms, which form the three-membered epoxide ring.The epoxy group is a well-known structural alert. Compounds bearing epoxy groups are alkylating agents in which carbon atoms represent electrophilic sites that react with nucleophilic DNA to form covalent bonds via nucleophilic substitution reaction, thus acting as direct genotoxins and carcinogens. Integrated Gradients recognized this structural alert in all cases, primarily highlighting the oxygen atom joined to one of the two carbon atoms. Most often, the electrophilic carbon atom responsible for chemical-DNA interaction was the most highlighted area, assuming that Integrated Gradients highlighted the most active site of the molecule from the genotoxicity point of view. Interestingly, the highlighting of the oxygen atom is always accompanied by one or two equally highlighted carbon atoms and never appears separately, only oxygen or carbon atoms. Presumably, the method highlights the bond of these atoms, which is the second most important characteristic of epoxy groups, since, during the nucleophilic substitution reaction, the three-membered ring is opened via one of the cleavages of the O-C bond, forming the favorable for the molecule unstrained acyclic intermediate.</p>
    </para>
    <para xml:id="A2.p5">
      <p>It is worth noting that in the case of nitroaromatic compounds, the components of the structural alerts have been detected, along with the high probability (<Math mode="inline" tex="&gt;0.8" text="absent &gt; 0.8" xml:id="A2.p5.m1">
          <XMath>
            <XMApp>
              <XMTok meaning="greater-than" role="RELOP">&gt;</XMTok>
              <XMTok meaning="absent"/>
              <XMTok meaning="0.8" role="NUMBER">0.8</XMTok>
            </XMApp>
          </XMath>
        </Math>) of correct prediction. In contrast, in the case of epoxy compounds, the structural alerts have been detected correctly, but the probability level of correct predictions was around 0.22-0.48, except for compounds 124 and 126 (probability levels 0.891 and 0.932).</p>
    </para>
    <para xml:id="A2.p6">
      <p>An example where the attributions given by Integrated Gradients is not correlated with the structural alerts is the case of the naphthalene group in polycyclic aromatic hydrocarbons (compounds under the numbers 8, 28, 31, 48, 58, 62, 61, 76, 99, 113, and 108). The naphthalene group is a bicyclic aromatic hydrocarbon representing a structural alert in all mentioned compounds. Although the attributions of Integrated Gradients were correct with the high probability level in all cases, the structural alert has not been recognized. Here, the activated neurons have been dispersed distributed throughout polycyclic aromatic hydrocarbons, rarely matching the part of the structural alert.</p>
    </para>
  </appendix>
  <appendix inlist="toc" labels="LABEL:sec:app-captum-esol" xml:id="A3">
    <tags>
      <tag>Appendix C</tag>
      <tag role="autoref">Appendix C</tag>
      <tag role="refnum">C</tag>
      <tag role="typerefnum">Appendix C</tag>
    </tags>
    <title><tag close=" ">Appendix C</tag>ESOL Interpretability: Detailed Analysis</title>
    <toctitle><tag close=" ">C</tag>ESOL Interpretability: Detailed Analysis</toctitle>
    <figure inlist="lof" labels="LABEL:fig:captum-esol-1" xml:id="A3.F12">
      <tags>
        <tag><text fontsize="90%">Figure 12</text></tag>
        <tag role="autoref">Figure 12</tag>
        <tag role="refnum">12</tag>
        <tag role="typerefnum">Figure 12</tag>
      </tags>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig1">
        <graphics candidates="figures/esol/esol0.pdf" graphic="figures/esol/esol0.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g1"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig2">
        <graphics candidates="figures/esol/esol1.pdf" graphic="figures/esol/esol1.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g2"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig3">
        <graphics candidates="figures/esol/esol2.pdf" graphic="figures/esol/esol2.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g3"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig4">
        <graphics candidates="figures/esol/esol3.pdf" graphic="figures/esol/esol3.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g4"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig5">
        <graphics candidates="figures/esol/esol4.pdf" graphic="figures/esol/esol4.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g5"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig6">
        <graphics candidates="figures/esol/esol5.pdf" graphic="figures/esol/esol5.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g6"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig7">
<!--  %**** appendix-esol.tex Line 25 **** -->        <graphics candidates="figures/esol/esol6.pdf" class="ltx_centering" graphic="figures/esol/esol6.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g7"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig8">
        <graphics candidates="figures/esol/esol7.pdf" graphic="figures/esol/esol7.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g8"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig9">
        <graphics candidates="figures/esol/esol8.pdf" graphic="figures/esol/esol8.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g9"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig10">
        <graphics candidates="figures/esol/esol9.pdf" graphic="figures/esol/esol9.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g10"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig11">
        <graphics candidates="figures/esol/esol10.pdf" graphic="figures/esol/esol10.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g11"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig12">
        <graphics candidates="figures/esol/esol12.pdf" graphic="figures/esol/esol12.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g12"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig13">
        <graphics candidates="figures/esol/esol13.pdf" graphic="figures/esol/esol13.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g13"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig14">
        <graphics candidates="figures/esol/esol14.pdf" graphic="figures/esol/esol14.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g14"/>
<!--  %**** appendix-esol.tex Line 50 **** -->      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig15">
        <graphics candidates="figures/esol/esol15.pdf" graphic="figures/esol/esol15.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g15"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig16">
        <graphics candidates="figures/esol/esol16.pdf" graphic="figures/esol/esol16.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g16"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig17">
        <graphics candidates="figures/esol/esol17.pdf" graphic="figures/esol/esol17.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g17"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig18">
        <graphics candidates="figures/esol/esol18.pdf" graphic="figures/esol/esol18.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g18"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig19">
        <graphics candidates="figures/esol/esol19.pdf" graphic="figures/esol/esol19.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g19"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig20">
        <graphics candidates="figures/esol/esol20.pdf" graphic="figures/esol/esol20.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g20"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F12.fig21">
        <graphics candidates="figures/esol/esol21.pdf" graphic="figures/esol/esol21.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F12.g21"/>
      </figure>
      <toccaption class="ltx_centering"><tag close=" ">12</tag>Explaining predictions of the fine-tuned model on ESOL dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 1/3</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 12</text></tag><text fontsize="90%">Explaining predictions of the fine-tuned model on ESOL dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 1/3</text></caption>
    </figure>
    <figure inlist="lof" labels="LABEL:fig:captum-esol-2" xml:id="A3.F13">
      <tags>
        <tag><text fontsize="90%">Figure 13</text></tag>
        <tag role="autoref">Figure 13</tag>
        <tag role="refnum">13</tag>
        <tag role="typerefnum">Figure 13</tag>
      </tags>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig1">
        <graphics candidates="figures/esol/esol23.pdf" graphic="figures/esol/esol23.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g1"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig2">
        <graphics candidates="figures/esol/esol24.pdf" graphic="figures/esol/esol24.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g2"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig3">
        <graphics candidates="figures/esol/esol25.pdf" graphic="figures/esol/esol25.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g3"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig4">
        <graphics candidates="figures/esol/esol26.pdf" graphic="figures/esol/esol26.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g4"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig5">
<!--  %**** appendix-esol.tex Line 100 **** -->        <graphics candidates="figures/esol/esol27.pdf" class="ltx_centering" graphic="figures/esol/esol27.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g5"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig6">
        <graphics candidates="figures/esol/esol28.pdf" graphic="figures/esol/esol28.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g6"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig7">
        <graphics candidates="figures/esol/esol29.pdf" graphic="figures/esol/esol29.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g7"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig8">
        <graphics candidates="figures/esol/esol30.pdf" graphic="figures/esol/esol30.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g8"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig9">
        <graphics candidates="figures/esol/esol31.pdf" graphic="figures/esol/esol31.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g9"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig10">
        <graphics candidates="figures/esol/esol32.pdf" graphic="figures/esol/esol32.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g10"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig11">
        <graphics candidates="figures/esol/esol33.pdf" graphic="figures/esol/esol33.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g11"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig12">
        <graphics candidates="figures/esol/esol34.pdf" graphic="figures/esol/esol34.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g12"/>
<!--  %**** appendix-esol.tex Line 125 **** -->      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig13">
        <graphics candidates="figures/esol/esol35.pdf" graphic="figures/esol/esol35.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g13"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig14">
        <graphics candidates="figures/esol/esol36.pdf" graphic="figures/esol/esol36.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g14"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig15">
        <graphics candidates="figures/esol/esol37.pdf" graphic="figures/esol/esol37.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g15"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig16">
        <graphics candidates="figures/esol/esol38.pdf" graphic="figures/esol/esol38.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g16"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig17">
        <graphics candidates="figures/esol/esol39.pdf" graphic="figures/esol/esol39.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g17"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig18">
        <graphics candidates="figures/esol/esol40.pdf" graphic="figures/esol/esol40.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g18"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig19">
        <graphics candidates="figures/esol/esol41.pdf" graphic="figures/esol/esol41.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g19"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig20">
<!--  %**** appendix-esol.tex Line 150 **** -->        <graphics candidates="figures/esol/esol42.pdf" class="ltx_centering" graphic="figures/esol/esol42.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g20"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F13.fig21">
        <graphics candidates="figures/esol/esol43.pdf" graphic="figures/esol/esol43.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F13.g21"/>
      </figure>
      <toccaption class="ltx_centering"><tag close=" ">13</tag>Explaining predictions of the fine-tuned model on ESOL dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 2/3</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 13</text></tag><text fontsize="90%">Explaining predictions of the fine-tuned model on ESOL dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 2/3</text></caption>
    </figure>
    <figure inlist="lof" labels="LABEL:fig:captum-esol-3" placement="h" xml:id="A3.F14">
      <tags>
        <tag><text fontsize="90%">Figure 14</text></tag>
        <tag role="autoref">Figure 14</tag>
        <tag role="refnum">14</tag>
        <tag role="typerefnum">Figure 14</tag>
      </tags>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig1">
        <graphics candidates="figures/esol/esol44.pdf" graphic="figures/esol/esol44.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g1"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig2">
        <graphics candidates="figures/esol/esol46.pdf" graphic="figures/esol/esol46.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g2"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig3">
        <graphics candidates="figures/esol/esol47.pdf" graphic="figures/esol/esol47.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g3"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig4">
        <graphics candidates="figures/esol/esol48.pdf" graphic="figures/esol/esol48.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g4"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig5">
        <graphics candidates="figures/esol/esol49.pdf" graphic="figures/esol/esol49.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g5"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig6">
        <graphics candidates="figures/esol/esol50.pdf" graphic="figures/esol/esol50.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g6"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig7">
        <graphics candidates="figures/esol/esol51.pdf" graphic="figures/esol/esol51.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g7"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig8">
        <graphics candidates="figures/esol/esol52.pdf" graphic="figures/esol/esol52.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g8"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig9">
        <graphics candidates="figures/esol/esol53.pdf" graphic="figures/esol/esol53.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g9"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig10">
        <graphics candidates="figures/esol/esol55.pdf" graphic="figures/esol/esol55.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g10"/>
<!--  %**** appendix-esol.tex Line 200 **** -->      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig11">
        <graphics candidates="figures/esol/esol56.pdf" graphic="figures/esol/esol56.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g11"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig12">
        <graphics candidates="figures/esol/esol57.pdf" graphic="figures/esol/esol57.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g12"/>
      </figure>
      <break class="ltx_break"/>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig13">
        <graphics candidates="figures/esol/esol58.pdf" graphic="figures/esol/esol58.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g13"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig14">
        <graphics candidates="figures/esol/esol59.pdf" graphic="figures/esol/esol59.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g14"/>
      </figure>
      <figure align="center" class="ltx_figure_panel" placement="b" xml:id="A3.F14.fig15">
        <graphics candidates="figures/esol/esol60.pdf" graphic="figures/esol/esol60.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A3.F14.g15"/>
      </figure>
      <toccaption class="ltx_centering"><tag close=" ">14</tag>Explaining predictions of the fine-tuned model on ESOL dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 3/3</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 14</text></tag><text fontsize="90%">Explaining predictions of the fine-tuned model on ESOL dataset. See Section <ref labelref="LABEL:sec:captum"/>. Part 3/3</text></caption>
    </figure>
    <para xml:id="A3.p1">
      <p>In <ref labelref="LABEL:fig:captum-esol-1" show="creftypepluralcap~refnum"/><ERROR class="undefined">\crefmiddleconjunction</ERROR><ref labelref="LABEL:fig:captum-esol-2" show="refnum"/><ERROR class="undefined">\creflastconjunction</ERROR><ref labelref="LABEL:fig:captum-esol-3" show="refnum"/> we show the attributions on <Math mode="inline" tex="60" text="60" xml:id="A3.p1.m1">
          <XMath>
            <XMTok meaning="60" role="NUMBER">60</XMTok>
          </XMath>
        </Math> molecules from the test set of ESOL dataset. The images with indices 10, 16, 19, 27, 30, 37, 50, 51 and 55 contain hydroxyl groups, which are properly highlighted in red by the Integrated Gradient method. Same with carbonyl groups, as seen on molecules with indices 2, 5, 7, 12,25, 26, 46 and 47.</p>
    </para>
    <para xml:id="A3.p2">
      <p>It is usually assumed that the noise in aqueous solubility datasets is roughly 0.7 <cite class="ltx_citemacro_citet"><bibref bibrefs="ml-solubility-noise" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>. It is noteworthy that in all instances, the errors of the predicted values are less than that and, hence, can be assumed to be accurate.</p>
    </para>
    <para xml:id="A3.p3">
      <p>In one case, out of all molecules bearing the hydroxyl group, in compound #48, the functional group was not highlighted and even marked as a negative contributor. Even so, the prediction value of solubility was correct.</p>
    </para>
    <para xml:id="A3.p4">
      <p>One more interesting example where the hydroxyl group has been recognized, but the prediction was incorrect (LogS ± 0.76) is compound #34. In this compound, in addition to the hydroxyl group, four alkyl chloride (C-Cl) groups exist and contribute to the molecule’s solubility (have a lower level of polarity). The lower proportion of alkyl chloride-bearing compounds in the training set may lead to underestimating these functional group contributions to the molecule’s solubility.</p>
    </para>
    <para xml:id="A3.p5">
      <p>In the case of carbonyl groups, there are more examples where the Integrated Gradients method didn’t highlight the functional groups, for instance, compounds under the numbers 13, 14, 17, 35, 36, 40, 41, 42 and 43. Note that logS values of all these compounds were in the range of 0 to <Math mode="inline" tex="-2" text="- 2" xml:id="A3.p5.m1">
          <XMath>
            <XMApp>
              <XMTok meaning="minus" role="ADDOP">-</XMTok>
              <XMTok meaning="2" role="NUMBER">2</XMTok>
            </XMApp>
          </XMath>
        </Math>, which is the range of less soluble compounds, compared to the solubility of aforementioned compounds with recognized carbonyl groups (in the range of 0 and higher, highly soluble).
Additionally, all compounds with unrecognized carbonyl groups are complex, polycyclic aromatic hydrocarbons, which, as mentioned in the Ames dataset, was more problematic for the analysis by the Integrated Gradients method.</p>
    </para>
    <para xml:id="A3.p6">
      <p>Other interesting examples are in the aliphatic compounds, which are hydrocarbon compounds containing carbon and hydrogen joined together in straight chains with carbonyl groups (Compounds # 7, 25, 26, 28) or hydroxyl groups (Compounds # 19, 48, 51). A general rule for these compounds is that the longer the carbon chain, the lower the solubility in polar solvents such as water. Here, we see that the increasing amount of CH2 groups in a carbon chain enhances the hydrophobic effect, which decreases the solubility value. Hence, compounds #28 and #51 have the lowest solubility compared to the abovementioned examples. It is remarkable that the model somehow followed the rule, and predicted values also exhibit the same pattern. As to the Integrated Gradients method, the negative contribution of CH2 groups is constantly highlighted in all compounds, whereas peripheric CH2 groups have been marked as small contributions to solubility.</p>
    </para>
    <para xml:id="A3.p7">
      <p>There are other examples where the predictions were accurate, but the underlying chemistry is more complicated, and the highlighted substructures sometimes are unexplainable.</p>
    </para>
  </appendix>
  <appendix inlist="toc" xml:id="A4">
    <tags>
      <tag>Appendix D</tag>
      <tag role="autoref">Appendix D</tag>
      <tag role="refnum">D</tag>
      <tag role="typerefnum">Appendix D</tag>
    </tags>
    <title><tag close=" ">Appendix D</tag>Hyperparameters</title>
    <toctitle><tag close=" ">D</tag>Hyperparameters</toctitle>
    <table inlist="lot" labels="LABEL:sec:hp LABEL:tab:hyperparams-pretrain" placement="h" xml:id="A4.T8">
      <tags>
        <tag><text fontsize="90%">Table 8</text></tag>
        <tag role="autoref">Table 8</tag>
        <tag role="refnum">8</tag>
        <tag role="typerefnum">Table 8</tag>
      </tags>
      <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
        <thead>
          <tr>
            <td align="left" border="tt" thead="column row">Hyper parameter</td>
            <td align="left" border="tt" class="ltx_nopad_r" thead="column">Value</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" border="t" thead="row">batch-size</td>
            <td align="left" border="t" class="ltx_nopad_r">32</td>
          </tr>
          <tr>
            <td align="left" thead="row">task</td>
            <td align="left" class="ltx_nopad_r">sentence_prediction</td>
          </tr>
          <tr>
            <td align="left" thead="row">add-prev-output-tokens</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">layernorm-embedding</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">share-all-embeddings</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">share-decoder-input-output-embed</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">skip-invalid-size-inputs-valid-test</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">required-batch-size-multiple</td>
            <td align="left" class="ltx_nopad_r">1</td>
          </tr>
          <tr>
            <td align="left" thead="row">criterion</td>
            <td align="left" class="ltx_nopad_r">cross_entropy</td>
          </tr>
          <tr>
            <td align="left" thead="row">arch</td>
            <td align="left" class="ltx_nopad_r">bart_large</td>
          </tr>
          <tr>
            <td align="left" thead="row">total-num-update</td>
            <td align="left" class="ltx_nopad_r">50000</td>
          </tr>
          <tr>
            <td align="left" thead="row">max-update</td>
            <td align="left" class="ltx_nopad_r">50000</td>
          </tr>
          <tr>
            <td align="left" thead="row">warmup-updates</td>
            <td align="left" class="ltx_nopad_r">1500</td>
          </tr>
          <tr>
            <td align="left" thead="row">max-target-positions</td>
            <td align="left" class="ltx_nopad_r">128</td>
          </tr>
          <tr>
            <td align="left" thead="row">max-source-positions</td>
            <td align="left" class="ltx_nopad_r">128</td>
          </tr>
          <tr>
            <td align="left" thead="row">best-checkpoint-metric</td>
            <td align="left" class="ltx_nopad_r">loss</td>
          </tr>
          <tr>
            <td align="left" thead="row">Dropout</td>
            <td align="left" class="ltx_nopad_r">0.1</td>
          </tr>
          <tr>
            <td align="left" thead="row">attention-dropout</td>
            <td align="left" class="ltx_nopad_r">0.2</td>
          </tr>
          <tr>
            <td align="left" thead="row">relu-dropout</td>
            <td align="left" class="ltx_nopad_r">0.1</td>
          </tr>
          <tr>
            <td align="left" thead="row">clip-norm</td>
            <td align="left" class="ltx_nopad_r">1.0</td>
          </tr>
          <tr>
            <td align="left" thead="row">max-source-positions</td>
            <td align="left" class="ltx_nopad_r">128</td>
          </tr>
          <tr>
            <td align="left" thead="row">weight-decay</td>
            <td align="left" class="ltx_nopad_r">0.01</td>
          </tr>
          <tr>
            <td align="left" thead="row">fp16</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">fp32</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">fp16-scale-window</td>
            <td align="left" class="ltx_nopad_r">128</td>
          </tr>
          <tr>
            <td align="left" thead="row">threshold-loss-scale</td>
            <td align="left" class="ltx_nopad_r">1</td>
          </tr>
          <tr>
            <td align="left" thead="row">Optimizer</td>
            <td align="left" class="ltx_nopad_r">Adam</td>
          </tr>
          <tr>
            <td align="left" thead="row">Adam-betas</td>
            <td align="left" class="ltx_nopad_r">(0.9, 0.999)</td>
          </tr>
          <tr>
            <td align="left" thead="row">Adam-eps</td>
            <td align="left" class="ltx_nopad_r">1e-08</td>
          </tr>
          <tr>
            <td align="left" thead="row">LR Scheduler</td>
            <td align="left" class="ltx_nopad_r">polynomial decay</td>
          </tr>
          <tr>
            <td align="left" thead="row">Learning Rate</td>
            <td align="left" class="ltx_nopad_r">1e-05</td>
          </tr>
          <tr>
            <td align="left" thead="row">mask</td>
            <td align="left" class="ltx_nopad_r">0.2</td>
          </tr>
          <tr>
            <td align="left" thead="row">mask_length</td>
            <td align="left" class="ltx_nopad_r">span-poisson</td>
          </tr>
          <tr>
            <td align="left" thead="row">mask_random</td>
            <td align="left" class="ltx_nopad_r">0.1</td>
          </tr>
          <tr>
            <td align="left" border="bb" thead="row">GPU</td>
            <td align="left" border="bb" class="ltx_nopad_r">1024</td>
          </tr>
        </tbody>
      </tabular>
      <toccaption class="ltx_centering"><tag close=" ">8</tag>Table shows the hyperparameters used for pre-training.</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Table 8</text></tag><text fontsize="90%">Table shows the hyperparameters used for pre-training.</text></caption>
    </table>
    <table inlist="lot" labels="LABEL:tab:hyperparams-finetune" placement="h" xml:id="A4.T9">
      <tags>
        <tag><text fontsize="90%">Table 9</text></tag>
        <tag role="autoref">Table 9</tag>
        <tag role="refnum">9</tag>
        <tag role="typerefnum">Table 9</tag>
      </tags>
      <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
        <thead>
          <tr>
            <td align="left" border="tt" thead="column row">Hyper parameter</td>
            <td align="left" border="tt" class="ltx_nopad_r" thead="column">Value</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" border="t" thead="row">batch-size</td>
            <td align="left" border="t" class="ltx_nopad_r">16</td>
          </tr>
          <tr>
            <td align="left" thead="row">task</td>
            <td align="left" class="ltx_nopad_r">sentence_prediction</td>
          </tr>
          <tr>
            <td align="left" thead="row">add-prev-output-tokens</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">layernorm-embedding</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">share-all-embeddings</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">share-decoder-input-output-embed</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">reset-optimizer</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">reset-dataloader</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">reset-meters</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">skip-invalid-size-inputs-valid-test</td>
            <td align="left" class="ltx_nopad_r">True</td>
          </tr>
          <tr>
            <td align="left" thead="row">required-batch-size-multiple</td>
            <td align="left" class="ltx_nopad_r">1</td>
          </tr>
          <tr>
            <td align="left" thead="row">criterion</td>
            <td align="left" class="ltx_nopad_r">sentence_prediction</td>
          </tr>
          <tr>
            <td align="left" thead="row">arch</td>
            <td align="left" class="ltx_nopad_r">bart_large</td>
          </tr>
          <tr>
            <td align="left" thead="row">total-num-update</td>
            <td align="left" class="ltx_nopad_r">dataset_size*0.8/batch_size*epoch</td>
          </tr>
          <tr>
            <td align="left" thead="row">max-update</td>
            <td align="left" class="ltx_nopad_r">total_num_update</td>
          </tr>
          <tr>
            <td align="left" thead="row">warmup-updates</td>
            <td align="left" class="ltx_nopad_r">total_num_update*0.16</td>
          </tr>
          <tr>
            <td align="left" thead="row">max-target-positions</td>
            <td align="left" class="ltx_nopad_r">128</td>
          </tr>
          <tr>
            <td align="left" thead="row">max-source-positions</td>
            <td align="left" class="ltx_nopad_r">128</td>
          </tr>
          <tr>
            <td align="left" thead="row">regression-target</td>
            <td align="left" class="ltx_nopad_r">{True, False}</td>
          </tr>
          <tr>
            <td align="left" thead="row">best-checkpoint-metric</td>
            <td align="left" class="ltx_nopad_r">loss</td>
          </tr>
          <tr>
            <td align="left" thead="row">Dropout</td>
            <td align="left" class="ltx_nopad_r">{0.1, 0.2, 0.3}</td>
          </tr>
          <tr>
            <td align="left" thead="row">attention-dropout</td>
            <td align="left" class="ltx_nopad_r">0.2</td>
          </tr>
          <tr>
            <td align="left" thead="row">relu-dropout</td>
            <td align="left" class="ltx_nopad_r">0.1</td>
          </tr>
          <tr>
            <td align="left" thead="row">clip-norm</td>
            <td align="left" class="ltx_nopad_r">0.1</td>
          </tr>
          <tr>
            <td align="left" thead="row">max-source-positions</td>
            <td align="left" class="ltx_nopad_r">128</td>
          </tr>
          <tr>
            <td align="left" thead="row">num-classes</td>
            <td align="left" class="ltx_nopad_r">{1, 2}</td>
          </tr>
          <tr>
            <td align="left" thead="row">weight-decay</td>
            <td align="left" class="ltx_nopad_r">0.01</td>
          </tr>
          <tr>
            <td align="left" thead="row">fp16</td>
            <td align="left" class="ltx_nopad_r">{True, null}</td>
          </tr>
          <tr>
            <td align="left" thead="row">fp32</td>
            <td align="left" class="ltx_nopad_r">{True, null}</td>
          </tr>
          <tr>
            <td align="left" thead="row">fp16-scale-window</td>
            <td align="left" class="ltx_nopad_r">128</td>
          </tr>
          <tr>
            <td align="left" thead="row">threshold-loss-scale</td>
            <td align="left" class="ltx_nopad_r">{1, null}</td>
          </tr>
          <tr>
            <td align="left" thead="row">Optimizer</td>
            <td align="left" class="ltx_nopad_r">Adam</td>
          </tr>
          <tr>
            <td align="left" thead="row">Adam-betas</td>
            <td align="left" class="ltx_nopad_r">(0.9, 0.999)</td>
          </tr>
          <tr>
            <td align="left" thead="row">Adam-eps</td>
            <td align="left" class="ltx_nopad_r">1e-08</td>
          </tr>
          <tr>
            <td align="left" thead="row">LR Scheduler</td>
            <td align="left" class="ltx_nopad_r">polynomial decay</td>
          </tr>
          <tr>
            <td align="left" border="bb" thead="row">Learning Rate</td>
            <td align="left" border="bb" class="ltx_nopad_r">{5e-06, 1e-05, 3e-05}</td>
          </tr>
        </tbody>
      </tabular>
      <toccaption class="ltx_centering"><tag close=" ">9</tag>Table shows the hyperparameters used for fine-tuning experiments for classification and regression tasks. We set the total number of updates to the number of steps required to do ten epochs and the number of warmup updates being 16% of the total number of updates.</toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Table 9</text></tag><text fontsize="90%">Table shows the hyperparameters used for fine-tuning experiments for classification and regression tasks. We set the total number of updates to the number of steps required to do ten epochs and the number of warmup updates being 16% of the total number of updates.</text></caption>
    </table>
    <table inlist="lot" labels="LABEL:tab:hyperparams-generative" placement="h" xml:id="A4.T10">
      <tags>
        <tag><text fontsize="90%">Table 10</text></tag>
        <tag role="autoref">Table 10</tag>
        <tag role="refnum">10</tag>
        <tag role="typerefnum">Table 10</tag>
      </tags>
      <tabular class="ltx_centering ltx_figure_panel ltx_guessed_headers" vattach="middle">
        <thead>
          <tr>
            <td align="left" border="tt" thead="column">Hyper parameter</td>
            <td align="left" border="tt" class="ltx_nopad_r" thead="column">Value</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" border="t">Optimizer</td>
            <td align="left" border="t" class="ltx_nopad_r">Adam</td>
          </tr>
          <tr>
            <td align="left">Adam-betas</td>
            <td align="left" class="ltx_nopad_r">(0.9, 0.98)</td>
          </tr>
          <tr>
            <td align="left">Adam-eps</td>
            <td align="left" class="ltx_nopad_r">1e-8</td>
          </tr>
          <tr>
            <td align="left">LR Scheduler</td>
            <td align="left" class="ltx_nopad_r">polynomial decay</td>
          </tr>
          <tr>
            <td align="left" border="bb">Learning Rate</td>
            <td align="left" border="bb" class="ltx_nopad_r">3e-05</td>
          </tr>
        </tbody>
      </tabular>
<!--  %**** arxiv˙version.tex Line 625 **** -->      <tabular class="ltx_centering ltx_figure_panel ltx_guessed_headers" vattach="middle">
        <tbody>
          <tr>
            <td align="left" border="tt" thead="row">Hyper parameter</td>
            <td align="left" border="tt" class="ltx_nopad_r">Value</td>
          </tr>
          <tr>
            <td align="left" border="t" thead="row"><Math mode="inline" tex="\lambda" text="lambda" xml:id="A4.T10.m1">
                <XMath>
                  <XMTok font="italic" name="lambda" role="UNKNOWN">λ</XMTok>
                </XMath>
              </Math></td>
            <td align="left" border="t" class="ltx_nopad_r">[0.001, 0.01, 0.1]</td>
          </tr>
          <tr>
            <td align="left" thead="row">Noise Types</td>
            <td align="left" class="ltx_nopad_r">[<Math mode="inline" tex="\mathcal{U}" text="U" xml:id="A4.T10.m2">
                <XMath>
                  <XMTok font="caligraphic" role="UNKNOWN">U</XMTok>
                </XMath>
              </Math>, <Math mode="inline" tex="\mathcal{N}" text="N" xml:id="A4.T10.m3">
                <XMath>
                  <XMTok font="caligraphic" role="UNKNOWN">N</XMTok>
                </XMath>
              </Math>]</td>
          </tr>
          <tr>
            <td align="left" thead="row"><Math mode="inline" tex="\sigma" text="sigma" xml:id="A4.T10.m4">
                <XMath>
                  <XMTok font="italic" name="sigma" role="UNKNOWN">σ</XMTok>
                </XMath>
              </Math></td>
            <td align="left" class="ltx_nopad_r"><Math mode="inline" tex="1e-5" text="1 * e - 5" xml:id="A4.T10.m5">
                <XMath>
                  <XMApp>
                    <XMTok meaning="minus" role="ADDOP">-</XMTok>
                    <XMApp>
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok meaning="1" role="NUMBER">1</XMTok>
                      <XMTok font="italic" role="UNKNOWN">e</XMTok>
                    </XMApp>
                    <XMTok meaning="5" role="NUMBER">5</XMTok>
                  </XMApp>
                </XMath>
              </Math></td>
          </tr>
          <tr>
            <td align="left" thead="row">Dropout</td>
            <td align="left" class="ltx_nopad_r">0.1</td>
          </tr>
          <tr>
            <td align="left" thead="row">Weight Decay</td>
            <td align="left" class="ltx_nopad_r">0.01</td>
          </tr>
          <tr>
            <td align="left" border="bb" thead="row">Clip Norm</td>
            <td align="left" border="bb" class="ltx_nopad_r">0.1</td>
          </tr>
        </tbody>
      </tabular>
      <toccaption class="ltx_centering"><tag close=" ">10</tag>Hyper parameters for the use of R3F on all generative fine-tuning experiments. All other hyper-parameters are in accordance with Table <ref labelref="LABEL:tab:hyperparams-finetune"/></toccaption>
      <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Table 10</text></tag><text fontsize="90%">Hyper parameters for the use of R3F on all generative fine-tuning experiments. All other hyper-parameters are in accordance with Table <ref labelref="LABEL:tab:hyperparams-finetune"/></text></caption>
      <p align="center" class="ltx_figure_panel">.</p>
    </table>
  </appendix>
</document>
