\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2020{\natexlab{a}})Aghajanyan, Shrivastava, Gupta,
  Goyal, Zettlemoyer, and Gupta]{RXF}
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke
  Zettlemoyer, and Sonal Gupta.
\newblock Better fine-tuning by reducing representational collapse.
\newblock \emph{arXiv preprint arXiv:2008.03156}, 2020{\natexlab{a}}.

\bibitem[Aghajanyan et~al.(2020{\natexlab{b}})Aghajanyan, Zettlemoyer, and
  Gupta]{intrinsic_dimensionality_finetuning}
Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta.
\newblock Intrinsic dimensionality explains the effectiveness of language model
  fine-tuning.
\newblock \emph{arXiv preprint arXiv:2012.13255}, 2020{\natexlab{b}}.

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Okhonko, Lewis, Joshi, Xu, Ghosh,
  and Zettlemoyer]{HTLM1}
Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu~Xu, Gargi Ghosh,
  and Luke Zettlemoyer.
\newblock Htlm: Hyper-text pre-training and prompting of language models.
\newblock \emph{arXiv preprint arXiv:2107.06955}, 2021.

\bibitem[Aghajanyan et~al.(2022)Aghajanyan, Huang, Ross, Karpukhin, Xu, Goyal,
  Okhonko, Joshi, Ghosh, Lewis, et~al.]{CM3}
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu~Xu, Naman
  Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et~al.
\newblock Cm3: A causal masked multimodal model of the internet.
\newblock \emph{arXiv preprint arXiv:2201.07520}, 2022.

\bibitem[Arakelyan et~al.(2020)Arakelyan, Soghomonyan, and {The Aim team}]{AIM}
Gor Arakelyan, Gevorg Soghomonyan, and {The Aim team}.
\newblock {Aim}.
\newblock 6 2020.
\newblock \doi{10.5281/zenodo.6536395}.
\newblock URL \url{https://github.com/aimhubio/aim}.

\bibitem[Baines et~al.(2021)Baines, Bhosale, Caggiano, Goyal, Goyal, Ott,
  Lefaudeux, Liptchinsky, Rabbat, Sheiffer, Sridhar, and Xu]{fairscale}
Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth
  Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam
  Sheiffer, Anjali Sridhar, and Min Xu.
\newblock Fairscale: A general purpose modular pytorch library for high
  performance and large scale training.
\newblock \url{https://github.com/facebookresearch/fairscale}, 2021.

\bibitem[Bao et~al.(2021)Bao, Dong, and Wei]{BEIT}
Hangbo Bao, Li~Dong, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock \emph{arXiv preprint arXiv:2106.08254}, 2021.

\bibitem[Bemis \& Murcko(1996)Bemis and Murcko]{scaffold1996}
Guy~W Bemis and Mark~A Murcko.
\newblock The properties of known drugs. 1. molecular frameworks.
\newblock \emph{Journal of medicinal chemistry}, 39\penalty0 (15):\penalty0
  2887--2893, 1996.

\bibitem[Boobier et~al.(2020)Boobier, Hose, Blacker, and
  Nguyen]{ml-solubility-noise}
Samuel Boobier, David~RJ Hose, A~John Blacker, and Bao~N Nguyen.
\newblock Machine learning with physicochemical relationships: solubility
  prediction in organic solvents and water.
\newblock \emph{Nature communications}, 11\penalty0 (1):\penalty0 1--10, 2020.

\bibitem[Bradshaw et~al.(2018)Bradshaw, Kusner, Paige, Segler, and
  Hern{\'a}ndez-Lobato]{bradshaw2018generative}
John Bradshaw, Matt~J Kusner, Brooks Paige, Marwin~HS Segler, and
  Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock A generative model for electron paths.
\newblock \emph{arXiv preprint arXiv:1805.10970}, 2018.

\bibitem[Chithrananda et~al.(2020)Chithrananda, Grand, and
  Ramsundar]{chemberta}
Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar.
\newblock Chemberta: Large-scale self-supervised pretraining for molecular
  property prediction.
\newblock \emph{arXiv preprint arXiv:2010.09885}, 2020.

\bibitem[Coley et~al.(2019)Coley, Jin, Rogers, Jamison, Jaakkola, Green,
  Barzilay, and Jensen]{coley2019graph}
Connor~W Coley, Wengong Jin, Luke Rogers, Timothy~F Jamison, Tommi~S Jaakkola,
  William~H Green, Regina Barzilay, and Klavs~F Jensen.
\newblock A graph-convolutional neural network model for the prediction of
  chemical reactivity.
\newblock \emph{Chemical science}, 10\penalty0 (2):\penalty0 370--377, 2019.

\bibitem[Conneau et~al.(2020)Conneau, Baevski, Collobert, Mohamed, and
  Auli]{XLSR}
Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and
  Michael Auli.
\newblock Unsupervised cross-lingual representation learning for speech
  recognition.
\newblock \emph{arXiv preprint arXiv:2006.13979}, 2020.

\bibitem[Dahl et~al.(2014)Dahl, Jaitly, and Salakhutdinov]{NN-QSAR-2014}
George~E Dahl, Navdeep Jaitly, and Ruslan Salakhutdinov.
\newblock Multi-task neural networks for qsar predictions.
\newblock \emph{arXiv preprint arXiv:1406.1231}, 2014.

\bibitem[Davey(2018)]{davey2018retrosynthesis}
Stephen~G Davey.
\newblock Retrosynthesis: Computer says yes.
\newblock \emph{Nature Reviews Chemistry}, 2\penalty0 (1):\penalty0 1--1, 2018.

\bibitem[Dearden(2016)]{qsar-history}
John Dearden.
\newblock The history and development of quantitative structure-activity
  relationships (qsars).
\newblock \emph{IJQSPR}, 1\penalty0 (1):\penalty0 1--44, 2016.
\newblock \doi{10.4018/978-1-5225-0549-5.ch003}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Do et~al.(2019)Do, Tran, and Venkatesh]{do2019graph}
Kien Do, Truyen Tran, and Svetha Venkatesh.
\newblock Graph transformation policy network for chemical reaction prediction.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  750--760, 2019.

\bibitem[Duvenaud et~al.(2015)Duvenaud, Maclaurin, Iparraguirre, Bombarell,
  Hirzel, Aspuru-Guzik, and Adams]{CNN-on-molecules}
David~K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell,
  Timothy Hirzel, Al{\'a}n Aspuru-Guzik, and Ryan~P Adams.
\newblock Convolutional networks on graphs for learning molecular fingerprints.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Fan et~al.(2018)Fan, Yang, Li, Sun, Di, Li, Tang, and
  Liu]{Micronucleus}
Defang Fan, Hongbin Yang, Fuxing Li, Lixia Sun, Peiwen Di, Weihua Li, Yun Tang,
  and Guixia Liu.
\newblock In silico prediction of chemical genotoxicity using machine learning
  methods and structural alerts.
\newblock \emph{Toxicology research}, 7\penalty0 (2):\penalty0 211--220, 2018.

\bibitem[Glen et~al.(2006)Glen, Bender, Arnby, Carlsson, Boyer, and
  Smith]{circular-fingerprints}
Robert~C Glen, Andreas Bender, Catrin~H Arnby, Lars Carlsson, Scott Boyer, and
  James Smith.
\newblock Circular fingerprints: flexible molecular descriptors with
  applications from physical chemistry to adme.
\newblock \emph{IDrugs}, 9\penalty0 (3):\penalty0 199, 2006.

\bibitem[Hansen et~al.(2009)Hansen, Mika, Schroeter, Sutter, ter Laak,
  {Steger-Hartmann}, Heinrich, and M\"uller]{Ames}
Katja Hansen, Sebastian Mika, Timon Schroeter, Andreas Sutter, Antonius ter
  Laak, Thomas {Steger-Hartmann}, Nikolaus Heinrich, and {Klaus-Robert}
  M\"uller.
\newblock Benchmark data set for in silico prediction of ames mutagenicity.
\newblock \emph{Journal of Chemical Information and Modeling}, 49\penalty0
  (9):\penalty0 2077--2081, 2009.
\newblock \doi{10.1021/ci900161g}.
\newblock URL \url{http://dx.doi.org/10.1021/ci900161g}.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{MAE}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  16000--16009, 2022.

\bibitem[Hu et~al.(2020)Hu, Liu, Gomes, Zitnik, Liang, Pande, and
  Leskovec]{GIN}
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,
  and Jure Leskovec.
\newblock Strategies for pre-training graph neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HJlWWJSFDH}.

\bibitem[Irwin et~al.(2020)Irwin, Tang, Young, Dandarchuluun, Wong,
  Khurelbaatar, Moroz, Mayfield, and Sayle]{ZINC20}
John~J Irwin, Khanh~G Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin~R
  Wong, Munkhzul Khurelbaatar, Yurii~S Moroz, John Mayfield, and Roger~A Sayle.
\newblock Zinc20—a free ultralarge-scale chemical database for ligand
  discovery.
\newblock \emph{Journal of chemical information and modeling}, 60\penalty0
  (12):\penalty0 6065--6073, 2020.

\bibitem[Irwin et~al.(2022)Irwin, Dimitriadis, He, and Bjerrum]{chemformer}
Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben~Jannik Bjerrum.
\newblock Chemformer: a pre-trained transformer for computational chemistry.
\newblock \emph{Machine Learning: Science and Technology}, 3\penalty0
  (1):\penalty0 015022, 2022.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{SWA}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Jaeger et~al.(2018)Jaeger, Fulle, and Turk]{mol2vec}
Sabrina Jaeger, Simone Fulle, and Samo Turk.
\newblock Mol2vec: unsupervised machine learning approach with chemical
  intuition.
\newblock \emph{Journal of chemical information and modeling}, 58\penalty0
  (1):\penalty0 27--35, 2018.

\bibitem[Jiang et~al.(2021)Jiang, Wu, Hsieh, Chen, Liao, Wang, Shen, Cao, Wu,
  and Hou]{GNN-drug-comparison}
Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang,
  Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou.
\newblock Could graph neural networks learn better molecular representation for
  drug discovery? a comparison study of descriptor-based and graph-based
  models.
\newblock \emph{Journal of cheminformatics}, 13\penalty0 (1):\penalty0 1--23,
  2021.

\bibitem[Jin et~al.(2017)Jin, Coley, Barzilay, and Jaakkola]{jin2017predicting}
Wengong Jin, Connor Coley, Regina Barzilay, and Tommi Jaakkola.
\newblock Predicting organic reaction outcomes with weisfeiler-lehman network.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Kearnes et~al.(2016)Kearnes, McCloskey, Berndl, Pande, and
  Riley]{GCN-on-molecules}
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley.
\newblock Molecular graph convolutions: moving beyond fingerprints.
\newblock \emph{Journal of computer-aided molecular design}, 30\penalty0
  (8):\penalty0 595--608, 2016.

\bibitem[Kokhlikyan et~al.(2020)Kokhlikyan, Miglani, Martin, Wang, Alsallakh,
  Reynolds, Melnikov, Kliushkina, Araya, Yan, and Reblitz-Richardson]{captum}
Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,
  Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi
  Yan, and Orion Reblitz-Richardson.
\newblock Captum: A unified and generic model interpretability library for
  pytorch, 2020.

\bibitem[Kramer(2016)]{scikit}
Oliver Kramer.
\newblock Scikit-learn.
\newblock In \emph{Machine learning for evolution strategies}, pp.\  45--53.
  Springer, 2016.

\bibitem[Kudo \& Richardson(2018)Kudo and Richardson]{sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock \emph{arXiv preprint arXiv:1808.06226}, 2018.

\bibitem[Landrum et~al.(2013)]{RDKIT}
Greg Landrum et~al.
\newblock Rdkit: A software suite for cheminformatics, computational chemistry,
  and predictive modeling, 2013.

\bibitem[Lemke(2003)]{book-review-organic-functional-groups}
Thomas~L Lemke.
\newblock \emph{Review of organic functional groups: introduction to medicinal
  organic chemistry}.
\newblock Lippincott Williams \& Wilkins, 2003.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{BART}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Lewis et~al.(2020)Lewis, Ghazvininejad, Ghosh, Aghajanyan, Wang, and
  Zettlemoyer]{MARGE}
Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and
  Luke Zettlemoyer.
\newblock Pre-training via paraphrasing, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{ROBERTA}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Lowe(2012)]{lowe-USPTO-2012}
Daniel~Mark Lowe.
\newblock \emph{Extraction of chemical structures and reactions from the
  literature}.
\newblock PhD thesis, University of Cambridge, 2012.

\bibitem[Martins et~al.(2012)Martins, Teixeira, Pinheiro, and Falcao]{bbbp}
Ines~Filipa Martins, Ana~L Teixeira, Luis Pinheiro, and Andre~O Falcao.
\newblock A bayesian approach to in silico blood-brain barrier penetration
  modeling.
\newblock \emph{Journal of chemical information and modeling}, 52\penalty0
  (6):\penalty0 1686--1697, 2012.

\bibitem[Morgan(1965)]{morgan-fingerprints}
Harry~L Morgan.
\newblock The generation of a unique machine description for chemical
  structures-a technique developed at chemical abstracts service.
\newblock \emph{Journal of chemical documentation}, 5\penalty0 (2):\penalty0
  107--113, 1965.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1904.01038}, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems},
  32:\penalty0 8026--8037, 2019.

\bibitem[Plo{\v{s}}nik et~al.(2016)Plo{\v{s}}nik, Vra{\v{c}}ko, and
  Sollner~Dolenc]{tox-structural-alerts}
Alja Plo{\v{s}}nik, Marjan Vra{\v{c}}ko, and Marija Sollner~Dolenc.
\newblock Mutagenic and carcinogenic structural alerts and their mechanisms of
  action.
\newblock \emph{Arhiv za higijenu rada i toksikologiju}, 67\penalty0
  (3):\penalty0 169--182, 2016.

\bibitem[Preuer et~al.(2018)Preuer, Renz, Unterthiner, Hochreiter, and
  Klambauer]{FrechetChemnet}
Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter
  Klambauer.
\newblock Fr{\'e}chet chemnet distance: a metric for generative models for
  molecules in drug discovery.
\newblock \emph{Journal of chemical information and modeling}, 58\penalty0
  (9):\penalty0 1736--1741, 2018.

\bibitem[Radford et~al.()Radford, Kim, Xu, Brockman, McLeavey, and
  Sutskever]{WHISPER}
Alec Radford, Jong~Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
  Ilya Sutskever.
\newblock Robust speech recognition via large-scale weak supervision.

\bibitem[Radford et~al.(2017)Radford, Jozefowicz, and
  Sutskever]{radford2017learning}
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
\newblock Learning to generate reviews and discovering sentiment.
\newblock \emph{arXiv preprint arXiv:1704.01444}, 2017.

\bibitem[Rogers \& Hahn(2010)Rogers and Hahn]{ECFP-fingerprints}
David Rogers and Mathew Hahn.
\newblock Extended-connectivity fingerprints.
\newblock \emph{Journal of chemical information and modeling}, 50\penalty0
  (5):\penalty0 742--754, 2010.

\bibitem[Rong et~al.(2020)Rong, Bian, Xu, Xie, Wei, Huang, and Huang]{GROVER}
Yu~Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and
  Junzhou Huang.
\newblock Self-supervised graph transformer on large-scale molecular data.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12559--12571, 2020.

\bibitem[Ross et~al.(2021)Ross, Belgodere, Chenthamarakshan, Padhi, Mroueh, and
  Das]{MolFormer}
Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef
  Mroueh, and Payel Das.
\newblock Do large scale molecular language representations capture important
  structural information?
\newblock \emph{arXiv preprint arXiv:2106.09553}, 2021.

\bibitem[Schwaller et~al.(2018)Schwaller, Gaudin, Lanyi, Bekas, and
  Laino]{reaction-prediction-seq2seq}
Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, and Teodoro
  Laino.
\newblock “found in translation”: predicting outcomes of complex organic
  chemistry reactions using neural sequence-to-sequence models.
\newblock \emph{Chemical science}, 9\penalty0 (28):\penalty0 6091--6098, 2018.

\bibitem[Schwaller et~al.(2019)Schwaller, Laino, Gaudin, Bolgar, Hunter, Bekas,
  and Lee]{molecular-transformer}
Philippe Schwaller, Teodoro Laino, Th{\'e}ophile Gaudin, Peter Bolgar,
  Christopher~A Hunter, Costas Bekas, and Alpha~A Lee.
\newblock Molecular transformer: a model for uncertainty-calibrated chemical
  reaction prediction.
\newblock \emph{ACS central science}, 5\penalty0 (9):\penalty0 1572--1583,
  2019.

\bibitem[Somnath et~al.(2020)Somnath, Bunne, Coley, Krause, and
  Barzilay]{graph_retro}
Vignesh~Ram Somnath, Charlotte Bunne, Connor~W Coley, Andreas Krause, and
  Regina Barzilay.
\newblock Learning graph models for template-free retrosynthesis.
\newblock \emph{arXiv preprint arXiv:2006.07038}, 2020.

\bibitem[St{\"a}rk et~al.(2022)St{\"a}rk, Beaini, Corso, Tossou, Dallago,
  G{\"u}nnemann, and Li{\`o}]{3dinfomaxGNN}
Hannes St{\"a}rk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian
  Dallago, Stephan G{\"u}nnemann, and Pietro Li{\`o}.
\newblock 3d infomax improves gnns for molecular property prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  20479--20502. PMLR, 2022.

\bibitem[Sun et~al.(2020)Sun, Dai, Li, Kearnes, and Dai]{retrosynthesis_energy}
Ruoxi Sun, Hanjun Dai, Li~Li, Steven Kearnes, and Bo~Dai.
\newblock Energy-based view of retrosynthesis.
\newblock \emph{arXiv preprint arXiv:2007.13437}, 2020.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and
  Yan]{integrated-gradients}
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In \emph{International conference on machine learning}, pp.\
  3319--3328. PMLR, 2017.

\bibitem[Tetko et~al.(2020)Tetko, Karpov, Van~Deursen, and
  Godin]{augmented_transformer}
Igor~V Tetko, Pavel Karpov, Ruud Van~Deursen, and Guillaume Godin.
\newblock State-of-the-art augmented nlp transformer models for direct and
  single-step retrosynthesis.
\newblock \emph{Nature communications}, 11\penalty0 (1):\penalty0 1--11, 2020.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{GLUE}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
  Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
  Analyzing and Interpreting Neural Networks for {NLP}}, pp.\  353--355,
  Brussels, Belgium, November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-5446}.
\newblock URL \url{https://www.aclweb.org/anthology/W18-5446}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Li, Jin, Cho, Ji, Han, and
  Burke]{reaction-aware-molRL}
Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han,
  and Martin Burke.
\newblock Chemical-reaction-aware molecule representation learning.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=6sh3pIzKS-}.

\bibitem[Wang et~al.(2019)Wang, Guo, Wang, Sun, and Huang]{SMILES_BERT}
Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang.
\newblock Smiles-bert: large scale unsupervised pre-training for molecular
  property prediction.
\newblock In \emph{Proceedings of the 10th ACM international conference on
  bioinformatics, computational biology and health informatics}, pp.\
  429--436, 2019.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Roberts, Hesslow, Scao, Chung,
  Beltagy, Launay, and Raffel]{lm_objective_architecture}
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven~Le Scao, Hyung~Won Chung,
  Iz~Beltagy, Julien Launay, and Colin Raffel.
\newblock What language model architecture and pretraining objective work best
  for zero-shot generalization?
\newblock \emph{arXiv preprint arXiv:2204.05832}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Magar, Liang, and
  Farimani]{iMolCLR}
Yuyang Wang, Rishikesh Magar, Chen Liang, and Amir~Barati Farimani.
\newblock Improving molecular contrastive learning via faulty negative
  mitigation and decomposed fragment contrast.
\newblock \emph{Journal of Chemical Information and Modeling}, 59\penalty0
  (8):\penalty0 3370--3388, 2022{\natexlab{c}}.
\newblock \doi{10.1021/acs.jcim.2c00495}.

\bibitem[Wang et~al.(2022{\natexlab{d}})Wang, Wang, Cao, and
  Barati~Farimani]{MolCLR}
Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati~Farimani.
\newblock Molecular contrastive learning of representations via graph neural
  networks.
\newblock \emph{Nature Machine Intelligence}, 4\penalty0 (3):\penalty0
  279--287, 2022{\natexlab{d}}.

\bibitem[Watson et~al.(2019)Watson, Wang, and
  Nicolaou]{watson2019retrosynthetic}
Ian~A Watson, Jibo Wang, and Christos~A Nicolaou.
\newblock A retrosynthetic analysis algorithm implementation.
\newblock \emph{Journal of cheminformatics}, 11\penalty0 (1):\penalty0 1--12,
  2019.

\bibitem[Wu et~al.(2018)Wu, Ramsundar, Feinberg, Gomes, Geniesse, Pappu,
  Leswing, and Pande]{moleculenet}
Zhenqin Wu, Bharath Ramsundar, Evan~N Feinberg, Joseph Gomes, Caleb Geniesse,
  Aneesh~S Pappu, Karl Leswing, and Vijay Pande.
\newblock Moleculenet: a benchmark for molecular machine learning.
\newblock \emph{Chemical science}, 9\penalty0 (2):\penalty0 513--530, 2018.

\bibitem[Xiong et~al.(2019)Xiong, Wang, Liu, Zhong, Wan, Li, Li, Luo, Chen,
  Jiang, et~al.]{AttentiveFP}
Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong
  Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et~al.
\newblock Pushing the boundaries of molecular representation for drug discovery
  with the graph attention mechanism.
\newblock \emph{Journal of medicinal chemistry}, 63\penalty0 (16):\penalty0
  8749--8760, 2019.

\bibitem[Yan et~al.(2021)Yan, Zhao, Lu, Yu, and Huang]{retrocomposer}
Chaochao Yan, Peilin Zhao, Chan Lu, Yang Yu, and Junzhou Huang.
\newblock Retrocomposer: Discovering novel reactions by composing templates for
  retrosynthesis prediction.
\newblock \emph{arXiv preprint arXiv:2112.11225}, 2021.

\bibitem[Yang et~al.(2019)Yang, Swanson, Jin, Coley, Eiden, Gao, Guzman-Perez,
  Hopper, Kelley, Mathea, et~al.]{DMPNN-original}
Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao,
  Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et~al.
\newblock Analyzing learned molecular representations for property prediction.
\newblock \emph{Journal of chemical information and modeling}, 59\penalty0
  (8):\penalty0 3370--3388, 2019.

\end{thebibliography}
