\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and
  Manning]{bowman2015large}
Bowman, S., Angeli, G., Potts, C., and Manning, C.~D.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  632--642, 2015.

\bibitem[Breiman(1996)]{breiman1996bagging}
Breiman, L.
\newblock Bagging predictors.
\newblock \emph{Machine learning}, 24\penalty0 (2):\penalty0 123--140, 1996.

\bibitem[Breiman(2001)]{breiman2001random}
Breiman, L.
\newblock Random forests.
\newblock \emph{Machine learning}, 45\penalty0 (1):\penalty0 5--32, 2001.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chai et~al.(2022)Chai, Wang, Sun, Tian, Wu, and Wang]{chai2022clip}
Chai, Y., Wang, S., Sun, Y., Tian, H., Wu, H., and Wang, H.
\newblock Clip-tuning: Towards derivative-free prompt learning with a mixture
  of rewards.
\newblock \emph{arXiv preprint arXiv:2210.12050}, 2022.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{dagan2005pascal}
Dagan, I., Glickman, O., and Magnini, B.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine learning challenges workshop}, pp.\  177--190, 2005.

\bibitem[Deng et~al.(2022)Deng, Wang, Hsieh, Wang, Guo, Shu, Song, Xing, and
  Hu]{deng2022rlprompt}
Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing,
  E.~P., and Hu, Z.
\newblock Rlprompt: Optimizing discrete text prompts with reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2205.12548}, 2022.

\bibitem[Diao et~al.(2022)Diao, Li, Lin, Huang, and Zhang]{diao2022black}
Diao, S., Li, X., Lin, Y., Huang, Z., and Zhang, T.
\newblock Black-box prompt learning for pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2201.08531}, 2022.

\bibitem[Dolan \& Brockett(2005)Dolan and Brockett]{dolan2005automatically}
Dolan, B. and Brockett, C.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Third International Workshop on Paraphrasing (IWP2005)},
  2005.

\bibitem[Freund \& Schapire(1997)Freund and Schapire]{freund1997decision}
Freund, Y. and Schapire, R.~E.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55:\penalty0
  119--139, 1997.

\bibitem[Friedman(2001)]{friedman2001greedy}
Friedman, J.~H.
\newblock Greedy function approximation: a gradient boosting machine.
\newblock \emph{Annals of statistics}, pp.\  1189--1232, 2001.

\bibitem[Gao et~al.(2021)Gao, Fisch, and Chen]{gao2021making}
Gao, T., Fisch, A., and Chen, D.
\newblock Making pre-trained language models better few-shot learners.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  3816--3830, 2021.

\bibitem[Han et~al.(2021)Han, Zhao, Ding, Liu, and Sun]{han2021ptr}
Han, X., Zhao, W., Ding, N., Liu, Z., and Sun, M.
\newblock Ptr: Prompt tuning with rules for text classification.
\newblock \emph{arXiv preprint arXiv:2105.11259}, 2021.

\bibitem[Hu \& Liu(2004)Hu and Liu]{hu2004mining}
Hu, M. and Liu, B.
\newblock Mining and summarizing customer reviews.
\newblock In \emph{Proceedings of the tenth ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  168--177, 2004.

\bibitem[Jiang et~al.(2020)Jiang, Xu, Araki, and Neubig]{jiang2020can}
Jiang, Z., Xu, F.~F., Araki, J., and Neubig, G.
\newblock How can we know what language models know?
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 423--438, 2020.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Lester, B., Al-Rfou, R., and Constant, N.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  3045--3059, 2021.

\bibitem[Li \& Liang(2021)Li and Liang]{li2021prefix}
Li, X.~L. and Liang, P.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  4582--4597, 2021.

\bibitem[Liu et~al.(2021)Liu, Zheng, Du, Ding, Qian, Yang, and
  Tang]{liu2021gpt}
Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J.
\newblock Gpt understands, too.
\newblock \emph{arXiv:2103.10385}, 2021.

\bibitem[Liu et~al.(2022)Liu, Ji, Fu, Tam, Du, Yang, and
  Tang]{liu2022ptuningv2}
Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang, J.
\newblock {P}-tuning: Prompt tuning can be comparable to fine-tuning across
  scales and tasks.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, 2022.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Pang \& Lee(2004)Pang and Lee]{pang2004sentimental}
Pang, B. and Lee, L.
\newblock A sentimental education: Sentiment analysis using subjectivity
  summarization based on minimum cuts.
\newblock \emph{arXiv preprint cs/0409058}, 2004.

\bibitem[Pang \& Lee(2005)Pang and Lee]{pang2005seeing}
Pang, B. and Lee, L.
\newblock Seeing stars: Exploiting class relationships for sentiment
  categorization with respect to rating scales.
\newblock In \emph{Proceedings of the 43rd Annual Meeting of the Association
  for Computational Linguistics (ACL’05)}, pp.\  115--124, 2005.

\bibitem[Prasad et~al.(2022)Prasad, Hase, Zhou, and Bansal]{prasad2022grips}
Prasad, A., Hase, P., Zhou, X., and Bansal, M.
\newblock Grips: Gradient-free, edit-based instruction search for prompting
  large language models.
\newblock \emph{arXiv preprint arXiv:2203.07281}, 2022.

\bibitem[Qin \& Eisner(2021)Qin and Eisner]{qin2021learning}
Qin, G. and Eisner, J.
\newblock Learning how to ask: Querying lms with mixtures of soft prompts.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  5203--5212, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, Liu, et~al.]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., Liu, P.~J., et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2383--2392, 2016.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Sanh, V., Debut, L., Chaumond, J., and Wolf, T.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock In \emph{NeurIPS $EMC^2$ Workshop}, 2019.

\bibitem[Schick \& Sch{\"u}tze(2020)Schick and Sch{\"u}tze]{schick2020few}
Schick, T. and Sch{\"u}tze, H.
\newblock Few-shot text generation with pattern-exploiting training.
\newblock \emph{arXiv preprint arXiv:2012.11926}, 2020.

\bibitem[Schick \& Sch{\"u}tze(2021{\natexlab{a}})Schick and
  Sch{\"u}tze]{schick2021exploiting}
Schick, T. and Sch{\"u}tze, H.
\newblock Exploiting cloze-questions for few-shot text classification and
  natural language inference.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume}, pp.\
  255--269, 2021{\natexlab{a}}.

\bibitem[Schick \& Sch{\"u}tze(2021{\natexlab{b}})Schick and
  Sch{\"u}tze]{schick2021s}
Schick, T. and Sch{\"u}tze, H.
\newblock It’s not just size that matters: Small language models are also
  few-shot learners.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  2339--2352, 2021{\natexlab{b}}.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and
  Singh]{shin2020autoprompt}
Shin, T., Razeghi, Y., Logan~IV, R.~L., Wallace, E., and Singh, S.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  4222--4235, 2020.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A.~Y., and
  Potts, C.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pp.\  1631--1642, 2013.

\bibitem[Sun et~al.(2022{\natexlab{a}})Sun, He, Qian, Zhou, Huang, and
  Qiu]{sun2022bbtv2}
Sun, T., He, Z., Qian, H., Zhou, Y., Huang, X., and Qiu, X.
\newblock Bbtv2: Towards a gradient-free future with large language models.
\newblock In \emph{Proceedings of {EMNLP}}, 2022{\natexlab{a}}.

\bibitem[Sun et~al.(2022{\natexlab{b}})Sun, Shao, Qian, Huang, and
  Qiu]{sun2022bbt}
Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X.
\newblock Black-box tuning for language-model-as-a-service.
\newblock In \emph{Proceedings of {ICML}}, 2022{\natexlab{b}}.

\bibitem[Tram{\`e}r et~al.(2016)Tram{\`e}r, Zhang, Juels, Reiter, and
  Ristenpart]{tramer2016stealing}
Tram{\`e}r, F., Zhang, F., Juels, A., Reiter, M.~K., and Ristenpart, T.
\newblock Stealing machine learning models via prediction $\{$APIs$\}$.
\newblock In \emph{25th USENIX security symposium (USENIX Security 16)}, pp.\
  601--618, 2016.

\bibitem[Voorhees \& Tice(2000)Voorhees and Tice]{voorhees2000building}
Voorhees, E.~M. and Tice, D.~M.
\newblock Building a question answering test collection.
\newblock In \emph{Proceedings of the 23rd annual international ACM SIGIR
  conference on Research and development in information retrieval}, pp.\
  200--207, 2000.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
  Analyzing and Interpreting Neural Networks for NLP}, 2018.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and
  Zhou]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem[Wiebe et~al.(2005)Wiebe, Wilson, and Cardie]{wiebe2005annotating}
Wiebe, J., Wilson, T., and Cardie, C.
\newblock Annotating expressions of opinions and emotions in language.
\newblock \emph{Language resources and evaluation}, 2005.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2018broad}
Williams, A., Nangia, N., and Bowman, S.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pp.\  1112--1122, 2018.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Yuan et~al.(2021)Yuan, Neubig, and Liu]{yuan2021bartscore}
Yuan, W., Neubig, G., and Liu, P.
\newblock Bartscore: Evaluating generated text as text generation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 27263--27277, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Li, Chen, Deng, Bi, Tan, Huang, and
  Chen]{zhang2021differentiable}
Zhang, N., Li, L., Chen, X., Deng, S., Bi, Z., Tan, C., Huang, F., and Chen, H.
\newblock Differentiable prompt makes pre-trained language models better
  few-shot learners.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Wang, Zhou, Schuurmans, and
  Gonzalez]{zhang2022tempera}
Zhang, T., Wang, X., Zhou, D., Schuurmans, D., and Gonzalez, J.~E.
\newblock Tempera: Test-time prompting via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2211.11890}, 2022.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{Zhang2015CharacterlevelCN}
Zhang, X., Zhao, J.~J., and LeCun, Y.
\newblock Character-level convolutional networks for text classification.
\newblock In \emph{NIPS}, 2015.

\end{thebibliography}
