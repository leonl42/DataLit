\section{Related Works}
%Conventionally, the contextual linear bandit problem only involves one learning agent, and 
Most existing bandit solutions assume a centralized learning setting, where data is readily available at a central server. Classical linear bandit algorithms, like LinUCB \citep{li2010contextual,abbasi2011improved} and LinTS \citep{agrawal2013thompson,abeille2017linear} %attain $O(\sqrt{T}\log{T})$ regret upper bound, which matches the $\Omega(\sqrt{T})$ minimax lower bound (up to a logarithmic factor) \citep{lattimore2020bandit}. But they 
only concern a single learning agent.  
Multi-agent bandits mostly focus on customizing algorithms that leverage relationships among the agents for collaborative learning \citep{cesa2013gang,wang2019distributed,gentile2014online,cesa2013gang,wu2016contextual,li2021unifying}, but the data about all agents is still on the central server.
%For example, graph Laplacian is used to capture known agent dependency to regularize each agent's model estimation \cite{cesa2013gang,yang2020laplacian}; and  agents' learned models are clustered for observation sharing \cite{gentile2014online,li2016collaborative,gentile2017context,li2019improved}. However, 
%For example, the series of works following CLUB assume the sub-problems form groups that is unknown to the learner, and different clustering methods are proposed to cluster the learned models during the interaction with the environment for improved model estimation \cite{gentile2014online,li2016collaborative,gentile2017context,li2019improved}. Another line of works assume the learner has information about the sub-problem relationships, e.g. friends in social network applications, and collaborative learning is achieved via applying regularization when optimizing each sub-problem based on the known relationships \cite{cesa2013gang,wu2016contextual,yang2020laplacian}.

Distributed bandit
% The most related work in multi-agent linear bandits to ours is the distributed bandits
\citep{korda2016distributed,wang2019distributed,dubey2020differentially} is the most relevant to ours, where designing an efficient communication strategy is the main focus. Existing algorithms mainly differ in the relations of learning problems solved by the agents (i.e., identical vs., clustered) and the type of communication network (i.e., peer-to-peer (P2P) vs., star-shaped). \citet{korda2016distributed} studied two problem settings with a P2P communication network: 1) all the agents solve a common linear bandit problem, and 2) the problems are clustered. However, they only tried to reduce \textit{per-round} communication, and thus the communication cost is still linear over time.
Two follow-up studies considered the setting where all agents solve a common problem and interact with the environment in a round-robin fashion \citep{wang2019distributed,dubey2020differentially}. Similar to our work, they also used event-triggered communications to obtain a sub-linear communication cost over time.
% In particular, \citet{wang2019distributed} worked under a star-shaped communication network, and designed an event-triggered communication based on the determinant of the covariance matrix, which is similar to the method proposed in our paper.
% However, it requires global synchronization in which all clients exchange their latest observations under the central server's control. 
%that when the determinant of covariance matrix for a client varies too much from that of the most recent synchronization, a new synchronization round will be triggered, so that each client will send its latest sufficient statistics to the server, and the server will send the aggregated sufficient statistics back to all the clients.
% \citet{dubey2020differentially} extended this synchronous protocol to  peer-to-peer network, with differential privacy.
In particular, \citet{wang2019distributed} considered a star-shaped network and proposed a synchronous communication protocol for all clients to exchange their latest observations under the central server's control. 
\citet{dubey2020differentially} extended this synchronous protocol to differentially private LinUCB algorithms under both star-shaped and P2P network.

There is also a rich literature in distributed machine learning/federated learning \citep{mcmahan2017communication,li2019convergence} that studies offline optimization problems. However, as we mentioned earlier, due to the fundamental difference in the learning objectives, they are not applicable to our problem. Specifically, their main focus is to collaboratively learn a good \textit{point estimate} over a fixed dataset, i.e., convergence to the minimizer with fewer communications, while the focus of federated bandit learning is collaborative \textit{confidence interval estimation} for efficient regret reduction, which requires exploration of the unknown data. 
This is also reflected by the difference in the triggering event designs. For distributed offline optimization, triggering event measuring the change in the learned parameters suffices \citep{kia2015distributed,yi2018distributed,george2020distributed}, while for federated bandit learning, triggering event needs to measure change in the volume of the confidence region, i.e., uncertainty in the problem space. This adds serious challenges in the design and analysis of the triggering events. In addition, for linear bandit problems, thanks to the existence of the closed-form solution, there is no need to use gradient-based optimization methods like FedAvg \citep{mcmahan2017communication}, because compared with transmitting the sufficient statistics, it only costs a lot more communication overhead without bringing in any gain in regret minimization.

% Therefore, though there are works in distributed optimization literature that studies event-triggered communications \citep{kia2015distributed,yi2018distributed,george2020distributed}, 

% It is also worth noting that event-triggered communications are commonly used in distributed optimization literature \citep{kia2015distributed,yi2018distributed,george2020distributed}. But we want to emphasize that due to the fundamental difference in the learning objectives, they are not applicable to our problem. As the purpose of distributed optimization is to collaboratively learn a good \textit{point estimate} with fewer communications, a triggering event measuring the change in the learned parameters suffices. However, for federated bandit learning, the purpose is collaborative \textit{confidence interval estimation} for efficient regret reduction, so that the triggering event needs to be designed based on the volume of the confidence region, i.e., the determinant of covariance matrices. This adds serious challenges in the design and analysis of the triggering events.