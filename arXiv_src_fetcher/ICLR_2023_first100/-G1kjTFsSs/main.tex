\documentclass[twoside,10pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Chuanhao Li and Hongning Wang}

% Short headings should be running head and authors last names

\ShortHeadings{Asynchronous Algorithms for Federated Linear Bandit}{Li and Wang}
\firstpageno{1}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{wrapfig}

% Additional packages
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage[section]{placeins}
\usepackage{enumitem}

\newcommand{\modelone}{{Async-LinUCB}}
\newcommand{\modeltwo}{{Async-LinUCB-AM}}
\newcommand{\modelbaseline}{{Sync-LinUCB}}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{corollary}{Corollary}[theorem]
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\def \cU {\mathcal{U}}
\def \cD {\mathcal{D}}
\def \cN {\mathcal{N}}
\def \bU {\mathbb{U}}
\def \bO {\mathbb{O}}
\def \bM {\mathbb{M}}
\def \bH {\mathbb{H}}
\def \cH {\mathcal{H}}
\def \cF {\mathcal{F}}
\def \bA {\mathbf{A}}
\def \bx {\mathbf{x}}
\def \bX {\mathbf{X}}
\def \bY {\mathbf{Y}}
\def \by {\mathbf{y}}
\def \cX {\mathcal{X}}
\def \cA {\mathcal{A}}
\def \cI {\mathcal{I}}
\def \bb {\mathbf{b}}
\def \bp {\mathbf{p}}
\def \bI {\mathbf{I}}
\def \bR {\mathbb{R}}
\def \bE {\mathbb{E}}
\def \bbE {\mathbf{E}}
\def \cE {\mathcal{E}}
\def \cS {\mathcal{S}}


\begin{document}

\title{Asynchronous Upper Confidence Bound Algorithms for Federated Linear Bandits}

\author{\name Chuanhao Li \email cl5ev@virginia.edu \\
       \addr Department of Computer Science\\
       University of Virginia\\
       Charlottesville, VA 22903, USA\\
       \AND
       \name Hongning Wang \email hw5x@virginia.edu \\
       \addr Department of Computer Science\\
       University of Virginia\\
       Charlottesville, VA 22903, USA
       }

% \editor{Leslie Pack Kaelbling}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Linear contextual bandit is a popular online learning problem. It has been mostly studied in centralized learning settings. With the surging demand of large-scale decentralized model learning, e.g., federated learning, how to retain regret minimization while reducing communication cost becomes an open challenge. 
In this paper, we study linear contextual bandit in a federated learning setting. We propose a general framework with asynchronous model update and communication for a collection of homogeneous clients and heterogeneous clients, respectively. Rigorous theoretical analysis is provided about the regret and communication cost under this distributed learning framework; and extensive empirical evaluations demonstrate the effectiveness of our solution.
\end{abstract}

\begin{keywords}
  Contextual bandit, Federated learning, Asynchronous Communication
\end{keywords}

\input{intro}
\input{related_works}
\input{method}
\input{exp}

\section{Conclusion}
%more and more people are reluctant to provide their own data and strict regulations on data usage like GDPR have also went into effect \cite{voigt2017eu}, which makes 
% The omnipresence of intelligent system in modern life has brought convenience but also increasing public awareness about data privacy. Coupled with the growing computaional power of mobile devices, decentralized learning method like federated learning has emerged as a promising solution to this dilemma.
In this paper, we propose an asynchronous event-triggered communication framework for federated linear bandit problem, which offers a flexible way to balance regret and communication cost. 
Based on this communication framework, two UCB-type algorithms are proposed for homogeneous clients and the more challenging heterogeneous clients, respectively.
From a theoretical aspect, we prove rigorously that our algorithm strikes a better tradeoff between regret and communication cost than existing works in the general case when the distribution over clients is non-uniform. From a practical aspect, 
%compared with existing works that all require synchronization of the clients, 
ours is the first asynchronous method for federated linear bandit. It is more robust against lagging communications, which are often inevitable in reality, and handles heterogeneity in different clients' learning tasks. Hence, it has greater potential in large-scale decentralized applications.
% we identified three challenges in applying bandit learning under the federated learning setting: 1. the trade-off between regret and communication cost; 2. 

% From the discussion in Section \ref{subsec:async_comm}, we know 
% The $\Omega(d\sqrt{T})$ minimax lower regret bound of standard linear bandit also holds in federated setting. However, the optimal trade-off between regret and communication cost is still unknown, e.g., lower bound on communication cost for consistent algorithms (with subpolynomial regret). 
The optimal trade-off between regret and communication cost is still unknown for this problem, e.g., lower bound on communication cost for a certain rate of regret.
Another interesting direction is a differential-private version of the proposed asynchronous algorithms, e.g., trading off regret and communication cost under given privacy budget. 
%e.g., carefully perturbing the transferred parameters, with the tree-based mechanism. 

% \bibliographystyle{plain}
\bibliography{bibfile}

% \newpage
\input{appendix}

\end{document}