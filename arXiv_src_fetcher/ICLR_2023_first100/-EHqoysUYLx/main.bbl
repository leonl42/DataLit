\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asad et~al.(2020)Asad, Moustafa, and Ito]{asad2020fedopt}
Muhammad Asad, Ahmed Moustafa, and Takayuki Ito.
\newblock Fedopt: Towards communication efficiency and privacy preservation in federated learning.
\newblock \emph{Applied Sciences}, 10\penalty0 (8):\penalty0 2864, 2020.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, Eckstein, et~al.]{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine learning}, 3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Caldarola et~al.(2022)Caldarola, Caputo, and Ciccone]{caldarola2022improving}
Debora Caldarola, Barbara Caputo, and Marco Ciccone.
\newblock Improving generalization in federated learning by seeking flat minima.
\newblock In \emph{European Conference on Computer Vision}, pages 654--672. Springer, 2022.

\bibitem[Caldarola et~al.(2023)Caldarola, Caputo, and Ciccone]{caldarola2023window}
Debora Caldarola, Barbara Caputo, and Marco Ciccone.
\newblock Window-based model averaging improves generalization in heterogeneous federated learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2263--2271, 2023.

\bibitem[Charles and Kone{\v{c}}n{\`y}(2021)]{inconsistency1}
Zachary Charles and Jakub Kone{\v{c}}n{\`y}.
\newblock Convergence and accuracy trade-offs in federated learning and meta-learning.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 2575--2583. PMLR, 2021.

\bibitem[Condat and Richt{\'a}rik(2022)]{condat2022randprox}
Laurent Condat and Peter Richt{\'a}rik.
\newblock Randprox: Primal-dual optimization algorithms with randomized proximal updates.
\newblock \emph{arXiv preprint arXiv:2207.12891}, 2022.

\bibitem[Condat et~al.(2023)Condat, Agarsk{\`y}, Malinovsky, and Richt{\'a}rik]{condat2023tamuna}
Laurent Condat, Ivan Agarsk{\`y}, Grigory Malinovsky, and Peter Richt{\'a}rik.
\newblock Tamuna: Doubly accelerated federated learning with local training, compression, and partial participation.
\newblock \emph{arXiv preprint arXiv:2302.09832}, 2023.

\bibitem[Dieuleveut et~al.(2021)Dieuleveut, Fort, Moulines, and Robin]{dieuleveut2021federated}
Aymeric Dieuleveut, Gersende Fort, Eric Moulines, and Genevi{\`e}ve Robin.
\newblock Federated-em with heterogeneity mitigation and variance reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 29553--29566, 2021.

\bibitem[Durmus et~al.(2021)Durmus, Yue, Ramon, Matthew, Paul, and Venkatesh]{durmus2021federated}
Alp~Emre Durmus, Zhao Yue, Matas Ramon, Mattina Matthew, Whatmough Paul, and Saligrama Venkatesh.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fan et~al.(2022)Fan, Wang, Yao, Lyu, Zhang, and Tian]{fan2022fedskip}
Ziqing Fan, Yanfeng Wang, Jiangchao Yao, Lingjuan Lyu, Ya~Zhang, and Qi~Tian.
\newblock Fedskip: Combatting statistical heterogeneity with federated skip aggregation.
\newblock In \emph{2022 IEEE International Conference on Data Mining (ICDM)}, pages 131--140. IEEE, 2022.

\bibitem[Fan et~al.(2023)Fan, Yao, Zhang, Lyu, Wang, and Zhang]{fan2023federated}
Ziqing Fan, Jiangchao Yao, Ruipeng Zhang, Lingjuan Lyu, Yanfeng Wang, and Ya~Zhang.
\newblock Federated learning under partially disjoint data via manifold reshaping.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem[Fan et~al.(2024{\natexlab{a}})Fan, Hu, Yao, Niu, Zhang, Sugiyama, and Wang]{fan2024locally}
Ziqing Fan, Shengchao Hu, Jiangchao Yao, Gang Niu, Ya~Zhang, Masashi Sugiyama, and Yanfeng Wang.
\newblock Locally estimated global perturbations are better than local perturbations for federated sharpness-aware minimization.
\newblock \emph{arXiv preprint arXiv:2405.18890}, 2024{\natexlab{a}}.

\bibitem[Fan et~al.(2024{\natexlab{b}})Fan, Yao, Han, Zhang, Wang, et~al.]{fan2024federated}
Ziqing Fan, Jiangchao Yao, Bo~Han, Ya~Zhang, Yanfeng Wang, et~al.
\newblock Federated learning with bilateral curation for partially class-disjoint data.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving generalization.
\newblock \emph{arXiv preprint arXiv:2010.01412}, 2020.

\bibitem[Gong et~al.(2022)Gong, Li, and Freris]{gong2022fedadmm}
Yonghai Gong, Yichuan Li, and Nikolaos~M Freris.
\newblock Fedadmm: A robust federated deep learning framework with adaptivity to system heterogeneity.
\newblock In \emph{2022 IEEE 38th International Conference on Data Engineering (ICDE)}, pages 2575--2587. IEEE, 2022.

\bibitem[Grudzie{\'n} et~al.(2023{\natexlab{a}})Grudzie{\'n}, Malinovsky, and Richt{\'a}rik]{grudzien2023can}
Micha{\l} Grudzie{\'n}, Grigory Malinovsky, and Peter Richt{\'a}rik.
\newblock Can 5th generation local training methods support client sampling? yes!
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1055--1092. PMLR, 2023{\natexlab{a}}.

\bibitem[Grudzie{\'n} et~al.(2023{\natexlab{b}})Grudzie{\'n}, Malinovsky, and Richt{\'a}rik]{grudzien2023improving}
Micha{\l} Grudzie{\'n}, Grigory Malinovsky, and Peter Richt{\'a}rik.
\newblock Improving accelerated federated learning with compression and importance sampling.
\newblock \emph{arXiv preprint arXiv:2306.03240}, 2023{\natexlab{b}}.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient descent.
\newblock In \emph{International conference on machine learning}, pages 1225--1234. PMLR, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem[Horv{\'a}th et~al.(2022)Horv{\'a}th, Sanjabi, Xiao, Richt{\'a}rik, and Rabbat]{horvath2022fedshuffle}
Samuel Horv{\'a}th, Maziar Sanjabi, Lin Xiao, Peter Richt{\'a}rik, and Michael Rabbat.
\newblock Fedshuffle: Recipes for better use of local work in federated learning.
\newblock \emph{arXiv preprint arXiv:2204.13169}, 2022.

\bibitem[Hsu et~al.(2019)Hsu, Qi, and Brown]{hsu2019measuring}
Tzu-Ming~Harry Hsu, Hang Qi, and Matthew Brown.
\newblock Measuring the effects of non-identical data distribution for federated visual classification.
\newblock \emph{arXiv preprint arXiv:1909.06335}, 2019.

\bibitem[Hu et~al.(2022)Hu, Li, and Liu]{hu2022generalization}
Xiaolin Hu, Shaojie Li, and Yong Liu.
\newblock Generalization bounds for federated learning: Fast rates, unparticipating clients and unbounded losses.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Jhunjhunwala et~al.(2022)Jhunjhunwala, Sharma, Nagarkatti, and Joshi]{jhunjhunwala2022fedvarp}
Divyansh Jhunjhunwala, Pranay Sharma, Aushim Nagarkatti, and Gauri Joshi.
\newblock Fedvarp: Tackling the variance due to partial client participation in federated learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 906--916. PMLR, 2022.

\bibitem[Kang et~al.(2024)Kang, Kim, Lee, and Kim]{kang2024fedand}
Heejoo Kang, Minsoo Kim, Bumsuk Lee, and Hongseok Kim.
\newblock Fedand: Federated learning exploiting consensus admm by nulling drift.
\newblock \emph{IEEE Transactions on Industrial Informatics}, 2024.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and Suresh]{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International conference on machine learning}, pages 5132--5143. PMLR, 2020.

\bibitem[Kim et~al.(2022)Kim, Kim, and Han]{kim2022communication}
Geeho Kim, Jinkyu Kim, and Bohyung Han.
\newblock Communication-efficient federated learning with acceleration of global momentum.
\newblock \emph{arXiv preprint arXiv:2201.03172}, 2022.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Lei and Ying(2020)]{lei2020fine}
Yunwen Lei and Yiming Ying.
\newblock Fine-grained analysis of stability and generalization for stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages 5809--5819. PMLR, 2020.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Shen, Li, Yin, and Tao]{li2023dfedadmm}
Qinglun Li, Li~Shen, Guanghao Li, Quanjun Yin, and Dacheng Tao.
\newblock Dfedadmm: Dual constraints controlled model inconsistency for decentralized federated learning.
\newblock \emph{arXiv preprint arXiv:2308.08290}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and Smith]{li2020federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine learning and systems}, 2:\penalty0 429--450, 2020.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Huang, Wang, Chi, and Quek]{li2023privacy}
Yiwei Li, Chien-Wei Huang, Shuai Wang, Chong-Yung Chi, and Tony~QS Quek.
\newblock Privacy-preserving federated primal-dual learning for non-convex problems with non-smooth regularization.
\newblock In \emph{2023 IEEE 33rd International Workshop on Machine Learning for Signal Processing (MLSP)}, pages 1--6. IEEE, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023)Liu, Sun, Ding, Shen, Liu, and Tao]{liu2023enhance}
Yixing Liu, Yan Sun, Zhengtao Ding, Li~Shen, Bo~Liu, and Dacheng Tao.
\newblock Enhance local consistency in federated learning: A multi-step inertial momentum approach.
\newblock \emph{arXiv preprint arXiv:2302.05726}, 2023.

\bibitem[Malinovskiy et~al.(2020)Malinovskiy, Kovalev, Gasanov, Condat, and Richtarik]{inconsistency2}
Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, and Peter Richtarik.
\newblock From local sgd to local fixed-point methods for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages 6692--6701. PMLR, 2020.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282. PMLR, 2017.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Malinovsky, Stich, and Richt{\'a}rik]{mishchenko2022proxskip}
Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richt{\'a}rik.
\newblock Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally!
\newblock In \emph{International Conference on Machine Learning}, pages 15750--15769. PMLR, 2022.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mehryar Mohri, Gary Sivek, and Ananda~Theertha Suresh.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages 4615--4625. PMLR, 2019.

\bibitem[Niu and Wei(2023)]{niu2023fedhybrid}
Xiaochun Niu and Ermin Wei.
\newblock Fedhybrid: A hybrid federated optimization method for heterogeneous clients.
\newblock \emph{IEEE Transactions on Signal Processing}, 71:\penalty0 150--163, 2023.

\bibitem[Ozfatura et~al.(2021)Ozfatura, Ozfatura, and G{\"u}nd{\"u}z]{ozfatura2021fedadc}
Emre Ozfatura, Kerem Ozfatura, and Deniz G{\"u}nd{\"u}z.
\newblock Fedadc: Accelerated federated learning with drift control.
\newblock In \emph{2021 IEEE International Symposium on Information Theory (ISIT)}, pages 467--472. IEEE, 2021.

\bibitem[Pathak and Wainwright(2020)]{pathak2020fedsplit}
Reese Pathak and Martin~J Wainwright.
\newblock Fedsplit: An algorithmic framework for fast federated optimization.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 7057--7066, 2020.

\bibitem[Qu et~al.(2022)Qu, Li, Duan, Liu, Tang, and Lu]{qu2022generalized}
Zhe Qu, Xingyu Li, Rui Duan, Yao Liu, Bo~Tang, and Zhuo Lu.
\newblock Generalized federated learning via sharpness aware minimization.
\newblock In \emph{International Conference on Machine Learning}, pages 18250--18280. PMLR, 2022.

\bibitem[Remedios et~al.(2020)Remedios, Butman, Landman, and Pham]{remedios2020federated}
Samuel~W Remedios, John~A Butman, Bennett~A Landman, and Dzung~L Pham.
\newblock Federated gradient averaging for multi-site training with momentum-based optimizers.
\newblock In \emph{Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning: Second MICCAI Workshop, DART 2020, and First MICCAI Workshop, DCL 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4--8, 2020, Proceedings 2}, pages 170--180. Springer, 2020.

\bibitem[Sarcheshmehpour et~al.(2021)Sarcheshmehpour, Leinonen, and Jung]{sarcheshmehpour2021federated}
Yasmin Sarcheshmehpour, M~Leinonen, and Alexander Jung.
\newblock Federated learning from big data over networks.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 3055--3059. IEEE, 2021.

\bibitem[Shen et~al.(2021)Shen, Cervino, Hassani, and Ribeiro]{shen2021agnostic}
Zebang Shen, Juan Cervino, Hamed Hassani, and Alejandro Ribeiro.
\newblock An agnostic approach to federated learning with class imbalance.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Sun et~al.(2023{\natexlab{a}})Sun, Shen, Chen, Ding, and Tao]{sun2023dynamic}
Yan Sun, Li~Shen, Shixiang Chen, Liang Ding, and Dacheng Tao.
\newblock Dynamic regularized sharpness aware minimization in federated learning: Approaching global consistency and smooth landscape.
\newblock \emph{arXiv preprint arXiv:2305.11584}, 2023{\natexlab{a}}.

\bibitem[Sun et~al.(2023{\natexlab{b}})Sun, Shen, Huang, Ding, and Tao]{sun2023fedspeed}
Yan Sun, Li~Shen, Tiansheng Huang, Liang Ding, and Dacheng Tao.
\newblock Fedspeed: Larger local interval, less communication round, and higher generalization accuracy.
\newblock \emph{arXiv preprint arXiv:2302.10429}, 2023{\natexlab{b}}.

\bibitem[Sun et~al.(2023{\natexlab{c}})Sun, Shen, Sun, Ding, and Tao]{sun2023efficient}
Yan Sun, Li~Shen, Hao Sun, Liang Ding, and Dacheng Tao.
\newblock Efficient federated learning via local adaptive amended optimizer with linear speedup.
\newblock \emph{arXiv preprint arXiv:2308.00522}, 2023{\natexlab{c}}.

\bibitem[Sun et~al.(2023{\natexlab{d}})Sun, Shen, and Tao]{sun2023mode}
Yan Sun, Li~Shen, and Dacheng Tao.
\newblock Which mode is better for federated learning? centralized or decentralized.
\newblock \emph{arXiv preprint arXiv:2310.03461}, 2023{\natexlab{d}}.

\bibitem[Sun et~al.(2023{\natexlab{e}})Sun, Shen, and Tao]{sun2023understanding}
Yan Sun, Li~Shen, and Dacheng Tao.
\newblock Understanding how consistency works in federated learning via stage-wise relaxed initialization.
\newblock \emph{arXiv preprint arXiv:2306.05706}, 2023{\natexlab{e}}.

\bibitem[Tran~Dinh et~al.(2021)Tran~Dinh, Pham, Phan, and Nguyen]{tran2021feddr}
Quoc Tran~Dinh, Nhan~H Pham, Dzung Phan, and Lam Nguyen.
\newblock Feddr--randomized douglas-rachford splitting algorithms for nonconvex federated composite optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 30326--30338, 2021.

\bibitem[Tyou et~al.(2023)Tyou, Murata, Fukami, Takezawa, and Niwa]{tyou2023localized}
Iifan Tyou, Tomoya Murata, Takumi Fukami, Yuki Takezawa, and Kenta Niwa.
\newblock A localized primal-dual method for centralized/decentralized federated learning robust to data heterogeneity.
\newblock \emph{IEEE Transactions on Signal and Information Processing over Networks}, 2023.

\bibitem[Wang et~al.(2022)Wang, Marella, and Anderson]{wang2022fedadmm}
Han Wang, Siddartha Marella, and James Anderson.
\newblock Fedadmm: A federated primal-dual algorithm allowing partial participation.
\newblock In \emph{2022 IEEE 61st Conference on Decision and Control (CDC)}, pages 287--294. IEEE, 2022.

\bibitem[Wang et~al.(2019)Wang, Tantia, Ballas, and Rabbat]{slowmo}
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat.
\newblock Slowmo: Improving communication-efficient distributed sgd with slow momentum.
\newblock \emph{arXiv preprint arXiv:1910.00643}, 2019.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Liu, Liang, Joshi, and Poor]{inconsistency3}
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H~Vincent Poor.
\newblock Tackling the objective inconsistency problem in heterogeneous federated optimization.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 7611--7623, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Liu, Liang, Joshi, and Poor]{wang2020tackling}
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H~Vincent Poor.
\newblock Tackling the objective inconsistency problem in heterogeneous federated optimization.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 7611--7623, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2023)Wang, Xu, Wang, Chang, Quek, and Sun]{wang2023beyond}
Shuai Wang, Yanqing Xu, Zhiguo Wang, Tsung-Hui Chang, Tony~QS Quek, and Defeng Sun.
\newblock Beyond admm: a unified client-variance-reduced adaptive federated learning framework.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 10175--10183, 2023.

\bibitem[Wu et~al.(2023)Wu, Xu, Yu, and Liu]{wu2023information}
Zheshun Wu, Zenglin Xu, Hongfang Yu, and Jie Liu.
\newblock Information-theoretic generalization analysis for topology-aware heterogeneous federated edge learning over noisy channels.
\newblock \emph{IEEE Signal Processing Letters}, 2023.

\bibitem[Xu et~al.(2021)Xu, Wang, Wang, and Yao]{xu2021fedcm}
Jing Xu, Sen Wang, Liwei Wang, and Andrew Chi-Chih Yao.
\newblock Fedcm: Federated learning with client-level momentum.
\newblock \emph{arXiv preprint arXiv:2106.10874}, 2021.

\bibitem[Yuan et~al.(2021)Yuan, Zaheer, and Reddi]{yuan2021federated}
Honglin Yuan, Manzil Zaheer, and Sashank Reddi.
\newblock Federated composite optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 12253--12266. PMLR, 2021.

\bibitem[Zhang et~al.(2024)Zhang, Fan, Yao, Zhang, and Wang]{zhang2024domain}
Ruipeng Zhang, Ziqing Fan, Jiangchao Yao, Ya~Zhang, and Yanfeng Wang.
\newblock Domain-inspired sharpness-aware minimization under domain shifts.
\newblock \emph{arXiv preprint arXiv:2405.18861}, 2024.

\bibitem[Zhang and Hong(2021)]{zhang2021connection}
Xinwei Zhang and Mingyi Hong.
\newblock On the connection between fed-dyn and fedpd.
\newblock \emph{FedDyn\_FedPD. pdf}, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Hong, Dhople, Yin, and Liu]{zhang2021fedpd}
Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu.
\newblock Fedpd: A federated learning framework with adaptivity to non-iid data.
\newblock \emph{IEEE Transactions on Signal Processing}, 69:\penalty0 6055--6070, 2021.

\bibitem[Zhou et~al.(2021)Zhou, Yan, Yuan, Feng, and Yan]{zhou2021towards}
Pan Zhou, Hanshu Yan, Xiaotong Yuan, Jiashi Feng, and Shuicheng Yan.
\newblock Towards understanding why lookahead generalizes better than sgd and beyond.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 27290--27304, 2021.

\bibitem[Zhou and Li(2023)]{zhou2023federated}
Shenglong Zhou and Geoffrey~Ye Li.
\newblock Federated learning via inexact admm.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2023.

\end{thebibliography}
