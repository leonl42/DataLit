\section{Related Work}
\label{related work}

\textit{Federated primal average.} Since \citet{mcmahan2017communication} propose the \textit{FedAvg} paradigm, a lot of \textit{primal average}-based methods are learned to enhance its performance. Most of them target strengthening global consistency and alleviating the ``client drift'' problem~\citep{karimireddy2020scaffold}. \citet{li2020federated} propose to adopt proxy terms to control local updates. \citet{karimireddy2020scaffold} propose the variance reduction version to handle the biases in the \textit{primal average}. Similar implementations include \citep{dieuleveut2021federated,jhunjhunwala2022fedvarp}. Moreover, momentum-based methods are also popular for correcting local biases. \citet{ozfatura2021fedadc,xu2021fedcm} propose to adopt the global consistency controller in the local training to force a high consensus. \citet{remedios2020federated} expand the local momentum to achieve higher accuracy in the training. Similarly, \citet{slowmo,kim2022communication} incorporate the global momentum which could further improve its performance. \citet{wang2020tackling} tackle the local inconsistency and utilize the \textit{weighted primal average} to balance different clients with different computing power. Based on this, \citet{horvath2022fedshuffle} further select the important clients set to balance the training trends under different heterogeneous datasets. \citet{liu2023enhance} summarize the inertial momentum implementation which could achieve a more stable result. \citet{qu2022generalized} utilize the \textit{Sharpeness Aware Minimization}~(\textit{SAM})~\citep{foret2020sharpness} to make the loss landscape smooth with higher generalization performance. Then \citet{caldarola2022improving,caldarola2023window} propose to improve its stability via \textit{Adaptive SAM} and window-based model averaging. In summary, \textit{Federated primal average} methods focus on alleviating the local inconsistency caused by ``client drift''~\citep{inconsistency2,inconsistency3,inconsistency1}. However, as analyzed by \citet{durmus2021federated}, the \textit{federated primal dual} methods will regularize the local objective gradually close to global consensus by dynamically adjusting the dual variables. This allows ``client drift'' to be effectively translated into a dual consistency problem on cross-silo devices.

\textit{Convex optimization of federated primal dual.} The primal-dual method was originally proposed to solve convex optimization problems and achieved high theoretical performance. In the FL setups, this method has also made significant advancements. \citet{grudzien2023can} compute inexactly a proximity operator to work as a variant of primal dual methods. Another technique is loopless instead of an inner loop of local steps. \citet{mishchenko2022proxskip} propose the \textit{Scaffnew} method to achieve the higher optimization efficiency, which is interpreted as a variant of primal dual approach~\citep{condat2022randprox}. These techniques can also be easily combined with existing efficient communication methods, e.g. compression and quantization~\citep{grudzien2023improving,condat2023tamuna}.

\textit{Non-convex optimization of federated primal dual.} Since \citet{tran2021feddr} adopt the \textit{Randomized Douglas-Rachford Splitting}, which unblocks the study of the important branch of utilizing \textit{primal dual} methods in FL~\citep{pathak2020fedsplit}. With further exploration of \citet{zhang2021fedpd}, \textit{federated primal dual} methods are proven to achieve the fast convergence rate. \citet{yuan2021federated} learn the composite optimization via a \textit{primal dual} method in FL. Meanwhile, \citet{sarcheshmehpour2021federated} empower its potential on the undirected empirical graph. \citet{shen2021agnostic} also study an agnostic approach under class imbalance targets. Then, \citet{gong2022fedadmm,wang2022fedadmm} expand its theoretical analysis to the partial participation scenarios with global regularization. \citet{durmus2021federated} improve its implementation by introducing the global dual variable. Moreover, \citet{zhou2023federated} learn the effect of subproblem precision on training efficiency. \citet{sun2023fedspeed,sun2023dynamic} incorporate it with the \textit{SAM} to achieve a higher generalization efficiency. \citet{niu2023fedhybrid} propose hybrid \textit{primal dual} updates with both first-order and second-order optimization. \citet{wang2023beyond} propose a variance reduction variant to further improve the training efficiency. \citet{li2023dfedadmm} expand it to a decentralized approach, which could achieve comparable performance in the centralized. \citet{tyou2023localized} propose a localized \textit{primal dual} approach for FL training. \citet{li2023privacy} further explore its efficiency on the specific non-convex objectives with non-smooth regularization. Current researches reveal the great application value of the \textit{primal dual} methods in FL. However, most of them still face the serious ``\textit{dual drift}'' problem at low participation ratios. Our improvements further alleviate this issue and make the \textit{federated primal dual} methods perform more stably in the FL community.