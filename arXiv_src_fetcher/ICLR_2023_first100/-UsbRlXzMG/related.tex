\section{Background and Related Work}
\noindent
In this section, we %set the stage for the present work by discussing 
discuss a few relevant prior works on fairness in text summarization and motivate the present work by contextualizing it in the existing literature.  

\subsection{Fairness in text summarization}
\noindent
Much like in the fairness in ML literature~\cite{ali2019fairness,friedler2019comparative,patro2020fairrec}, the proposed methodologies for fair text summarization can be divided into three categories e.g., (1)~pre-processing, (2)~in-processing, (3)~post-processing based algorithms based on the stage at which fairness intervention is performed. 
%In our previous work~\cite{dash2019summarizing}, we proposed fairness preserving text summarization algorithms adhering to each of the classes in the aforementioned taxonomy. 
In the pre-processing based algorithms, the dataset is fed to the summarization algorithms in a way such that the generated summaries will end up being fair. Similarly, in post processing algorithm, fairness interventions are applied on the output of standard summarization algorithms to generate fair summaries. 
Finally, in the in-processing based approach~\cite{dash2019summarizing, mukherjee2020read}, the algorithm designers often treat summarization as an optimization problem and solve the same by either modifying the optimization function or adding fairness constraints to generate fair summaries. Next, we briefly discuss the FairSumm algorithm that was proposed in~\cite{dash2019summarizing}.

\vspace{2 mm}
\noindent \textbf{FairSumm algorithm for fair summarization:} Our prior work~\cite{dash2019summarizing} developed an in-processing fair summarization algorithm, called `\textbf{FairSumm}'. %In this taxonomy of approaches, the algorithm designers often treat summarization as an optimization problem and solve the same by either modifying the optimization function or adding fairness constraints to generate fair summaries. 
FairSumm treats the summarization task as a sub-modular optimization problem with fairness constraints and solves it to maximize coverage and diversity across the textual units while adhering to standard fairness notions~\cite{dash2019summarizing}. Given a heterogeneous set of micro-blogs (coming from different socially salient groups), and a desired target representation of the groups, the algorithm produces extractive summaries that reconcile between textual quality of the summaries (as quantified by ROUGE scores), and fair representation of different social salient groups in the summary. 
For instance, FairSumm can be applied over a set of tweets posted by male and female authors, to obtain a good summary having equal fractions of tweets posted by male authors and tweets posted by female authors.
We shall be using \textit{FairSumm} algorithm extensively for the experiments throughout this paper.

%Taking the cue from our approach, another similar in-processing based approach has been proposed to generate fair summaries of tourist reviews by Mukherjee et.al.~\cite{mukherjee2020read}. They considered summarization to be an integer linear program with fairness constraints.

\subsection{Notions of fairness in text summarization} \label{sub:fairness-notions}
\noindent
Most of the prior works on fair summarization deal with the idea of group fairness. Specifically, when the input data (e.g. tweets or reviews) are generated by users from different socially salient groups, the algorithms explicitly enforce the summaries to fairly represent these different groups. 

\vspace{2mm}
\noindent
\textbf{Equal Representation:} The notion of equality finds its roots in the field of morality and justice, which advocates for the redress of undeserved inequalities (e.g. inequalities of birth or due to natural endowment)\cite{rawls2009theory}. In the context of summarization this ensures that the final summary must include equal number of textual units coming from different socially salient groups.

\vspace{2mm}
\noindent
\textbf{Proportional Representation:} Often it may not be possible to equally  represent different user groups in the summary, especially if the input data contains very different proportions from different groups. 
Hence, we consider another notion of fairness: {\it Proportional Representation} (also known as \textit{Statistical Parity}~\cite{luong2011k}).
In the context of summarization, Proportional Representation requires that the proportion of content from different user groups in the summary should be  same as in the original input. 

%A relaxed notion of proportional fairness is one which would ensure {\it no adverse impact} in the generated summary. 
%In other words, `no adverse impact'  requires that the fraction of textual units from any class, that is selected for inclusion in the summary, \textit{should not be} less than $80\%$ of the fraction of selected units from the class having the highest selection rate (in the summary).

\noindent These notions of fairness ensure that the probability of selecting an item is \textbf{independent} of which user group generated it.

%\noindent
\subsection{Drawbacks in the current literature}
\noindent
The process of summarizing involves two parties: namely producers of the information a.k.a `writers' and consumers of summarized information a.k.a `readers'. All of the prior works on fairness in summarization have attempted to ensure the fair representation of the producers; whereas the fairness toward consumers or readers has been completely ignored. 
The inclusion or exclusion of certain opinions/voices tend to have the maximum effect on the consumers of the summaries. As the summary is what is read by the consumers, the summary shapes their opinion on the topic. Hence bias in the final summary can have severe impact on shaping the public discourse. Hence, in this work we focus on exploring the interplay of existing fairness definitions and how they are perceived by the readers.

\vspace{2mm}
\noindent
\textbf{Limitations of existing measures in quantification of (un)fairness in summaries:} For evaluation of algorithm-generated summaries, all of the prior works have evaluated the generated summaries based on ROUGE metric. However, in this work, we observe that ROUGE metric is unable to capture the (un)fairness aspect of the generated summaries. 
To this end, in this work we also propose a metric for perceived fairness of textual summaries. Further, we also propose an automated quantification of the perceived bias of textual summaries that correlates significantly with the aforementioned perceived fairness. 

To the best of our knowledge, this is the first work towards quantification of (un)fairness in summaries, and understanding the interplay between the perceived fairness in text summarization from the perspective of both writers and readers of the textual content. 


%================Commented from here==========================
\if 0
\section{The Need For Fairness In Summarization}
Traditionally summarization algorithms have only focused on summary-worthiness of textual units while deciding on whether to exclude or include them. However, whenever we have authors of textual units belonging to socially-salient groups, further considerations need to be given. \cite{dash2019summarizing} have shown that generally textual units written by various social groups are of comparable quality. Further it has also been noted that textual units written by different social groups contain different opinions/ideas\cite{dash2019summarizing}. In this context overrepresentation or underrepresentation of ideas selected for inclusion from different social groups assumes significance. This calls for ‘equality of opportunity’ for different social groups’ opinion to be reflected in the summary. This is along the lines of the need for fairness in recommendation systems and search engine results. The core idea is that of imminent exposure received by ideas present in the summary vis-a-vis excluded ones, thus putting some opinion at an advantage at the expense of others.



\subsection{Current Paradigms of Fairness in Summarization}
{Most of the current works on fair summarization deal with the idea of group fairness in the context of summarization. Essentially, when the input data (e.g. tweets) are generated by users belonging to different social groups, we require the summaries to fairly represent these groups. 


\textbf{Equal Representation:} The notion of equality finds its roots in the field of morality and justice, which advocates for the redress of undeserved inequalities (e.g. inequalities of birth or due to natural endowment)\cite{rawls2009theory}. Formal equality suggests that when two people or two groups of people have equal status in at least one normatively relevant aspect, they must be treated equally\cite{sep-equality}. In terms of selection, equal representation requires that the number of representatives from different classes in the society having comparable relevance has to be equal. In the context of user-generated content, we observed that different sections of the society have different opinions on the same topic, either because of their gender or ideological leaning\cite{babaei2018purple}. However, if we consider the textual quality, i.e. their candidature for inclusion in the summary, then tweets from both the groups are comparable. Thus, the notion of equal representation requires that a summarization algorithm will be fair if different groups generating the input data are represented equally in the output summary. Given the usefulness of summaries in many downstream applications, this notion of fairness ensures equal exposure to the opinions of different socially salient groups.
Often it may not be possible to equally represent different user groups in the summary, especially if the input data contains very different proportions from different groups. Hence, we consider another notion of fairness: 

\textbf{Proportional Representation:} (also known as Statistical Parity\cite{luong2011k}). Proportional representation requires that the representation of different groups in the selected set should be proportional to their distribution in the input data. In certain scenarios such as hiring for jobs, relaxations of this notion are often used. For instance, the U.S. Equal Employment Opportunity Commission uses a variant of Proportional Representation to determine whether a company’s hiring policy is biased against (has any adverse impact on) a demographic group\cite{gajane2017formalizing}. According to this policy, a particular class c is under-represented in the selected set (or adversely impacted), if the fraction of selected people belonging to class c is less than 80 \% of the fraction of selected people from the class having the highest selection rate. In the context of summarization, Proportional Representation requires that the proportion of content from different user groups in the summary should be the same as in the original input.

\textbf{No Adverse Impact:} This notion relies on the idea that some social classes/groups need to be protected more than others if they have a high probability of facing bias. In some cases, ‘no adverse impact’ requires that the fraction of textual units from any class, that is selected for inclusion in the summary, should not be less than 80 percent of the fraction of selected units from the class having the highest selection rate (in the summary). These notions of fairness ensure that the probability of selecting an item is independent of which user group generated it.
}
\subsection{Problems With The Current Paradigm}
{The process of summarizing involves two parties: namely producers of the information a.k.a ‘producers’ and consumers/readers of summarised information a.k.a ‘consumers. Almost all of the work on fairness in summarization relies on ensuring the fairness for the producers. In this process they completely ignore the other party of the process: the consumers. The effect of inclusion or exclusion of certain opinions/voices tend to have the maximum effect on the consumers of the information. As the summary is used by consumers to shape their opinion on topics, any bias that creeps into the summarization process has severe impact on shaping the public discourse. In this work we aim to quantify biasedness in the process of summarization as understood by consumers of the summary.

Despite the recent surge of interest in Fair ML, there is no consensus on a precise definition of (un)fairness. Numerous mathematical
definitions of fairness have been proposed; as discussed in Section 2.1. Notwithstanding the utility of these definitiona, it has been shown that they are incompatible with one another and do not hold simultaneously \cite{chouldechova2017fair}. The literature so far
has dealt with optimising the tradeoffs between various definition under the presumption that practitioners would be best suited to determine the tradeoffs (see, e.g., \cite{corbett2017algorithmic}).
However, there have been some recent works that takes a different perspective on these impossibility results. 


In ethics, there are two distinct ways of addressing moral dilemmas: descriptive vs. normative approach. Normative ethics involves creating or evaluating moral standards to decide what people should do or whether their current moral behavior is reasonable. Descriptive (or comparative) ethics is a form of empirical research into the attitudes of individuals or groups of people towards morality and moral decision-making. Several prior papers have taken a normative perspective on algorithmic fairness. For instance, Gajane and Pechenizkiy\cite{gajane2017formalizing} attempt to cast algorithmic notions of fairness as instances of existing theories of justice. Heidari et al.\cite{heidari2019moral} propose a framework for evaluating the assumptions underlying different notions of fairness by casting them as special cases of economic models of equality of opportunity. We emphasize that there is no simple, widely-accepted, normative principle to settle the ethical problem of algorithmic fairness. 
Several recent papers empirically investigate the issues of fairness and interpretability utilizing human-subject experiments. MIT’s moral machine\cite{awad2018moral} provides a crowd-sourcing platform for aggregating human opinion on how self-driving cars should make decisions when faced with moral dilemmas. For the same setting, Noothigattu et al\cite{noothigattu2017voting} have proposed  learning a random utility model of individual preferences, and then efficiently aggregating those individual preferences through a social choice function. \cite{lee2019webuildai} proposes a similar approach for general ethical decision-making. More recently, Shrivastva et al\cite{srivastava2019mathematical} have opined that fairness definitions are very context dependent and the involvement of people is pertinent to coming up with the right definition. Similar to them, we conduct a study to understand the consumers’ perception of fairness in the process of automatic summarization. To the best of our knowledge, none of the work till now have taken a descriptive approach towards dealing with fairness in textual summarization processes.

Further, we posit that fairness, in the context of summarization, is a highly context-dependent ideal involving multiple stakeholders. As discussed before, two broad categories that can be identified are the producers/writers of information to be summarised and consumers/readers of the summarised information. Depending on the societal domain in which the summarization
algorithm is deployed, one mathematical notion of fairness may be considered ethically more desirable than other alternatives. Similarly, mathematical notions of fairness that have been traditionally been seen from the viewpoint of producers(for eg. demographic parity, proportional representation etc) may not not hold true from the viewpoint of consumers. We invesigate the interplay between the traditional definitions of fairness from the viewpoint of producers and the consumers' perceptions of fairness and how this interplay varies with the context of the summarization process.

}
\fi 