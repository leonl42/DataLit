\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2021)Bai, Hou, Shang, Jiang, King, and Lyu]{bai2021towards}
Bai, H., Hou, L., Shang, L., Jiang, X., King, I., and Lyu, M.~R.
\newblock Towards efficient post-training quantization of pre-trained language
  models.
\newblock \emph{arXiv preprint arXiv:2109.15082}, 2021.

\bibitem[Bengio et~al.(2013)Bengio, Leonard, and Courville]{Bengio2013ste}
Bengio, Y., Leonard, N., and Courville, A.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Bisk, Y., Zellers, R., Gao, J., Choi, Y., et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pp.\  7432--7439, 2020.

\bibitem[Black et~al.(2021)Black, Gao, Wang, Leahy, and Biderman]{gpt-neo}
Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, March 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5297715}.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem[Bondarenko et~al.(2021)Bondarenko, Nagel, and
  Blankevoort]{bondarenko2021understanding}
Bondarenko, Y., Nagel, M., and Blankevoort, T.
\newblock Understanding and overcoming the challenges of efficient transformer
  quantization.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  7947--7969. Association for Computational
  Linguistics, November 2021.
\newblock \doi{10.18653/v1/2021.emnlp-main.627}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.627}.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and
  Toutanova]{clark2019boolq}
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova,
  K.
\newblock {B}ool{Q}: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  2924--2936,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1300}.
\newblock URL \url{https://aclanthology.org/N19-1300}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{clark2018arc}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and
  Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Courbariaux et~al.(2016)Courbariaux, Hubara, Soudry, El-Yaniv, and
  Bengio]{courbariaux2016binarized}
Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to +1 or-1.
\newblock \emph{arXiv preprint arXiv:1602.02830}, 2016.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022llm}
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Esser et~al.(2020)Esser, McKinstry, Bablani, Appuswamy, and
  Modha]{esser2020learned}
Esser, S.~K., McKinstry, J.~L., Bablani, D., Appuswamy, R., and Modha, D.~S.
\newblock Learned step size quantization.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgO66VKDS}.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and
  Zou]{eval-harness}
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L.,
  Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E.,
  Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation, September 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5371628}.

\bibitem[Gardent et~al.(2017)Gardent, Shimorina, Narayan, and
  Perez-Beltrachini]{gardent2017webnlg}
Gardent, C., Shimorina, A., Narayan, S., and Perez-Beltrachini, L.
\newblock The {W}eb{NLG} challenge: Generating text from {RDF} data.
\newblock In \emph{Proceedings of the 10th International Conference on Natural
  Language Generation}, pp.\  124--133, Santiago de Compostela, Spain,
  September 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W17-3518}.
\newblock URL \url{https://aclanthology.org/W17-3518}.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2016deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In \emph{International Conference on Learning Representations}, 2016.
\newblock URL \url{https://arxiv.org/pdf/1510.00149.pdf}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q.,
  Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2790--2799. PMLR, 2019.

\bibitem[Hu et~al.(2022)Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2022lora}
Hu, E.~J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
  and Chen, W.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Hubara et~al.(2021)Hubara, Nahshan, Hanani, Banner, and
  Soudry]{hubara2021adaquant}
Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry, D.
\newblock Accurate post training quantization with small calibration sets.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of \emph{Proceedings of Machine Learning Research},
  pp.\  4466--4475. PMLR, 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/hubara21a.html}.

\bibitem[Jain et~al.(2019)Jain, Gural, Wu, and Dick]{jain2019tqt}
Jain, S.~R., Gural, A., Wu, M., and Dick, C.~H.
\newblock Trained quantization thresholds for accurate and efficient
  fixed-point inference of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1903.08066}, 2019.

\bibitem[Jung et~al.(2019)Jung, Son, Lee, Son, Han, Kwak, Ju~Hwang, and
  Choi]{jung2019learning}
Jung, S., Son, C., Lee, S., Son, J., Han, J.-J., Kwak, Y., Ju~Hwang, S., and
  Choi, C.
\newblock Learning to quantize deep networks by optimizing quantization
  intervals with task loss.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  4350--4359, 2019.

\bibitem[Kim et~al.(2021)Kim, Park, and Yi]{kim2021performance}
Kim, S., Park, G., and Yi, Y.
\newblock Performance evaluation of int8 quantized inference on mobile gpus.
\newblock \emph{IEEE Access}, 9:\penalty0 164245--164255, 2021.

\bibitem[Lee et~al.(2021)Lee, Yun, Hwang, and Yang]{lee2021cluster}
Lee, J.~H., Yun, J., Hwang, S.~J., and Yang, E.
\newblock Cluster-promoting quantization with bit-drop for minimizing network
  quantization loss.
\newblock In \emph{2021 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pp.\  5350--5359. IEEE Computer Society, 2021.
\newblock URL
  \url{https://doi.ieeecomputersociety.org/10.1109/ICCV48922.2021.00532}.

\bibitem[Li et~al.(2021)Li, Gong, Tan, Yang, Hu, Zhang, Yu, Wang, and
  Gu]{li2021brecq}
Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang, W., and
  Gu, S.
\newblock {BRECQ}: Pushing the limit of post-training quantization by block
  reconstruction.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=POWv6hDd9XH}.

\bibitem[Liu et~al.(2022)Liu, Ji, Fu, Tam, Du, Yang, and Tang]{liu2022p}
Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang, J.
\newblock P-tuning: Prompt tuning can be comparable to fine-tuning across
  scales and tasks.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pp.\  61--68, 2022.

\bibitem[Lou et~al.(2020)Lou, Guo, Kim, Liu, and Jiang.]{lou2020autoq}
Lou, Q., Guo, F., Kim, M., Liu, L., and Jiang., L.
\newblock Autoq: Automated kernel-wise neural network quantization.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rygfnn4twS}.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and
  Marcinkiewicz]{marcus-etal-1993-building}
Marcus, M.~P., Santorini, B., and Marcinkiewicz, M.~A.
\newblock Building a large annotated corpus of {E}nglish: The {P}enn
  {T}reebank.
\newblock \emph{Computational Linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.
\newblock URL \url{https://www.aclweb.org/anthology/J93-2004}.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models, 2016.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal]{mihaylov2018obqa}
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}, 2018.

\bibitem[Nagel et~al.(2019)Nagel, Baalen, Blankevoort, and
  Welling]{nagel2019data}
Nagel, M., Baalen, M.~v., Blankevoort, T., and Welling, M.
\newblock Data-free quantization through weight equalization and bias
  correction.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  1325--1334, 2019.

\bibitem[Nagel et~al.(2020)Nagel, Amjad, Van~Baalen, Louizos, and
  Blankevoort]{nagel2020adaround}
Nagel, M., Amjad, R.~A., Van~Baalen, M., Louizos, C., and Blankevoort, T.
\newblock Up or down? {A}daptive rounding for post-training quantization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of \emph{Proceedings of Machine Learning Research},
  pp.\  7197--7206. PMLR, 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/nagel20a.html}.

\bibitem[Nagel et~al.(2021)Nagel, Fournarakis, Amjad, Bondarenko, van Baalen,
  and Blankevoort]{nagel2021white}
Nagel, M., Fournarakis, M., Amjad, R.~A., Bondarenko, Y., van Baalen, M., and
  Blankevoort, T.
\newblock A white paper on neural network quantization.
\newblock \emph{arXiv preprint arXiv:2106.08295}, 2021.

\bibitem[Nahshan et~al.(2021)Nahshan, Chmiel, Baskin, Zheltonozhskii, Banner,
  Bronstein, and Mendelson]{nahshan2021loss}
Nahshan, Y., Chmiel, B., Baskin, C., Zheltonozhskii, E., Banner, R., Bronstein,
  A.~M., and Mendelson, A.
\newblock Loss aware post-training quantization.
\newblock \emph{Machine Learning}, 110\penalty0 (11):\penalty0 3245--3262,
  2021.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019gpt2}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020c4}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[{Rajpurkar} et~al.(2016){Rajpurkar}, {Zhang}, {Lopyrev}, and
  {Liang}]{2016arXiv160605250R}
{Rajpurkar}, P., {Zhang}, J., {Lopyrev}, K., and {Liang}, P.
\newblock {SQuAD: 100,000+ Questions for Machine Comprehension of Text}.
\newblock \emph{arXiv e-prints}, art. arXiv:1606.05250, 2016.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and
  Choi]{sakaguchi2021winogrande}
Sakaguchi, K., Bras, R.~L., Bhagavatula, C., and Choi, Y.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106,
  2021.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4510--4520, 2018.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin,
  A., Grave, E., and Lample, G.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Wang et~al.(2020)Wang, Chen, He, and Cheng]{wang2020towards}
Wang, P., Chen, Q., He, X., and Cheng, J.
\newblock Towards accurate post-training network quantization via bit-split and
  stitching.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9847--9856. PMLR, 2020.

\bibitem[Wei et~al.(2022)Wei, Gong, Li, Liu, and Yu]{wei2022qdrop}
Wei, X., Gong, R., Li, Y., Liu, X., and Yu, F.
\newblock {QD}rop: Randomly dropping quantization for extremely low-bit
  post-training quantization.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=ySQH0oDyp7}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Le~Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
  P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le~Scao, T., Gugger, S., Drame, M.,
  Lhoest, Q., and Rush, A.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-demos.6}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-demos.6}.

\bibitem[Wu et~al.(2020)Wu, Judd, Zhang, Isaev, and
  Micikevicius]{wu2020integer}
Wu, H., Judd, P., Zhang, X., Isaev, M., and Micikevicius, P.
\newblock Integer quantization for deep learning inference: Principles and
  empirical evaluation.
\newblock \emph{arXiv preprint arXiv:2004.09602}, 2020.

\bibitem[Xiao et~al.(2022)Xiao, Lin, Seznec, Demouth, and
  Han]{xiao2022smoothquant}
Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock \emph{arXiv preprint arXiv:2211.10438}, 2022.

\bibitem[Yao et~al.(2022)Yao, Aminabadi, Zhang, Wu, Li, and
  He]{yao2022zeroquant}
Yao, Z., Aminabadi, R.~Y., Zhang, M., Wu, X., Li, C., and He, Y.
\newblock Zeroquant: Efficient and affordable post-training quantization for
  large-scale transformers.
\newblock \emph{arXiv preprint arXiv:2206.01861}, 2022.

\bibitem[Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and
  Wasserblat]{zafrir2019q8bert}
Zafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M.
\newblock Q8bert: Quantized 8bit bert.
\newblock In \emph{2019 Fifth Workshop on Energy Efficient Machine Learning and
  Cognitive Computing-NeurIPS Edition (EMC2-NIPS)}, pp.\  36--39. IEEE, 2019.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,
  Diab, M., Li, X., Lin, X.~V., et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Hou, Yin, Shang, Chen, Jiang, and
  Liu]{zhang2020ternarybert}
Zhang, W., Hou, L., Yin, Y., Shang, L., Chen, X., Jiang, X., and Liu, Q.
\newblock Ternarybert: Distillation-aware ultra-low bit bert.
\newblock \emph{arXiv preprint arXiv:2009.12812}, 2020.

\bibitem[Zhao et~al.(2019)Zhao, Hu, Dotzel, De~Sa, and
  Zhang]{zhao2019improving}
Zhao, R., Hu, Y., Dotzel, J., De~Sa, C., and Zhang, Z.
\newblock Improving neural network quantization without retraining using
  outlier channel splitting.
\newblock In \emph{International conference on machine learning}, pp.\
  7543--7552. PMLR, 2019.

\bibitem[Zhao et~al.(2020)Zhao, Wang, Cai, Liu, and Zhang]{zhao2020linear}
Zhao, X., Wang, Y., Cai, X., Liu, C., and Zhang, L.
\newblock Linear symmetric quantization of neural networks for low-precision
  integer hardware.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1lBj2VFPS}.

\end{thebibliography}
