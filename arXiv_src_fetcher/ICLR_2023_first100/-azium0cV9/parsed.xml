<?xml version="1.0" encoding="UTF-8"?>
<?latexml searchpaths="/home/miri/Documents/DataLit/arXiv_src_fetcher/ICLR_2023_first100/-azium0cV9"?>
<?latexml class="article"?>
<?latexml package="microtype"?>
<?latexml package="graphicx"?>
<?latexml package="booktabs"?>
<?latexml package="hyperref" options="breaklinks"?>
<?latexml package="icml2023" options="accepted"?>
<?latexml package="amsmath"?>
<?latexml package="amssymb"?>
<?latexml package="mathtools"?>
<?latexml package="amsthm"?>
<?latexml package="url"?>
<?latexml package="inputenc" options="utf8"?>
<?latexml package="fontenc" options="T1"?>
<?latexml package="hyperref" options="breaklinks"?>
<?latexml package="url"?>
<?latexml package="booktabs"?>
<?latexml package="amsfonts"?>
<?latexml package="amsmath"?>
<?latexml package="amssymb"?>
<?latexml package="mathtools"?>
<?latexml package="amsthm"?>
<?latexml package="nicefrac"?>
<?latexml package="microtype"?>
<?latexml package="xcolor"?>
<?latexml package="wrapfig"?>
<?latexml package="graphicx"?>
<?latexml package="caption"?>
<?latexml package="multirow,makecell"?>
<?latexml package="xspace"?>
<?latexml package="subcaption"?>
<?latexml package="algorithm"?>
<?latexml package="algorithmic"?>
<?latexml package="cleveref" options="capitalize,noabbrev"?>
<?latexml package="todonotes" options="textsize=tiny"?>
<?latexml RelaxNGSchema="LaTeXML"?>
<document xmlns="http://dlmf.nist.gov/LaTeXML" class="ltx_pruned_first">
  <resource src="LaTeXML.css" type="text/css"/>
  <resource src="ltx-article.css" type="text/css"/>
  <title>SWARM Parallelism: Training Large Models<break/>Can Be Surprisingly Communication-Efficient</title>
  <creator role="author">
    <personname><!--  %****␣main.tex␣Line␣75␣**** -->Max Ryabinin</personname>
  </creator>
  <creator before="  " role="author">
    <personname>Tim Dettmers</personname>
  </creator>
  <creator before="  " role="author">
    <personname>Michael Diskin</personname>
  </creator>
  <creator before="  " role="author">
    <personname>Alexander Borzunov</personname>
  </creator>
  <abstract name="Abstract">
    <p>Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap “preemptible” instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where <text font="italic">training larger models becomes less communication-intensive</text>.
Based on these findings, we propose SWARM parallelism<note mark="1" role="footnote" xml:id="footnote1"><tags>
          <tag>1</tag>
          <tag role="autoref">footnote 1</tag>
          <tag role="refnum">1</tag>
          <tag role="typerefnum">footnote 1</tag>
        </tags>SWARM parallelism is a backronym for Stochastically Wired Adaptively Rebalanced Model Parallelism.</note>, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure.
We empirically validate our findings and compare SWARM parallelism with existing large-scale training approaches.
Finally, we combine our insights with compression strategies to train a large Transformer language model with 1B shared parameters (<Math mode="inline" tex="{\approx}13" text="absent approximately-equals 13" xml:id="m1">
        <XMath>
          <XMApp>
            <XMTok meaning="approximately-equals" name="approx" role="RELOP">≈</XMTok>
            <XMTok meaning="absent"/>
            <XMTok meaning="13" role="NUMBER">13</XMTok>
          </XMApp>
        </XMath>
      </Math>B before sharing) on preemptible T4 GPUs with less than 200Mb/s network.</p>
  </abstract>
  <keywords>Machine Learning, ICML</keywords>
  <para xml:id="p2">
    <break/>
  </para>
  <section inlist="toc" labels="LABEL:sect:intro" xml:id="S1">
    <tags>
      <tag>1</tag>
      <tag role="autoref">section 1</tag>
      <tag role="refnum">1</tag>
      <tag role="typerefnum">§1</tag>
    </tags>
    <title><tag close=" ">1</tag>Introduction</title>
    <para xml:id="S1.p1">
      <p>For the past several years, the deep learning community has been growing more reliant on large pretrained neural networks. The most evident example of this trend is natural language processing, where the parameter count of models has grown from hundreds of millions <cite class="ltx_citemacro_citep">(<bibref bibrefs="transformer,gpt,bert" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> to billions <cite class="ltx_citemacro_citep">(<bibref bibrefs="megatron2,t5,gptj,ernie3" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> to hundreds of billions <cite class="ltx_citemacro_citep">(<bibref bibrefs="gpt3,fedus2021switch,palm,gopher" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> with consistent gains in quality <cite class="ltx_citemacro_citep">(<bibref bibrefs="kaplan2020scaling" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>. Likewise, many models in computer vision are reaching the billion-parameter scale <cite class="ltx_citemacro_citep">(<bibref bibrefs="dalle,scaling_vit,coatnet,guided_diffusion" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>.</p>
    </para>
    <para xml:id="S1.p2">
      <p>At this scale, the models no longer fit into a single accelerator and require specialized training algorithms that partition the parameters across devices <cite class="ltx_citemacro_citep">(<bibref bibrefs="alexnet,dean12" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>. While these model-parallel algorithms use different partitioning strategies, they all share the need to perform intensive device-to-device communication <cite class="ltx_citemacro_citep">(<bibref bibrefs="pipedream,megatron2" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>. Also, if a single device fails, it will cause the entire training process to break down. As a result, model-parallel algorithms are typically deployed in dedicated high-performance computing (HPC) clusters or supercomputers <cite class="ltx_citemacro_citep">(<bibref bibrefs="shoeybi2019megatron,zero,megatron2" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>.</p>
    </para>
    <para xml:id="S1.p3">
      <p>This kind of infrastructure is notoriously expensive to build and operate, which makes it available only to a few well-resourced organizations <cite class="ltx_citemacro_citep">(<bibref bibrefs="summit,fugaku,microsoft_supercomputer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>. Most researchers cannot afford the experiments necessary for a proper evaluation of their ideas. This ultimately limits the scientific progress for many important research areas, such as solving NLP problems in “non-mainstream” languages.</p>
    </para>
    <para xml:id="S1.p4">
      <p>Several recent works propose more cost-efficient distributed training strategies that leverage fleets of temporary “preemptible” instances that can be dynamically allocated in regions with low demand for hardware and electricity, making them 2–10 times cheaper than their dedicated counterparts <cite class="ltx_citemacro_citep">(<bibref bibrefs="proteus" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>. Another solution is to train in “collaborations” by pooling together preexisting resources or using the help of volunteers <cite class="ltx_citemacro_citep">(<bibref bibrefs="dedloc,eydle,hivemind_dmoe,yuan2022decentralized" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>.</p>
    </para>
    <para xml:id="S1.p5">
      <p>However, training in either of those setups requires specialized algorithms that can adapt to the changing number of workers, utilize heterogeneous devices and recover from hardware and network failures. While there are several practical algorithms for unreliable hardware <cite class="ltx_citemacro_citep">(<bibref bibrefs="volunteer_dl_async,lin2020multinode,moshpit" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>, they can only train relatively small models that <text font="italic">fit into the memory of the smallest device</text>. This limits the practical impact of cost-efficient strategies, because today’s large-scale experiments often involve models with billions of parameters.</p>
    </para>
    <para xml:id="S1.p6">
      <p>In this work, we aim to find a practical way of training large neural networks using <text font="bold">unreliable heterogeneous devices with slow interconnect</text>.
We begin by studying the impact of model size on the balance between communication and computation costs of pipeline-parallel training.
Specifically, increasing the size leads computation costs to grow faster than the network footprint, thus making <text font="bold">household-grade connection speeds</text> more practical than one might think.
This idea inspires the creation of <text font="bold">SWARM parallelism</text>, a pipeline-parallel approach designed to handle peer failures by prioritizing stable peers with lower latency.
In addition, this approach periodically rebalances the pipeline stages, which allows handling devices with different hardware and network speeds.</p>
    </para>
    <para xml:id="S1.p7">
      <p>In summary, we make the following contributions:</p>
    </para>
    <para xml:id="S1.p8">
      <itemize xml:id="S1.I1">
        <item xml:id="S1.I1.i1">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">1st item</tag>
          </tags>
          <para xml:id="S1.I1.i1.p1">
            <p>We analyze the existing model-parallel training techniques and formulate the “Square-Cube Law” of distributed training: a counterintuitive observation that, for some methods, <text font="italic">training larger models can actually decrease the network overhead</text>.</p>
          </para>
        </item>
        <item xml:id="S1.I1.i2">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">2nd item</tag>
          </tags>
          <para xml:id="S1.I1.i2.p1">
            <p>We develop SWARM parallelism, a decentralized model-parallel algorithm<note mark="2" role="footnote" xml:id="footnote2"><tags>
                  <tag>2</tag>
                  <tag role="autoref">footnote 2</tag>
                  <tag role="refnum">2</tag>
                  <tag role="typerefnum">footnote 2</tag>
                </tags>The code for our experiments can be found at <ref class="ltx_href" font="typewriter" href="https://github.com/yandex-research/swarm">github.com/yandex-research/swarm</ref>.</note>that leverages randomized fault-tolerant pipelines and dynamically rebalances nodes between pipeline stages. To the best of our knowledge, this is the first decentralized algorithm capable of billion-scale training on heterogeneous unreliable devices with slow interconnect.</p>
          </para>
        </item>
        <item xml:id="S1.I1.i3">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">3rd item</tag>
          </tags>
          <para xml:id="S1.I1.i3.p1">
            <p>Combining insights from the square-cube law, SWARM parallelism, and 8-bit compression, we show that it is possible to train a billion-scale Transformer language model on preemptible servers with low-power GPUs and the network bandwidth of less than <Math mode="inline" tex="200" text="200" xml:id="S1.I1.i3.p1.m1">
                <XMath>
                  <XMTok meaning="200" role="NUMBER">200</XMTok>
                </XMath>
              </Math>Mb/s while achieving high training throughput.</p>
          </para>
        </item>
      </itemize>
    </para>
  </section>
  <section inlist="toc" xml:id="S2">
    <tags>
      <tag>2</tag>
      <tag role="autoref">section 2</tag>
      <tag role="refnum">2</tag>
      <tag role="typerefnum">§2</tag>
    </tags>
    <title><tag close=" ">2</tag>Background &amp; Related Work</title>
    <subsection inlist="toc" labels="LABEL:sect:related_model_parallel" xml:id="S2.SS1">
      <tags>
        <tag>2.1</tag>
        <tag role="autoref">subsection 2.1</tag>
        <tag role="refnum">2.1</tag>
        <tag role="typerefnum">§2.1</tag>
      </tags>
      <title><tag close=" ">2.1</tag>Model-Parallel Training</title>
      <para xml:id="S2.SS1.p1">
        <p>Over the past decade, the deep learning community has developed several algorithms for training large neural networks.
Most of them work by dividing the model between multiple workers, which is known as model parallelism.
The exact way in which these algorithms divide the model determines their training performance and the maximum model size they can support.</p>
      </para>
      <paragraph inlist="toc" xml:id="S2.SS1.SSS0.Px1">
        <title>Traditional model parallelism.</title>
        <para xml:id="S2.SS1.SSS0.Px1.p1">
          <p>Historically, the first general strategy for training large models was to assign each device to compute a subset of each layer (e.g., a subset of neurons), then communicate the results between each other <cite class="ltx_citemacro_citep">(<bibref bibrefs="alexnet,model_parallelism_survey1,model_parallelism_survey2" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>.
Since each device stores a fraction of layer parameters, this technique can train models with extremely wide layers that would not fit into a single GPU. However, applying traditional model parallelism to deep neural networks comes at a significant performance penalty, as it requires all-to-all communication after each layer.
As a result, while intra-layer parallelism is still widely used <cite class="ltx_citemacro_citep">(<bibref bibrefs="meshtensorflow,zero" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>, it is usually applied within one physical server in combination with other strategies <cite class="ltx_citemacro_citep">(<bibref bibrefs="krizhevsky2014oneweirdtrick,projectadam,beyond_data_and_model,megatron2" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>.</p>
        </para>
      </paragraph>
      <paragraph inlist="toc" xml:id="S2.SS1.SSS0.Px2">
        <title>Pipeline parallelism</title>
        <para xml:id="S2.SS1.SSS0.Px2.p1">
          <p>circumvents the need for expensive all-to-all communication by assigning each device with one or several layers <cite class="ltx_citemacro_citep">(<bibref bibrefs="huang2019gpipe" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>. During the forward pass, each stage applies its subset of layers to the inputs supplied by the previous stage, then sends the outputs of the last layer to the next stage. For the backward pass, this process is reversed, with each pipeline stage passing the gradients to the device that supplied it with input activations.</p>
        </para>
        <para xml:id="S2.SS1.SSS0.Px2.p2">
          <p>To better utilize the available devices, the pipeline must process multiple microbatches per step, allowing each stage to run in parallel on a different batch of inputs. In practice, the number of microbatches is limited by the device memory: this results in reduced device utilization when processing the first and the last microbatches, known as the “bubble” overhead <cite class="ltx_citemacro_citep">(<bibref bibrefs="huang2019gpipe" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>. To combat this issue, subsequent studies propose using activation checkpointing, interleaved scheduling, and even asynchronous training <cite class="ltx_citemacro_citep">(<bibref bibrefs="pipedream,megatron2,huang2019gpipe,shoeybi2019megatron,pipemare" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>.</p>
        </para>
        <para xml:id="S2.SS1.SSS0.Px2.p3">
          <p>Aside from model parallelism, there two more strategies for training large models: data parallelism with dynamic parameter loading <cite class="ltx_citemacro_citep">(<bibref bibrefs="zero" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite> and model-specific algorithms such as Mixture-of-Experts <cite class="ltx_citemacro_citep">(<bibref bibrefs="shazeer2017outrageously" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>. We discuss these algorithms in Appendix <ref labelref="LABEL:appendix:related"/> and compare the performance of offloading with SWARM in Section <ref labelref="LABEL:appendix:training_throughput"/> and Appendix <ref labelref="LABEL:appendix:equivalence"/>.</p>
        </para>
      </paragraph>
    </subsection>
    <subsection inlist="toc" labels="LABEL:sect:related_cost_efficent_collaborative" xml:id="S2.SS2">
      <tags>
        <tag>2.2</tag>
        <tag role="autoref">subsection 2.2</tag>
        <tag role="refnum">2.2</tag>
        <tag role="typerefnum">§2.2</tag>
      </tags>
      <title><tag close=" ">2.2</tag>Distributed Training Outside HPC</title>
      <para xml:id="S2.SS2.p1">
        <p>The techniques described in Section <ref labelref="LABEL:sect:related_model_parallel"/> are designed for clusters of identical devices with rapid and reliable communication, making them a natural fit for the HPC setup. As we discussed earlier, such infrastructure is not always available, and a more cost-efficient alternative is to use “preemptible” instances <cite class="ltx_citemacro_citep">(<bibref bibrefs="li2019speeding,zhang2020machine,proteus" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> or volunteer computing <cite class="ltx_citemacro_citep">(<bibref bibrefs="volunteer_dl_async,hivemind_dmoe,eydle,dedloc" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. However, these environments are more difficult for distributed training: each machine can disconnect abruptly due to a failure or preemption. Besides, since there is a limited number of available instances per region, training at scale often requires operating across multiple locations or using different instance types.</p>
      </para>
      <para xml:id="S2.SS2.p2">
        <p>To handle unstable peers and heterogeneous devices, the research community has proposed elastic and asynchronous training methods, correspondingly.
Moreover, training large models over heterogeneous devices can be optimized with global scheduling <cite class="ltx_citemacro_citep">(<bibref bibrefs="yuan2022decentralized" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.
We describe these methods in more detail in Appendix <ref labelref="LABEL:appendix:related"/>; importantly, neither of them are unable to satisfy all the constraints of our setup.</p>
      </para>
      <para xml:id="S2.SS2.p3">
        <p>By contrast, the largest models have billions of parameters, which exceeds the memory limits of most low-end computers.
However, model-parallel algorithms are not redundant, which makes them more vulnerable to hardware and network failures.
There exist two methods that allow training large models with unreliable devices <cite class="ltx_citemacro_citep">(<bibref bibrefs="hivemind_dmoe,thorpe2022bamboo" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>: however, the first one supports only specific architectures and requires at least 1Gb/s bandwidth, whereas the second one has no publicly available implementations, relies on redundant computations for fault tolerance and considers only the homogeneous setup.</p>
      </para>
    </subsection>
    <subsection inlist="toc" labels="LABEL:sect:related_communication_eficiency" xml:id="S2.SS3">
      <tags>
        <tag>2.3</tag>
        <tag role="autoref">subsection 2.3</tag>
        <tag role="refnum">2.3</tag>
        <tag role="typerefnum">§2.3</tag>
      </tags>
      <title><tag close=" ">2.3</tag>Communication Efficiency and Compression</title>
      <para xml:id="S2.SS3.p1">
        <p>In this section, we discuss techniques that address training with limited network bandwidth or high latency, such as gradient compression or overlapping computation with communication phases. These techniques are often necessary for distributed training without high-speed connectivity, because otherwise the performance of the system becomes severely bottlenecked by communication.</p>
      </para>
      <paragraph inlist="toc" xml:id="S2.SS3.SSS0.Px1">
        <title>Efficient gradient communication.</title>
        <para xml:id="S2.SS3.SSS0.Px1.p1">
          <p>Data-parallel training requires synchronization of gradients after each backward pass, which can be costly if the model has many parameters or the network bandwidth is limited. There exist several methods that approach this problem: for example, Deep Gradient Compression <cite class="ltx_citemacro_citep">(<bibref bibrefs="deepgradientcompression" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite> sparsifies the gradients and corrects the momentum after synchronization, while PowerSGD <cite class="ltx_citemacro_citep">(<bibref bibrefs="vogels2019powersgd" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite> factorizes the gradients and uses error feedback to reduce the approximation error.
Recently, <cite class="ltx_citemacro_citet"><bibref bibrefs="wang2022finetuning" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase>(</bibrefphrase>
                <bibrefphrase>)</bibrefphrase>
              </bibref></cite> proposed to compress the changes of model activations, achieving high-speed communication for finetuning models of up to 1.5B parameters.
Alternatively, <cite class="ltx_citemacro_citet"><bibref bibrefs="Dettmers20158BitAF" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                <bibrefphrase>(</bibrefphrase>
                <bibrefphrase>)</bibrefphrase>
              </bibref></cite> uses 8-bit quantization to compress gradients before communication. We evaluate it along with compression-aware architectures, leaving the exploration of more advanced approaches to future work.</p>
        </para>
        <para xml:id="S2.SS3.SSS0.Px1.p2">
          <p>Besides gradient compression, another effective technique is to use layer sharing <cite class="ltx_citemacro_citep">(<bibref bibrefs="albert" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>, which reduces the number of aggregated gradients by a factor of how many times each layer is reused.</p>
        </para>
      </paragraph>
      <paragraph inlist="toc" xml:id="S2.SS3.SSS0.Px2">
        <title>Overlapping communication and computation.</title>
        <para xml:id="S2.SS3.SSS0.Px2.p1">
          <p>Model, pipeline, and data parallelism all have synchronization points and require transfer of gradients or activations. One way to reduce the transfer cost is to overlap communication with computation, <text font="italic">hiding</text> the synchronization latency. This overlap can be achieved by combining parallelism techniques <cite class="ltx_citemacro_citep">(<bibref bibrefs="krizhevsky2014oneweirdtrick,zero" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>, by synchronizing gradients layer-by-layer in lockstep with backpropagation <cite class="ltx_citemacro_citep">(<bibref bibrefs="paszke2019pytorch" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>, or by using pure pipeline parallelism <cite class="ltx_citemacro_citep">(<bibref bibrefs="huang2019gpipe,pipedream" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>.
However, pure pipeline parallelism requires many stages to effectively hide the latency. To overcome this problem, we study inter-layer compression techniques that work well even with relatively few pipeline stages.</p>
        </para>
        <figure align="center" class="ltx_figure_panel ltx_minipage" inlist="lof" labels="LABEL:fig:squarecube" placement="t" vattach="middle" width="277.5pt" xml:id="S2.F1.fig1">
          <tags>
            <tag><text fontsize="90%">Figure 1</text></tag>
            <tag role="autoref">Figure 1</tag>
            <tag role="refnum">1</tag>
            <tag role="typerefnum">Figure 1</tag>
          </tags>
          <graphics candidates="resources/squarecube_short_v2_max.pdf" graphic="resources/squarecube_short_v2_max.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S2.F1.g1"/>
          <toccaption><tag close=" ">1</tag><text font="bold">(Left)</text> An intuitive explanation of the square-cube law, <text font="bold">(Right)</text> Relative device utilization for Transformer layers using Tesla V100 and 500Mb/s network bandwidth. See Section <ref labelref="LABEL:sect:experiments_square_cube"/> and Appendix <ref labelref="LABEL:appendix:detailed_setup"/> for a detailed setup.</toccaption>
          <caption><tag close=": "><text fontsize="90%">Figure 1</text></tag><text font="bold" fontsize="90%">(Left)<text font="medium"> An intuitive explanation of the square-cube law, </text>(Right)<text font="medium"> Relative device utilization for Transformer layers using Tesla V100 and 500Mb/s network bandwidth. See Section <ref labelref="LABEL:sect:experiments_square_cube"/> and Appendix <ref labelref="LABEL:appendix:detailed_setup"/> for a detailed setup.</text></text></caption>
          <block align="center" class="ltx_figure_panel ltx_minipage" vattach="middle" width="151.8pt">
            <graphics candidates="resources/perf_relative_notitle.pdf" graphic="resources/perf_relative_notitle.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S2.F1.g2"/>
          </block>
        </figure>
      </paragraph>
    </subsection>
  </section>
  <section inlist="toc" labels="LABEL:sect:method" xml:id="S3">
    <tags>
      <tag>3</tag>
      <tag role="autoref">section 3</tag>
      <tag role="refnum">3</tag>
      <tag role="typerefnum">§3</tag>
    </tags>
    <title><tag close=" ">3</tag>Communication-Efficient Model Parallelism</title>
    <para xml:id="S3.p1">
      <p>In this section, we outline our approach for training large models with heterogeneous unreliable poorly-connected devices.
To that end, the section is organized as follows:</p>
    </para>
    <para xml:id="S3.p2">
      <itemize xml:id="S3.I1">
        <item xml:id="S3.I1.i1">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">1st item</tag>
          </tags>
          <para xml:id="S3.I1.i1.p1">
            <p>Section <ref labelref="LABEL:sect:method_squarecube"/> analyzes how existing model-parallel algorithms scale with model size and shows conditions where training increasingly larger models leads to less intense network usage;</p>
          </para>
        </item>
        <item xml:id="S3.I1.i2">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">2nd item</tag>
          </tags>
          <para xml:id="S3.I1.i2.p1">
            <p>Section <ref labelref="LABEL:sect:method_swarm"/> describes SWARM parallelism — a decentralized algorithm for training large models under the conditions outlined in Section <ref labelref="LABEL:sect:related_cost_efficent_collaborative"/>.</p>
          </para>
        </item>
      </itemize>
    </para>
    <subsection inlist="toc" labels="LABEL:sect:method_squarecube" xml:id="S3.SS1">
      <tags>
        <tag>3.1</tag>
        <tag role="autoref">subsection 3.1</tag>
        <tag role="refnum">3.1</tag>
        <tag role="typerefnum">§3.1</tag>
      </tags>
      <title><tag close=" ">3.1</tag>The Square-Cube Law of Distributed Training</title>
      <para xml:id="S3.SS1.p1">
        <p>To better understand the general scaling properties of model parallelism, we need to abstract away from the application-specific parameters, such as model architecture, batch size, and system design. To that end, we first consider a simplified model of pipeline parallelism. Our “pipeline” consists of <Math mode="inline" tex="k" text="k" xml:id="S3.SS1.p1.m1">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">k</XMTok>
            </XMath>
          </Math> stages, each represented by <Math mode="inline" tex="n{\times}n" text="n * n" xml:id="S3.SS1.p1.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">×</XMTok>
                <XMTok font="italic" role="UNKNOWN">n</XMTok>
                <XMTok font="italic" role="UNKNOWN">n</XMTok>
              </XMApp>
            </XMath>
          </Math> matrices. Intuitively, the first matrix represents the input data and all subsequent matrices are linear “layers” applied to that data. This model abstracts away from application-specific details, allowing us to capture general relationships that hold for many models.</p>
      </para>
      <para xml:id="S3.SS1.p2">
        <p>During “training”, stages iteratively perform matrix multiplication and then send the output to the subsequent pipeline stage over a throughput-limited network. These two operations have different scaling properties.
The compute time for naïve matrix multiplication scales as <Math mode="inline" tex="O(n^{3})" text="O * n ^ 3" xml:id="S3.SS1.p2.m1">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p2.m1.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p2.m1.1">
                      <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" role="UNKNOWN">n</XMTok>
                      <XMTok fontsize="70%" meaning="3" role="NUMBER">3</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math>. While this can be reduced further in theory <cite class="ltx_citemacro_citep">(<bibref bibrefs="coppersmith_winograd,refined_laser" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, it is only used for very large matrices <cite class="ltx_citemacro_citep">(<bibref bibrefs="practical_matmul_best,practical_matmul_earlier,strassen_reloaded" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. Therefore, deep learning on GPUs typically relies on <Math mode="inline" tex="O(n^{3})" text="O * n ^ 3" xml:id="S3.SS1.p2.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p2.m2.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p2.m2.1">
                      <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" role="UNKNOWN">n</XMTok>
                      <XMTok fontsize="70%" meaning="3" role="NUMBER">3</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> algorithms.</p>
      </para>
      <para xml:id="S3.SS1.p3">
        <p>In turn, the communication phase requires at most <Math mode="inline" tex="O(n^{2})" text="O * n ^ 2" xml:id="S3.SS1.p3.m1">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p3.m1.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p3.m1.1">
                      <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" role="UNKNOWN">n</XMTok>
                      <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> time to transfer a batch of <Math mode="inline" tex="n{\times}n" text="n * n" xml:id="S3.SS1.p3.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">×</XMTok>
                <XMTok font="italic" role="UNKNOWN">n</XMTok>
                <XMTok font="italic" role="UNKNOWN">n</XMTok>
              </XMApp>
            </XMath>
          </Math> activations or gradients. Therefore, as we increase the model size, the computation time grows faster than communication time, regardless of which matrix multiplication algorithm we use. We refer to this idea as the <text font="italic">square-cube law</text> after the eponymous principle in physics <cite class="ltx_citemacro_citep">(<bibref bibrefs="square_cube,mechanics" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.</p>
      </para>
      <para xml:id="S3.SS1.p4">
        <p>This principle applies to many real-world neural network architectures, albeit with some confounding variables. In convolutional neural networks <cite class="ltx_citemacro_citep">(<bibref bibrefs="conv_first" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, the computation time scales as <Math mode="inline" tex="O(BHWC^{2})" text="O * B * H * W * C ^ 2" xml:id="S3.SS1.p4.m1">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p4.m1.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p4.m1.1">
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">B</XMTok>
                      <XMTok font="italic" role="UNKNOWN">H</XMTok>
                      <XMTok font="italic" role="UNKNOWN">W</XMTok>
                      <XMApp>
                        <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" role="UNKNOWN">C</XMTok>
                        <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> and the communication is <Math mode="inline" tex="O(BHWC)" text="O * B * H * W * C" xml:id="S3.SS1.p4.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p4.m2.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p4.m2.1">
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">B</XMTok>
                      <XMTok font="italic" role="UNKNOWN">H</XMTok>
                      <XMTok font="italic" role="UNKNOWN">W</XMTok>
                      <XMTok font="italic" role="UNKNOWN">C</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math>, where <Math mode="inline" tex="B" text="B" xml:id="S3.SS1.p4.m3">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">B</XMTok>
            </XMath>
          </Math>, <Math mode="inline" tex="H" text="H" xml:id="S3.SS1.p4.m4">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">H</XMTok>
            </XMath>
          </Math>, <Math mode="inline" tex="W" text="W" xml:id="S3.SS1.p4.m5">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">W</XMTok>
            </XMath>
          </Math> and <Math mode="inline" tex="C" text="C" xml:id="S3.SS1.p4.m6">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">C</XMTok>
            </XMath>
          </Math> stand for batch size, height, width and the number of channels. Recurrent neural networks <cite class="ltx_citemacro_citep">(<bibref bibrefs="backprop_rnn,lstm" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> need <Math mode="inline" tex="O(BLH^{2})" text="O * B * L * H ^ 2" xml:id="S3.SS1.p4.m7">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p4.m7.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p4.m7.1">
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">B</XMTok>
                      <XMTok font="italic" role="UNKNOWN">L</XMTok>
                      <XMApp>
                        <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" role="UNKNOWN">H</XMTok>
                        <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> compute in terms of batch size, sequence length, and hidden size, respectively, and <Math mode="inline" tex="O(BLH)" text="O * B * L * H" xml:id="S3.SS1.p4.m8">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p4.m8.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p4.m8.1">
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">B</XMTok>
                      <XMTok font="italic" role="UNKNOWN">L</XMTok>
                      <XMTok font="italic" role="UNKNOWN">H</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> or <Math mode="inline" tex="O(BH)" text="O * B * H" xml:id="S3.SS1.p4.m9">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p4.m9.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p4.m9.1">
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">B</XMTok>
                      <XMTok font="italic" role="UNKNOWN">H</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> communication, depending on the architecture. With the same notation, Transformers <cite class="ltx_citemacro_citep">(<bibref bibrefs="transformer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> require <Math mode="inline" tex="O(BL^{2}H)" text="O * B * L ^ 2 * H" xml:id="S3.SS1.p4.m10">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p4.m10.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p4.m10.1">
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">B</XMTok>
                      <XMApp>
                        <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" role="UNKNOWN">L</XMTok>
                        <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                      </XMApp>
                      <XMTok font="italic" role="UNKNOWN">H</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> compute for attention layers, <Math mode="inline" tex="O(BLH^{2})" text="O * B * L * H ^ 2" xml:id="S3.SS1.p4.m11">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p4.m11.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p4.m11.1">
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">B</XMTok>
                      <XMTok font="italic" role="UNKNOWN">L</XMTok>
                      <XMApp>
                        <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" role="UNKNOWN">H</XMTok>
                        <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> compute for feedforward layers, but only <Math mode="inline" tex="O(BLH)" text="O * B * L * H" xml:id="S3.SS1.p4.m12">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">O</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS1.p4.m12.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS1.p4.m12.1">
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">B</XMTok>
                      <XMTok font="italic" role="UNKNOWN">L</XMTok>
                      <XMTok font="italic" role="UNKNOWN">H</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> communication.</p>
      </para>
      <para xml:id="S3.SS1.p5">
        <p>Based on these observations, we conclude that pipeline parallelism naturally grows more communication-efficient with model size. More precisely, increasing the hidden dimension will reduce the communication load per device per unit of time, making it possible to train the model efficiently <text font="italic">with lower network bandwidth</text> and <text font="italic">higher latency<note mark="3" role="footnote" xml:id="footnote3"><tags>
                <tag><text font="upright">3</text></tag>
                <tag role="autoref"><text font="upright">footnote 3</text></tag>
                <tag role="refnum"><text font="upright">3</text></tag>
                <tag role="typerefnum"><text font="upright">footnote 3</text></tag>
              </tags><text font="upright">Latency slows the communication down by a constant factor that also grows less important with model size.</text></note></text>. While the exact practical ramifications depend on the use case, Section <ref labelref="LABEL:sect:experiments_square_cube"/> demonstrates that some of the larger models trained with pipeline parallelism can already train at peak efficiency with only hundreds of Mb/s bandwidth.</p>
      </para>
      <para xml:id="S3.SS1.p6">
        <p>In theory, the square-cube principle also applies to intra-layer parallelism, but using this technique at 500 Mb/s would become practical only for layer sizes of more than <Math mode="inline" tex="2^{16}" text="2 ^ 16" xml:id="S3.SS1.p6.m1">
            <XMath>
              <XMApp>
                <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                <XMTok meaning="2" role="NUMBER">2</XMTok>
                <XMTok fontsize="70%" meaning="16" role="NUMBER">16</XMTok>
              </XMApp>
            </XMath>
          </Math> units. Data-parallel training with sharding or offloading <cite class="ltx_citemacro_citep">(<bibref bibrefs="zerooffload" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> does not scale as well, as its communication time scales with the size of <text font="italic">model parameters</text> instead of activations. However, it may be possible to achieve similar scaling with gradient compression algorithms.</p>
      </para>
    </subsection>
    <subsection inlist="toc" labels="LABEL:sect:method_swarm" xml:id="S3.SS2">
      <tags>
        <tag>3.2</tag>
        <tag role="autoref">subsection 3.2</tag>
        <tag role="refnum">3.2</tag>
        <tag role="typerefnum">§3.2</tag>
      </tags>
      <title><tag close=" ">3.2</tag>SWARM Parallelism</title>
      <para xml:id="S3.SS2.p1">
        <p>Traditional pipeline parallelism can be communication-efficient, but this alone is not enough for our setups. Since training devices can have different compute and network capabilities, a pipeline formed out of such devices would be bottlenecked by the single “weakest link”, i.e., the participant with the smallest training throughput. As a result, the more powerful nodes along the pipeline would be underutilized due to either lack of inputs or slow subsequent stages. On top of that, if any node fails or leaves training prematurely, it will stall the entire training procedure.</p>
      </para>
      <figure inlist="lof" labels="LABEL:fig:swarm" placement="t" xml:id="S3.F2">
        <tags>
          <tag><text fontsize="90%">Figure 2</text></tag>
          <tag role="autoref">Figure 2</tag>
          <tag role="refnum">2</tag>
          <tag role="typerefnum">Figure 2</tag>
        </tags>
        <graphics candidates="resources/swarm_v2_max.pdf" class="ltx_centering" graphic="resources/swarm_v2_max.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S3.F2.g1"/>
        <toccaption class="ltx_centering"><tag close=" ">2</tag>An overview of SWARM parallelism, illustrating both normal operation, device failures and adaptive rebalancing. One of the workers at stage 2 leaves; another peer from stage 3 takes its place by downloading the latest stage 2 parameters and statistics from peers.</toccaption>
        <caption class="ltx_centering"><tag close=": "><text fontsize="90%">Figure 2</text></tag><text fontsize="90%">An overview of SWARM parallelism, illustrating both normal operation, device failures and adaptive rebalancing. One of the workers at stage 2 leaves; another peer from stage 3 takes its place by downloading the latest stage 2 parameters and statistics from peers.</text></caption>
      </figure>
      <para xml:id="S3.SS2.p2">
        <p>To overcome these two challenges, we replace the rigid pipeline structure with temporary “pipelines” that are built stochastically on the fly during each iteration. Each participant can send their outputs to any peer that serves the next pipeline stage. Thus, if one peer is faster than others, it can process inputs from multiple predecessors and distribute its outputs across several weaker peers to maximize utilization. Also, if any participant disconnects, its predecessors can reroute their requests to its neighbors. New peers can download up-to-date parameters and optimizer statistics from remaining workers at the chosen stage. This allows the training to proceed as long as there is at least one active participant per stage: we elaborate on the fault tolerance of SWARM parallelism in Appendix <ref labelref="LABEL:appendix:faq"/>.</p>
      </para>
      <para xml:id="S3.SS2.p3">
        <p>The resulting system consists of several consecutive swarms, as depicted in Figure <ref labelref="LABEL:fig:swarm"/>. Peers within one swarm serve the same pipeline stage (i.e., the <text font="bold">same subset of layers</text> with <text font="bold">the same parameters</text>).
We assume that the model consists of similar “blocks” and thus partition it into evenly sized stages, leaving the study of better strategies <cite class="ltx_citemacro_citep">(<bibref bibrefs="huang2019gpipe,pipedream" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> as future work.
During the <text font="italic">forward</text> pass, peers receive inputs from predecessors (determined on each iteration) and send activations to peers in the next stage. For the <text font="italic">backward</text> pass, peers receive gradients for outputs, compute gradients for layer inputs and accumulate gradients for parameters. Once enough gradients are accumulated, peers form groups, run All-Reduce to average gradients within their pipeline stages and perform the optimizer step.</p>
      </para>
      <para xml:id="S3.SS2.p4">
        <p>SWARM parallelism can also use Delayed Parameter Updates (DPU) <cite class="ltx_citemacro_citep">(<bibref bibrefs="zerooffload" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> to further improve hardware utilization by performing the optimizer step in parallel with processing the next batch. While it is technically asynchronous, DPU was shown to achieve similar per-iteration convergence as fully synchronous training, both theoretically <cite class="ltx_citemacro_citep">(<bibref bibrefs="stich2020error,arjevani2020tight" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> and empirically <cite class="ltx_citemacro_citep">(<bibref bibrefs="zerooffload,dedloc" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.</p>
      </para>
      <para xml:id="S3.SS2.p5">
        <p>Each peer has queues for incoming and outgoing requests to maintain high GPU utilization under latency and to compensate for varying network speeds. Similarly to other pipeline implementations <cite class="ltx_citemacro_citep">(<bibref bibrefs="huang2019gpipe,megatron2" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, SWARM parallelism uses activation checkpointing <cite class="ltx_citemacro_citep">(<bibref bibrefs="gradient_checkpointing_autograd,gradient_checkpointing_dl" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> to reduce the memory footprint.</p>
      </para>
      <paragraph inlist="toc" xml:id="S3.SS2.SSS0.Px1">
        <title>Stochastic wiring.</title>
        <para xml:id="S3.SS2.SSS0.Px1.p1">
          <p>To better utilize heterogeneous devices and recover from faults, we dynamically “wire” each input through each stage and pick devices in proportion to their training throughput. To achieve this, SWARM peers run “trainer” processes that route training data through the “stages” of SWARM, balancing the load between peers.</p>
        </para>
        <para xml:id="S3.SS2.SSS0.Px1.p2">
          <p>For each pipeline stage, trainers discover which peers currently serve this stage via a Distributed Hash Table (DHT, <cite class="ltx_citemacro_citep"><bibref bibrefs="kademlia" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref></cite>). Trainers then assign a microbatch to one of those peers based on their performance. If that peer fails, it is temporarily banned and the microbatch is sent to another peer within the same stage. Note that trainers themselves do not use GPUs and have no trainable parameters, which makes it possible to run multiple trainers per peer.</p>
        </para>
        <para xml:id="S3.SS2.SSS0.Px1.p3">
          <p>Each trainer assigns data independently using the Interleaved Weighted Round-Robin <cite class="ltx_citemacro_citep">(<bibref bibrefs="iwrr,interleaved_round_robin" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite> scheduler. Our specific implementation of IWRR uses a priority queue: each peer is associated with <text font="italic">the total processing time over all previous requests</text>. A training minibatch is then routed to the node that has the smallest total processing time. Thus, for instance, if device A takes half as long to process a sample as device B, the routing algorithm will choose A twice as often as B. Finally, if a peer does not respond or fails to process the batch, trainer will “ban” this peer until it reannounces itself in the DHT, which is done every few minutes. For a more detailed description of stochastic wiring, please refer to Appendix <ref labelref="LABEL:appendix:wiring_details"/>.</p>
        </para>
        <para xml:id="S3.SS2.SSS0.Px1.p4">
          <p>Curiously, different trainers can have different throughput estimates for the same device because of the network topology. For instance, if training nodes are split between two cloud regions, a given peer’s trainer will have a higher throughput estimate for peers in the same data center. In other words, trainers automatically adjust to the network topology by routing more traffic to peers that are “nearby”.</p>
        </para>
      </paragraph>
      <paragraph inlist="toc" xml:id="S3.SS2.SSS0.Px2">
        <title>Adaptive swarm rebalancing.</title>
        <para xml:id="S3.SS2.SSS0.Px2.p1">
          <p>While stochastic wiring allows for automatic rebalancing within a stage, additional cross-stage rebalancing may be required to maximize throughput, especially when devices are very unreliable. As we described in Section <ref labelref="LABEL:sect:related_cost_efficent_collaborative"/>, our workers can join and leave training at any time. If any single pipeline stage loses too many peers, the remaining ones will face an increased processing load, which will inevitably form a bottleneck.</p>
        </para>
        <para xml:id="S3.SS2.SSS0.Px2.p2">
          <p>SWARM parallelism addresses this problem by allowing peers to dynamically switch between “pipeline stages” to maximize the training throughput. Every <Math mode="inline" tex="T" text="T" xml:id="S3.SS2.SSS0.Px2.p2.m1">
              <XMath>
                <XMTok font="italic" role="UNKNOWN">T</XMTok>
              </XMath>
            </Math> seconds, peers measure the utilization rate of each pipeline stage as the queue size.
Peers from the most underutilized pipeline stage will then switch to the most overutilized one (see Figure <ref labelref="LABEL:fig:swarm"/> for an overview and Appendix <ref labelref="LABEL:appendix:rebalancing_formal"/> for a formal description and complexity analysis), download the latest training state from their new neighbors and continue training. Similarly, if a new peer joins midway through training, it is assigned to the optimal pipeline stage by following the same protocol. As a side effect, if one pipeline stage requires more compute than others, SWARM will allocate more peers to that stage. In Section <ref labelref="LABEL:sect:experiments_adaptive"/>, we evaluate our approach to dynamic rebalancing in realistic conditions.</p>
        </para>
      </paragraph>
    </subsection>
  </section>
  <section inlist="toc" xml:id="S4">
    <tags>
      <tag>4</tag>
      <tag role="autoref">section 4</tag>
      <tag role="refnum">4</tag>
      <tag role="typerefnum">§4</tag>
    </tags>
    <title><tag close=" ">4</tag>Experiments</title>
    <subsection inlist="toc" labels="LABEL:sect:experiments_square_cube" xml:id="S4.SS1">
      <tags>
        <tag>4.1</tag>
        <tag role="autoref">subsection 4.1</tag>
        <tag role="refnum">4.1</tag>
        <tag role="typerefnum">§4.1</tag>
      </tags>
      <title><tag close=" ">4.1</tag>Communication Efficiency at Scale</title>
      <para xml:id="S4.SS1.p1">
        <p>Before we can meaningfully evaluate SWARM parallelism, we must verify our theoretical observations on communication efficiency. Here we run several controlled experiments that measure the GPU utilization and network usage for different model sizes, using the Transformer architecture <cite class="ltx_citemacro_citep">(<bibref bibrefs="transformer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> that has been widely adopted in various fields <cite class="ltx_citemacro_citep">(<bibref bibrefs="lin2021survey" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. To decouple the performance impact from other factors, we run these experiments on homogeneous V100 GPU nodes that serve one pipeline stage over the network with varying latency and bandwidth. We use a batch size of 1 and sequences of 512 tokens; the complete configuration is deferred to Appendix <ref labelref="LABEL:appendix:detailed_setup"/>.</p>
      </para>
      <para xml:id="S4.SS1.p2">
        <p>First, we measure how the model size affects the computation to communication ratio at 500 Mb/s network bandwidth in both directions. We consider 4 model configurations: the base configuration from the BERT paper <cite class="ltx_citemacro_citep">(<bibref bibrefs="bert" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, “xxlarge" (“large” with <Math mode="inline" tex="d_{model}{=}4096" text="d _ (m * o * d * e * l) = 4096" xml:id="S4.SS1.p2.m1">
            <XMath>
              <XMApp>
                <XMTok meaning="equals" role="RELOP">=</XMTok>
                <XMApp>
                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                  <XMTok font="italic" role="UNKNOWN">d</XMTok>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">m</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">l</XMTok>
                  </XMApp>
                </XMApp>
                <XMTok meaning="4096" role="NUMBER">4096</XMTok>
              </XMApp>
            </XMath>
          </Math>), which is used in several recent works <cite class="ltx_citemacro_citep">(<bibref bibrefs="albert,ernie3,deberta" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, and a GPT-3-scale model with <Math mode="inline" tex="d_{model}{=}12288" text="d _ (m * o * d * e * l) = 12288" xml:id="S4.SS1.p2.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="equals" role="RELOP">=</XMTok>
                <XMApp>
                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                  <XMTok font="italic" role="UNKNOWN">d</XMTok>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">m</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">l</XMTok>
                  </XMApp>
                </XMApp>
                <XMTok meaning="12288" role="NUMBER">12288</XMTok>
              </XMApp>
            </XMath>
          </Math> <cite class="ltx_citemacro_citep">(<bibref bibrefs="gpt3" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. We also evaluate a modified Transformer architecture (“Ours”) as defined in Section <ref labelref="LABEL:sect:experiments_large"/> with <Math mode="inline" tex="d_{model}{=}4096" text="d _ (m * o * d * e * l) = 4096" xml:id="S4.SS1.p2.m3">
            <XMath>
              <XMApp>
                <XMTok meaning="equals" role="RELOP">=</XMTok>
                <XMApp>
                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                  <XMTok font="italic" role="UNKNOWN">d</XMTok>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">m</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">l</XMTok>
                  </XMApp>
                </XMApp>
                <XMTok meaning="4096" role="NUMBER">4096</XMTok>
              </XMApp>
            </XMath>
          </Math>, 3 layers per pipeline stage and 8-bit quantized activations. As we demonstrate in Appendix <ref labelref="LABEL:appendix:compression"/>, this compression strategy can significantly reduce network usage with little effect on convergence. In the first three configurations, the model consists of 12 Transformer layers placed on 12 servers with a single GPU; in the last one, there are 4 servers, each hosting 3 layers.
Appendix <ref labelref="LABEL:appendix:detailed_setup"/> contains FLOP and parameter counts of each configuration.</p>
      </para>
      <figure align="center" class="ltx_figure_panel" inlist="lof" labels="LABEL:fig:throughput_exps" placement="b" xml:id="S4.F3">
        <graphics candidates="resources/perf_absolute.pdf" class="ltx_figure_panel" graphic="resources/perf_absolute.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S4.SS1.g1"/>
        <tags>
          <tag><text fontsize="90%">Figure 3</text></tag>
          <tag role="autoref">Figure 3</tag>
          <tag role="refnum">3</tag>
          <tag role="typerefnum">Figure 3</tag>
        </tags>
        <toccaption><tag close=" ">3</tag>Pipeline computation and idle time per batch at 500 Mb/s bandwidth.</toccaption>
        <caption><tag close=": "><text fontsize="90%">Figure 3</text></tag><text fontsize="90%">Pipeline computation and idle time per batch at 500 Mb/s bandwidth.</text></caption>
      </figure>
      <table class="ltx_figure_panel" inlist="lot" labels="LABEL:tab:latency" xml:id="S4.T1">
        <tags>
          <tag><text fontsize="90%">Table 1</text></tag>
          <tag role="autoref">Table 1</tag>
          <tag role="refnum">1</tag>
          <tag role="typerefnum">Table 1</tag>
        </tags>
<!--  %****␣experiments.tex␣Line␣25␣**** -->        <toccaption><tag close=" "><!--  %****␣experiments.tex␣Line␣25␣**** -->1<!--  %****␣experiments.tex␣Line␣25␣**** 
     %****␣experiments.tex␣Line␣25␣**** 
     %****␣experiments.tex␣Line␣25␣**** 
     %****␣experiments.tex␣Line␣25␣**** 
     %****␣experiments.tex␣Line␣25␣**** --></tag><!--  %****␣experiments.tex␣Line␣25␣**** -->Relative device utilization at 500 Mb/s bandwidth and varying network latency.</toccaption>
        <caption><tag close=": "><text fontsize="90%">Table 1</text></tag><!--  %****␣experiments.tex␣Line␣25␣**** 
     %****␣experiments.tex␣Line␣25␣**** 
     %****␣experiments.tex␣Line␣25␣**** 
     %****␣experiments.tex␣Line␣25␣**** --><text fontsize="90%">Relative device utilization at 500 Mb/s bandwidth and varying network latency.</text></caption>
<!--  %****␣experiments.tex␣Line␣25␣**** -->        <tabular class="ltx_figure_panel" colsep="8.0pt" vattach="bottom">
          <tr>
            <td align="left" border="tt"><text fontsize="90%"><ERROR class="undefined">\thead</ERROR>Latency</text><text fontsize="90%">
(RTT)</text></td>
            <td align="right" border="tt" class="ltx_nopad_r" colspan="4"><text fontsize="90%">
</text><ERROR class="undefined">\thead</ERROR><text fontsize="90%">
Relative GPU utilization</text></td>
          </tr>
          <tr>
            <td align="left"><text fontsize="90%">(100% - idle time)</text></td>
            <td/>
            <td/>
            <td/>
            <td/>
          </tr>
          <tr>
            <td/>
            <td align="center" border="t"><text fontsize="90%">base</text></td>
            <td align="center" border="t"><text fontsize="90%">xxlarge</text></td>
            <td align="center" border="t"><text fontsize="90%">GPT-3</text></td>
            <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">Ours</text></td>
          </tr>
          <tr>
            <td align="left" border="t"><text fontsize="90%">None</text></td>
            <td align="center" border="t"><text fontsize="90%">18.0%</text></td>
            <td align="center" border="t"><text fontsize="90%">32.1%</text></td>
            <td align="center" border="t"><text fontsize="90%">82.1%</text></td>
            <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">89.5%</text></td>
          </tr>
          <tr>
            <td align="left"><text fontsize="90%">10ms</text></td>
            <td align="center"><text fontsize="90%">11.8%</text></td>
            <td align="center"><text fontsize="90%">28.9%</text></td>
            <td align="center"><text fontsize="90%">79.3%</text></td>
            <td align="center" class="ltx_nopad_r"><text fontsize="90%">87.2%</text></td>
          </tr>
          <tr>
            <td align="left"><text fontsize="90%">50ms</text></td>
            <td align="center"><text fontsize="90%">4.88%</text></td>
            <td align="center"><text fontsize="90%">20.1%</text></td>
            <td align="center"><text fontsize="90%">70.3%</text></td>
            <td align="center" class="ltx_nopad_r"><text fontsize="90%">79.5%</text></td>
          </tr>
          <tr>
            <td align="left"><text fontsize="90%">100ms</text></td>
            <td align="center"><text fontsize="90%">2.78%</text></td>
            <td align="center"><text fontsize="90%">14.9%</text></td>
            <td align="center"><text fontsize="90%">60.2%</text></td>
            <td align="center" class="ltx_nopad_r"><text fontsize="90%">71.5%</text></td>
          </tr>
          <tr>
            <td align="left" border="bb"><text fontsize="90%">200ms</text></td>
            <td align="center" border="bb"><text fontsize="90%">1.53%</text></td>
            <td align="center" border="bb"><text fontsize="90%">10.1%</text></td>
            <td align="center" border="bb"><text fontsize="90%">48.5%</text></td>
            <td align="center" border="bb" class="ltx_nopad_r"><text fontsize="90%">59.2%</text></td>
          </tr>
        </tabular>
        <break class="ltx_break"/>
        <p class="ltx_figure_panel"><text fontsize="90%">As depicted in Figure <ref labelref="LABEL:fig:squarecube"/> (right) and Figure <ref labelref="LABEL:fig:throughput_exps"/>, larger models achieve better GPU utilization rate in the same network conditions, since their communication load grows slower than computation. More importantly, even at 500 Mb/s, the resulting GPU idle time can be pushed into the 10–20% range, either naturally for GPT-3-sized models or through activation compression for smaller models. In addition, large models maintain most of their training efficiency at the 100ms latency (Table <ref labelref="LABEL:tab:latency"/>), which is roughly equivalent to training on different continents <cite class="ltx_citemacro_citep">(<bibref bibrefs="verizon_latency" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                <bibrefphrase>, </bibrefphrase>
              </bibref>)</cite>.</text></p>
        <break class="ltx_break"/>
        <subsection class="ltx_figure_panel" inlist="toc" labels="LABEL:appendix:training_throughput" xml:id="S4.SS2">
          <tags>
            <tag>4.2</tag>
            <tag role="autoref">subsection 4.2</tag>
            <tag role="refnum">4.2</tag>
            <tag role="typerefnum">§4.2</tag>
          </tags>
          <title fontsize="90%"><tag close=" ">4.2</tag>Detailed Performance Comparison</title>
          <para xml:id="S4.SS2.p1">
            <p><text fontsize="90%">Here we investigate how SWARM parallelism compares to existing systems for training large models: </text><text font="bold" fontsize="90%">GPipe</text><text fontsize="90%"> </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="huang2019gpipe" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                  <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> and </text><text font="bold" fontsize="90%">ZeRO-Offload</text><text fontsize="90%"> </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="zerooffload" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                  <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.
The purpose of this section is to compare the training throughput in “ideal” conditions (with homogeneous reliable devices and balanced layers), as deviating from these conditions makes it </text><text font="italic" fontsize="90%">infeasible</text><text fontsize="90%"> to train with baseline systems.
Still, even in such conditions the performance of different systems can vary across model architectures, and hence we want to identify the cases in which using SWARM is preferable to other approaches.
We benchmark individual SWARM components in preemptible setups in Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_adaptive"/><text fontsize="90%"> and Appendix </text><ref fontsize="90%" labelref="LABEL:appendix:scaling"/><text fontsize="90%">.</text></p>
          </para>
          <para xml:id="S4.SS2.p2">
            <p><text fontsize="90%">We evaluate training performance for sequences of 4 Transformer layers of identical size distributed over 16 workers. Similarly to Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_square_cube"/><text fontsize="90%">, we use three layer configurations: “xxlarge” (</text><Math mode="inline" tex="d_{model}{=}4096" text="d _ (m * o * d * e * l) = 4096" xml:id="S4.SS2.p2.m1">
                <XMath>
                  <XMApp>
                    <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">o</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">e</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">l</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMTok fontsize="90%" meaning="4096" role="NUMBER">4096</XMTok>
                  </XMApp>
                </XMath>
              </Math><text fontsize="90%">, </text><Math mode="inline" tex="d_{\text{FFN}}{=}16384" text="d _ [FFN] = 16384" xml:id="S4.SS2.p2.m2">
                <XMath>
                  <XMApp>
                    <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                      <XMText><text fontsize="63%">FFN</text></XMText>
                    </XMApp>
                    <XMTok fontsize="90%" meaning="16384" role="NUMBER">16384</XMTok>
                  </XMApp>
                </XMath>
              </Math><text fontsize="90%">, 32 heads), “GPT-3” (</text><Math mode="inline" tex="d_{model}{=}12288" text="d _ (m * o * d * e * l) = 12288" xml:id="S4.SS2.p2.m3">
                <XMath>
                  <XMApp>
                    <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">o</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">e</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">l</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMTok fontsize="90%" meaning="12288" role="NUMBER">12288</XMTok>
                  </XMApp>
                </XMath>
              </Math><text fontsize="90%">, </text><Math mode="inline" tex="d_{\text{FFN}}{=}49152" text="d _ [FFN] = 49152" xml:id="S4.SS2.p2.m4">
                <XMath>
                  <XMApp>
                    <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                      <XMText><text fontsize="63%">FFN</text></XMText>
                    </XMApp>
                    <XMTok fontsize="90%" meaning="49152" role="NUMBER">49152</XMTok>
                  </XMApp>
                </XMath>
              </Math><text fontsize="90%">, 96 heads), and “Ours” (</text><Math mode="inline" tex="d_{model}{=}4096" text="d _ (m * o * d * e * l) = 4096" xml:id="S4.SS2.p2.m5">
                <XMath>
                  <XMApp>
                    <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">o</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">e</XMTok>
                        <XMTok font="italic" fontsize="63%" role="UNKNOWN">l</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMTok fontsize="90%" meaning="4096" role="NUMBER">4096</XMTok>
                  </XMApp>
                </XMath>
              </Math><text fontsize="90%">, </text><Math mode="inline" tex="d_{\text{FFN}}{=}16384" text="d _ [FFN] = 16384" xml:id="S4.SS2.p2.m6">
                <XMath>
                  <XMApp>
                    <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                      <XMText><text fontsize="63%">FFN</text></XMText>
                    </XMApp>
                    <XMTok fontsize="90%" meaning="16384" role="NUMBER">16384</XMTok>
                  </XMApp>
                </XMath>
              </Math><text fontsize="90%">, 32 heads, 16 shared layers per block, last stage holds only the vocabulary projection layer). The microbatch size is 4 for “xxlarge” and 1 for “GPT-3” and “Ours”, and the sequence length is 512.</text></p>
          </para>
          <para xml:id="S4.SS2.p3">
            <p><text fontsize="90%">To provide a more detailed view of the training performance, we measure two separate performance statistics: the training throughput and the All-Reduce time.
The training throughput measures the rate at which the system can process training sequences, i.e., run forward and backward passes.
More specifically, we measure the time required to process 6250 sequences of 512 tokens, which corresponds to the largest batch size used in </text><cite class="ltx_citemacro_citet"><bibref bibrefs="gpt3" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                  <bibrefphrase><text fontsize="90%">(</text></bibrefphrase>
                  <bibrefphrase><text fontsize="90%">)</text></bibrefphrase>
                </bibref></cite><text fontsize="90%">.
In turn, the All-Reduce time is the time each system spends to aggregate accumulated gradients across devices.
Intuitively, training with small batch sizes is more sensitive to the All-Reduce time (since the algorithm needs to run All-Reduce more frequently) and vice versa.</text></p>
          </para>
          <para xml:id="S4.SS2.p4">
            <p><text font="bold" fontsize="90%">Hardware setup:</text><text fontsize="90%"> Each worker uses a V100-PCIe GPU with 16 CPU threads (E5 v5-2660v4) and 128 GB RAM. The only exception is for ZeRO-Offload with “GPT-3” layers, where we had to double the RAM size because the system required 190 gigabytes at peak. Similarly to Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_square_cube"/><text fontsize="90%">, each worker can communicate at a 500 Mb/s bandwidth for both upload and download for a total of 1 Gb/s.
In terms of network latency, we consider two setups: with </text><text font="bold" fontsize="90%">no latency</text><text fontsize="90%">, where workers communicate normally within the same rack, and with </text><text font="bold" fontsize="90%">latency</text><text fontsize="90%">, where we introduce additional </text><Math mode="inline" tex="100\pm 50" text="100 plus-or-minus 50" xml:id="S4.SS2.p4.m1">
                <XMath>
                  <XMApp>
                    <XMTok fontsize="90%" meaning="plus-or-minus" name="pm" role="ADDOP">±</XMTok>
                    <XMTok fontsize="90%" meaning="100" role="NUMBER">100</XMTok>
                    <XMTok fontsize="90%" meaning="50" role="NUMBER">50</XMTok>
                  </XMApp>
                </XMath>
              </Math><text fontsize="90%">ms latency directly in the kernel</text><note mark="4" role="footnote" xml:id="footnote4"><tags>
                  <tag>4</tag>
                  <tag role="autoref">footnote 4</tag>
                  <tag role="refnum">4</tag>
                  <tag role="typerefnum">footnote 4</tag>
                </tags>More specifically, <text font="typewriter">tc qdisc add dev &lt;...&gt; root netem delay 100ms 50ms</text></note><text fontsize="90%">.</text></p>
          </para>
          <para xml:id="S4.SS2.p5">
            <p><text font="bold" fontsize="90%">GPipe configuration:</text><text fontsize="90%"> We use a popular PyTorch-based implementation of GPipe</text><note mark="5" role="footnote" xml:id="footnote5"><tags>
                  <tag>5</tag>
                  <tag role="autoref">footnote 5</tag>
                  <tag role="refnum">5</tag>
                  <tag role="typerefnum">footnote 5</tag>
                </tags>The source code is available at <ref class="ltx_url" font="typewriter" href="https://github.com/kakaobrain/torchgpipe">https://github.com/kakaobrain/torchgpipe</ref></note><text fontsize="90%">. The model is partitioned into 4 stages repeated over 4 model-parallel groups. To fit into the GPU memory for the “GPT-3” configuration, we offload the optimizer into RAM using ZeRO-Offload. Before averaging, we use PyTorch’s built-in All-Reduce to aggregate gradients.
We evaluate both the standard GPipe schedule and the 1F1B schedule </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="pipedream" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                  <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.</text></p>
          </para>
          <para xml:id="S4.SS2.p6">
            <p><text font="bold" fontsize="90%">ZeRO-Offload configuration:</text><text fontsize="90%"> Each worker runs the entire model individually, then exchanges gradients with peers. For “xxlarge”, we use the official implementation from </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="zerooffload" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                  <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. However, for “GPT-3”, we found that optimizer offloading still does not allow us to fit 4 layers into the GPU. For this reason, we also offload the model parameters using the </text><text font="typewriter" fontsize="90%">offload_param</text><text fontsize="90%"> option.</text></p>
          </para>
          <table class="ltx_figure_panel" inlist="lot" labels="LABEL:tab:throughput_gpt" xml:id="S4.T2">
            <tags>
              <tag><text fontsize="90%">Table 2</text></tag>
              <tag role="autoref">Table 2</tag>
              <tag role="refnum">2</tag>
              <tag role="typerefnum">Table 2</tag>
            </tags>
            <toccaption><tag close=" "><text fontsize="90%">2</text></tag><text fontsize="90%">Training performance for different model sizes.</text></toccaption>
            <caption fontsize="90%"><tag close=": ">Table 2</tag>Training performance for different model sizes.</caption>
            <tabular class="ltx_centering ltx_figure_panel" colsep="4.0pt" vattach="bottom">
              <tr>
                <td align="left" border="tt" rowspan="2"><text fontsize="90%">System</text></td>
                <td align="center" border="tt" colspan="2"><text fontsize="90%">Throughput, min/batch</text></td>
                <td align="center" border="tt" colspan="2"><text fontsize="90%">All-Reduce time, min</text></td>
              </tr>
              <tr>
                <td align="center" border="t"><text fontsize="90%">No latency</text></td>
                <td align="center" border="t"><text fontsize="90%">Latency</text></td>
                <td align="center" border="t"><text fontsize="90%">No latency</text></td>
                <td align="center" border="t"><text fontsize="90%">Latency</text></td>
              </tr>
              <tr>
                <td align="center" border="t" colspan="5"><text fontsize="90%">“GPT-3” (4 layers)</text></td>
              </tr>
              <tr>
                <td align="left" border="t"><text fontsize="90%">SWARM</text></td>
                <td align="center" border="t"><text fontsize="90%">168.3</text></td>
                <td align="center" border="t"><text font="bold" fontsize="90%">186.7</text></td>
                <td align="center" border="t"><text fontsize="90%">7.4</text></td>
                <td align="center" border="t"><text font="bold" fontsize="90%">7.6</text></td>
              </tr>
              <tr>
                <td align="left"><text fontsize="90%">GPipe</text></td>
                <td align="center"><text fontsize="90%">164.5</text></td>
                <td align="center"><text fontsize="90%">218.4</text></td>
                <td align="center" rowspan="2"><text font="bold" fontsize="90%">6.7</text></td>
                <td align="center" rowspan="2"><text fontsize="90%">7.8</text></td>
              </tr>
              <tr>
                <td align="left"><text fontsize="90%">1F1B</text></td>
                <td align="center"><text font="bold" fontsize="90%">163.3</text></td>
                <td align="center"><text fontsize="90%">216.1</text></td>
              </tr>
              <tr>
                <td align="left"><text fontsize="90%">Offload</text></td>
                <td align="center"><text fontsize="90%">272.7</text></td>
                <td align="center"><text fontsize="90%">272.7</text></td>
                <td align="center"><text fontsize="90%">25.5</text></td>
                <td align="center"><text fontsize="90%">27.3</text></td>
              </tr>
              <tr>
                <td align="center" border="t" colspan="5"><text fontsize="90%">“xxlarge” (4 layers)</text></td>
              </tr>
              <tr>
                <td align="left" border="t"><text fontsize="90%">SWARM</text></td>
                <td align="center" border="t"><text fontsize="90%">44.2</text></td>
                <td align="center" border="t"><text fontsize="90%">48.2</text></td>
                <td align="center" border="t"><text fontsize="90%">0.8</text></td>
                <td align="center" border="t"><text font="bold" fontsize="90%">0.9</text></td>
              </tr>
              <tr>
                <td align="left"><text fontsize="90%">GPipe</text></td>
                <td align="center"><text fontsize="90%">40.1</text></td>
                <td align="center"><text fontsize="90%">108.8</text></td>
                <td align="center" rowspan="2"><text font="bold" fontsize="90%">0.7</text></td>
                <td align="center" rowspan="2"><text fontsize="90%">1.1</text></td>
              </tr>
              <tr>
                <td align="left"><text fontsize="90%">1F1B</text></td>
                <td align="center"><text fontsize="90%">40.8</text></td>
                <td align="center"><text fontsize="90%">105.5</text></td>
              </tr>
              <tr>
                <td align="left"><text fontsize="90%">Offload</text></td>
                <td align="center"><text font="bold" fontsize="90%">33.8</text></td>
                <td align="center"><text font="bold" fontsize="90%">33.8</text></td>
                <td align="center"><text fontsize="90%">2.8</text></td>
                <td align="center"><text fontsize="90%">4.2</text></td>
              </tr>
              <tr>
                <td align="center" border="t" colspan="5"><text fontsize="90%">Full “Ours” model (48 shared layers + embeddings)</text></td>
              </tr>
              <tr>
                <td align="left" border="t"><text fontsize="90%">SWARM</text></td>
                <td align="center" border="t"><text fontsize="90%">432.2</text></td>
                <td align="center" border="t"><text fontsize="90%">452.9</text></td>
                <td align="center" border="t"><text fontsize="90%">0.8</text></td>
                <td align="center" border="t"><text font="bold" fontsize="90%">1.0</text></td>
              </tr>
              <tr>
                <td align="left"><text fontsize="90%">GPipe</text></td>
                <td align="center"><text fontsize="90%">420.0</text></td>
                <td align="center"><text fontsize="90%">602.1</text></td>
                <td align="center" rowspan="2"><text font="bold" fontsize="90%">0.7</text></td>
                <td align="center" rowspan="2"><text fontsize="90%">1.1</text></td>
              </tr>
              <tr>
                <td align="left"><text fontsize="90%">1F1B</text></td>
                <td align="center"><text fontsize="90%">408.5</text></td>
                <td align="center"><text fontsize="90%">569.2</text></td>
              </tr>
              <tr>
                <td align="left" border="bb"><text fontsize="90%">Offload</text></td>
                <td align="center" border="bb"><text font="bold" fontsize="90%">372.0</text></td>
                <td align="center" border="bb"><text font="bold" fontsize="90%">372.0</text></td>
                <td align="center" border="bb"><text fontsize="90%">3.2</text></td>
                <td align="center" border="bb"><text fontsize="90%">4.8</text></td>
              </tr>
            </tabular>
          </table>
          <figure align="center" class="ltx_figure_panel" inlist="lof" labels="LABEL:fig:convergence" placement="b" xml:id="S4.F4">
            <graphics candidates="resources/learning_3stages.pdf" class="ltx_figure_panel" graphic="resources/learning_3stages.pdf" options="width=281.85034pt,keepaspectratio=true" xml:id="S4.SS2.g1"/>
            <tags>
              <tag><text fontsize="90%">Figure 4</text></tag>
              <tag role="autoref">Figure 4</tag>
              <tag role="refnum">4</tag>
              <tag role="typerefnum">Figure 4</tag>
            </tags>
            <toccaption><tag close=" "><text fontsize="90%">4</text></tag><text fontsize="90%">Training convergence comparison.</text></toccaption>
            <caption fontsize="90%"><tag close=": ">Figure 4</tag>Training convergence comparison.</caption>
          </figure>
          <para xml:id="S4.SS2.p7">
            <p><text fontsize="90%">In turn, when training smaller models, ZeRO-Offload outperforms both SWARM and GPipe. This result aligns with our earlier observations in Figure </text><ref fontsize="90%" labelref="LABEL:fig:squarecube"/><text fontsize="90%">, where the same model spent most of the time waiting for the communication between pipeline stages.</text></p>
          </para>
          <para xml:id="S4.SS2.p8">
            <p><text fontsize="90%">We also observe that ZeRO-Offload takes longer to aggregate gradients, likely because each peer must aggregate the entire model, whereas in SWARM and GPipe, peers aggregate a single pipeline stage. The variation between All-Reduce time in GPipe and SWARM is due to implementation differences. Overall, SWARM is competitive to HPC baselines even in an idealized homogeneous environment.</text></p>
          </para>
          <subsection inlist="toc" labels="LABEL:sect:experiments_large" xml:id="S4.SS3">
            <tags>
              <tag>4.3</tag>
              <tag role="autoref">subsection 4.3</tag>
              <tag role="refnum">4.3</tag>
              <tag role="typerefnum">§4.3</tag>
            </tags>
            <title fontsize="90%"><tag close=" ">4.3</tag>Large-Scale Distributed Training</title>
            <para xml:id="S4.SS3.p1">
              <p><text fontsize="90%">To verify the efficiency of SWARM parallelism in a practical scenario, we conduct a series of large-scale distributed experiments using preemptible (unreliable) cloud T4 and A100 GPUs over a public cloud network.</text></p>
            </para>
            <para xml:id="S4.SS3.p2">
              <p><text fontsize="90%">We train a Transformer language model with the architecture similar to prior work </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="gpt3,gptj,gptneo" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                    <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                  </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> and 1.01 billion parameters in total. Our model consists of 3 stages, each containing a single Transformer decoder block with </text><Math mode="inline" tex="d_{model}=4096" text="d _ (m * o * d * e * l) = 4096" xml:id="S4.SS3.p2.m1">
                  <XMath>
                    <XMApp>
                      <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                      <XMApp>
                        <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                        <XMApp>
                          <XMTok meaning="times" role="MULOP">⁢</XMTok>
                          <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                          <XMTok font="italic" fontsize="63%" role="UNKNOWN">o</XMTok>
                          <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                          <XMTok font="italic" fontsize="63%" role="UNKNOWN">e</XMTok>
                          <XMTok font="italic" fontsize="63%" role="UNKNOWN">l</XMTok>
                        </XMApp>
                      </XMApp>
                      <XMTok fontsize="90%" meaning="4096" role="NUMBER">4096</XMTok>
                    </XMApp>
                  </XMath>
                </Math><text fontsize="90%"> and 16 layers per pipeline stage. All workers within a stage serve the same group of layers, and all layers within each group use the same set of parameters, similarly to ALBERT </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="albert" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                    <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                  </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. On top of this, the first stage also contains the embedding layer, and the last stage includes the language modeling head. Because of layer sharing, this model is equivalent to a 13B model from </text><cite class="ltx_citemacro_citet"><bibref bibrefs="gpt3" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                    <bibrefphrase><text fontsize="90%">(</text></bibrefphrase>
                    <bibrefphrase><text fontsize="90%">)</text></bibrefphrase>
                  </bibref></cite><text fontsize="90%"> in terms of compute costs.</text></p>
            </para>
            <para xml:id="S4.SS3.p3">
              <p><text fontsize="90%">We use 8-bit compression </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="adam8bit" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                    <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                  </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> for activations and gradients to reduce the communication intensity. Additional training setup details are covered in Appendix </text><ref fontsize="90%" labelref="LABEL:appendix:detailed_large"/><text fontsize="90%">.
SWARM nodes run rebalancing every </text><Math mode="inline" tex="T=300" text="T = 300" xml:id="S4.SS3.p3.m1">
                  <XMath>
                    <XMApp>
                      <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                      <XMTok fontsize="90%" meaning="300" role="NUMBER">300</XMTok>
                    </XMApp>
                  </XMath>
                </Math><text fontsize="90%"> seconds, and trainers measure peer performance using a moving average with </text><Math mode="inline" tex="\alpha=0.1" text="alpha = 0.1" xml:id="S4.SS3.p3.m2">
                  <XMath>
                    <XMApp>
                      <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                      <XMTok font="italic" fontsize="90%" name="alpha" role="UNKNOWN">α</XMTok>
                      <XMTok fontsize="90%" meaning="0.1" role="NUMBER">0.1</XMTok>
                    </XMApp>
                  </XMath>
                </Math><text fontsize="90%">. However, as we show in Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_adaptive"/><text fontsize="90%">, the throughput of SWARM is not very sensitive to the choice of these hyperparameters.</text></p>
            </para>
            <para xml:id="S4.SS3.p4">
              <p><text fontsize="90%">First, to verify that model parallelism with asynchronous updates does not have significant convergence issues, we train the model on the Pile </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="gao2020pile" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                    <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                  </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> dataset with 400 preemptible T4 instances, each hosting one accelerator. As a baseline, we use regular data-parallel training with offloading on 128 A100 GPUs.
We run both experiments for approximately 4 weeks and compare the learning curves.</text></p>
            </para>
            <para xml:id="S4.SS3.p5">
              <p><text fontsize="90%">Figure </text><ref fontsize="90%" labelref="LABEL:fig:convergence"/><text fontsize="90%"> shows the results of this experiment: it can be seen that the training dynamics of two approaches are indeed similar, which demonstrates the viability of SWARM parallelism for heterogeneous and poorly-connected devices.</text></p>
            </para>
            <para xml:id="S4.SS3.p6">
              <p><text fontsize="90%">In the next experiment, we aim to measure the pipeline throughput in different hardware conditions and to compare it with an estimate of best-case pipeline performance.
We consider several setups: first, we use the same 400 preemptible T4 nodes; in another setup, we use 7 instances with 8 A100 GPU each; finally, we combine these fleets to create a heterogeneous setup. We examine the performance of the pipeline both with weight sharing and with standard, more common, Transformer blocks.</text></p>
            </para>
            <table class="ltx_figure_panel" inlist="lot" labels="LABEL:tab:throughput" xml:id="S4.T3">
              <tags>
                <tag><text fontsize="90%">Table 3</text></tag>
                <tag role="autoref">Table 3</tag>
                <tag role="refnum">3</tag>
                <tag role="typerefnum">Table 3</tag>
              </tags>
<!--  %****␣experiments.tex␣Line␣150␣**** -->              <toccaption><tag close=" "><!--  %****␣experiments.tex␣Line␣150␣**** --><text fontsize="90%">3</text><!--  %****␣experiments.tex␣Line␣150␣**** 
     %****␣experiments.tex␣Line␣150␣**** 
     %****␣experiments.tex␣Line␣150␣**** 
     %****␣experiments.tex␣Line␣150␣**** 
     %****␣experiments.tex␣Line␣150␣**** --></tag><!--  %****␣experiments.tex␣Line␣150␣**** --><text fontsize="90%">Pipeline throughput, layer sharing.</text></toccaption>
              <caption fontsize="90%"><tag close=": ">Table 3</tag><!--  %****␣experiments.tex␣Line␣150␣**** 
     %****␣experiments.tex␣Line␣150␣**** 
     %****␣experiments.tex␣Line␣150␣**** 
     %****␣experiments.tex␣Line␣150␣**** -->Pipeline throughput, layer sharing.</caption>
<!--  %****␣experiments.tex␣Line␣150␣**** -->              <tabular class="ltx_centering ltx_figure_panel" colsep="8.0pt" vattach="middle">
                <tr>
                  <td align="left" border="tt" rowspan="2"><text fontsize="90%"><tabular colsep="8.0pt" vattach="middle">
                        <tr>
                          <td align="left" class="ltx_nopad_r">Hardware</td>
                        </tr>
                        <tr>
                          <td align="left" class="ltx_nopad_r">setup</td>
                        </tr>
                      </tabular></text></td>
                  <td align="center" border="tt" colspan="2"><tabular colsep="8.0pt" vattach="middle">
                      <tr>
                        <td align="center" class="ltx_nopad_r"><text fontsize="90%">Throughput,</text></td>
                      </tr>
                      <tr>
                        <td align="center" class="ltx_nopad_r"><text fontsize="90%">samples/s</text></td>
                      </tr>
                    </tabular></td>
                  <td align="center" border="tt" colspan="2"><tabular colsep="8.0pt" vattach="middle">
                      <tr>
                        <td align="center" class="ltx_nopad_r"><text fontsize="90%">Optimal</text></td>
                      </tr>
                      <tr>
                        <td align="center" class="ltx_nopad_r"><text fontsize="90%">bandwidth, Mb/s</text></td>
                      </tr>
                    </tabular></td>
                </tr>
                <tr>
                  <td align="center" border="t"><text fontsize="90%">Actual</text></td>
                  <td align="center" border="t"><text fontsize="90%">Best-case</text></td>
                  <td align="center" border="t"><text fontsize="90%">Upload</text></td>
                  <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">Download</text></td>
                </tr>
                <tr>
                  <td align="left" border="t"><text fontsize="90%">T4</text></td>
                  <td align="center" border="t"><text fontsize="90%">17.6</text></td>
                  <td align="center" border="t"><text fontsize="90%">19.2</text></td>
                  <td align="center" border="t"><text fontsize="90%">317.8</text></td>
                  <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">397.9</text></td>
                </tr>
                <tr>
                  <td align="left"><text fontsize="90%">A100</text></td>
                  <td align="center"><text fontsize="90%">16.9</text></td>
                  <td align="center"><text fontsize="90%">25.5</text></td>
                  <td align="center"><text fontsize="90%">436.1</text></td>
                  <td align="center" class="ltx_nopad_r"><text fontsize="90%">545.1</text></td>
                </tr>
                <tr>
                  <td align="left" border="bb"><text fontsize="90%">T4 &amp; A100</text></td>
                  <td align="center" border="bb"><text fontsize="90%">27.3</text></td>
                  <td align="center" border="bb"><text fontsize="90%">—</text></td>
                  <td align="center" border="bb"><text fontsize="90%">—</text></td>
                  <td align="center" border="bb" class="ltx_nopad_r"><text fontsize="90%">—</text></td>
                </tr>
              </tabular>
            </table>
            <table class="ltx_figure_panel" inlist="lot" labels="LABEL:tab:throughput_standard" xml:id="S4.T4">
              <tags>
                <tag><text fontsize="90%">Table 4</text></tag>
                <tag role="autoref">Table 4</tag>
                <tag role="refnum">4</tag>
                <tag role="typerefnum">Table 4</tag>
              </tags>
              <toccaption><tag close=" "><text fontsize="90%">4</text></tag><text fontsize="90%">Pipeline throughput, default Transformer.</text></toccaption>
              <caption fontsize="90%"><tag close=": ">Table 4</tag>Pipeline throughput, default Transformer.</caption>
              <tabular class="ltx_centering ltx_figure_panel" colsep="8.0pt" vattach="middle">
                <tr>
                  <td align="left" border="tt" rowspan="2"><text fontsize="90%"><tabular colsep="8.0pt" vattach="middle">
                        <tr>
                          <td align="left" class="ltx_nopad_r">Hardware</td>
                        </tr>
                        <tr>
                          <td align="left" class="ltx_nopad_r">setup</td>
                        </tr>
                      </tabular></text></td>
                  <td align="center" border="tt" colspan="2"><tabular colsep="8.0pt" vattach="middle">
                      <tr>
                        <td align="center" class="ltx_nopad_r"><text fontsize="90%">Throughput,</text></td>
                      </tr>
                      <tr>
                        <td align="center" class="ltx_nopad_r"><text fontsize="90%">samples/s</text></td>
                      </tr>
                    </tabular></td>
                </tr>
                <tr>
                  <td align="center" border="t"><text fontsize="90%">Actual</text></td>
                  <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">Best-case</text></td>
                </tr>
                <tr>
                  <td align="left" border="t"><text fontsize="90%">T4</text></td>
                  <td align="center" border="t"><text fontsize="90%">8.8</text></td>
                  <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">19.3</text></td>
                </tr>
                <tr>
                  <td align="left"><text fontsize="90%">A100</text></td>
                  <td align="center"><text fontsize="90%">8.0</text></td>
                  <td align="center" class="ltx_nopad_r"><text fontsize="90%">25.1</text></td>
                </tr>
                <tr>
                  <td align="left" border="bb"><text fontsize="90%">T4 &amp; A100</text></td>
                  <td align="center" border="bb"><text fontsize="90%">13.4</text></td>
                  <td align="center" border="bb" class="ltx_nopad_r"><text fontsize="90%">—</text></td>
                </tr>
              </tabular>
            </table>
            <para xml:id="S4.SS3.p7">
              <p><text fontsize="90%">We measure the number of randomly generated samples processed by the pipeline both in our infrastructure and the ideal case that ignores all network-related operations (i.e., has infinite bandwidth and zero latency). The ideal case is emulated by executing a single pipeline stage 3 times locally on a single server and multiplying the single-node estimates by the number of nodes.</text></p>
            </para>
            <para xml:id="S4.SS3.p8">
              <p><text fontsize="90%">As demonstrated in the left two columns of Table </text><ref fontsize="90%" labelref="LABEL:tab:throughput"/><text fontsize="90%"> and Table </text><ref fontsize="90%" labelref="LABEL:tab:throughput_standard"/><text fontsize="90%">, asynchronous training of compute-intensive models with 8-bit compressed activations regardless of the architecture specifics allows us to achieve high performance without a dedicated networking solution. Furthermore, the load balancing algorithm of SWARM allows us to dynamically and efficiently utilize different hardware without being bottlenecked by slower devices.</text></p>
            </para>
            <para xml:id="S4.SS3.p9">
              <p><text fontsize="90%">Next, we use the same load testing scenario to estimate the bandwidth required to fully utilize each device type in the above infrastructure. For this, we measure the average incoming and outgoing bandwidth on the nodes that serve the intermediate stage of the pipeline. We summarize our findings in the right two columns of Table </text><ref fontsize="90%" labelref="LABEL:tab:throughput"/><text fontsize="90%">: it turns out that with layer sharing and 8-bit compression, medium-performance GPUs (such as T4) can be saturated even with moderate network speeds. Based on our main experiment, the optimal total bandwidth is roughly 100Mb/s higher than the values reported in Table 3 due to gradient averaging, loading state from peers, maintaining the DHT and streaming the training data.
Although training over the Internet with more efficient hardware might indeed underutilize the accelerator, this issue can be offset by advanced compression strategies such as compression-aware architectures or layer sharing, as shown in Table </text><ref fontsize="90%" labelref="LABEL:tab:throughput"/><text fontsize="90%">.</text></p>
            </para>
            <figure align="center" class="ltx_figure_panel" inlist="lof" labels="LABEL:fig:rebalancing" placement="t" xml:id="S4.F5">
              <graphics candidates="resources/rebalancing_activity.pdf" class="ltx_figure_panel" graphic="resources/rebalancing_activity.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="S4.SS3.g1"/>
              <tags>
                <tag><text fontsize="90%">Figure 5</text></tag>
                <tag role="autoref">Figure 5</tag>
                <tag role="refnum">5</tag>
                <tag role="typerefnum">Figure 5</tag>
              </tags>
              <toccaption><tag close=" "><text fontsize="90%">5</text></tag><text fontsize="90%">Throughput of rebalancing methods over time.</text></toccaption>
              <caption fontsize="90%"><tag close=": ">Figure 5</tag>Throughput of rebalancing methods over time.</caption>
            </figure>
            <subsection inlist="toc" xml:id="S4.SS4">
              <tags>
                <tag>4.4</tag>
                <tag role="autoref">subsection 4.4</tag>
                <tag role="refnum">4.4</tag>
                <tag role="typerefnum">§4.4</tag>
              </tags>
              <title fontsize="90%"><tag close=" ">4.4</tag>Adaptive Rebalancing Evaluation</title>
              <figure inlist="lof" labels="LABEL:fig:rebalancing_stages_all LABEL:sect:experiments_adaptive" placement="h!" xml:id="S4.F6">
                <tags>
                  <tag><text fontsize="90%">Figure 6</text></tag>
                  <tag role="autoref">Figure 6</tag>
                  <tag role="refnum">6</tag>
                  <tag role="typerefnum">Figure 6</tag>
                </tags>
                <figure class="ltx_figure_panel" inlist="lof" labels="LABEL:fig:rebalancing_stages" xml:id="S4.F6.sf1">
                  <tags>
                    <tag><text fontsize="90%">(a)</text></tag>
                    <tag role="autoref">6(a)</tag>
                    <tag role="refnum">6(a)</tag>
                  </tags>
                  <graphics candidates="resources/rebalancing_stages.pdf" class="ltx_centering" graphic="resources/rebalancing_stages.pdf" options="width=420.61192pt,keepaspectratio=true" xml:id="S4.F6.sf1.g1"/>
                  <toccaption class="ltx_centering"><tag close=" "><text fontsize="90%">(a)</text></tag><text fontsize="90%">Adaptive rebalancing of SWARM parallelism.</text></toccaption>
                  <caption class="ltx_centering" fontsize="90%"><tag close=" ">(a)</tag>Adaptive rebalancing of SWARM parallelism.</caption>
                </figure>
                <figure class="ltx_figure_panel" inlist="lof" labels="LABEL:fig:rebalancing_stages_baseline" xml:id="S4.F6.sf2">
                  <tags>
                    <tag><text fontsize="90%">(b)</text></tag>
                    <tag role="autoref">6(b)</tag>
                    <tag role="refnum">6(b)</tag>
                  </tags>
                  <graphics candidates="resources/rebalancing_stages_baseline.pdf" class="ltx_centering" graphic="resources/rebalancing_stages_baseline.pdf" options="width=420.61192pt,keepaspectratio=true" xml:id="S4.F6.sf2.g1"/>
                  <toccaption class="ltx_centering"><tag close=" "><text fontsize="90%">(b)</text></tag><text fontsize="90%">No rebalancing.</text></toccaption>
                  <caption class="ltx_centering" fontsize="90%"><tag close=" ">(b)</tag>No rebalancing.</caption>
                </figure>
                <toccaption><tag close=" "><text fontsize="90%">6</text></tag><text fontsize="90%">Scaling of pipeline-parallel strategies with respect to the number of stages.</text></toccaption>
                <caption fontsize="90%"><tag close=": ">Figure 6</tag>Scaling of pipeline-parallel strategies with respect to the number of stages.</caption>
              </figure>
              <para xml:id="S4.SS4.p1">
                <p><text fontsize="90%">In this experiment, we evaluate the efficiency of adaptive peer rebalancing between stages proposed in Section </text><ref fontsize="90%" labelref="LABEL:sect:method_swarm"/><text fontsize="90%">.
We use statistics of the number of active T4 nodes from the 32-hour segment of the experiment described in Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_large"/><text fontsize="90%">.
We use this data to simulate training dynamics by viewing it as sequence of events, each consisting of a timestamp and a change in the number of peers (which can be positive or negative).
When a worker is removed from the pipeline, we randomly choose the stage it was removed from: that is, removing </text><Math mode="inline" tex="N" text="N" xml:id="S4.SS4.p1.m1">
                    <XMath>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">N</XMTok>
                    </XMath>
                  </Math><text fontsize="90%"> peers corresponds to </text><Math mode="inline" tex="N" text="N" xml:id="S4.SS4.p1.m2">
                    <XMath>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">N</XMTok>
                    </XMath>
                  </Math><!--  %****␣experiments.tex␣Line␣225␣**** --><text fontsize="90%"> samples from the uniform distribution over four pipeline stages.
We run 10 simulations with different random seeds and average the resulting trajectories.
We compare our strategy with two different values of </text><Math mode="inline" tex="T" text="T" xml:id="S4.SS4.p1.m3">
                    <XMath>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                    </XMath>
                  </Math><text fontsize="90%"> to the baseline that has no rebalancing.</text></p>
              </para>
              <para xml:id="S4.SS4.p2">
                <p><text fontsize="90%">The results of this evaluation are available in </text><ref class="ltx_refmacro_autoref" fontsize="90%" labelref="LABEL:fig:rebalancing" show="autoref"/><text fontsize="90%">; for reference, we also provide the performance of a theoretically optimal rebalancing strategy that maintains the highest possible throughput at every moment. It can be seen that even with the rebalancing period </text><Math mode="inline" tex="T=300" text="T = 300" xml:id="S4.SS4.p2.m1">
                    <XMath>
                      <XMApp>
                        <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                        <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                        <XMTok fontsize="90%" meaning="300" role="NUMBER">300</XMTok>
                      </XMApp>
                    </XMath>
                  </Math><text fontsize="90%">, our approach significantly improves the overall throughput of the pipeline. When the number of peers is relatively stable, the rebalanced pipeline also approaches the optimal one in terms of throughput, which shows the efficiency of rebalancing even when moving only one node at a time.</text></p>
              </para>
              <para xml:id="S4.SS4.p3">
                <p><text fontsize="90%">In addition, we observed that for some brief periods, the performance of the unbalanced pipeline exceeded the throughput of the balanced one due to random choice of disconnecting peers (dropping more from the “overrepresented” stages affects the imbalanced pipeline less). However, this held true only for </text><Math mode="inline" tex="\approx 4.5\%" text="absent approximately-equals 4.5percent" xml:id="S4.SS4.p3.m1">
                    <XMath>
                      <XMApp>
                        <XMTok fontsize="90%" meaning="approximately-equals" name="approx" role="RELOP">≈</XMTok>
                        <XMTok meaning="absent"/>
                        <XMApp>
                          <XMTok fontsize="90%" meaning="percent" role="POSTFIX">%</XMTok>
                          <XMTok fontsize="90%" meaning="4.5" role="NUMBER">4.5</XMTok>
                        </XMApp>
                      </XMApp>
                    </XMath>
                  </Math><text fontsize="90%"> of the experiment and was quickly mitigated by adaptive rebalancing.</text></p>
              </para>
              <para xml:id="S4.SS4.p4">
                <p><text fontsize="90%">As expected, decreasing </text><Math mode="inline" tex="T" text="T" xml:id="S4.SS4.p4.m1">
                    <XMath>
                      <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                    </XMath>
                  </Math><text fontsize="90%"> from 300 to 60 seconds improves both the overall throughput and the speed of convergence to optimal pipeline performance. However, the effect is not as drastic compared to the increase in DHT data transfer volume. This is also demonstrated by </text><ref class="ltx_refmacro_autoref" fontsize="90%" labelref="LABEL:tab:rebalancing_speedup" show="autoref"/><text fontsize="90%">, which shows the relative throughput of the three configurations compared to the optimal one. Furthermore, the table displays that while initially there is little difference between rebalancing choices, it becomes more pronounced later on as the imbalanced version “drifts further” from the optimal state.</text></p>
              </para>
              <table class="ltx_figure_panel" inlist="lot" labels="LABEL:tab:rebalancing_speedup" placement="b" xml:id="S4.T5">
                <tags>
                  <tag><text fontsize="90%">Table 5</text></tag>
                  <tag role="autoref">Table 5</tag>
                  <tag role="refnum">5</tag>
                  <tag role="typerefnum">Table 5</tag>
                </tags>
                <toccaption><tag close=" "><text fontsize="90%">5</text></tag><text fontsize="90%">Relative throughput comparison of pipeline rebalancing methods.</text></toccaption>
                <caption fontsize="90%"><tag close=": ">Table 5</tag>Relative throughput comparison of pipeline rebalancing methods.</caption>
                <tabular class="ltx_centering ltx_figure_panel" colsep="8.0pt" vattach="bottom">
                  <tr>
                    <td align="left" border="tt" rowspan="2"><text fontsize="90%"><ERROR class="undefined">\thead</ERROR>Rebalancing</text></td>
                    <td align="center" border="tt" colspan="3"><text fontsize="90%">% of optimal</text></td>
                  </tr>
                  <tr>
                    <td align="center" border="t"><text fontsize="90%">Overall</text></td>
                    <td align="center" border="t"><text fontsize="90%">First 1 hour</text></td>
                    <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">Last 1 hour</text></td>
                  </tr>
                  <tr>
                    <td align="left" border="t"><text fontsize="90%">None</text></td>
                    <td align="center" border="t"><text fontsize="90%">82.7</text></td>
                    <td align="center" border="t"><text fontsize="90%">99.0</text></td>
                    <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">45.4</text></td>
                  </tr>
                  <tr>
                    <td align="left"><Math mode="inline" tex="T=300" text="T = 300" xml:id="S4.SS4.m1">
                        <XMath>
                          <XMApp>
                            <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                            <XMTok fontsize="90%" meaning="300" role="NUMBER">300</XMTok>
                          </XMApp>
                        </XMath>
                      </Math></td>
                    <td align="center"><text fontsize="90%">95.8</text></td>
                    <td align="center"><text fontsize="90%">99.4</text></td>
                    <td align="center" class="ltx_nopad_r"><text fontsize="90%">88.9</text></td>
                  </tr>
                  <tr>
                    <td align="left" border="bb"><Math mode="inline" tex="T=60" text="T = 60" xml:id="S4.SS4.m2">
                        <XMath>
                          <XMApp>
                            <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                            <XMTok fontsize="90%" meaning="60" role="NUMBER">60</XMTok>
                          </XMApp>
                        </XMath>
                      </Math></td>
                    <td align="center" border="bb"><text fontsize="90%">97.6</text></td>
                    <td align="center" border="bb"><text fontsize="90%">99.8</text></td>
                    <td align="center" border="bb" class="ltx_nopad_r"><text fontsize="90%">91.7</text></td>
                  </tr>
                </tabular>
              </table>
              <para xml:id="S4.SS4.p5">
                <p><text fontsize="90%">Finally, we analyze the scaling properties of rebalancing with respect to the number of stages. To do this, we conduct experiments in the same setup as above (</text><Math mode="inline" tex="T=300" text="T = 300" xml:id="S4.SS4.p5.m1">
                    <XMath>
                      <XMApp>
                        <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                        <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                        <XMTok fontsize="90%" meaning="300" role="NUMBER">300</XMTok>
                      </XMApp>
                    </XMath>
                  </Math><text fontsize="90%">) while changing the number of pipeline stages from 4 to </text><Math mode="inline" tex="\{4,\ 8,\ 16,\ 32\}" text="set@(4, 8, 16, 32)" xml:id="S4.SS4.p5.m2">
                    <XMath>
                      <XMDual>
                        <XMApp>
                          <XMTok meaning="set"/>
                          <XMRef idref="S4.SS4.p5.m2.1"/>
                          <XMRef idref="S4.SS4.p5.m2.2"/>
                          <XMRef idref="S4.SS4.p5.m2.3"/>
                          <XMRef idref="S4.SS4.p5.m2.4"/>
                        </XMApp>
                        <XMWrap>
                          <XMTok fontsize="90%" role="OPEN" stretchy="false">{</XMTok>
                          <XMTok fontsize="90%" meaning="4" role="NUMBER" xml:id="S4.SS4.p5.m2.1">4</XMTok>
                          <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                          <XMTok fontsize="90%" meaning="8" role="NUMBER" xml:id="S4.SS4.p5.m2.2"> 8</XMTok>
                          <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                          <XMTok fontsize="90%" meaning="16" role="NUMBER" xml:id="S4.SS4.p5.m2.3"> 16</XMTok>
                          <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                          <XMTok fontsize="90%" meaning="32" role="NUMBER" xml:id="S4.SS4.p5.m2.4"> 32</XMTok>
                          <XMTok fontsize="90%" role="CLOSE" stretchy="false">}</XMTok>
                        </XMWrap>
                      </XMDual>
                    </XMath>
                  </Math><text fontsize="90%">. To ensure the consistency of throughput across all experiments, we increase the starting number of peers accordingly while keeping the preemption rate constant. As a baseline, we also evaluate the throughput of the pipeline that has no rebalancing.</text></p>
              </para>
              <para xml:id="S4.SS4.p6">
                <p><!--  %****␣experiments.tex␣Line␣250␣**** --><text fontsize="90%">Figure </text><ref fontsize="90%" labelref="LABEL:fig:rebalancing_stages_all"/><text fontsize="90%"> shows the outcome of this experiment. As displayed in the plots, both strategies drop in performance with the increase in the stage count: while all stages should drop in performance equally in expectation, in practice, the variances are too large while the number of peers is relatively too small for the asymptotic properties to take place. This effect results in more outliers (large drops in the number of peers) in the preemption distribution for more stages. Still, rebalancing allows to partially mitigate the issue: while we observe a more consistent downward trend for the baseline strategy, the rebalanced pipeline regains its performance over time and achieves a higher overall throughput.</text></p>
              </para>
              <section inlist="toc" xml:id="S5">
                <tags>
                  <tag>5</tag>
                  <tag role="autoref">section 5</tag>
                  <tag role="refnum">5</tag>
                  <tag role="typerefnum">§5</tag>
                </tags>
                <title fontsize="90%"><tag close=" ">5</tag>Conclusion</title>
                <para xml:id="S5.p1">
                  <p><text fontsize="90%">In this work, we evaluate the feasibility of high-throughput training of billion-scale neural networks on unreliable peers with low network bandwidth.
We find that training in this setup can be possible with very large models and pipeline parallelism.
To this end, we propose SWARM parallelism to overcome the challenges of pipeline parallelism for preemptible devices with heterogeneous network bandwidths and computational throughputs.
We show that our method is highly effective at rebalancing peers and maximizing the aggregate training throughput even in presence of unstable nodes.
We also show that training </text><text font="bold" fontsize="90%">large models</text><text fontsize="90%"> with </text><text font="bold" fontsize="90%">SWARM parallelism</text><text fontsize="90%"> and </text><text font="bold" fontsize="90%">compression</text><text fontsize="90%">-aware architectures enables high utilization of cheap preemptible instances with slow interconnect.
As such, our work makes training of large models accessible to researchers that do not have access to dedicated compute infrastructure.</text></p>
                </para>
                <pagination role="newpage"/>
                <bibliography citestyle="authoryear" files="bibliography" xml:id="bib">
                  <title fontsize="90%">References</title>
                </bibliography>
                <pagination role="newpage"/>
                <appendix xml:id="Ax1">
                  <title fontsize="90%">Supplementary Material</title>
                  <para xml:id="Ax1.p1">
                    <p><text fontsize="90%">This part of the paper is organized as follows:</text></p>
                    <itemize xml:id="Ax1.I1">
                      <item xml:id="Ax1.I1.i1">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">1st item</tag>
                        </tags>
                        <para xml:id="Ax1.I1.i1.p1">
                          <p><ref fontsize="90%" labelref="LABEL:appendix:faq" show="creftype~refnum"/><text fontsize="90%"> overviews several common questions about the details of our study and addresses the limitations of SWARM parallelism;</text></p>
                        </para>
                      </item>
                      <item xml:id="Ax1.I1.i2">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">2nd item</tag>
                        </tags>
                        <para xml:id="Ax1.I1.i2.p1">
                          <p><text fontsize="90%">In </text><ref fontsize="90%" labelref="LABEL:appendix:related" show="creftype~refnum"/><text fontsize="90%">, we list further related works on topics relevant to the problem setting we study;</text></p>
                        </para>
                      </item>
                      <item xml:id="Ax1.I1.i3">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">3rd item</tag>
                        </tags>
                        <para xml:id="Ax1.I1.i3.p1">
                          <p><text fontsize="90%">In </text><ref fontsize="90%" labelref="LABEL:appendix:wiring_details" show="creftype~refnum"/><text fontsize="90%"> and </text><ref fontsize="90%" labelref="LABEL:appendix:rebalancing_formal" show="creftype~refnum"/><text fontsize="90%">, we give a more formal description and outline the details of stochastic wiring and adaptive rebalancing, accordingly;</text></p>
                        </para>
                      </item>
                      <item xml:id="Ax1.I1.i4">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">4th item</tag>
                        </tags>
                        <para xml:id="Ax1.I1.i4.p1">
                          <p><text fontsize="90%">In </text><ref fontsize="90%" labelref="LABEL:appendix:equivalence" show="creftype~refnum"/><text fontsize="90%">, we outline the relation between training with SWARM and using methods for offloading.</text></p>
                        </para>
                      </item>
                      <item xml:id="Ax1.I1.i5">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">5th item</tag>
                        </tags>
                        <para xml:id="Ax1.I1.i5.p1">
                          <p><ref fontsize="90%" labelref="LABEL:appendix:detailed_setup" show="creftype~refnum"/><text fontsize="90%"> and </text><ref fontsize="90%" labelref="LABEL:appendix:detailed_large" show="creftype~refnum"/><text fontsize="90%"> contain additional details of our experimental setup, whereas </text><ref fontsize="90%" labelref="LABEL:appendix:scaling" show="creftype~refnum"/><text fontsize="90%"> reports further experiments on specific aspects and components of SWARM parallelism;</text></p>
                        </para>
                      </item>
                      <item xml:id="Ax1.I1.i6">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">6th item</tag>
                        </tags>
                        <para xml:id="Ax1.I1.i6.p1">
                          <p><text fontsize="90%">Lastly, we investigate compression-aware architectures in </text><ref fontsize="90%" labelref="LABEL:appendix:compression" show="creftype~refnum"/><text fontsize="90%"> and evaluate their impact in a practical setting in </text><ref fontsize="90%" labelref="LABEL:appendix:time_to_solution" show="creftype~refnum"/><text fontsize="90%">.</text></p>
                        </para>
                      </item>
                    </itemize>
                  </para>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:faq" xml:id="A1">
                  <tags>
                    <tag>Appendix A</tag>
                    <tag role="autoref">Appendix A</tag>
                    <tag role="refnum">A</tag>
                    <tag role="typerefnum">Appendix A</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix A</tag>Answers to Common Questions</title>
                  <toctitle><tag close=" "><text fontsize="90%">A</text></tag><text fontsize="90%">Answers to Common Questions</text></toctitle>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px1">
                    <title fontsize="90%">Why not just use data parallelism with offloading?</title>
                    <para xml:id="A1.SS0.SSS0.Px1.p1">
                      <p><text fontsize="90%">Regular data parallelism requires all-reduce steps where peers exchange gradients, which can be prohibitively expensive for large models. For example, a 1 billion parameter model with 16-bit gradients requires 2 GB of data to be synchronized between all </text><Math mode="inline" tex="n" text="n" xml:id="A1.SS0.SSS0.Px1.p1.m1">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">n</XMTok>
                          </XMath>
                        </Math><text fontsize="90%"> devices. We need at least </text><Math mode="inline" tex="n" text="n" xml:id="A1.SS0.SSS0.Px1.p1.m2">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">n</XMTok>
                          </XMath>
                        </Math><text fontsize="90%"> messages to perform this synchronization. If we have 100 devices with bidirectional communication, each client would need to send 2 GB of data to finish the synchronization. Thus, with slow interconnects, such synchronizations are not practical.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px2">
                    <title fontsize="90%">Why not just use fully sharded data parallelism with elasticity?</title>
                    <para xml:id="A1.SS0.SSS0.Px2.p1">
                      <p><!--  %****␣supplementary.tex␣Line␣25␣**** --><text fontsize="90%">Sharded data parallelism requires all-to-all communication of parameter buffers at each layer. Each of these communications can be done in parallel and has a size of parameter count divided by </text><Math mode="inline" tex="n" text="n" xml:id="A1.SS0.SSS0.Px2.p1.m1">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">n</XMTok>
                          </XMath>
                        </Math><text fontsize="90%">; in total, </text><Math mode="inline" tex="n" text="n" xml:id="A1.SS0.SSS0.Px2.p1.m2">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">n</XMTok>
                          </XMath>
                        </Math><text fontsize="90%"> messages are required. Thus, for 1B parameters in 16-bit precision, a total of 2 GB need to be synchronized for both the forward and backward pass. For low-bandwidth devices with 100 Mb/s speed, this would entail an overhead of 5.5 minutes per forward/backward pass, which is difficult to overlap with computation. This is exacerbated further, because all-to-all communication latency is determined by the slowest peer. Thus, sharded data parallelism can be particularly inefficient for setups where peers have different network bandwidths.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px3">
                    <title fontsize="90%">Should I use SWARM in a supercomputer?</title>
                    <para xml:id="A1.SS0.SSS0.Px3.p1">
                      <p><text fontsize="90%">By default, SWARM is worse than traditional parallelism due to its extra complexity (see experiments in Section </text><ref fontsize="90%" labelref="LABEL:appendix:training_throughput"/><text fontsize="90%">). However, SWARM can be useful in case of supercomputers that have heterogeneous devices.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px4">
                    <title fontsize="90%">ZeRO-Offload allows one to train 13B parameters on a single V100, so why do I need SWARM?</title>
                    <para xml:id="A1.SS0.SSS0.Px4.p1">
                      <p><text fontsize="90%">Using ZeRO-Offload can slow down training due to the slow data transfer between external memory and the accelerator. Training with SWARM can </text><text font="italic" fontsize="90%">accelerate</text><text fontsize="90%"> training while also allowing to train larger models; see Appendix </text><ref fontsize="90%" labelref="LABEL:appendix:equivalence"/><text fontsize="90%"> for a detailed comparison.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px5">
                    <title fontsize="90%">Is it worth using preemptible instances and SWARM from an economic standpoint?</title>
                    <para xml:id="A1.SS0.SSS0.Px5.p1">
                      <p><text fontsize="90%">Due to a significantly smaller cost per hour, one can leverage a larger amount of computation when using spot instances compared to on-demand cloud VMs or dedicated HPC setups. See </text><ref class="ltx_refmacro_autoref" fontsize="90%" labelref="LABEL:appendix:time_to_solution" show="autoref"/><text fontsize="90%"> and </text><ref class="ltx_refmacro_autoref" fontsize="90%" labelref="LABEL:tab:cost" show="autoref"/><text fontsize="90%"> for a comparison of both hourly and total costs for an example large-scale pretraining task.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px6">
                    <title fontsize="90%">When should I avoid using SWARM?</title>
                    <para xml:id="A1.SS0.SSS0.Px6.p1">
                      <p><text fontsize="90%">SWARM is efficient at training compute-intensive models with more than 1B parameters. For smaller models, a sharded data-parallel approach can be more optimal. For homogeneous HPC environments, standard sharded data-parallel or pipeline-parallel training will be more efficient than SWARM, because the rebalancing is not required. For HPC environments that are so extensive that the failure of a node is likely, the practicality of SWARM depends on how many nodes are expected to fail. Elastic sharded data parallelism is better than SWARM if the number of expected failures is relatively low.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px7">
                    <title fontsize="90%">Can I use SWARM without layer sharing or quantization?</title>
                    <para xml:id="A1.SS0.SSS0.Px7.p1">
                      <p><text fontsize="90%">Yes, SWARM can still be effective in these scenarios. Our bandwidth experiments in the main part of the work give an estimate of its network overhead. By using no quantization, which means using regular 16-bit activations, the network overhead increases approximately by a factor of two. Without layer sharing, the overhead within each pipeline stage to synchronize the gradients is increased by the number of layers not being shared. As such, a rough estimate of the efficiency of SWARM in these scenarios can be estimated by taking our model size and network bandwidth requirements data and multiplying it by the relevant factor.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px8">
                    <title fontsize="90%">Do the compression-aware architecture modifications apply only to Transformers?</title>
                    <para xml:id="A1.SS0.SSS0.Px8.p1">
                      <p><text fontsize="90%">Bottleneck and maxout compression are general compression techniques that can be applied to any layer in any architecture. However, their effectiveness may vary depending on where in the model they are applied and what kind of model these are applied to (for example, CNNs vs. RNNs vs. Transformers).</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px9">
                    <title fontsize="90%">How many pipeline stages can SWARM have?</title>
                    <para xml:id="A1.SS0.SSS0.Px9.p1">
                      <p><text fontsize="90%">While its design allows for any number of stages, using long pipelines can result in a reduced training throughput. Similarly to regular pipeline parallelism, SWARM suffers from the pipeline “bubble” problem </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="huang2019gpipe" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">: at the beginning of the initial batch processing, peers near the end of the pipeline will be waiting for inputs. Likewise, early layers will be idle after processing the final microbatch.
In theory, this can be mitigated with asynchronous updates </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="pipedream,pipemare" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, but we did not investigate them in this work due to potential convergence issues.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px10">
                    <title fontsize="90%">How much failure can SWARM handle?</title>
                    <para xml:id="A1.SS0.SSS0.Px10.p1">
                      <p><text fontsize="90%">As long as there is at least one operational peer at every pipeline stage and at least one trainer, SWARM can work without any issues.
The key factors defining the training run state at a given SGD step are the model parameters, the optimizer statistics, the data loader state, and the step number (required for proper scheduling). The up-to-date parameters and optimizer statistics, as well as the step number, are naturally located on all active nodes of a given stage, since they are required for training. Thus, when a peer joins the network, it can download the checkpoint corresponding to the current training state from other peers.</text></p>
                    </para>
                    <para xml:id="A1.SS0.SSS0.Px10.p2">
                      <p><text fontsize="90%">As we mention in Section </text><ref fontsize="90%" labelref="LABEL:sect:method_swarm"/><text fontsize="90%">, peer failures do not affect forward and backward passes as long as there is at least one peer at the required stage: because of rewiring, it is possible to resend activations or gradients to another worker that has identical model weights by construction. Similarly, the data loader state can be recomputed from the last known SGD step. However, we do not track the order of examples sampled within the same batch; because of the i.i.d. assumption in the large-scale training setup, the distribution of gradients is expected to be the same. Hence, if the peer leaves from the pipeline stage, other workers can compute gradients and replace those accumulated by the disconnected peer, so that the number of examples for an SGD step stays the same.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A1.SS0.SSS0.Px11">
                    <title fontsize="90%">Some configurations in Section <ref labelref="LABEL:sect:experiments_square_cube"/> measure less than <Math mode="inline" tex="\bf 20\%" text="20percent" xml:id="A1.SS0.SSS0.Px11.m1">
                        <XMath>
                          <XMApp>
                            <XMTok meaning="percent" role="POSTFIX">%</XMTok>
                            <XMTok font="bold" meaning="20" role="NUMBER">20</XMTok>
                          </XMApp>
                        </XMath>
                      </Math> GPU idle time, while many HPC systems only achieve <Math mode="inline" tex="\bf\approx 80\%" text="absent approximately-equals 80percent" xml:id="A1.SS0.SSS0.Px11.m2">
                        <XMath>
                          <XMApp>
                            <XMTok meaning="approximately-equals" name="approx" role="RELOP">≈</XMTok>
                            <XMTok meaning="absent"/>
                            <XMApp>
                              <XMTok meaning="percent" role="POSTFIX">%</XMTok>
                              <XMTok font="bold" meaning="80" role="NUMBER">80</XMTok>
                            </XMApp>
                          </XMApp>
                        </XMath>
                      </Math> GPU utilization. Does this mean that SWARM is <Math mode="inline" tex="\bf 30\%" text="30percent" xml:id="A1.SS0.SSS0.Px11.m3">
                        <XMath>
                          <XMApp>
                            <XMTok meaning="percent" role="POSTFIX">%</XMTok>
                            <XMTok font="bold" meaning="30" role="NUMBER">30</XMTok>
                          </XMApp>
                        </XMath>
                      </Math> faster?</title>
                    <para xml:id="A1.SS0.SSS0.Px11.p1">
                      <p><text fontsize="90%">No, because these are different measurement types. </text><cite class="ltx_citemacro_citet"><bibref bibrefs="megatron2" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">(</text></bibrefphrase>
                            <bibrefphrase><text fontsize="90%">)</text></bibrefphrase>
                          </bibref></cite><text fontsize="90%"> measures GPU utilization as a fraction of theoretical peak FLOP/s of their GPUs. In contrast, we only measure what fraction of time the GPU is running the model, regardless of efficiency. Since any realistic deep learning workload cannot achieve </text><Math mode="inline" tex="100\%" text="100percent" xml:id="A1.SS0.SSS0.Px11.p1.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="percent" role="POSTFIX">%</XMTok>
                              <XMTok fontsize="90%" meaning="100" role="NUMBER">100</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> peak FLOP/s, </text><Math mode="inline" tex="20\%" text="20percent" xml:id="A1.SS0.SSS0.Px11.p1.m2">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="percent" role="POSTFIX">%</XMTok>
                              <XMTok fontsize="90%" meaning="20" role="NUMBER">20</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> GPU idle time for SWARM means that it can reach </text><Math mode="inline" tex="\approx 0.8" text="absent approximately-equals 0.8" xml:id="A1.SS0.SSS0.Px11.p1.m3">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="approximately-equals" name="approx" role="RELOP">≈</XMTok>
                              <XMTok meaning="absent"/>
                              <XMTok fontsize="90%" meaning="0.8" role="NUMBER">0.8</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">x the training throughput compared to training with an infinitely fast network. As a rule of thumb, one can say that SWARM will run at a </text><Math mode="inline" tex="20\%" text="20percent" xml:id="A1.SS0.SSS0.Px11.p1.m4">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="percent" role="POSTFIX">%</XMTok>
                              <XMTok fontsize="90%" meaning="20" role="NUMBER">20</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> slower speed than systems described by </text><cite class="ltx_citemacro_citet"><bibref bibrefs="megatron2" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">(</text></bibrefphrase>
                            <bibrefphrase><text fontsize="90%">)</text></bibrefphrase>
                          </bibref></cite><text fontsize="90%"> using the infrastructure that is several times cheaper.</text></p>
                    </para>
                  </paragraph>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:related" xml:id="A2">
                  <tags>
                    <tag>Appendix B</tag>
                    <tag role="autoref">Appendix B</tag>
                    <tag role="refnum">B</tag>
                    <tag role="typerefnum">Appendix B</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix B</tag>Additional Related Work</title>
                  <toctitle><tag close=" "><text fontsize="90%">B</text></tag><text fontsize="90%">Additional Related Work</text></toctitle>
                  <paragraph inlist="toc" xml:id="A2.SS0.SSS0.Px1">
                    <title fontsize="90%">Dynamic parameter loading.</title>
                    <para xml:id="A2.SS0.SSS0.Px1.p1">
                      <p><text fontsize="90%">Several recent studies propose alternative execution algorithms that allow training large models with data parallelism. Since neural networks typically use a small fraction of weights at any given moment, the remaining “inactive” parameters can be sharded </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="zero" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> or offloaded to external memory </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="l2l,zerooffload,zero_ssd" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. In sharded data parallelism </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="zero" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, inactive tensors are distributed across all </text><Math mode="inline" tex="n" text="n" xml:id="A2.SS0.SSS0.Px1.p1.m1">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">n</XMTok>
                          </XMath>
                        </Math><text fontsize="90%"> devices such that each device stores </text><Math mode="inline" tex="\frac{1}{n}" text="1 / n" xml:id="A2.SS0.SSS0.Px1.p1.m2">
                          <XMath>
                            <XMApp>
                              <XMTok mathstyle="text" meaning="divide" role="FRACOP"/>
                              <XMTok fontsize="63%" meaning="1" role="NUMBER">1</XMTok>
                              <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">th of all parameters. For active layers, the shards are gathered such that each device holds the entire tensor just-in-time for computation. After the computation, the parameters’ memory is freed so that only the sharded memory remains (</text><Math mode="inline" tex="\frac{1}{n}" text="1 / n" xml:id="A2.SS0.SSS0.Px1.p1.m3">
                          <XMath>
                            <XMApp>
                              <XMTok mathstyle="text" meaning="divide" role="FRACOP"/>
                              <XMTok fontsize="63%" meaning="1" role="NUMBER">1</XMTok>
                              <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">th per device). This makes it very memory efficient to store model and optimizer states for inactive layers if many devices are available. Similarly to tensor parallelism, these algorithms can support arbitrary models without the need for layer partitioning and can, in principle, run a large model on a single GPU, which is useful for finetuning and inference.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A2.SS0.SSS0.Px2">
                    <title fontsize="90%">Architecture-specific methods.</title>
                    <para xml:id="A2.SS0.SSS0.Px2.p1">
                      <p><text fontsize="90%">Finally, some distributed training algorithms take advantage of specific layers, such as locally connected layers </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="dean12,coates13" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, Mixture-of-Experts </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="moe_first,shazeer2017outrageously,Lepikhin2020GShardSG" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, Switch layers </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="fedus2021switch" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> or Product Key Memory </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="pkm" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. These layers contain many near-independent parts that can be assigned to different devices. They can easily scale to an extremely large number of parameters with a relatively small increase in compute </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="shazeer2017outrageously" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. However, they are also less parameter-efficient </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="fedus2021switch" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> and may not apply to all architectures.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A2.SS0.SSS0.Px3">
                    <title fontsize="90%">Optimal scheduling for distributed training.</title>
                    <para xml:id="A2.SS0.SSS0.Px3.p1">
                      <p><text fontsize="90%">When the configuration of each peer is known, it is possible to significantly optimize the pipeline scheduling by going beyond the greedy approach with global optimization techniques </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="alpa,piper" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, even with heterogeneous hardware </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="yuan2022decentralized" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.
However, we consider a setup in which this is not possible: preemptible and volunteer peers can join at any point of the experiment, and dynamically rescheduling and orchestrating them in a centralized manner is technically difficult because of the communication and reliability constraints.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A2.SS0.SSS0.Px4">
                    <title fontsize="90%">Elastic training.</title>
                    <para xml:id="A2.SS0.SSS0.Px4.p1">
                      <p><text fontsize="90%">To train with a dynamic number of workers, deep learning practitioners have developed elastic training algorithms </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="pytorch_elastic,elastic_horovod" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. If a worker leaves or fails during training, these algorithms rebalance the load between the remaining nodes and continue the training procedure </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="proteus,moshpit" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. If new workers join during training, they get the latest model parameters from their peers and train alongside them.</text></p>
                    </para>
                  </paragraph>
                  <paragraph inlist="toc" xml:id="A2.SS0.SSS0.Px5">
                    <title fontsize="90%">Asynchronous training.</title>
                    <para xml:id="A2.SS0.SSS0.Px5.p1">
                      <p><text fontsize="90%">Another important problem is distributed training on devices with uneven performance. One way to solve this problem is to use asynchronous training, where nodes compute gradients at their own pace and aggregate them using a parameter server </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="recht2011hogwild,volunteer_dl_async" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> or a decentralized network </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="dp_sgd" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. This idea allows full utilization of each device, but may reduce the convergence rate due to “stale” gradients </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="recht2011hogwild,aji2019making" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. Several studies </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="wagma,moshpit,zerooffload,dedloc" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> propose hybrid techniques that remove some synchronization points while maintaining the per-iteration convergence.</text></p>
                    </para>
                  </paragraph>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:wiring_details" xml:id="A3">
                  <tags>
                    <tag>Appendix C</tag>
                    <tag role="autoref">Appendix C</tag>
                    <tag role="refnum">C</tag>
                    <tag role="typerefnum">Appendix C</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix C</tag>Stochastic Wiring Details</title>
                  <toctitle><tag close=" "><text fontsize="90%">C</text></tag><text fontsize="90%">Stochastic Wiring Details</text></toctitle>
                  <para xml:id="A3.p1">
                    <p><text fontsize="90%">Our approach uses </text><text font="italic" fontsize="90%">stochastic wiring</text><text fontsize="90%">, a specialized routing algorithm designed around heterogeneous unreliable devices and high network latency. The core idea of stochastic wiring is to route each training microbatch through random devices from each pipeline stage, such that the workload of each device is proportional to its performance.
The performance of the peer is measured as an exponentially weighted average of its response time, and all peers serving a specific stage are stored in a priority queue.
We formally describe the components of stochastic wiring in Algorithm </text><ref fontsize="90%" labelref="LABEL:alg:wiring"/><text fontsize="90%">.</text></p>
                  </para>
                  <para xml:id="A3.p2">
                    <p><text fontsize="90%">From a system design perspective, each worker runs a separate </text><text font="italic" fontsize="90%">trainer</text><text fontsize="90%"> process that forms microbatches and routes them through pipeline stages (forward and backward pass). As we describe earlier in Section </text><ref fontsize="90%" labelref="LABEL:sect:method_swarm"/><text fontsize="90%">, trainers run Interleaved Weighted Round Robin </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="iwrr,interleaved_round_robin" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> (IWRR) scheduling to dynamically assign microbatches to peers based on each peer’s training throughput (“samples per second”) in a balanced way.</text></p>
                  </para>
                  <para xml:id="A3.p3">
                    <p><text fontsize="90%">An important observation is that </text><text font="italic" fontsize="90%">stochastic wiring allows SWARM to mitigate network latency</text><text fontsize="90%">. Unlike existing pipeline algorithms </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="huang2019gpipe" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, SWARM workers do not get blocked if their neighbors take too long to process a minibatch. Instead, each SWARM device maintains a queue of microbatches assigned by trainers. In case of a latency spike, workers keep processing previously queued microbatches, maintaining high device utilization.</text></p>
                  </para>
                  <figure placement="t" xml:id="A3.fig1">
                    <float class="ltx_figure_panel ltx_float_algorithm" framed="top" inlist="loa" labels="LABEL:alg:wiring" placement="H" xml:id="alg1">
                      <tags>
                        <tag><text font="bold">Algorithm 1</text></tag>
                        <tag role="autoref">1</tag>
                        <tag role="refnum">1</tag>
                        <tag role="typerefnum">Algorithm 1</tag>
                      </tags>
                      <toccaption><tag close=" "><text fontsize="90%">1</text></tag><text fontsize="90%">Pseudocode of stochastic wiring</text></toccaption>
                      <caption fontsize="90%"><tag close=" "><text font="bold">Algorithm 1</text></tag> Pseudocode of stochastic wiring</caption>
                      <break class="ltx_break"/>
                      <listing class="ltx_figure_panel">
                        <listingline xml:id="alg1.l0"><tags>
                            <tag><text fontsize="80%">0:</text></tag>
                            <tag role="autoref">0</tag>
                            <tag role="refnum">0</tag>
                          </tags><text fontsize="90%">  the number of pipeline stages </text><Math mode="inline" tex="N" text="N" xml:id="alg1.l0.m1">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">N</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">, the set of active servers </text><Math mode="inline" tex="S" text="S" xml:id="alg1.l0.m2">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">S</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">, smoothing parameter </text><Math mode="inline" tex="\gamma" text="gamma" xml:id="alg1.l0.m3">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" name="gamma" role="UNKNOWN">γ</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">, initial priority </text><Math mode="inline" tex="\epsilon" text="epsilon" xml:id="alg1.l0.m4">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" name="epsilon" role="UNKNOWN">ϵ</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l1"><tags>
                            <tag><text fontsize="80%">1:</text></tag>
                            <tag role="autoref">1</tag>
                            <tag role="refnum">1</tag>
                          </tags><text fontsize="90%">  </text><Math mode="inline" tex="\triangleright" text="triangleright" xml:id="alg1.l1.m1">
                            <XMath>
                              <XMTok fontsize="90%" name="triangleright" role="ADDOP">▷</XMTok>
                            </XMath>
                          </Math><text fontsize="90%"> Initialization
</text></listingline>
                        <listingline xml:id="alg1.l2"><tags>
                            <tag><text fontsize="80%">2:</text></tag>
                            <tag role="autoref">2</tag>
                            <tag role="refnum">2</tag>
                          </tags><text fontsize="90%">  ema = dict()
</text></listingline>
                        <listingline xml:id="alg1.l3"><tags>
                            <tag><text fontsize="80%">3:</text></tag>
                            <tag role="autoref">3</tag>
                            <tag role="refnum">3</tag>
                          </tags><text fontsize="90%">  queues = list()
</text></listingline>
                        <listingline xml:id="alg1.l4"><tags>
                            <tag><text fontsize="80%">4:</text></tag>
                            <tag role="autoref">4</tag>
                            <tag role="refnum">4</tag>
                          </tags><text fontsize="90%">  </text><text font="bold" fontsize="90%">for</text><text fontsize="90%"> </text><Math mode="inline" tex="\text{i}\in 1,\ldots,N" text="[i] element-of list@(1, ldots, N)" xml:id="alg1.l4.m1">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMText><text fontsize="90%">i</text></XMText>
                                <XMDual>
                                  <XMApp>
                                    <XMTok meaning="list"/>
                                    <XMRef idref="alg1.l4.m1.1"/>
                                    <XMRef idref="alg1.l4.m1.2"/>
                                    <XMRef idref="alg1.l4.m1.3"/>
                                  </XMApp>
                                  <XMWrap>
                                    <XMTok fontsize="90%" meaning="1" role="NUMBER" xml:id="alg1.l4.m1.1">1</XMTok>
                                    <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                    <XMTok fontsize="90%" name="ldots" role="ID" xml:id="alg1.l4.m1.2">…</XMTok>
                                    <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN" xml:id="alg1.l4.m1.3">N</XMTok>
                                  </XMWrap>
                                </XMDual>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%"> </text><text font="bold" fontsize="90%">do</text><text fontsize="90%">


</text></listingline>
                        <listingline xml:id="alg1.l5"><tags>
                            <tag><text fontsize="80%">5:</text></tag>
                            <tag role="autoref">5</tag>
                            <tag role="refnum">5</tag>
                          </tags><text fontsize="90%">    queues.append(PriorityQueue())
</text></listingline>
                        <listingline xml:id="alg1.l6"><tags>
                            <tag><text fontsize="80%">6:</text></tag>
                            <tag role="autoref">6</tag>
                            <tag role="refnum">6</tag>
                          </tags><text fontsize="90%">  </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">for</text></listingline>
                        <listingline xml:id="alg1.l7"><tags>
                            <tag><text fontsize="80%">7:</text></tag>
                            <tag role="autoref">7</tag>
                            <tag role="refnum">7</tag>
                          </tags><text fontsize="90%">  </text><text font="bold" fontsize="90%">def</text><text fontsize="90%"> add_server(server)</text><text font="bold" fontsize="90%">:</text><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l8"><tags>
                            <tag><text fontsize="80%">8:</text></tag>
                            <tag role="autoref">8</tag>
                            <tag role="refnum">8</tag>
                          </tags><text fontsize="90%">       ema[server] = </text><Math mode="inline" tex="\varepsilon" text="varepsilon" xml:id="alg1.l8.m1">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" name="varepsilon" role="UNKNOWN">ε</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l9"><tags>
                            <tag><text fontsize="80%">9:</text></tag>
                            <tag role="autoref">9</tag>
                            <tag role="refnum">9</tag>
                          </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">for</text><text fontsize="90%"> </text><Math mode="inline" tex="\text{i}\in\text{get\_blocks\_served\_by(server)}" text="[i] element-of [get_blocks_served_by(server)]" xml:id="alg1.l9.m1">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMText><text fontsize="90%">i</text></XMText>
                                <XMText><text fontsize="90%">get_blocks_served_by(server)</text></XMText>
                              </XMApp>
                            </XMath>
                          </Math><text font="bold" fontsize="90%">:</text><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l10"><tags>
                            <tag><text fontsize="80%">10:</text></tag>
                            <tag role="autoref">10</tag>
                            <tag role="refnum">10</tag>
                          </tags><text fontsize="90%">           queues[i].update(server, priority=</text><Math mode="inline" tex="\varepsilon" text="varepsilon" xml:id="alg1.l10.m1">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" name="varepsilon" role="UNKNOWN">ε</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">)
</text></listingline>
                        <listingline xml:id="alg1.l11"><tags>
                            <tag><text fontsize="80%">11:</text></tag>
                            <tag role="autoref">11</tag>
                            <tag role="refnum">11</tag>
                          </tags><text fontsize="90%">  </text><text font="bold" fontsize="90%">def</text><text fontsize="90%"> ban_server(server) </text><text font="bold" fontsize="90%">:</text><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l12"><tags>
                            <tag><text fontsize="80%">12:</text></tag>
                            <tag role="autoref">12</tag>
                            <tag role="refnum">12</tag>
                          </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">for</text><text fontsize="90%"> </text><Math mode="inline" tex="\text{i}\in\text{get\_blocks\_served\_by(server)}" text="[i] element-of [get_blocks_served_by(server)]" xml:id="alg1.l12.m1">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMText><text fontsize="90%">i</text></XMText>
                                <XMText><text fontsize="90%">get_blocks_served_by(server)</text></XMText>
                              </XMApp>
                            </XMath>
                          </Math><text font="bold" fontsize="90%">:</text><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l13"><tags>
                            <tag><text fontsize="80%">13:</text></tag>
                            <tag role="autoref">13</tag>
                            <tag role="refnum">13</tag>
                          </tags><text fontsize="90%">           queues[i].update(server, priority=</text><Math mode="inline" tex="\infty" text="infinity" xml:id="alg1.l13.m1">
                            <XMath>
                              <XMTok fontsize="90%" meaning="infinity" name="infty" role="ID">∞</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">)
</text></listingline>
                        <listingline xml:id="alg1.l14"><tags>
                            <tag><text fontsize="80%">14:</text></tag>
                            <tag role="autoref">14</tag>
                            <tag role="refnum">14</tag>
                          </tags><text fontsize="90%">  </text><text font="bold" fontsize="90%">def</text><text fontsize="90%"> choose_server(i)</text><text font="bold" fontsize="90%">:</text><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l15"><tags>
                            <tag><text fontsize="80%">15:</text></tag>
                            <tag role="autoref">15</tag>
                            <tag role="refnum">15</tag>
                          </tags><text fontsize="90%">       server, priority = queues[i].top()
</text></listingline>
                        <listingline xml:id="alg1.l16"><tags>
                            <tag><text fontsize="80%">16:</text></tag>
                            <tag role="autoref">16</tag>
                            <tag role="refnum">16</tag>
                          </tags><text fontsize="90%">       new_priority = priority + ema[server]
</text></listingline>
                        <listingline xml:id="alg1.l17"><tags>
                            <tag><text fontsize="80%">17:</text></tag>
                            <tag role="autoref">17</tag>
                            <tag role="refnum">17</tag>
                          </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">for</text><text fontsize="90%"> </text><Math mode="inline" tex="\text{j}\in\text{get\_blocks\_served\_by(server)}" text="[j] element-of [get_blocks_served_by(server)]" xml:id="alg1.l17.m1">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMText><text fontsize="90%">j</text></XMText>
                                <XMText><text fontsize="90%">get_blocks_served_by(server)</text></XMText>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%"> </text><text font="bold" fontsize="90%">:</text><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l18"><tags>
                            <tag><text fontsize="80%">18:</text></tag>
                            <tag role="autoref">18</tag>
                            <tag role="refnum">18</tag>
                          </tags><text fontsize="90%">           queues[j].update(server, priority=new_priority)
</text></listingline>
                        <listingline xml:id="alg1.l19"><tags>
                            <tag><text fontsize="80%">19:</text></tag>
                            <tag role="autoref">19</tag>
                            <tag role="refnum">19</tag>
                          </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">return</text><text fontsize="90%"> server
</text></listingline>
                        <listingline xml:id="alg1.l20"><tags>
                            <tag><text fontsize="80%">20:</text></tag>
                            <tag role="autoref">20</tag>
                            <tag role="refnum">20</tag>
                          </tags><text fontsize="90%">  </text><Math mode="inline" tex="\triangleright" text="triangleright" xml:id="alg1.l20.m1">
                            <XMath>
                              <XMTok fontsize="90%" name="triangleright" role="ADDOP">▷</XMTok>
                            </XMath>
                          </Math><!--  %****␣supplementary.tex␣Line␣125␣**** --><text fontsize="90%"> Forward pass with stochastic wiring
</text></listingline>
                        <listingline xml:id="alg1.l21"><tags>
                            <tag><text fontsize="80%">21:</text></tag>
                            <tag role="autoref">21</tag>
                            <tag role="refnum">21</tag>
                          </tags><text fontsize="90%">  </text><text font="bold" fontsize="90%">def</text><text fontsize="90%"> forward(inputs):
</text></listingline>
                        <listingline xml:id="alg1.l22"><tags>
                            <tag><text fontsize="80%">22:</text></tag>
                            <tag role="autoref">22</tag>
                            <tag role="refnum">22</tag>
                          </tags><text fontsize="90%">       layer_index = 0
</text></listingline>
                        <listingline xml:id="alg1.l23"><tags>
                            <tag><text fontsize="80%">23:</text></tag>
                            <tag role="autoref">23</tag>
                            <tag role="refnum">23</tag>
                          </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">while</text><text fontsize="90%"> </text><Math mode="inline" tex="\text{layer\_index}&lt;N" text="[layer_index] &lt; N" xml:id="alg1.l23.m1">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="less-than" role="RELOP">&lt;</XMTok>
                                <XMText><text fontsize="90%">layer_index</text></XMText>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">N</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text font="bold" fontsize="90%">:</text><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l24"><tags>
                            <tag><text fontsize="80%">24:</text></tag>
                            <tag role="autoref">24</tag>
                            <tag role="refnum">24</tag>
                          </tags><text fontsize="90%">           server = choose_server(layer_index)
</text></listingline>
                        <listingline xml:id="alg1.l25"><tags>
                            <tag><text fontsize="80%">25:</text></tag>
                            <tag role="autoref">25</tag>
                            <tag role="refnum">25</tag>
                          </tags><text fontsize="90%">           t = get_current_time()
</text></listingline>
                        <listingline xml:id="alg1.l26"><tags>
                            <tag><text fontsize="80%">26:</text></tag>
                            <tag role="autoref">26</tag>
                            <tag role="refnum">26</tag>
                          </tags><text fontsize="90%">           </text><text font="bold" fontsize="90%">try:</text><text fontsize="90%">
</text></listingline>
                        <listingline xml:id="alg1.l27"><tags>
                            <tag><text fontsize="80%">27:</text></tag>
                            <tag role="autoref">27</tag>
                            <tag role="refnum">27</tag>
                          </tags><text fontsize="90%">               inputs = server.forward(inputs)
</text></listingline>
                        <listingline xml:id="alg1.l28"><tags>
                            <tag><text fontsize="80%">28:</text></tag>
                            <tag role="autoref">28</tag>
                            <tag role="refnum">28</tag>
                          </tags><text fontsize="90%">               layer_index = layer_index + 1
</text></listingline>
                        <listingline xml:id="alg1.l29"><tags>
                            <tag><text fontsize="80%">29:</text></tag>
                            <tag role="autoref">29</tag>
                            <tag role="refnum">29</tag>
                          </tags><text fontsize="90%">               </text><Math mode="inline" tex="\Delta t" text="Delta * t" xml:id="alg1.l29.m1">
                            <XMath>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMTok fontsize="90%" name="Delta" role="UNKNOWN">Δ</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">t</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%"> = get_current_time() - t
</text></listingline>
                        <listingline xml:id="alg1.l30"><tags>
                            <tag><text fontsize="80%">30:</text></tag>
                            <tag role="autoref">30</tag>
                            <tag role="refnum">30</tag>
                          </tags><text fontsize="90%">               ema[server] = </text><Math class="ltx_math_unparsed" mode="inline" tex="\gamma\cdot\Delta t+(1-\gamma)\cdot" xml:id="alg1.l30.m1">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" name="gamma" role="UNKNOWN">γ</XMTok>
                              <XMTok fontsize="90%" name="cdot" role="MULOP">⋅</XMTok>
                              <XMTok fontsize="90%" name="Delta" role="UNKNOWN">Δ</XMTok>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">t</XMTok>
                              <XMTok fontsize="90%" meaning="plus" role="ADDOP">+</XMTok>
                              <XMWrap>
                                <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                <XMTok fontsize="90%" meaning="1" role="NUMBER">1</XMTok>
                                <XMTok fontsize="90%" meaning="minus" role="ADDOP">-</XMTok>
                                <XMTok font="italic" fontsize="90%" name="gamma" role="UNKNOWN">γ</XMTok>
                                <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                              </XMWrap>
                              <XMTok fontsize="90%" name="cdot" role="MULOP">⋅</XMTok>
                            </XMath>
                          </Math><text fontsize="90%"> ema[server]
</text></listingline>
                        <listingline xml:id="alg1.l31"><tags>
                            <tag><text fontsize="80%">31:</text></tag>
                            <tag role="autoref">31</tag>
                            <tag role="refnum">31</tag>
                          </tags><text fontsize="90%">           </text><text font="bold" fontsize="90%">catch</text><text fontsize="90%"> (ServerFault, Timeout):
</text></listingline>
                        <listingline xml:id="alg1.l32"><tags>
                            <tag><text fontsize="80%">32:</text></tag>
                            <tag role="autoref">32</tag>
                            <tag role="refnum">32</tag>
                          </tags><text fontsize="90%">               ban_server(server)
</text></listingline>
                        <listingline xml:id="alg1.l33"><tags>
                            <tag><text fontsize="80%">33:</text></tag>
                            <tag role="autoref">33</tag>
                            <tag role="refnum">33</tag>
                          </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">return</text><text fontsize="90%"> inputs
</text></listingline>
                      </listing>
                    </float>
                  </figure>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:rebalancing_formal" xml:id="A4">
                  <tags>
                    <tag>Appendix D</tag>
                    <tag role="autoref">Appendix D</tag>
                    <tag role="refnum">D</tag>
                    <tag role="typerefnum">Appendix D</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix D</tag>Description and Complexity of Adaptive Rebalancing</title>
                  <toctitle><tag close=" "><text fontsize="90%">D</text></tag><text fontsize="90%">Description and Complexity of Adaptive Rebalancing</text></toctitle>
                  <para xml:id="A4.p1">
                    <p><text fontsize="90%">Algorithm </text><ref fontsize="90%" labelref="LABEL:alg:adaptive_rebalancing"/><text fontsize="90%"> contains the formal definition of the adaptive rebalancing procedure. As described previously, each worker of SWARM that hosts model layers continuously updates the information about its load in parallel with processing the incoming requests. Each </text><Math mode="inline" tex="T" text="T" xml:id="A4.p1.m1">
                        <XMath>
                          <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                        </XMath>
                      </Math><text fontsize="90%"> seconds, the peers measure the total load for all stages of the pipeline, and the peer with the lowest queue size from the stage with the minimum load moves to the stage with the maximum load. In principle, the algorithm could be extended to support moving multiple peers simultaneously; however, as we have shown in Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_adaptive"/><text fontsize="90%">, even in the current form the algorithm bridges most of the gap between the optimally balanced pipeline and the system without any rebalancing.</text></p>
                  </para>
                  <para xml:id="A4.p2">
                    <p><text fontsize="90%">The complexity of Algorithm </text><ref fontsize="90%" labelref="LABEL:alg:adaptive_rebalancing"/><text fontsize="90%"> can be estimated as follows: for </text><Math mode="inline" tex="M" text="M" xml:id="A4.p2.m1">
                        <XMath>
                          <XMTok font="italic" fontsize="90%" role="UNKNOWN">M</XMTok>
                        </XMath>
                      </Math><text fontsize="90%"> as the highest number of peers over all stages, we have </text><Math mode="inline" tex="O(M)" text="O * M" xml:id="A4.p2.m2">
                        <XMath>
                          <XMApp>
                            <XMTok meaning="times" role="MULOP">⁢</XMTok>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">O</XMTok>
                            <XMDual>
                              <XMRef idref="A4.p2.m2.1"/>
                              <XMWrap>
                                <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN" xml:id="A4.p2.m2.1">M</XMTok>
                                <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                              </XMWrap>
                            </XMDual>
                          </XMApp>
                        </XMath>
                      </Math><text fontsize="90%"> operations in Lines 9–11 and Lines 22–24, and all other operations take constant time for a single stage. These operations are nested in the loop over all stages, which means that the total complexity of the algorithm is </text><Math mode="inline" tex="O(MS)" text="O * M * S" xml:id="A4.p2.m3">
                        <XMath>
                          <XMApp>
                            <XMTok meaning="times" role="MULOP">⁢</XMTok>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">O</XMTok>
                            <XMDual>
                              <XMRef idref="A4.p2.m3.1"/>
                              <XMWrap>
                                <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                <XMApp xml:id="A4.p2.m3.1">
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">M</XMTok>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">S</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                              </XMWrap>
                            </XMDual>
                          </XMApp>
                        </XMath>
                      </Math><text fontsize="90%">. For practical numbers of both peers (e.g., &lt; 10,000) and stages (fewer than 100), this incurs a negligible overhead on performance, as all communication and computation is done in parallel with the actual forward and backward passes.</text></p>
                  </para>
                  <para xml:id="A4.p3">
                    <p><!--  %****␣supplementary.tex␣Line␣150␣**** --><text fontsize="90%">Also, notice that only one migrating peer needs to stop processing requests and download the weights and optimizer statistics of the pipeline stage it starts serving: this means that the overall network load of this procedure is relatively small, as all DHT requests handle scalar data and do not exceed the number of active peers for each worker.</text></p>
                  </para>
                  <para xml:id="A4.p4">
                    <p><text fontsize="90%">In practice, the algorithm handles slight deviations in local time and network/DHT latencies by allowing the peers to wait for straggling nodes in Line 9 for a predefined timeout. If a node does not join the rebalancing procedure by reporting its load in time or joins the network too late, it is omitted from the current iteration.</text></p>
                  </para>
                  <float class="ltx_float_algorithm" framed="top" inlist="loa" labels="LABEL:alg:adaptive_rebalancing" xml:id="alg2">
                    <tags>
                      <tag><text font="bold">Algorithm 2</text></tag>
                      <tag role="autoref">2</tag>
                      <tag role="refnum">2</tag>
                      <tag role="typerefnum">Algorithm 2</tag>
                    </tags>
                    <toccaption><tag close=" "><text fontsize="90%">2</text></tag><text fontsize="90%">Adaptive rebalancing for SWARM parallelism</text></toccaption>
                    <caption fontsize="90%"><tag close=" "><text font="bold">Algorithm 2</text></tag> Adaptive rebalancing for SWARM parallelism</caption>
                    <listing>
                      <listingline xml:id="alg2.l0"><tags>
                          <tag><text fontsize="80%">0:</text></tag>
                          <tag role="autoref">0</tag>
                          <tag role="refnum">0</tag>
                        </tags><text fontsize="90%">  peer index </text><Math mode="inline" tex="i" text="i" xml:id="alg2.l0.m1">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">i</XMTok>
                          </XMath>
                        </Math><text fontsize="90%">, current peer stage </text><Math mode="inline" tex="s_{cur}" text="s _ (c * u * r)" xml:id="alg2.l0.m2">
                          <XMath>
                            <XMApp>
                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">u</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">r</XMTok>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">, total number of stages </text><Math mode="inline" tex="S" text="S" xml:id="alg2.l0.m3">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">S</XMTok>
                          </XMath>
                        </Math><text fontsize="90%">, rebalancing period </text><Math mode="inline" tex="T" text="T" xml:id="alg2.l0.m4">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l1"><tags>
                          <tag><text fontsize="80%">1:</text></tag>
                          <tag role="autoref">1</tag>
                          <tag role="refnum">1</tag>
                        </tags><text fontsize="90%">  </text><text font="bold" fontsize="90%">while</text><text fontsize="90%"> active </text><text font="bold" fontsize="90%">do</text><text fontsize="90%">


</text></listingline>
                      <listingline xml:id="alg2.l2"><tags>
                          <tag><text fontsize="80%">2:</text></tag>
                          <tag role="autoref">2</tag>
                          <tag role="refnum">2</tag>
                        </tags><text fontsize="90%">    Sleep for </text><Math mode="inline" tex="T" text="T" xml:id="alg2.l2.m1">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                          </XMath>
                        </Math><text fontsize="90%"> seconds
</text></listingline>
                      <listingline xml:id="alg2.l3"><tags>
                          <tag><text fontsize="80%">3:</text></tag>
                          <tag role="autoref">3</tag>
                          <tag role="refnum">3</tag>
                        </tags><text fontsize="90%">    Measure </text><Math mode="inline" tex="q_{i}" text="q _ i" xml:id="alg2.l3.m1">
                          <XMath>
                            <XMApp>
                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                              <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> as the local request queue size
</text></listingline>
                      <listingline xml:id="alg2.l4"><tags>
                          <tag><text fontsize="80%">4:</text></tag>
                          <tag role="autoref">4</tag>
                          <tag role="refnum">4</tag>
                        </tags><text fontsize="90%">    Write </text><Math mode="inline" tex="(i,q_{i})" text="open-interval@(i, q _ i)" xml:id="alg2.l4.m1">
                          <XMath>
                            <XMDual>
                              <XMApp>
                                <XMTok meaning="open-interval"/>
                                <XMRef idref="alg2.l4.m1.1"/>
                                <XMRef idref="alg2.l4.m1.2"/>
                              </XMApp>
                              <XMWrap>
                                <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN" xml:id="alg2.l4.m1.1">i</XMTok>
                                <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                <XMApp xml:id="alg2.l4.m1.2">
                                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                              </XMWrap>
                            </XMDual>
                          </XMath>
                        </Math><text fontsize="90%"> as the key-subkey pair to DHT[</text><Math mode="inline" tex="s_{cur}" text="s _ (c * u * r)" xml:id="alg2.l4.m2">
                          <XMath>
                            <XMApp>
                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">u</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">r</XMTok>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">]
</text></listingline>
                      <listingline xml:id="alg2.l5"><tags>
                          <tag><text fontsize="80%">5:</text></tag>
                          <tag role="autoref">5</tag>
                          <tag role="refnum">5</tag>
                        </tags><text fontsize="90%">    Initialize minimum and maximum load stages: </text><Math mode="inline" tex="s_{min}=s_{max}:=-1" text="s _ (m * i * n) = s _ (m * a * x) assign - 1" xml:id="alg2.l5.m1">
                          <XMath>
                            <XMApp>
                              <XMTok meaning="multirelation"/>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                </XMApp>
                              </XMApp>
                              <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">a</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">x</XMTok>
                                </XMApp>
                              </XMApp>
                              <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="minus" role="ADDOP">-</XMTok>
                                <XMTok fontsize="90%" meaning="1" role="NUMBER">1</XMTok>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">,
</text></listingline>
                      <listingline xml:id="alg2.l6"><tags>
                          <tag><text fontsize="80%">6:</text></tag>
                          <tag role="autoref">6</tag>
                          <tag role="refnum">6</tag>
                        </tags><text fontsize="90%">    </text><Math mode="inline" tex="l_{min}:=\infty,l_{max}:=-\infty" text="formulae@(l _ (m * i * n) assign infinity, l _ (m * a * x) assign - infinity)" xml:id="alg2.l6.m1">
                          <XMath>
                            <XMDual>
                              <XMApp>
                                <XMTok meaning="formulae"/>
                                <XMRef idref="alg2.l6.m1.1"/>
                                <XMRef idref="alg2.l6.m1.2"/>
                              </XMApp>
                              <XMWrap>
                                <XMApp xml:id="alg2.l6.m1.1">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">l</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="infinity" name="infty" role="ID">∞</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                <XMApp xml:id="alg2.l6.m1.2">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">l</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">a</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">x</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMApp>
                                    <XMTok fontsize="90%" meaning="minus" role="ADDOP">-</XMTok>
                                    <XMTok fontsize="90%" meaning="infinity" name="infty" role="ID">∞</XMTok>
                                  </XMApp>
                                </XMApp>
                              </XMWrap>
                            </XMDual>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l7"><tags>
                          <tag><text fontsize="80%">7:</text></tag>
                          <tag role="autoref">7</tag>
                          <tag role="refnum">7</tag>
                        </tags><text fontsize="90%">    </text><text font="bold" fontsize="90%">for</text><text fontsize="90%"> </text><Math mode="inline" tex="s" text="s" xml:id="alg2.l7.m1">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                          </XMath>
                        </Math><text fontsize="90%"> in </text><Math mode="inline" tex="1,\ldots,S" text="list@(1, ldots, S)" xml:id="alg2.l7.m2">
                          <XMath>
                            <XMDual>
                              <XMApp>
                                <XMTok meaning="list"/>
                                <XMRef idref="alg2.l7.m2.1"/>
                                <XMRef idref="alg2.l7.m2.2"/>
                                <XMRef idref="alg2.l7.m2.3"/>
                              </XMApp>
                              <XMWrap>
                                <XMTok fontsize="90%" meaning="1" role="NUMBER" xml:id="alg2.l7.m2.1">1</XMTok>
                                <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                <XMTok fontsize="90%" name="ldots" role="ID" xml:id="alg2.l7.m2.2">…</XMTok>
                                <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN" xml:id="alg2.l7.m2.3">S</XMTok>
                              </XMWrap>
                            </XMDual>
                          </XMath>
                        </Math><text fontsize="90%"> </text><text font="bold" fontsize="90%">do</text><text fontsize="90%">


</text></listingline>
                      <listingline xml:id="alg2.l8"><tags>
                          <tag><text fontsize="80%">8:</text></tag>
                          <tag role="autoref">8</tag>
                          <tag role="refnum">8</tag>
                        </tags><text fontsize="90%">       Initialize the load buffer </text><Math mode="inline" tex="L=0" text="L = 0" xml:id="alg2.l8.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                              <XMTok fontsize="90%" meaning="0" role="NUMBER">0</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l9"><tags>
                          <tag><text fontsize="80%">9:</text></tag>
                          <tag role="autoref">9</tag>
                          <tag role="refnum">9</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">for</text><text fontsize="90%"> </text><Math mode="inline" tex="(j,q_{j})" text="open-interval@(j, q _ j)" xml:id="alg2.l9.m1">
                          <XMath>
                            <XMDual>
                              <XMApp>
                                <XMTok meaning="open-interval"/>
                                <XMRef idref="alg2.l9.m1.1"/>
                                <XMRef idref="alg2.l9.m1.2"/>
                              </XMApp>
                              <XMWrap>
                                <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN" xml:id="alg2.l9.m1.1">j</XMTok>
                                <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                <XMApp xml:id="alg2.l9.m1.2">
                                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">j</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                              </XMWrap>
                            </XMDual>
                          </XMath>
                        </Math><text fontsize="90%"> in DHT[</text><Math mode="inline" tex="s" text="s" xml:id="alg2.l9.m2">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                          </XMath>
                        </Math><text fontsize="90%">] </text><text font="bold" fontsize="90%">do</text><text fontsize="90%">


</text></listingline>
                      <listingline xml:id="alg2.l10"><tags>
                          <tag><text fontsize="80%">10:</text></tag>
                          <tag role="autoref">10</tag>
                          <tag role="refnum">10</tag>
                        </tags><text fontsize="90%">          </text><Math mode="inline" tex="L:=L+q_{j}" text="L assign L + q _ j" xml:id="alg2.l10.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="plus" role="ADDOP">+</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                                <XMApp>
                                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">j</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l11"><tags>
                          <tag><text fontsize="80%">11:</text></tag>
                          <tag role="autoref">11</tag>
                          <tag role="refnum">11</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">for</text></listingline>
                      <listingline xml:id="alg2.l12"><tags>
                          <tag><text fontsize="80%">12:</text></tag>
                          <tag role="autoref">12</tag>
                          <tag role="refnum">12</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">if</text><text fontsize="90%"> </text><Math mode="inline" tex="L&gt;L_{max}" text="L &gt; L _ (m * a * x)" xml:id="alg2.l12.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="greater-than" role="RELOP">&gt;</XMTok>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">a</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">x</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> </text><text font="bold" fontsize="90%">then</text><text fontsize="90%">


</text></listingline>
                      <listingline xml:id="alg2.l13"><tags>
                          <tag><text fontsize="80%">13:</text></tag>
                          <tag role="autoref">13</tag>
                          <tag role="refnum">13</tag>
                        </tags><text fontsize="90%">          </text><Math mode="inline" tex="s_{max}:=s,\ L_{max}:=L" text="formulae@(s _ (m * a * x) assign s, L _ (m * a * x) assign L)" xml:id="alg2.l13.m1">
                          <XMath>
                            <XMDual>
                              <XMApp>
                                <XMTok meaning="formulae"/>
                                <XMRef idref="alg2.l13.m1.1"/>
                                <XMRef idref="alg2.l13.m1.2"/>
                              </XMApp>
                              <XMWrap>
                                <XMApp xml:id="alg2.l13.m1.1">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">a</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">x</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" role="PUNCT" rpadding="4.5pt">,</XMTok>
                                <XMApp xml:id="alg2.l13.m1.2">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">a</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">x</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                                </XMApp>
                              </XMWrap>
                            </XMDual>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l14"><tags>
                          <tag><text fontsize="80%">14:</text></tag>
                          <tag role="autoref">14</tag>
                          <tag role="refnum">14</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">if</text></listingline>
                      <listingline xml:id="alg2.l15"><tags>
                          <tag><text fontsize="80%">15:</text></tag>
                          <tag role="autoref">15</tag>
                          <tag role="refnum">15</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">if</text><text fontsize="90%"> </text><Math mode="inline" tex="L&lt;L_{min}" text="L &lt; L _ (m * i * n)" xml:id="alg2.l15.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="less-than" role="RELOP">&lt;</XMTok>
<!--  %**** supplementary.tex Line 175 **** -->                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> </text><text font="bold" fontsize="90%">then</text><text fontsize="90%">


</text></listingline>
                      <listingline xml:id="alg2.l16"><tags>
                          <tag><text fontsize="80%">16:</text></tag>
                          <tag role="autoref">16</tag>
                          <tag role="refnum">16</tag>
                        </tags><text fontsize="90%">          </text><Math mode="inline" tex="s_{min}:=s,\ L_{min}:=L" text="formulae@(s _ (m * i * n) assign s, L _ (m * i * n) assign L)" xml:id="alg2.l16.m1">
                          <XMath>
                            <XMDual>
                              <XMApp>
                                <XMTok meaning="formulae"/>
                                <XMRef idref="alg2.l16.m1.1"/>
                                <XMRef idref="alg2.l16.m1.2"/>
                              </XMApp>
                              <XMWrap>
                                <XMApp xml:id="alg2.l16.m1.1">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" role="PUNCT" rpadding="4.5pt">,</XMTok>
                                <XMApp xml:id="alg2.l16.m1.2">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">L</XMTok>
                                </XMApp>
                              </XMWrap>
                            </XMDual>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l17"><tags>
                          <tag><text fontsize="80%">17:</text></tag>
                          <tag role="autoref">17</tag>
                          <tag role="refnum">17</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">if</text></listingline>
                      <listingline xml:id="alg2.l18"><tags>
                          <tag><text fontsize="80%">18:</text></tag>
                          <tag role="autoref">18</tag>
                          <tag role="refnum">18</tag>
                        </tags><text fontsize="90%">    </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">for</text></listingline>
                      <listingline xml:id="alg2.l19"><tags>
                          <tag><text fontsize="80%">19:</text></tag>
                          <tag role="autoref">19</tag>
                          <tag role="refnum">19</tag>
                        </tags><text fontsize="90%">    </text><text font="bold" fontsize="90%">if</text><text fontsize="90%"> </text><Math mode="inline" tex="s_{cur}=s_{min}" text="s _ (c * u * r) = s _ (m * i * n)" xml:id="alg2.l19.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">u</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">r</XMTok>
                                </XMApp>
                              </XMApp>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> </text><text font="bold" fontsize="90%">then</text><text fontsize="90%">


</text></listingline>
                      <listingline xml:id="alg2.l20"><tags>
                          <tag><text fontsize="80%">20:</text></tag>
                          <tag role="autoref">20</tag>
                          <tag role="refnum">20</tag>
                        </tags><text fontsize="90%">       // Migrate to the maximum load stage
</text></listingline>
                      <listingline xml:id="alg2.l21"><tags>
                          <tag><text fontsize="80%">21:</text></tag>
                          <tag role="autoref">21</tag>
                          <tag role="refnum">21</tag>
                        </tags><text fontsize="90%">       Initialize the minimum load peer </text><Math mode="inline" tex="i_{min}:=-1,q_{min}:=\infty" text="formulae@(i _ (m * i * n) assign - 1, q _ (m * i * n) assign infinity)" xml:id="alg2.l21.m1">
                          <XMath>
                            <XMDual>
                              <XMApp>
                                <XMTok meaning="formulae"/>
                                <XMRef idref="alg2.l21.m1.1"/>
                                <XMRef idref="alg2.l21.m1.2"/>
                              </XMApp>
                              <XMWrap>
                                <XMApp xml:id="alg2.l21.m1.1">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">i</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMApp>
                                    <XMTok fontsize="90%" meaning="minus" role="ADDOP">-</XMTok>
                                    <XMTok fontsize="90%" meaning="1" role="NUMBER">1</XMTok>
                                  </XMApp>
                                </XMApp>
                                <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                <XMApp xml:id="alg2.l21.m1.2">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="infinity" name="infty" role="ID">∞</XMTok>
                                </XMApp>
                              </XMWrap>
                            </XMDual>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l22"><tags>
                          <tag><text fontsize="80%">22:</text></tag>
                          <tag role="autoref">22</tag>
                          <tag role="refnum">22</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">for</text><text fontsize="90%"> </text><Math mode="inline" tex="(j,q_{j})" text="open-interval@(j, q _ j)" xml:id="alg2.l22.m1">
                          <XMath>
                            <XMDual>
                              <XMApp>
                                <XMTok meaning="open-interval"/>
                                <XMRef idref="alg2.l22.m1.1"/>
                                <XMRef idref="alg2.l22.m1.2"/>
                              </XMApp>
                              <XMWrap>
                                <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN" xml:id="alg2.l22.m1.1">j</XMTok>
                                <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                <XMApp xml:id="alg2.l22.m1.2">
                                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">j</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                              </XMWrap>
                            </XMDual>
                          </XMath>
                        </Math><text fontsize="90%"> in DHT[</text><Math mode="inline" tex="s" text="s" xml:id="alg2.l22.m2">
                          <XMath>
                            <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                          </XMath>
                        </Math><text fontsize="90%">] </text><text font="bold" fontsize="90%">do</text><text fontsize="90%">


</text></listingline>
                      <listingline xml:id="alg2.l23"><tags>
                          <tag><text fontsize="80%">23:</text></tag>
                          <tag role="autoref">23</tag>
                          <tag role="refnum">23</tag>
                        </tags><text fontsize="90%">          </text><text font="bold" fontsize="90%">if</text><text fontsize="90%"> </text><Math mode="inline" tex="q_{j}&lt;q_{min}" text="q _ j &lt; q _ (m * i * n)" xml:id="alg2.l23.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="less-than" role="RELOP">&lt;</XMTok>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">j</XMTok>
                              </XMApp>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> </text><text font="bold" fontsize="90%">then</text><text fontsize="90%">


</text></listingline>
                      <listingline xml:id="alg2.l24"><tags>
                          <tag><text fontsize="80%">24:</text></tag>
                          <tag role="autoref">24</tag>
                          <tag role="refnum">24</tag>
                        </tags><text fontsize="90%">             </text><Math mode="inline" tex="i_{min}:=j,\ q_{min}:=q_{j}" text="formulae@(i _ (m * i * n) assign j, q _ (m * i * n) assign q _ j)" xml:id="alg2.l24.m1">
                          <XMath>
                            <XMDual>
                              <XMApp>
                                <XMTok meaning="formulae"/>
                                <XMRef idref="alg2.l24.m1.1"/>
                                <XMRef idref="alg2.l24.m1.2"/>
                              </XMApp>
                              <XMWrap>
                                <XMApp xml:id="alg2.l24.m1.1">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">i</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">j</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" role="PUNCT" rpadding="4.5pt">,</XMTok>
                                <XMApp xml:id="alg2.l24.m1.2">
                                  <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">q</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">j</XMTok>
                                  </XMApp>
                                </XMApp>
                              </XMWrap>
                            </XMDual>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l25"><tags>
                          <tag><text fontsize="80%">25:</text></tag>
                          <tag role="autoref">25</tag>
                          <tag role="refnum">25</tag>
                        </tags><text fontsize="90%">          </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">if</text></listingline>
                      <listingline xml:id="alg2.l26"><tags>
                          <tag><text fontsize="80%">26:</text></tag>
                          <tag role="autoref">26</tag>
                          <tag role="refnum">26</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">for</text></listingline>
                      <listingline xml:id="alg2.l27"><tags>
                          <tag><text fontsize="80%">27:</text></tag>
                          <tag role="autoref">27</tag>
                          <tag role="refnum">27</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">if</text><text fontsize="90%"> </text><Math mode="inline" tex="i_{min}=i" text="i _ (m * i * n) = i" xml:id="alg2.l27.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">i</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">i</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">n</XMTok>
                                </XMApp>
                              </XMApp>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">i</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> </text><text font="bold" fontsize="90%">then</text><text fontsize="90%">


</text></listingline>
                      <listingline xml:id="alg2.l28"><tags>
                          <tag><text fontsize="80%">28:</text></tag>
                          <tag role="autoref">28</tag>
                          <tag role="refnum">28</tag>
                        </tags><text fontsize="90%">          // This peer should migrate
</text></listingline>
                      <listingline xml:id="alg2.l29"><tags>
                          <tag><text fontsize="80%">29:</text></tag>
                          <tag role="autoref">29</tag>
                          <tag role="refnum">29</tag>
                        </tags><text fontsize="90%">          </text><Math mode="inline" tex="s_{cur}:=s_{max}" text="s _ (c * u * r) assign s _ (m * a * x)" xml:id="alg2.l29.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="assign" role="RELOP">:=</XMTok>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">u</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">r</XMTok>
                                </XMApp>
                              </XMApp>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">a</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">x</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l30"><tags>
                          <tag><text fontsize="80%">30:</text></tag>
                          <tag role="autoref">30</tag>
                          <tag role="refnum">30</tag>
                        </tags><text fontsize="90%">          Download up-to-date parameters from peers in </text><Math mode="inline" tex="s_{max}" text="s _ (m * a * x)" xml:id="alg2.l30.m1">
                          <XMath>
                            <XMApp>
                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">a</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">x</XMTok>
                              </XMApp>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%">
</text></listingline>
                      <listingline xml:id="alg2.l31"><tags>
                          <tag><text fontsize="80%">31:</text></tag>
                          <tag role="autoref">31</tag>
                          <tag role="refnum">31</tag>
                        </tags><text fontsize="90%">       </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">if</text></listingline>
                      <listingline xml:id="alg2.l32"><tags>
                          <tag><text fontsize="80%">32:</text></tag>
                          <tag role="autoref">32</tag>
                          <tag role="refnum">32</tag>
                        </tags><text fontsize="90%">    </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">if</text></listingline>
                      <listingline xml:id="alg2.l33"><tags>
                          <tag><text fontsize="80%">33:</text></tag>
                          <tag role="autoref">33</tag>
                          <tag role="refnum">33</tag>
                        </tags><text fontsize="90%">  </text><text font="bold" fontsize="90%">end</text><text fontsize="90%"> </text><text font="bold" fontsize="90%">while</text></listingline>
                    </listing>
                  </float>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:equivalence" xml:id="A5">
                  <tags>
                    <tag>Appendix E</tag>
                    <tag role="autoref">Appendix E</tag>
                    <tag role="refnum">E</tag>
                    <tag role="typerefnum">Appendix E</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix E</tag>Relation between SWARM and ZeRO-Offload</title>
                  <toctitle><tag close=" "><text fontsize="90%">E</text></tag><text fontsize="90%">Relation between SWARM and ZeRO-Offload</text></toctitle>
                  <para xml:id="A5.p1">
                    <p><!--  %****␣supplementary.tex␣Line␣200␣**** --><text fontsize="90%">In this section, we argue that depending on the use of DPU, SWARM-parallel training is equivalent to either fully synchronous training or the semi-synchronous training proposed in ZeRO-Offload </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="zerooffload" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.
That is, SWARM produces exactly the same stepwise updates as conventional distributed training algorithms and will therefore achieve a solution in the same number of steps.</text></p>
                  </para>
                  <para xml:id="A5.p2">
                    <p><text fontsize="90%">This observation is similar to how many advanced distributed training techniques </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="huang2019gpipe,zero" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> are computationally equivalent to regular synchronous training on a single device. For instance, despite using advanced distributed computation strategies, GPipe </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="huang2019gpipe" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> computes exactly the same mathematical expression to obtain gradients and applies those gradients in the same order as any other </text><text font="italic" fontsize="90%">synchronous</text><text fontsize="90%"> training algorithm. On the other hand, PipeDream </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="pipedream" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> changes the order in which the updates are applied, introducing the so-called stale gradients </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="recht2011hogwild" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. This allows PipeDream to improve device utilization but has been shown to reduce the final model quality in some setups </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="MLSYS2020_96da2f59" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.</text></p>
                  </para>
                  <para xml:id="A5.p3">
                    <p><text fontsize="90%">Despite using randomized routing and asynchronous communication between pipeline stages, SWARM still performs optimizer steps synchronously after peers collectively reach the required global batch size (which is a hyperparameter). While different peers may accumulate a different number of samples, they will all use the same gradient after averaging.
Any peer that fails or does not meet this condition is considered a straggler and must reload its state from neighbors before it can resume training.
This procedure ensures that all surviving peers use non-stale aggregated gradients over the specified batch size when performing the optimizer step.</text></p>
                  </para>
                  <para xml:id="A5.p4">
                    <p><text fontsize="90%">The only deviation from fully synchronous training is that SWARM uses the same approach for CPU offloading as ZeRO-Offload, and by extension, delayed parameter updates (DPU). While DPU was shown not to affect convergence </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="zerooffload,stich2020error,arjevani2020tight" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, one can disable this functionality and make SWARM fully equivalent to standard training.</text></p>
                  </para>
                  <para xml:id="A5.p5">
                    <p><text fontsize="90%">Naturally, these guarantees come at the cost of reduced hardware utilization, as a small portion of devices will need to wait after every step. However, as we show in Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_large"/><text fontsize="90%">, SWARM can still train with competitive training throughput due to the fact that large models are trained with increased batch sizes </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="gpt3" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.</text></p>
                  </para>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:detailed_setup" xml:id="A6">
                  <tags>
                    <tag>Appendix F</tag>
                    <tag role="autoref">Appendix F</tag>
                    <tag role="refnum">F</tag>
                    <tag role="typerefnum">Appendix F</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix F</tag>Additional Details for Section <ref labelref="LABEL:sect:experiments_square_cube"/></title>
                  <toctitle><tag close=" "><text fontsize="90%">F</text></tag><text fontsize="90%">Additional Details for Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_square_cube"/></toctitle>
                  <para xml:id="A6.p1">
                    <p><text fontsize="90%">We benchmark four versions of the Transformer layer:</text></p>
                  </para>
                  <para xml:id="A6.p2">
                    <itemize xml:id="A6.I1">
                      <item xml:id="A6.I1.i1">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">1st item</tag>
                        </tags>
                        <para xml:id="A6.I1.i1.p1">
                          <p><text fontsize="90%">“base”: </text><Math mode="inline" tex="d_{model}=768" text="d _ (m * o * d * e * l) = 768" xml:id="A6.I1.i1.p1.m1">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">o</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">e</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">l</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="768" role="NUMBER">768</XMTok>
                                </XMApp>
                              </XMath>
                            </Math><text fontsize="90%">, </text><Math mode="inline" tex="d_{\text{FFN}}=3072" text="d _ [FFN] = 3072" xml:id="A6.I1.i1.p1.m2">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                                    <XMText><text fontsize="63%">FFN</text></XMText>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="3072" role="NUMBER">3072</XMTok>
                                </XMApp>
                              </XMath>
                            </Math><text fontsize="90%">, 12 heads;</text></p>
                        </para>
                      </item>
                      <item xml:id="A6.I1.i2">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">2nd item</tag>
                        </tags>
                        <para xml:id="A6.I1.i2.p1">
                          <p><text fontsize="90%">“xxlarge”: </text><Math mode="inline" tex="d_{model}=4096" text="d _ (m * o * d * e * l) = 4096" xml:id="A6.I1.i2.p1.m1">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">o</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">e</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">l</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="4096" role="NUMBER">4096</XMTok>
                                </XMApp>
                              </XMath>
                            </Math><text fontsize="90%">, </text><Math mode="inline" tex="d_{\text{FFN}}=16384" text="d _ [FFN] = 16384" xml:id="A6.I1.i2.p1.m2">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                                    <XMText><text fontsize="63%">FFN</text></XMText>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="16384" role="NUMBER">16384</XMTok>
                                </XMApp>
                              </XMath>
                            </Math><text fontsize="90%">, 32 heads;</text></p>
                        </para>
                      </item>
                      <item xml:id="A6.I1.i3">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">3rd item</tag>
                        </tags>
                        <para xml:id="A6.I1.i3.p1">
                          <p><text fontsize="90%">“GPT-3” </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="gpt3" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                                <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                              </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">: </text><Math mode="inline" tex="d_{model}=12288" text="d _ (m * o * d * e * l) = 12288" xml:id="A6.I1.i3.p1.m1">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">o</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">e</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">l</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="12288" role="NUMBER">12288</XMTok>
                                </XMApp>
                              </XMath>
                            </Math><text fontsize="90%">, </text><Math mode="inline" tex="d_{\text{FFN}}=49152" text="d _ [FFN] = 49152" xml:id="A6.I1.i3.p1.m2">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                                    <XMText><text fontsize="63%">FFN</text></XMText>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="49152" role="NUMBER">49152</XMTok>
                                </XMApp>
                              </XMath>
                            </Math><text fontsize="90%">, 96 heads.</text></p>
                        </para>
                      </item>
                      <item xml:id="A6.I1.i4">
                        <tags>
                          <tag>•</tag>
                          <tag role="autoref">item </tag>
                          <tag role="typerefnum">4th item</tag>
                        </tags>
                        <para xml:id="A6.I1.i4.p1">
                          <p><text fontsize="90%">“Ours”: </text><Math mode="inline" tex="d_{model}=4096" text="d _ (m * o * d * e * l) = 4096" xml:id="A6.I1.i4.p1.m1">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                                    <XMApp>
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">o</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">e</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">l</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="4096" role="NUMBER">4096</XMTok>
                                </XMApp>
                              </XMath>
                            </Math><text fontsize="90%">, </text><Math mode="inline" tex="d_{\text{FFN}}=16384" text="d _ [FFN] = 16384" xml:id="A6.I1.i4.p1.m2">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                                    <XMText><text fontsize="63%">FFN</text></XMText>
                                  </XMApp>
                                  <XMTok fontsize="90%" meaning="16384" role="NUMBER">16384</XMTok>
                                </XMApp>
                              </XMath>
                            </Math><text fontsize="90%">, 32 heads, 3 layers per pipeline stage.</text></p>
                        </para>
                      </item>
                    </itemize>
                  </para>
                  <para xml:id="A6.p3">
                    <p><text fontsize="90%">In Table </text><ref fontsize="90%" labelref="LABEL:tab:flops_params"/><text fontsize="90%">, we report FLOP and parameter counts of each version based on the expressions from </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="kaplan2020scaling" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.
For simplicity, we set up each experiment with 12 Transformer layers using 12 servers (4 for “Ours”) with a single V100-PCIE GPU each. The servers communicate at 500Mbps under 3–6ms latency.</text></p>
                  </para>
                  <table inlist="lot" labels="LABEL:tab:flops_params" placement="b" xml:id="A6.T6">
                    <tags>
                      <tag><text fontsize="90%">Table 6</text></tag>
                      <tag role="autoref">Table 6</tag>
                      <tag role="refnum">6</tag>
                      <tag role="typerefnum">Table 6</tag>
                    </tags>
                    <toccaption class="ltx_centering"><tag close=" "><text fontsize="90%">6</text></tag><text fontsize="90%">Parameter and FLOP counts of each architecture.</text></toccaption>
                    <caption class="ltx_centering" fontsize="90%"><tag close=": ">Table 6</tag>Parameter and FLOP counts of each architecture.</caption>
                    <tabular class="ltx_centering" colsep="8.0pt" vattach="middle">
                      <tr>
                        <td align="left" border="tt"><text fontsize="90%">Architecture</text></td>
                        <td align="center" border="tt"><text fontsize="90%">Parameters</text></td>
                        <td align="center" border="tt" class="ltx_nopad_r"><text fontsize="90%">FLOP count</text></td>
                      </tr>
                      <tr>
                        <td align="left" border="t"><text fontsize="90%">“base”</text></td>
                        <td align="center" border="t"><text fontsize="90%">7.08M</text></td>
                        <td align="center" border="t" class="ltx_nopad_r"><Math mode="inline" tex="2.2\times 10^{10}" text="2.2 * 10 ^ 10" xml:id="A6.T6.m1">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="times" role="MULOP">×</XMTok>
                                <XMTok fontsize="90%" meaning="2.2" role="NUMBER">2.2</XMTok>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMTok fontsize="90%" meaning="10" role="NUMBER">10</XMTok>
                                  <XMTok fontsize="63%" meaning="10" role="NUMBER">10</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math></td>
                      </tr>
                      <tr>
                        <td align="left"><text fontsize="90%">“xxlarge”</text></td>
                        <td align="center"><text fontsize="90%">201M</text></td>
                        <td align="center" class="ltx_nopad_r"><Math mode="inline" tex="6.2\times 10^{11}" text="6.2 * 10 ^ 11" xml:id="A6.T6.m2">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="times" role="MULOP">×</XMTok>
                                <XMTok fontsize="90%" meaning="6.2" role="NUMBER">6.2</XMTok>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMTok fontsize="90%" meaning="10" role="NUMBER">10</XMTok>
                                  <XMTok fontsize="63%" meaning="11" role="NUMBER">11</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math></td>
                      </tr>
                      <tr>
                        <td align="left"><text fontsize="90%">“GPT-3”</text></td>
                        <td align="center"><text fontsize="90%">1.81B</text></td>
                        <td align="center" class="ltx_nopad_r"><Math mode="inline" tex="5.5\times 10^{12}" text="5.5 * 10 ^ 12" xml:id="A6.T6.m3">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="times" role="MULOP">×</XMTok>
                                <XMTok fontsize="90%" meaning="5.5" role="NUMBER">5.5</XMTok>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMTok fontsize="90%" meaning="10" role="NUMBER">10</XMTok>
                                  <XMTok fontsize="63%" meaning="12" role="NUMBER">12</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math></td>
                      </tr>
                      <tr>
                        <td align="left" border="bb"><text fontsize="90%">“Ours”</text></td>
                        <td align="center" border="bb"><text fontsize="90%">201M</text></td>
                        <td align="center" border="bb" class="ltx_nopad_r"><Math mode="inline" tex="1.8\times 10^{12}" text="1.8 * 10 ^ 12" xml:id="A6.T6.m4">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="times" role="MULOP">×</XMTok>
                                <XMTok fontsize="90%" meaning="1.8" role="NUMBER">1.8</XMTok>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMTok fontsize="90%" meaning="10" role="NUMBER">10</XMTok>
                                  <XMTok fontsize="63%" meaning="12" role="NUMBER">12</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math></td>
                      </tr>
                    </tabular>
<!--  %****␣supplementary.tex␣Line␣250␣**** -->                  </table>
                  <para xml:id="A6.p4">
                    <p><text fontsize="90%">Due to a modest communication bandwidth, smaller models spend most of the time waiting for the network. However, that same bandwidth allows for </text><Math mode="inline" tex="&gt;80\%" text="absent &gt; 80percent" xml:id="A6.p4.m1">
                        <XMath>
                          <XMApp>
                            <XMTok fontsize="90%" meaning="greater-than" role="RELOP">&gt;</XMTok>
                            <XMTok meaning="absent"/>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="percent" role="POSTFIX">%</XMTok>
                              <XMTok fontsize="90%" meaning="80" role="NUMBER">80</XMTok>
                            </XMApp>
                          </XMApp>
                        </XMath>
                      </Math><text fontsize="90%"> GPU utilization when dealing with GPT-3-sized layers. If we colocate 3 “GPT-3” layers per pipeline stage, the GPU utilization can further improved to </text><Math mode="inline" tex="&gt;90\%" text="absent &gt; 90percent" xml:id="A6.p4.m2">
                        <XMath>
                          <XMApp>
                            <XMTok fontsize="90%" meaning="greater-than" role="RELOP">&gt;</XMTok>
                            <XMTok meaning="absent"/>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="percent" role="POSTFIX">%</XMTok>
                              <XMTok fontsize="90%" meaning="90" role="NUMBER">90</XMTok>
                            </XMApp>
                          </XMApp>
                        </XMath>
                      </Math><text fontsize="90%">.</text></p>
                  </para>
                  <para xml:id="A6.p5">
                    <p><text fontsize="90%">The time reported in Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_square_cube"/><text fontsize="90%"> is the time required to run forward and backward pass for all layers with a batch of 1x512 tokens, not including the Adam updates. All results are averaged over 1000 consecutive batches; the standard deviations are below 0.1%. All four GPUs are in the same data center but on different servers. Each layer is a </text><text font="typewriter" fontsize="90%">TransformerEncoderLayer</text><text fontsize="90%"> from PyTorch 1.7.0 </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="paszke2019pytorch" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> wrapped with activation checkpointing. We use </text><text font="typewriter" fontsize="90%">hivemind==0.8.15</text><text fontsize="90%"> </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="hivemind_dmoe" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> with a single synchronous trainer based on the BERT training code from the Transformers library </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="wolf-etal-2020-transformers" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. However, these results are not specific to hivemind and are likely reproducible in FairScale </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="FairScale2021" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> or PyTorch RPC. The only important detail is that the training code should run as much communication as possible in the background while the GPUs are busy processing batches.
It is important to reuse the same connection for multiple RPC calls so that the TCP buffer does not have to warm up during each call. Also, our implementation performs quantization asynchronously with communication and other computations.</text></p>
                  </para>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:detailed_large" xml:id="A7">
                  <tags>
                    <tag>Appendix G</tag>
                    <tag role="autoref">Appendix G</tag>
                    <tag role="refnum">G</tag>
                    <tag role="typerefnum">Appendix G</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix G</tag>Additional Details for Section <ref labelref="LABEL:sect:experiments_large"/></title>
                  <toctitle><tag close=" "><text fontsize="90%">G</text></tag><text fontsize="90%">Additional Details for Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_large"/></toctitle>
                  <para xml:id="A7.p1">
                    <p><text fontsize="90%">We use the standard Transformer architecture with two modifications: Rotary Positional Embeddings </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="su2021roformer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> and GeGLU activations </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="gated_improve" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.
Similarly to other models trained on Pile </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="gao2020pile,gptj" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, we use the tokenizer of GPT-2 </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="radford2019language" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. Following </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="curriculum_minja" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, we linearly increase training sequence length during the initial phase. More specifically, we begin training with sequences of up to 256 tokens and increase them to the maximum length of 2048 over the first </text><Math mode="inline" tex="12,000" text="list@(12, 000)" xml:id="A7.p1.m1">
                        <XMath>
                          <XMDual>
                            <XMApp>
                              <XMTok meaning="list"/>
                              <XMRef idref="A7.p1.m1.1"/>
                              <XMRef idref="A7.p1.m1.2"/>
                            </XMApp>
                            <XMWrap>
                              <XMTok fontsize="90%" meaning="12" role="NUMBER" xml:id="A7.p1.m1.1">12</XMTok>
                              <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                              <XMTok fontsize="90%" meaning="000" role="NUMBER" xml:id="A7.p1.m1.2">000</XMTok>
                            </XMWrap>
                          </XMDual>
                        </XMath>
                      </Math><text fontsize="90%"> optimizer steps.
We train the model with LAMB </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="lamb" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, following the configuration from the original paper for a batch size of 16384. On top of that, we set </text><Math mode="inline" tex="\eta=10^{-3}" text="eta = 10 ^ (- 3)" xml:id="A7.p1.m2">
                        <XMath>
                          <XMApp>
                            <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                            <XMTok font="italic" fontsize="90%" name="eta" role="UNKNOWN">η</XMTok>
                            <XMApp>
                              <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                              <XMTok fontsize="90%" meaning="10" role="NUMBER">10</XMTok>
                              <XMApp>
                                <XMTok fontsize="63%" meaning="minus" role="ADDOP">-</XMTok>
                                <XMTok fontsize="63%" meaning="3" role="NUMBER">3</XMTok>
                              </XMApp>
                            </XMApp>
                          </XMApp>
                        </XMath>
                      </Math><text fontsize="90%"> and </text><Math mode="inline" tex="\beta_{2}=0.95" text="beta _ 2 = 0.95" xml:id="A7.p1.m3">
                        <XMath>
                          <XMApp>
                            <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                            <XMApp>
                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                              <XMTok font="italic" fontsize="90%" name="beta" role="UNKNOWN">β</XMTok>
                              <XMTok fontsize="63%" meaning="2" role="NUMBER">2</XMTok>
                            </XMApp>
                            <XMTok fontsize="90%" meaning="0.95" role="NUMBER">0.95</XMTok>
                          </XMApp>
                        </XMath>
                      </Math><text fontsize="90%"> to account for the increased model size.</text></p>
                  </para>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:scaling" xml:id="A8">
                  <tags>
                    <tag>Appendix H</tag>
                    <tag role="autoref">Appendix H</tag>
                    <tag role="refnum">H</tag>
                    <tag role="typerefnum">Appendix H</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix H</tag>Additional Scaling Evaluation</title>
                  <toctitle><tag close=" "><text fontsize="90%">H</text></tag><text fontsize="90%">Additional Scaling Evaluation</text></toctitle>
                  <para xml:id="A8.p1">
                    <p><text fontsize="90%">In this experiment, we investigate the influence of the number of nodes training with SWARM parallelism on the throughput of the pipeline. Specifically, we measure the performance of training the same model as in Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_large"/><text fontsize="90%"> in several configurations that differ in the size of the data-parallel group at each pipeline stage, with the number of single-GPU instances ranging from 8 to 128 (the highest quantity of preemptible nodes that we could reliably maintain for a long time). To isolate the effect of worker heterogeneity, here we use only the T4 accelerators and measure the average performance over 30 minutes of training.</text></p>
                  </para>
                  <figure inlist="lof" labels="LABEL:fig:scaling_t4" placement="b" xml:id="A8.F7">
                    <tags>
                      <tag><text fontsize="90%">Figure 7</text></tag>
                      <tag role="autoref">Figure 7</tag>
                      <tag role="refnum">7</tag>
                      <tag role="typerefnum">Figure 7</tag>
                    </tags>
                    <graphics candidates="resources/scaling_t4.pdf" class="ltx_centering" graphic="resources/scaling_t4.pdf" options="width=303.53267pt,keepaspectratio=true" xml:id="A8.F7.g1"/>
                    <toccaption class="ltx_centering"><tag close=" "><text fontsize="90%">7</text></tag><text fontsize="90%">Scaling of SWARM parallelism throughput with the number of nodes.</text></toccaption>
                    <caption class="ltx_centering" fontsize="90%"><tag close=": ">Figure 7</tag>Scaling of SWARM parallelism throughput with the number of nodes.</caption>
                  </figure>
                  <para xml:id="A8.p2">
                    <p><text fontsize="90%">Figure </text><ref fontsize="90%" labelref="LABEL:fig:scaling_t4"/><text fontsize="90%"> shows the results of our evaluation. It can be seen that the training performance exhibits an approximately linear scaling pattern, which can be explained by the high efficiency of both the stochastic wiring strategy and the auxiliary training components such as the DHT and the All-Reduce protocol used for gradient averaging.</text></p>
                  </para>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:compression" xml:id="A9">
                  <tags>
                    <tag>Appendix I</tag>
                    <tag role="autoref">Appendix I</tag>
                    <tag role="refnum">I</tag>
                    <tag role="typerefnum">Appendix I</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix I</tag>Compression-Aware Architectures</title>
                  <toctitle><tag close=" "><text fontsize="90%">I</text></tag><text fontsize="90%">Compression-Aware Architectures</text></toctitle>
                  <para xml:id="A9.p1">
                    <p><text fontsize="90%">Since pipeline parallelism has several distinct points of communication, the network overhead can be reduced considerably by reducing the size of data at these communication points. To exploit this, we develop compression-aware architectures that apply extreme compression at these points. We study two distinct communication bottleneck layers: (1) compression through a linear bottleneck layer, and (2) compression through a bottleneck induced by the maxout activation function </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="goodfellow2013maxout" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. We also study how compressing the activations and gradients at the communication points to 8 bits affects the predictive performance.</text></p>
                  </para>
                  <subsection inlist="toc" labels="LABEL:appendix:compression_detailed" xml:id="A9.SS1">
                    <tags>
                      <tag>I.1</tag>
                      <tag role="autoref">subsection I.1</tag>
                      <tag role="refnum">I.1</tag>
                      <tag role="typerefnum">§I.1</tag>
                    </tags>
                    <title fontsize="90%"><tag close=" ">I.1</tag>Description</title>
                    <paragraph inlist="toc" xml:id="A9.SS1.SSS0.Px1">
                      <title fontsize="90%">Fully connected layers (baseline):</title>
                      <para xml:id="A9.SS1.SSS0.Px1.p1">
                        <p><text fontsize="90%">Fully connected layers in models such as Transformers consist of a multilayer perceptron with a single hidden layer and a nonlinear activation function. Without biases and with a residual connection </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="resnet" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                              <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                            </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> from the inputs to the outputs, this can be described as </text><Math mode="inline" tex="\text{MLP}(\mathbf{x},\mathbf{w}_{1},\mathbf{w}_{2})=\sigma(\mathbf{x}\mathbf{%&#10;w}_{1})\mathbf{w}_{2}+\mathbf{x}" text="[MLP] * vector@(x, w _ 1, w _ 2) = sigma * xw _ 1 * w _ 2 + x" xml:id="A9.SS1.SSS0.Px1.p1.m1">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMText><text fontsize="90%">MLP</text></XMText>
                                  <XMDual>
                                    <XMApp>
                                      <XMTok meaning="vector"/>
                                      <XMRef idref="A9.SS1.SSS0.Px1.p1.m1.1"/>
                                      <XMRef idref="A9.SS1.SSS0.Px1.p1.m1.2"/>
                                      <XMRef idref="A9.SS1.SSS0.Px1.p1.m1.3"/>
                                    </XMApp>
                                    <XMWrap>
                                      <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                      <XMTok font="bold" fontsize="90%" role="UNKNOWN" xml:id="A9.SS1.SSS0.Px1.p1.m1.1">x</XMTok>
                                      <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                      <XMApp xml:id="A9.SS1.SSS0.Px1.p1.m1.2">
                                        <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                        <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                        <XMTok fontsize="63%" meaning="1" role="NUMBER">1</XMTok>
                                      </XMApp>
                                      <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                      <XMApp xml:id="A9.SS1.SSS0.Px1.p1.m1.3">
                                        <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                        <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                        <XMTok fontsize="63%" meaning="2" role="NUMBER">2</XMTok>
                                      </XMApp>
                                      <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                    </XMWrap>
                                  </XMDual>
                                </XMApp>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="plus" role="ADDOP">+</XMTok>
                                  <XMApp>
                                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                    <XMTok font="italic" fontsize="90%" name="sigma" role="UNKNOWN">σ</XMTok>
                                    <XMDual>
                                      <XMRef idref="A9.SS1.SSS0.Px1.p1.m1.4"/>
                                      <XMWrap>
                                        <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                        <XMApp xml:id="A9.SS1.SSS0.Px1.p1.m1.4">
                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">xw</XMTok>
                                          <XMTok fontsize="63%" meaning="1" role="NUMBER">1</XMTok>
                                        </XMApp>
                                        <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                      </XMWrap>
                                    </XMDual>
                                    <XMApp>
                                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                      <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                      <XMTok fontsize="63%" meaning="2" role="NUMBER">2</XMTok>
                                    </XMApp>
                                  </XMApp>
                                  <XMTok font="bold" fontsize="90%" role="UNKNOWN">x</XMTok>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">,
where </text><Math mode="inline" tex="\mathbf{x}\in\mathbb{R}^{b\times s\times m}" text="x element-of R ^ (b * s * m)" xml:id="A9.SS1.SSS0.Px1.p1.m2">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMTok font="bold" fontsize="90%" role="UNKNOWN">x</XMTok>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="blackboard" fontsize="90%" role="UNKNOWN">R</XMTok>
                                  <XMApp>
                                    <XMTok fontsize="63%" meaning="times" role="MULOP">×</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">b</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">s</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  </XMApp>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">, </text><Math mode="inline" tex="\mathbf{w}_{1}\in\mathbb{R}^{m\times h}" text="w _ 1 element-of R ^ (m * h)" xml:id="A9.SS1.SSS0.Px1.p1.m3">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMApp>
                                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                  <XMTok fontsize="63%" meaning="1" role="NUMBER">1</XMTok>
                                </XMApp>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="blackboard" fontsize="90%" role="UNKNOWN">R</XMTok>
                                  <XMApp>
                                    <XMTok fontsize="63%" meaning="times" role="MULOP">×</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">h</XMTok>
                                  </XMApp>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">, </text><Math mode="inline" tex="\mathbf{w}_{2}\in\mathbb{R}T^{h\times m}" text="w _ 2 element-of R * T ^ (h * m)" xml:id="A9.SS1.SSS0.Px1.p1.m4">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMApp>
                                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                  <XMTok fontsize="63%" meaning="2" role="NUMBER">2</XMTok>
                                </XMApp>
                                <XMApp>
                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                  <XMTok font="blackboard" fontsize="90%" role="UNKNOWN">R</XMTok>
                                  <XMApp>
                                    <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                    <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                                    <XMApp>
                                      <XMTok fontsize="63%" meaning="times" role="MULOP">×</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">h</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                    </XMApp>
                                  </XMApp>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">, and </text><Math mode="inline" tex="\sigma(\cdot)" text="sigma * cdot" xml:id="A9.SS1.SSS0.Px1.p1.m5">
                            <XMath>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMTok font="italic" fontsize="90%" name="sigma" role="UNKNOWN">σ</XMTok>
                                <XMDual>
                                  <XMRef idref="A9.SS1.SSS0.Px1.p1.m5.1"/>
                                  <XMWrap>
                                    <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                    <XMTok fontsize="90%" name="cdot" role="MULOP" xml:id="A9.SS1.SSS0.Px1.p1.m5.1">⋅</XMTok>
                                    <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                  </XMWrap>
                                </XMDual>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%"> is a nonlinear activation function such as ReLU </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="alexnet" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                              <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                            </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">; </text><Math mode="inline" tex="b" text="b" xml:id="A9.SS1.SSS0.Px1.p1.m6">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">b</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">, </text><Math mode="inline" tex="s" text="s" xml:id="A9.SS1.SSS0.Px1.p1.m7">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">s</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">, </text><Math mode="inline" tex="m" text="m" xml:id="A9.SS1.SSS0.Px1.p1.m8">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">m</XMTok>
                            </XMath>
                          </Math><text fontsize="90%">, and </text><Math mode="inline" tex="h" text="h" xml:id="A9.SS1.SSS0.Px1.p1.m9">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">h</XMTok>
                            </XMath>
                          </Math><text fontsize="90%"> are the batch, sequence, model, and hidden dimensions of the neural network. To compress the output of the MLP layer, we want to apply a compression layer between two consecutive stages. For example, if we have 24 layers and 4 stages, we need 3 compression layers at layers 6, 12, and 18.</text></p>
                      </para>
                    </paragraph>
                    <paragraph inlist="toc" xml:id="A9.SS1.SSS0.Px2">
                      <title fontsize="90%">Quantized activations:</title>
                      <para xml:id="A9.SS1.SSS0.Px2.p1">
                        <p><text fontsize="90%">A natural way to reduce the communication intensity is to send activations and gradients with respect to activations in reduced precision. However, simply casting tensors to a lower precision may slow down convergence and cause instabilities. Instead, we use dynamic 8-bit quantization with blockwise scaling from </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="adam8bit" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                              <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                            </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. This technique reduces communication by </text><Math mode="inline" tex="{\approx}2" text="absent approximately-equals 2" xml:id="A9.SS1.SSS0.Px2.p1.m1">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="approximately-equals" name="approx" role="RELOP">≈</XMTok>
                                <XMTok meaning="absent"/>
                                <XMTok fontsize="90%" meaning="2" role="NUMBER">2</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">x and </text><Math mode="inline" tex="{\approx}4" text="absent approximately-equals 4" xml:id="A9.SS1.SSS0.Px2.p1.m2">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="approximately-equals" name="approx" role="RELOP">≈</XMTok>
                                <XMTok meaning="absent"/>
                                <XMTok fontsize="90%" meaning="4" role="NUMBER">4</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">x for half and full precision, respectively.</text></p>
                      </para>
                      <para xml:id="A9.SS1.SSS0.Px2.p2">
                        <p><text fontsize="90%">On the other hand, quantizing and dequantizing activations can add compute overhead on every microbatch processed. Our implementation circumvents that overhead by performing quantization asynchronously on the CPU. However, this is not required, as blockwise (de)quantization takes less than 1% of total computation time: see Appendix </text><ref fontsize="90%" labelref="LABEL:appendix:time_to_solution"/><text fontsize="90%"> for details.</text></p>
                      </para>
                    </paragraph>
                    <paragraph inlist="toc" xml:id="A9.SS1.SSS0.Px3">
                      <title fontsize="90%">Bottleneck layers:</title>
                      <para xml:id="A9.SS1.SSS0.Px3.p1">
                        <p><text fontsize="90%">We experiment with simple bottleneck layers that work by compressing the output features of the MLP by linear projection:</text></p>
                        <equationgroup class="ltx_eqn_gather" xml:id="A10.EGx1">
                          <equation xml:id="A9.Ex1">
                            <Math mode="display" tex="\displaystyle\text{Bottleneck}(\mathbf{x},\mathbf{w}_{1},\mathbf{w}_{2},%&#10;\mathbf{w}_{c},\mathbf{w}_{d})=" text="[Bottleneck] * vector@(x, w _ 1, w _ 2, w _ c, w _ d) = absent" xml:id="A9.Ex1.m1">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                    <XMText><text fontsize="90%">Bottleneck</text></XMText>
                                    <XMDual>
                                      <XMApp>
                                        <XMTok meaning="vector"/>
                                        <XMRef idref="A9.Ex1.m1.1"/>
                                        <XMRef idref="A9.Ex1.m1.2"/>
                                        <XMRef idref="A9.Ex1.m1.3"/>
                                        <XMRef idref="A9.Ex1.m1.4"/>
                                        <XMRef idref="A9.Ex1.m1.5"/>
                                      </XMApp>
                                      <XMWrap>
                                        <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                        <XMTok font="bold" fontsize="90%" role="UNKNOWN" xml:id="A9.Ex1.m1.1">x</XMTok>
                                        <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                        <XMApp xml:id="A9.Ex1.m1.2">
                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                          <XMTok fontsize="63%" meaning="1" role="NUMBER">1</XMTok>
                                        </XMApp>
                                        <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                        <XMApp xml:id="A9.Ex1.m1.3">
                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                          <XMTok fontsize="63%" meaning="2" role="NUMBER">2</XMTok>
                                        </XMApp>
                                        <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                        <XMApp xml:id="A9.Ex1.m1.4">
                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                          <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                                        </XMApp>
                                        <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                        <XMApp xml:id="A9.Ex1.m1.5">
                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                          <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                                        </XMApp>
                                        <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                      </XMWrap>
                                    </XMDual>
                                  </XMApp>
                                  <XMTok meaning="absent"/>
                                </XMApp>
                              </XMath>
                            </Math>
                          </equation>
                          <equation xml:id="A9.Ex2">
                            <Math mode="display" tex="\displaystyle=\text{LayerNorm}(\text{LayerNorm}(\text{MLP}(\mathbf{x},\mathbf{%&#10;w}_{1},\mathbf{w}_{2}))\mathbf{w}_{c})\mathbf{w_{d}}," text="absent = [LayerNorm] * [LayerNorm] * [MLP] * vector@(x, w _ 1, w _ 2) * w _ c * w _ d" xml:id="A9.Ex2.m1">
                              <XMath>
                                <XMDual>
                                  <XMRef idref="A9.Ex2.m1.2"/>
                                  <XMWrap>
                                    <XMApp xml:id="A9.Ex2.m1.2">
                                      <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                      <XMTok meaning="absent"/>
                                      <XMApp>
                                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                        <XMText><text fontsize="90%">LayerNorm</text></XMText>
                                        <XMDual>
                                          <XMRef idref="A9.Ex2.m1.2.1"/>
                                          <XMWrap>
                                            <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                            <XMApp xml:id="A9.Ex2.m1.2.1">
                                              <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                              <XMText><text fontsize="90%">LayerNorm</text></XMText>
                                              <XMDual>
                                                <XMRef idref="A9.Ex2.m1.2.1.1"/>
                                                <XMWrap>
                                                  <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                                  <XMApp xml:id="A9.Ex2.m1.2.1.1">
                                                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                                    <XMText><text fontsize="90%">MLP</text></XMText>
                                                    <XMDual>
                                                      <XMApp>
                                                        <XMTok meaning="vector"/>
                                                        <XMRef idref="A9.Ex2.m1.1"/>
                                                        <XMRef idref="A9.Ex2.m1.2.1.1.1"/>
                                                        <XMRef idref="A9.Ex2.m1.2.1.1.2"/>
                                                      </XMApp>
                                                      <XMWrap>
                                                        <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                                        <XMTok font="bold" fontsize="90%" role="UNKNOWN" xml:id="A9.Ex2.m1.1">x</XMTok>
                                                        <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                                        <XMApp xml:id="A9.Ex2.m1.2.1.1.1">
                                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                                          <XMTok fontsize="63%" meaning="1" role="NUMBER">1</XMTok>
                                                        </XMApp>
                                                        <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                                        <XMApp xml:id="A9.Ex2.m1.2.1.1.2">
                                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                                          <XMTok fontsize="63%" meaning="2" role="NUMBER">2</XMTok>
                                                        </XMApp>
                                                        <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                                      </XMWrap>
                                                    </XMDual>
                                                  </XMApp>
                                                  <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                                </XMWrap>
                                              </XMDual>
                                              <XMApp>
                                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                                <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                                              </XMApp>
                                            </XMApp>
                                            <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                          </XMWrap>
                                        </XMDual>
                                        <XMApp>
                                          <XMTok role="SUBSCRIPTOP" scriptpos="post2"/>
                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                          <XMTok font="bold" fontsize="63%" role="UNKNOWN">d</XMTok>
                                        </XMApp>
                                      </XMApp>
                                    </XMApp>
                                    <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                  </XMWrap>
                                </XMDual>
                              </XMath>
                            </Math>
                          </equation>
                        </equationgroup>
                      </para>
                      <para xml:id="A9.SS1.SSS0.Px3.p2">
                        <p><text fontsize="90%">where </text><Math mode="inline" tex="\mathbf{w}_{c}\in\mathbb{R}^{m\times c}" text="w _ c element-of R ^ (m * c)" xml:id="A9.SS1.SSS0.Px3.p2.m1">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMApp>
                                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                                </XMApp>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="blackboard" fontsize="90%" role="UNKNOWN">R</XMTok>
                                  <XMApp>
                                    <XMTok fontsize="63%" meaning="times" role="MULOP">×</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                                  </XMApp>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">, </text><Math mode="inline" tex="\mathbf{w}_{d}\in\mathbb{R}^{c\times m}" text="w _ d element-of R ^ (c * m)" xml:id="A9.SS1.SSS0.Px3.p2.m2">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMApp>
                                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                                </XMApp>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="blackboard" fontsize="90%" role="UNKNOWN">R</XMTok>
                                  <XMApp>
                                    <XMTok fontsize="63%" meaning="times" role="MULOP">×</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  </XMApp>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%"> are compression and decompression parameters with compression dimension </text><Math mode="inline" tex="c&lt;m" text="c &lt; m" xml:id="A9.SS1.SSS0.Px3.p2.m3">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="less-than" role="RELOP">&lt;</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">c</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">m</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">. We find it critical to use layer normalization </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="ba2016layernorm" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                              <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                            </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> to ensure training without divergence. The parameter matrix </text><Math mode="inline" tex="\mathbf{w}_{c}" text="w _ c" xml:id="A9.SS1.SSS0.Px3.p2.m4">
                            <XMath>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">c</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%"> resides in one stage and its outputs are transferred to the next stage that holds the parameters </text><Math mode="inline" tex="\mathbf{w}_{d}" text="w _ d" xml:id="A9.SS1.SSS0.Px3.p2.m5">
                            <XMath>
                              <XMApp>
                                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">, which requires </text><Math mode="inline" tex="m/c" text="m / c" xml:id="A9.SS1.SSS0.Px3.p2.m6">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="divide" role="MULOP">/</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">m</XMTok>
                                <XMTok font="italic" fontsize="90%" role="UNKNOWN">c</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%"> times less communication compared to the original model. Note that adding a bottleneck only adds two linear layers for the forward pass and decreases the size of MLP activations; thus, its computational overhead is negligible (less than 1% for typical sizes, see Appendix </text><ref fontsize="90%" labelref="LABEL:appendix:time_to_solution"/><text fontsize="90%">).</text></p>
                      </para>
                    </paragraph>
                    <paragraph inlist="toc" xml:id="A9.SS1.SSS0.Px4">
                      <title fontsize="90%">Maxout compression:</title>
                      <para xml:id="A9.SS1.SSS0.Px4.p1">
                        <p><text fontsize="90%">Compared to bottleneck compression, maxout compression works by using the maxout activation function </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="goodfellow2013maxout" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                              <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                            </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> for compression rather than a linear projection. The maxout function of factor </text><Math mode="inline" tex="k" text="k" xml:id="A9.SS1.SSS0.Px4.p1.m1">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">k</XMTok>
                            </XMath>
                          </Math><text fontsize="90%"> takes inputs with a hidden dimension of </text><Math mode="inline" tex="d" text="d" xml:id="A9.SS1.SSS0.Px4.p1.m2">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">d</XMTok>
                            </XMath>
                          </Math><text fontsize="90%"> and reduces this dimension by a factor of </text><Math mode="inline" tex="k" text="k" xml:id="A9.SS1.SSS0.Px4.p1.m3">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">k</XMTok>
                            </XMath>
                          </Math><text fontsize="90%"> by computing the maximum value for each non-overlapping window of </text><Math mode="inline" tex="k" text="k" xml:id="A9.SS1.SSS0.Px4.p1.m4">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">k</XMTok>
                            </XMath>
                          </Math><text fontsize="90%"> features. We use maxout compression as follows:</text></p>
                        <equationgroup class="ltx_eqn_gather" xml:id="A10.EGx2">
                          <equation xml:id="A9.Ex3">
                            <Math mode="display" tex="\displaystyle\text{Maxout}(\mathbf{x},\mathbf{w}_{1},\mathbf{w}_{2},\mathbf{w}%&#10;_{d})=" text="[Maxout] * vector@(x, w _ 1, w _ 2, w _ d) = absent" xml:id="A9.Ex3.m1">
                              <XMath>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="equals" role="RELOP">=</XMTok>
                                  <XMApp>
                                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                    <XMText><text fontsize="90%">Maxout</text></XMText>
                                    <XMDual>
                                      <XMApp>
                                        <XMTok meaning="vector"/>
                                        <XMRef idref="A9.Ex3.m1.1"/>
                                        <XMRef idref="A9.Ex3.m1.2"/>
                                        <XMRef idref="A9.Ex3.m1.3"/>
                                        <XMRef idref="A9.Ex3.m1.4"/>
                                      </XMApp>
                                      <XMWrap>
                                        <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                        <XMTok font="bold" fontsize="90%" role="UNKNOWN" xml:id="A9.Ex3.m1.1">x</XMTok>
                                        <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                        <XMApp xml:id="A9.Ex3.m1.2">
                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                          <XMTok fontsize="63%" meaning="1" role="NUMBER">1</XMTok>
                                        </XMApp>
                                        <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                        <XMApp xml:id="A9.Ex3.m1.3">
                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                          <XMTok fontsize="63%" meaning="2" role="NUMBER">2</XMTok>
                                        </XMApp>
                                        <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                        <XMApp xml:id="A9.Ex3.m1.4">
                                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                          <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                          <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                                        </XMApp>
                                        <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                      </XMWrap>
                                    </XMDual>
                                  </XMApp>
                                  <XMTok meaning="absent"/>
                                </XMApp>
                              </XMath>
                            </Math>
                          </equation>
                          <equation xml:id="A9.Ex4">
                            <Math mode="display" tex="\displaystyle\text{LayerNorm}(\text{maxout}_{k}(\text{LayerNorm}(\text{MLP}(%&#10;\mathbf{x},\mathbf{w}_{1},\mathbf{w}_{2}))))\mathbf{w_{d}}," text="[LayerNorm] * [maxout] _ k * [LayerNorm] * [MLP] * vector@(x, w _ 1, w _ 2) * w _ d" xml:id="A9.Ex4.m1">
                              <XMath>
                                <XMDual>
                                  <XMRef idref="A9.Ex4.m1.2"/>
                                  <XMWrap>
                                    <XMApp xml:id="A9.Ex4.m1.2">
                                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                      <XMText><text fontsize="90%">LayerNorm</text></XMText>
                                      <XMDual>
                                        <XMRef idref="A9.Ex4.m1.2.1"/>
                                        <XMWrap>
                                          <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                          <XMApp xml:id="A9.Ex4.m1.2.1">
                                            <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                            <XMApp>
                                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                              <XMText><text fontsize="90%">maxout</text></XMText>
                                              <XMTok font="italic" fontsize="63%" role="UNKNOWN">k</XMTok>
                                            </XMApp>
                                            <XMDual>
                                              <XMRef idref="A9.Ex4.m1.2.1.1"/>
                                              <XMWrap>
                                                <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                                <XMApp xml:id="A9.Ex4.m1.2.1.1">
                                                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                                  <XMText><text fontsize="90%">LayerNorm</text></XMText>
                                                  <XMDual>
                                                    <XMRef idref="A9.Ex4.m1.2.1.1.1"/>
                                                    <XMWrap>
                                                      <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                                      <XMApp xml:id="A9.Ex4.m1.2.1.1.1">
                                                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                                        <XMText><text fontsize="90%">MLP</text></XMText>
                                                        <XMDual>
                                                          <XMApp>
                                                            <XMTok meaning="vector"/>
                                                            <XMRef idref="A9.Ex4.m1.1"/>
                                                            <XMRef idref="A9.Ex4.m1.2.1.1.1.1"/>
                                                            <XMRef idref="A9.Ex4.m1.2.1.1.1.2"/>
                                                          </XMApp>
                                                          <XMWrap>
                                                            <XMTok fontsize="90%" role="OPEN" stretchy="false">(</XMTok>
                                                            <XMTok font="bold" fontsize="90%" role="UNKNOWN" xml:id="A9.Ex4.m1.1">x</XMTok>
                                                            <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                                            <XMApp xml:id="A9.Ex4.m1.2.1.1.1.1">
                                                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                                              <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                                              <XMTok fontsize="63%" meaning="1" role="NUMBER">1</XMTok>
                                                            </XMApp>
                                                            <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                                            <XMApp xml:id="A9.Ex4.m1.2.1.1.1.2">
                                                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                                              <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                                              <XMTok fontsize="63%" meaning="2" role="NUMBER">2</XMTok>
                                                            </XMApp>
                                                            <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                                          </XMWrap>
                                                        </XMDual>
                                                      </XMApp>
                                                      <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                                    </XMWrap>
                                                  </XMDual>
                                                </XMApp>
                                                <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                              </XMWrap>
                                            </XMDual>
                                          </XMApp>
                                          <XMTok fontsize="90%" role="CLOSE" stretchy="false">)</XMTok>
                                        </XMWrap>
                                      </XMDual>
                                      <XMApp>
                                        <XMTok role="SUBSCRIPTOP" scriptpos="post2"/>
                                        <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                        <XMTok font="bold" fontsize="63%" role="UNKNOWN">d</XMTok>
                                      </XMApp>
                                    </XMApp>
                                    <XMTok fontsize="90%" role="PUNCT">,</XMTok>
                                  </XMWrap>
                                </XMDual>
                              </XMath>
                            </Math>
                          </equation>
                        </equationgroup>
                      </para>
                      <para xml:id="A9.SS1.SSS0.Px4.p2">
                        <p><text fontsize="90%">where the output is reduced by a factor of </text><Math mode="inline" tex="k" text="k" xml:id="A9.SS1.SSS0.Px4.p2.m1">
                            <XMath>
                              <XMTok font="italic" fontsize="90%" role="UNKNOWN">k</XMTok>
                            </XMath>
                          </Math><text fontsize="90%"> through the maxout function in the previous stage, and then sent to the next stage which holds the decompression matrix </text><Math mode="inline" tex="\mathbf{w}_{d}{\in}\mathbb{R}^{m/k\times m}" text="w _ d element-of R ^ ((m / k) * m)" xml:id="A9.SS1.SSS0.Px4.p2.m2">
                            <XMath>
                              <XMApp>
                                <XMTok fontsize="90%" meaning="element-of" name="in" role="RELOP">∈</XMTok>
                                <XMApp>
                                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="bold" fontsize="90%" role="UNKNOWN">w</XMTok>
                                  <XMTok font="italic" fontsize="63%" role="UNKNOWN">d</XMTok>
                                </XMApp>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMTok font="blackboard" fontsize="90%" role="UNKNOWN">R</XMTok>
                                  <XMApp>
                                    <XMTok fontsize="63%" meaning="times" role="MULOP">×</XMTok>
                                    <XMApp>
                                      <XMTok fontsize="63%" meaning="divide" role="MULOP">/</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                      <XMTok font="italic" fontsize="63%" role="UNKNOWN">k</XMTok>
                                    </XMApp>
                                    <XMTok font="italic" fontsize="63%" role="UNKNOWN">m</XMTok>
                                  </XMApp>
                                </XMApp>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">.</text></p>
                      </para>
                      <table class="ltx_figure_panel" inlist="lot" labels="LABEL:tab:steps_to_22" placement="h!" xml:id="A9.T7">
                        <tags>
                          <tag><text fontsize="90%">Table 7</text></tag>
                          <tag role="autoref">Table 7</tag>
                          <tag role="refnum">7</tag>
                          <tag role="typerefnum">Table 7</tag>
                        </tags>
                        <toccaption><tag close=" "><text fontsize="90%">7</text></tag><text fontsize="90%">Performance of compression methods for a Transformer language model with adaptive inputs on WikiText-103. The asterisk denotes that the difference is not statistically significant.</text></toccaption>
                        <caption fontsize="90%"><tag close=": ">Table 7</tag>Performance of compression methods for a Transformer language model with adaptive inputs on WikiText-103. The asterisk denotes that the difference is not statistically significant.</caption>
                        <tabular class="ltx_centering ltx_figure_panel" colsep="8.0pt" vattach="middle">
                          <tr>
                            <td align="left" border="tt"><ERROR class="undefined">\multirowcell</ERROR><text fontsize="90%">2[-0.5ex][l]Method</text></td>
                            <td align="right" border="tt" class="ltx_nopad_r"><ERROR class="undefined">\multirowcell</ERROR><text fontsize="90%">2[-0.5ex]Ppl after</text></td>
                            <td border="tt"/>
                            <td border="tt"/>
                            <td border="tt"/>
                            <td border="tt"/>
                          </tr>
                          <tr>
                            <td align="left"><text fontsize="90%">286K steps</text></td>
                            <td align="right" class="ltx_nopad_r"><ERROR class="undefined">\multirowcell</ERROR><text fontsize="90%">2[-0.5ex]Steps to</text></td>
                            <td/>
                            <td/>
                            <td/>
                            <td/>
                          </tr>
                          <tr>
                            <td align="left"><text fontsize="90%">ppl 22</text></td>
                            <td align="right" class="ltx_nopad_r"><ERROR class="undefined">\multirowcell</ERROR><text fontsize="90%">2[-0.5ex]Data</text></td>
                            <td/>
                            <td/>
                            <td/>
                            <td/>
                          </tr>
                          <tr>
                            <td align="left"><text fontsize="90%">transfer</text></td>
                            <td align="center" colspan="2"><text fontsize="90%">Extra compute</text></td>
                            <td/>
                            <td/>
                            <td/>
                          </tr>
                          <tr>
                            <td/>
                            <td/>
                            <td/>
                            <td/>
                            <td align="center" border="t"><text fontsize="90%">Absolute</text></td>
                            <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">Relative</text></td>
                          </tr>
                          <tr>
                            <td align="left" border="t"><text fontsize="90%">No compression</text></td>
                            <td align="center" border="t"><text fontsize="90%">21.02</text></td>
                            <td align="center" border="t"><text fontsize="90%">1x</text></td>
                            <td align="center" border="t"><text fontsize="90%">1x</text></td>
                            <td align="center" border="t"><text fontsize="90%">0</text></td>
                            <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">None</text></td>
                          </tr>
                          <tr>
                            <td align="left"><text fontsize="90%">8-bit compression</text></td>
                            <td align="center"><text fontsize="90%">21.13</text></td>
                            <td align="center"><Math mode="inline" tex="\text{0.97x}^{*}" text="[0.97x] ^ *" xml:id="A9.SS1.SSS0.Px4.m1">
                                <XMath>
                                  <XMApp>
                                    <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                    <XMText><text fontsize="90%">0.97x</text></XMText>
                                    <XMTok fontsize="63%" meaning="times" role="MULOP">∗</XMTok>
                                  </XMApp>
                                </XMath>
                              </Math></td>
                            <td align="center"><text fontsize="90%">0.5x</text></td>
                            <td align="center"><text fontsize="90%">1.2ms</text></td>
                            <td align="center" class="ltx_nopad_r"><text fontsize="90%">None </text><text fontsize="50%">(overlapped)</text></td>
                          </tr>
                          <tr>
                            <td align="left"><text fontsize="90%">Bottleneck</text></td>
                            <td align="center"><text fontsize="90%">21.76</text></td>
                            <td align="center"><text fontsize="90%">1.26x</text></td>
                            <td align="center"><text fontsize="90%">0.5x</text></td>
                            <td align="center"><text fontsize="90%">1.96ms</text></td>
                            <td align="center" class="ltx_nopad_r"><Math mode="inline" tex="\leq 1\%" text="absent &lt;= 1percent" xml:id="A9.SS1.SSS0.Px4.m2">
                                <XMath>
                                  <XMApp>
                                    <XMTok fontsize="90%" meaning="less-than-or-equals" name="leq" role="RELOP">≤</XMTok>
                                    <XMTok meaning="absent"/>
                                    <XMApp>
                                      <XMTok fontsize="90%" meaning="percent" role="POSTFIX">%</XMTok>
                                      <XMTok fontsize="90%" meaning="1" role="NUMBER">1</XMTok>
                                    </XMApp>
                                  </XMApp>
                                </XMath>
                              </Math></td>
                          </tr>
                          <tr>
                            <td align="left" border="bb"><text fontsize="90%">Maxout</text></td>
                            <td align="center" border="bb"><text fontsize="90%">21.83</text></td>
                            <td align="center" border="bb"><text fontsize="90%">1.28x</text></td>
                            <td align="center" border="bb"><text fontsize="90%">0.5x</text></td>
                            <td align="center" border="bb"><text fontsize="90%">2.04ms</text></td>
                            <td align="center" border="bb" class="ltx_nopad_r"><Math mode="inline" tex="\leq 1\%" text="absent &lt;= 1percent" xml:id="A9.SS1.SSS0.Px4.m3">
                                <XMath>
                                  <XMApp>
                                    <XMTok fontsize="90%" meaning="less-than-or-equals" name="leq" role="RELOP">≤</XMTok>
                                    <XMTok meaning="absent"/>
                                    <XMApp>
                                      <XMTok fontsize="90%" meaning="percent" role="POSTFIX">%</XMTok>
                                      <XMTok fontsize="90%" meaning="1" role="NUMBER">1</XMTok>
                                    </XMApp>
                                  </XMApp>
                                </XMath>
                              </Math></td>
                          </tr>
                        </tabular>
                      </table>
                    </paragraph>
                  </subsection>
                  <subsection inlist="toc" labels="LABEL:appendix:compression_tradeoff" xml:id="A9.SS2">
                    <tags>
                      <tag>I.2</tag>
                      <tag role="autoref">subsection I.2</tag>
                      <tag role="refnum">I.2</tag>
                      <tag role="typerefnum">§I.2</tag>
                    </tags>
                    <title fontsize="90%"><tag close=" ">I.2</tag>Evaluating the Speed-Quality Tradeoff</title>
                    <para xml:id="A9.SS2.p1">
                      <p><text fontsize="90%">While compression techniques reduce the communication overhead, they might also degrade the perplexity reached in a certain time and the final perplexity after a specific number of steps. To study these tradeoffs, we train a Transformer language model with adaptive inputs </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="baevski2019adaptiveinputs" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> on the WikiText-103 dataset and measure how compression-aware architecture variants affect convergence.</text></p>
                    </para>
                    <para xml:id="A9.SS2.p2">
                      <p><text fontsize="90%">Our setup follows that of </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="baevski2019adaptiveinputs" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> with one difference: we use a sequence length of 2048 instead of 3072 to fit this model into our smaller GPUs.
To measure the time to solution, we look at the number of iterations it takes to converge to the training perplexity of </text><text font="bold" fontsize="90%">22</text><text fontsize="90%">. We evaluate the baseline model and three compression-aware modifications from Section </text><ref fontsize="90%" labelref="LABEL:appendix:compression_detailed"/><text fontsize="90%">: bottleneck, maxout, and block-wise dynamic 8-bit quantization, each with 2 pipeline stages and each a compression factor of 2x.</text></p>
                    </para>
                    <para xml:id="A9.SS2.p3">
                      <p><text fontsize="90%">The results can be seen in Table </text><ref fontsize="90%" labelref="LABEL:tab:steps_to_22"/><text fontsize="90%">. We can see that 8-bit compression does not degrade the time to 22 perplexity and maintains close to the final perplexity of the baseline. The compression-aware bottleneck and maxout architectures perform equal to each other, but degrade final perplexity slightly and increase time to a perplexity of 22 by 26–28%.</text></p>
                    </para>
                    <para xml:id="A9.SS2.p4">
                      <p><text fontsize="90%">Using these results, one can determine which method is optimal for their hardware setup. For instance, training with maxout with 2 pipeline stages needs </text><Math mode="inline" tex="28\%" text="28percent" xml:id="A9.SS2.p4.m1">
                          <XMath>
                            <XMApp>
                              <XMTok fontsize="90%" meaning="percent" role="POSTFIX">%</XMTok>
                              <XMTok fontsize="90%" meaning="28" role="NUMBER">28</XMTok>
                            </XMApp>
                          </XMath>
                        </Math><text fontsize="90%"> more steps, but accelerates the communication phase by </text><Math mode="inline" tex="2" text="2" xml:id="A9.SS2.p4.m2">
                          <XMath>
                            <XMTok fontsize="90%" meaning="2" role="NUMBER">2</XMTok>
                          </XMath>
                        </Math><text fontsize="90%">x. If communication is the limiting factor, using maxout or bottleneck compression layers will offer </text><text font="italic" fontsize="90%">improved</text><text fontsize="90%"> time to perplexity despite the performance degradation. However, the same two techniques would result in slower training in a setup where network bandwidth is unlimited.</text></p>
                    </para>
                    <para xml:id="A9.SS2.p5">
                      <p><text fontsize="90%">In turn, 8-bit quantization reduces communication cost without slowing down per-iteration convergence, making it a “safe bet” for situations where the per-iteration convergence must be preserved.
In our large-scale experiments (Section </text><ref fontsize="90%" labelref="LABEL:sect:experiments_large"/><text fontsize="90%">), we opt to using quantization since it was enough to fully saturate the GPUs.
If network bandwidth is still a limiting factor, one can combine quantization with bottleneck or maxout compression to further reduce communication.</text></p>
                    </para>
                  </subsection>
                  <subsection inlist="toc" labels="LABEL:appendix:compression_extra" xml:id="A9.SS3">
                    <tags>
                      <tag>I.3</tag>
                      <tag role="autoref">subsection I.3</tag>
                      <tag role="refnum">I.3</tag>
                      <tag role="typerefnum">§I.3</tag>
                    </tags>
                    <title fontsize="90%"><tag close=" ">I.3</tag>Additional Experiments</title>
                    <para xml:id="A9.SS3.p1">
                      <p><text fontsize="90%">The additional experiments in this section have two purposes: (1) to evaluate how compression methods vary with the number of stages and (2) to evaluate an additional setting that is closer to modern pretraining setups such as GPT-2/3.</text></p>
                    </para>
                    <para xml:id="A9.SS3.p2">
                      <p><text fontsize="90%">While (1) has further implications for scaling, (2) is helpful to account for confounding factors that might have been overlooked in the main experiments on WikiText-103. The WikiText-103 baseline uses non-BPE vocabulary, a long sequence length, and uses adaptive inputs </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="baevski2019adaptiveinputs" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">, all of which are not frequently used in modern pretrained Transformers since GPT-2 </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="radford2019language" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                            <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                          </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.</text></p>
                    </para>
                    <table inlist="lot" labels="LABEL:tab:compression" xml:id="A9.T8">
                      <tags>
                        <tag><text fontsize="90%">Table 8</text></tag>
                        <tag role="autoref">Table 8</tag>
                        <tag role="refnum">8</tag>
                        <tag role="typerefnum">Table 8</tag>
                      </tags>
                      <toccaption class="ltx_centering"><tag close=" "><text fontsize="90%">8</text></tag><text fontsize="90%">Results of language models trained on the OpenWebText Corpus (OWT). The baseline model has 253M parameters and is trained for 8 GPU-days. We apply bottleneck and maxout compression to our baseline in 2 and 4 stages with a compression factor between 2–4x. PTB=Penn Treebank, 1BW=Billion word corpus.</text></toccaption>
                      <caption class="ltx_centering" fontsize="90%"><tag close=": ">Table 8</tag>Results of language models trained on the OpenWebText Corpus (OWT). The baseline model has 253M parameters and is trained for 8 GPU-days. We apply bottleneck and maxout compression to our baseline in 2 and 4 stages with a compression factor between 2–4x. PTB=Penn Treebank, 1BW=Billion word corpus.</caption>
<!--  %****␣supplementary.tex␣Line␣350␣**** -->                      <tabular class="ltx_centering" colsep="8.0pt" vattach="middle">
                        <tr>
                          <td border="tt"/>
                          <td border="tt"/>
                          <td border="tt"/>
                          <td align="center" border="tt" colspan="6"><text fontsize="90%">Validation perplexity</text></td>
                        </tr>
                        <tr>
                          <td align="left"><text fontsize="90%">Model</text></td>
                          <td align="center"><text fontsize="90%">Stages</text></td>
                          <td align="center"><text fontsize="90%">Compression</text></td>
                          <td align="center" border="t"><text fontsize="90%">OWT</text></td>
                          <td align="center" border="t"><text fontsize="90%">LAMBADA</text></td>
                          <td align="center" border="t"><text fontsize="90%">WikiText-2</text></td>
                          <td align="center" border="t"><text fontsize="90%">WikiText-103</text></td>
                          <td align="center" border="t"><text fontsize="90%">PTB</text></td>
                          <td align="center" border="t"><text fontsize="90%">1BW</text></td>
                        </tr>
                        <tr>
                          <td align="left" border="t"><text fontsize="90%">Baseline</text></td>
                          <td align="center" border="t"><text fontsize="90%">–</text></td>
                          <td align="center" border="t"><text fontsize="90%">–</text></td>
                          <td align="center" border="t"><text fontsize="90%">19.7</text></td>
                          <td align="center" border="t"><text fontsize="90%">86.4</text></td>
                          <td align="center" border="t"><text fontsize="90%">56.2</text></td>
                          <td align="center" border="t"><text fontsize="90%">35.4</text></td>
                          <td align="center" border="t"><text fontsize="90%">133.0</text></td>
                          <td align="center" border="t"><text fontsize="90%">80.9</text></td>
                        </tr>
                        <tr>
                          <td align="left" border="t"><text fontsize="90%">8-bit Quantization</text></td>
                          <td align="center" border="t"><text fontsize="90%">2</text></td>
                          <td align="center" border="t"><text fontsize="90%">2x</text></td>
                          <td align="center" border="t"><text fontsize="90%">19.6</text></td>
                          <td align="center" border="t"><text fontsize="90%">89.1</text></td>
                          <td align="center" border="t"><text font="bold" fontsize="90%">56.0</text></td>
                          <td align="center" border="t"><text font="bold" fontsize="90%">35.0</text></td>
                          <td align="center" border="t"><text fontsize="90%">132.7</text></td>
                          <td align="center" border="t"><text fontsize="90%">79.8</text></td>
                        </tr>
                        <tr>
                          <td align="left"><text fontsize="90%">Bottleneck</text></td>
                          <td align="center"><text fontsize="90%">2</text></td>
                          <td align="center"><text fontsize="90%">2x</text></td>
                          <td align="center"><text font="bold" fontsize="90%">19.5</text></td>
                          <td align="center"><text fontsize="90%">87.7</text></td>
                          <td align="center"><text fontsize="90%">56.5</text></td>
                          <td align="center"><text fontsize="90%">35.2</text></td>
                          <td align="center"><text fontsize="90%">129.8</text></td>
                          <td align="center"><text fontsize="90%">79.2</text></td>
                        </tr>
                        <tr>
                          <td align="left"><text fontsize="90%">Maxout</text></td>
                          <td align="center"><text fontsize="90%">2</text></td>
                          <td align="center"><text fontsize="90%">2x</text></td>
                          <td align="center"><text fontsize="90%">19.6</text></td>
                          <td align="center"><text font="bold" fontsize="90%">85.4</text></td>
                          <td align="center"><text fontsize="90%">56.6</text></td>
                          <td align="center"><text fontsize="90%">35.2</text></td>
                          <td align="center"><text font="bold" fontsize="90%">126.8</text></td>
                          <td align="center"><text font="bold" fontsize="90%">78.8</text></td>
                        </tr>
                        <tr>
                          <td align="left" border="t"><text fontsize="90%">8-bit Quantization</text></td>
                          <td align="center" border="t"><text fontsize="90%">4</text></td>
                          <td align="center" border="t"><text fontsize="90%">2x</text></td>
                          <td align="center" border="t"><text font="bold" fontsize="90%">19.7</text></td>
                          <td align="center" border="t"><text font="bold" fontsize="90%">87.9</text></td>
                          <td align="center" border="t"><text font="bold" fontsize="90%">56.3</text></td>
                          <td align="center" border="t"><text font="bold" fontsize="90%">35.2</text></td>
                          <td align="center" border="t"><text font="bold" fontsize="90%">133.9</text></td>
                          <td align="center" border="t"><text font="bold" fontsize="90%">79.8</text></td>
                        </tr>
                        <tr>
                          <td align="left"><text fontsize="90%">Bottleneck</text></td>
                          <td align="center"><text fontsize="90%">4</text></td>
                          <td align="center"><text fontsize="90%">2x</text></td>
                          <td align="center"><text fontsize="90%">21.7</text></td>
                          <td align="center"><text fontsize="90%">100.0</text></td>
                          <td align="center"><text fontsize="90%">66.4</text></td>
                          <td align="center"><text fontsize="90%">40.0</text></td>
                          <td align="center"><text fontsize="90%">149.6</text></td>
                          <td align="center"><text fontsize="90%">89.5</text></td>
                        </tr>
                        <tr>
                          <td align="left"><text fontsize="90%">Maxout</text></td>
                          <td align="center"><text fontsize="90%">4</text></td>
                          <td align="center"><text fontsize="90%">2x</text></td>
                          <td align="center"><text fontsize="90%">21.4</text></td>
                          <td align="center"><text fontsize="90%">89.9</text></td>
                          <td align="center"><text fontsize="90%">63.9</text></td>
                          <td align="center"><text fontsize="90%">39.5</text></td>
                          <td align="center"><text fontsize="90%">142.1</text></td>
                          <td align="center"><text fontsize="90%">86.2</text></td>
                        </tr>
                        <tr>
                          <td align="left" border="t"><text fontsize="90%">Bottleneck</text></td>
                          <td align="center" border="t"><text fontsize="90%">2</text></td>
                          <td align="center" border="t"><text fontsize="90%">4x</text></td>
                          <td align="center" border="t"><text fontsize="90%">21.6</text></td>
                          <td align="center" border="t"><text fontsize="90%">99.8</text></td>
                          <td align="center" border="t"><text fontsize="90%">64.8</text></td>
                          <td align="center" border="t"><text fontsize="90%">39.6</text></td>
                          <td align="center" border="t"><text fontsize="90%">145.6</text></td>
                          <td align="center" border="t"><text fontsize="90%">88.3</text></td>
                        </tr>
                        <tr>
                          <td align="left"><text fontsize="90%">Maxout</text></td>
                          <td align="center"><text fontsize="90%">2</text></td>
                          <td align="center"><text fontsize="90%">4x</text></td>
                          <td align="center"><text font="bold" fontsize="90%">20.5</text></td>
                          <td align="center"><text font="bold" fontsize="90%">89.6</text></td>
                          <td align="center"><text font="bold" fontsize="90%">60.0</text></td>
                          <td align="center"><text font="bold" fontsize="90%">37.1</text></td>
                          <td align="center"><text font="bold" fontsize="90%">141.7</text></td>
                          <td align="center"><text font="bold" fontsize="90%">83.5</text></td>
                        </tr>
                        <tr>
                          <td align="left" border="t"><text fontsize="90%">Bottleneck</text></td>
                          <td align="center" border="t"><text fontsize="90%">4</text></td>
                          <td align="center" border="t"><text fontsize="90%">4x</text></td>
                          <td align="center" border="t"><text fontsize="90%">28.9</text></td>
                          <td align="center" border="t"><text fontsize="90%">141.6</text></td>
                          <td align="center" border="t"><text fontsize="90%">100.2</text></td>
                          <td align="center" border="t"><text fontsize="90%">58.1</text></td>
                          <td align="center" border="t"><text fontsize="90%">235.5</text></td>
                          <td align="center" border="t"><text fontsize="90%">118.3</text></td>
                        </tr>
                        <tr>
                          <td align="left" border="bb"><text fontsize="90%">Maxout</text></td>
                          <td align="center" border="bb"><text fontsize="90%">4</text></td>
                          <td align="center" border="bb"><text fontsize="90%">4x</text></td>
                          <td align="center" border="bb"><text font="bold" fontsize="90%">21.3</text></td>
                          <td align="center" border="bb"><text font="bold" fontsize="90%">93.5</text></td>
                          <td align="center" border="bb"><text font="bold" fontsize="90%">63.6</text></td>
                          <td align="center" border="bb"><text font="bold" fontsize="90%">39.2</text></td>
                          <td align="center" border="bb"><text font="bold" fontsize="90%">147.7</text></td>
                          <td align="center" border="bb"><text font="bold" fontsize="90%">89.1</text></td>
                        </tr>
                      </tabular>
                    </table>
                    <paragraph inlist="toc" xml:id="A9.SS3.SSS0.Px1">
                      <title fontsize="90%">Experimental setup:</title>
                      <para xml:id="A9.SS3.SSS0.Px1.p1">
                        <p><text fontsize="90%">As a baseline, we train a Transformer language model </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="transformer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                              <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                            </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> on the OpenWebText corpus </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="gokaslan2019openwebtext" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                              <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                            </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. We use the following hyperparameters: sequence size 512, 16 layers with model dimension 1024, and hidden dimension 4096 for a total of 253M parameters. We use byte pair encoding </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="sennrich-etal-2016-neural,radford2019language" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                              <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                            </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> with a vocabulary size of 50264 symbols. We do not use dropout or other regularization, since our models underfit. We run these experiments in Fairseq </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="Ott2019fairseqAF" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                              <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                            </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">.</text></p>
                      </para>
                      <para xml:id="A9.SS3.SSS0.Px1.p2">
                        <p><text fontsize="90%">We test bottleneck and maxout compression for a compression factor of 50% and 75% compared to the original size over two and four stages. We look at how using these compression-aware architectures affects the performance compared to the compression that they achieve.</text></p>
                      </para>
                    </paragraph>
                    <paragraph inlist="toc" xml:id="A9.SS3.SSS0.Px2">
                      <title fontsize="90%">Results:</title>
                      <para xml:id="A9.SS3.SSS0.Px2.p1">
                        <p><text fontsize="90%">The results of our compression-aware architectures are shown in Table </text><ref fontsize="90%" labelref="LABEL:tab:compression"/><text fontsize="90%">. We can see that while the bottleneck architecture is competitive with maxout for a compression factor of 2x with two stages, maxout has better perplexities if more stages or a higher compression ratio is used. The out-of-distribution perplexities vary consistently with the in-distribution perplexity, which suggests compression-aware architectures do not degrade the out-of-distribution performance more than the in-distribution performance. As such, the maxout compression is an effective technique to reduce the bandwidth requirements of pipeline parallel training further.</text></p>
                      </para>
                      <para xml:id="A9.SS3.SSS0.Px2.p2">
                        <p><text fontsize="90%">While the 8-bit blockwise quantization can only compress the activations by a factor of two (16-bit </text><Math mode="inline" tex="\rightarrow" text="rightarrow" xml:id="A9.SS3.SSS0.Px2.p2.m1">
                            <XMath>
                              <XMTok fontsize="90%" name="rightarrow" role="ARROW">→</XMTok>
                            </XMath>
                          </Math><text fontsize="90%"> 8-bit), it does not affect the quality as much when compared to the baseline. As such, the 8-bit quantization appears to be a reliable default choice to reduce the communication overhead for pipeline parallelism.</text></p>
                      </para>
                      <para xml:id="A9.SS3.SSS0.Px2.p3">
                        <p><text fontsize="90%">When considered together with the square-cube law for distributed training and SWARM parallelism, compression-aware architectures allow for better scaling of large neural networks trained over preemptible low-bandwidth peers. Thus, compression-aware architectures improve the accessibility and affordability of training large models outside HPC environments.</text></p>
                      </para>
                    </paragraph>
                  </subsection>
                </appendix>
                <appendix inlist="toc" labels="LABEL:appendix:time_to_solution" xml:id="A10">
                  <tags>
                    <tag>Appendix J</tag>
                    <tag role="autoref">Appendix J</tag>
                    <tag role="refnum">J</tag>
                    <tag role="typerefnum">Appendix J</tag>
                  </tags>
                  <title fontsize="90%"><tag close=" ">Appendix J</tag>Time To Solution</title>
                  <toctitle><tag close=" "><text fontsize="90%">J</text></tag><text fontsize="90%">Time To Solution</text></toctitle>
                  <para xml:id="A10.p1">
                    <p><text fontsize="90%">In this section, we evaluate the compression-aware techniques proposed in Appendix </text><ref fontsize="90%" labelref="LABEL:appendix:compression_detailed"/><text fontsize="90%"> from a practitioner’s point of view. A natural way to compare these techniques is in terms of “the time to solution”, i.e., the wall-clock time it takes to achieve the desired validation objective.
In practice, this time depends on three main factors: the compression strategy, the distributed training algorithm, and the computational infrastructure.</text></p>
                  </para>
                  <para xml:id="A10.p2">
                    <p><text fontsize="90%">In order to disentangle these factors, we first address the relationship between the training algorithm and the infrastructure.
As we discuss in Section </text><ref fontsize="90%" labelref="LABEL:sect:method_swarm"/><text fontsize="90%"> (and later in Appendix </text><ref fontsize="90%" labelref="LABEL:appendix:equivalence"/><text fontsize="90%">), SWARM parallelism has the same per-iteration behavior as other synchronous methods. Theoretically, the choice of an optimal training system should come down to whichever algorithm has the highest training throughput.</text></p>
                  </para>
                  <para xml:id="A10.p3">
                    <p><text fontsize="90%">To verify this argument in practice, we compare the per-iteration and per-hour performance of SWARM against fully synchronous training. For this experiment, we train the ALBERT model </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="albert" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> on the WikiText-103 dataset </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="wikitext103" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%">. We use the ALBERT-Large architecture with 4 layer groups that correspond to 4 SWARM stages </text><text font="italic" fontsize="90%">without the architecture modifications from Appendix <ref labelref="LABEL:appendix:compression"/></text><text fontsize="90%">. We follow the exact hyperparameters from the original paper: for example, we use the LAMB optimizer </text><cite class="ltx_citemacro_citep"><text fontsize="90%">(</text><bibref bibrefs="lamb" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
                          <bibrefphrase><text fontsize="90%">, </text></bibrefphrase>
                        </bibref><text fontsize="90%">)</text></cite><text fontsize="90%"> with the batch size of 4096 and the sequence length of 512. We train this model in three setups: traditional distributed training with 8 V100 workers, SWARM with 8 preemptible V100 GPUs, and SWARM with 32 preemptible T4 workers.</text></p>
                  </para>
                  <table class="ltx_figure_panel" inlist="lot" labels="LABEL:tab:cost" placement="t" xml:id="A10.T9">
                    <tags>
                      <tag><text fontsize="90%">Table 9</text></tag>
                      <tag role="autoref">Table 9</tag>
                      <tag role="refnum">9</tag>
                      <tag role="typerefnum">Table 9</tag>
                    </tags>
                    <toccaption><tag close=" "><text fontsize="90%">9</text></tag><text fontsize="90%">Training time and costs.</text></toccaption>
                    <caption fontsize="90%"><tag close=": ">Table 9</tag>Training time and costs.</caption>
<!--  %****␣supplementary.tex␣Line␣400␣**** -->                    <tabular class="ltx_centering ltx_figure_panel" colsep="8.0pt" vattach="middle">
                      <tr>
                        <td align="left" border="tt"><ERROR class="undefined">\multirowcell</ERROR><text fontsize="90%">2[-0.5ex][l]Setup</text></td>
                        <td align="center" border="tt"><ERROR class="undefined">\multirowcell</ERROR><text fontsize="90%">2[-0.5ex]Time, hours</text></td>
                        <td align="center" border="tt" colspan="2"><text fontsize="90%">Cost, $</text></td>
                      </tr>
                      <tr>
                        <td/>
                        <td/>
                        <td align="center" border="t"><text fontsize="90%">Hourly</text></td>
                        <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">Total</text></td>
                      </tr>
                      <tr>
                        <td align="left" border="t"><Math mode="inline" tex="8\times V100" text="8 * V * 100" xml:id="A10.m1">
                            <XMath>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="times" role="MULOP">×</XMTok>
                                  <XMTok fontsize="90%" meaning="8" role="NUMBER">8</XMTok>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">V</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" meaning="100" role="NUMBER">100</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">, reliable</text></td>
                        <td align="center" border="t"><text fontsize="90%">175.4</text></td>
                        <td align="center" border="t"><text fontsize="90%">7.834</text></td>
                        <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">1374</text></td>
                      </tr>
                      <tr>
                        <td align="left" border="t"><Math mode="inline" tex="8\times V100" text="8 * V * 100" xml:id="A10.m2">
                            <XMath>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="times" role="MULOP">×</XMTok>
                                  <XMTok fontsize="90%" meaning="8" role="NUMBER">8</XMTok>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">V</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" meaning="100" role="NUMBER">100</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">, preemptible</text></td>
                        <td align="center" border="t"><text fontsize="90%">192.6</text></td>
                        <td align="center" border="t"><text fontsize="90%">5.383</text></td>
                        <td align="center" border="t" class="ltx_nopad_r"><text fontsize="90%">1037</text></td>
                      </tr>
                      <tr>
                        <td align="left" border="bb t"><Math mode="inline" tex="32\times T4" text="32 * T * 4" xml:id="A10.m3">
                            <XMath>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMApp>
                                  <XMTok fontsize="90%" meaning="times" role="MULOP">×</XMTok>
                                  <XMTok fontsize="90%" meaning="32" role="NUMBER">32</XMTok>
                                  <XMTok font="italic" fontsize="90%" role="UNKNOWN">T</XMTok>
                                </XMApp>
                                <XMTok fontsize="90%" meaning="4" role="NUMBER">4</XMTok>
                              </XMApp>
                            </XMath>
                          </Math><text fontsize="90%">, preemptible</text></td>
                        <td align="center" border="bb t"><text fontsize="90%">140.8</text></td>
                        <td align="center" border="bb t"><text fontsize="90%">3.536</text></td>
                        <td align="center" border="bb t" class="ltx_nopad_r"><text fontsize="90%">497.8</text></td>
                      </tr>
                    </tabular>
                  </table>
                  <para xml:id="A10.p4">
                    <p><text fontsize="90%">To quantify the time to solution, we measure the wall time required to achieve the ALBERT objective equal to </text><text font="bold" fontsize="90%">1.5</text><text fontsize="90%">. Additionally, we report the per-hour cost of each experimental setup and the total cost of achieving a loss of 1.5 using public cloud provider pricing estimates in </text><ref fontsize="90%" labelref="LABEL:tab:cost" show="creftype~refnum"/><text fontsize="90%">.</text></p>
                  </para>
                  <para xml:id="A10.p5">
                    <p><text fontsize="90%">Figure </text><ref fontsize="90%" labelref="LABEL:fig:convergence_iterations"/><text fontsize="90%"> demonstrates that SWARM matches the per-iteration learning curves of traditional distributed training (PyTorch DistributedDataParallel) up to the variation comparable to caused by changing the random seed. However, SWARM parallelism can achieve the loss of 1.5 more cost-efficiently and faster by using preemptible instances. In turn, </text><text font="italic" fontsize="90%">when forced to use homogeneous and reliable GPUs</text><text fontsize="90%">, SWARM would have slightly inferior performance compared to conventional algorithms, which was first demonstrated in Section </text><ref fontsize="90%" labelref="LABEL:appendix:training_throughput"/><text fontsize="90%">.</text></p>
                  </para>
                  <figure align="center" class="ltx_figure_panel" inlist="lof" labels="LABEL:fig:convergence_iterations" xml:id="A10.F8">
                    <graphics candidates="resources/albert_learning_curves.pdf" class="ltx_figure_panel" graphic="resources/albert_learning_curves.pdf" options="width=433.62pt,keepaspectratio=true" xml:id="A10.g1"/>
                    <tags>
                      <tag><text fontsize="90%">Figure 8</text></tag>
                      <tag role="autoref">Figure 8</tag>
                      <tag role="refnum">8</tag>
                      <tag role="typerefnum">Figure 8</tag>
                    </tags>
                    <toccaption><tag close=" "><text fontsize="90%">8</text></tag><text fontsize="90%">Convergence curves of ALBERT with SWARM and standard data-parallel training.</text></toccaption>
                    <caption fontsize="90%"><tag close=": ">Figure 8</tag>Convergence curves of ALBERT with SWARM and standard data-parallel training.</caption>
                  </figure>
                </appendix>
              </section>
            </subsection>
          </subsection>
        </subsection>
      </table>
    </subsection>
  </section>
</document>
