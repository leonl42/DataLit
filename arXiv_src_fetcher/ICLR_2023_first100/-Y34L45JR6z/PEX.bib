

@inproceedings{mild_cql,
  title     = {Mildly Conservative Q-Learning for Offline Reinforcement Learning},
  author    = {Jiafei Lyu and Xiaoteng Ma and Xiu Li and Zongqing Lu},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2022}
}



@inproceedings{sql,
  title     = {Reinforcement Learning with Deep Energy-Based Policies},
  author    = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle = {International Conference on Machine Learning},
  year      = {2017}
}





@article{yang21,
  author     = {Chao{-}Han Huck Yang and
                Zhengling Qi and
                Yifan Cui and
                Pin{-}Yu Chen},
  title      = {Pessimistic model selection for offline deep reinforcement learning},
  journal    = {CoRR},
  volume     = {abs/2111.14346},
  year       = {2021}
}



@article{Lu22,
  author    = {Lu, Cong and Ball, Philip J. and Rudner, Tim G. J. and Parker-Holder, Jack and Osborne, Michael A. and Teh, Yee Whye},
  title     = {Challenges and opportunities in offline reinforcement learning from visual observations},
  journal    = {CoRR},
  volume     = {abs/2206.04779},
  year      = {2022}
}



@article{zheng22,
  author = {Zheng, Qinqing and Henaff, Mikael and Amos, Brandon and Grover, Aditya},
  title  = {Semi-supervised offline reinforcement learning with action-free trajectories},
  journal    = {CoRR},
  volume     = {abs/2210.06518},
  year   = {2022}
}





@inproceedings{dqn_demo,
  author    = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and Dulac-Arnold, Gabriel and Agapiou, John and Leibo, Joel Z. and Gruslys, Audrunas},
  title     = {Deep {Q}-learning from demonstrations},
  year      = {2018},
  booktitle = {AAAI Conference on Artificial Intelligence}
}

@inproceedings{sac_bc,
  author    = {Ashvin Nair and
               Bob McGrew and
               Marcin Andrychowicz and
               Wojciech Zaremba and
               Pieter Abbeel},
  title     = {{Overcoming exploration in reinforcement learning with demonstrations}},
  booktitle = {{IEEE} International Conference on Robotics and Automation},
  year      = {2018}
}


@article{use_offline_buffer,
  author     = {Matej Vecer{\'{\i}}k and
                Todd Hester and
                Jonathan Scholz and
                Fumin Wang and
                Olivier Pietquin and
                Bilal Piot and
                Nicolas Heess and
                Thomas Roth{\"{o}}rl and
                Thomas Lampe and
                Martin A. Riedmiller},
  title      = {{Leveraging demonstrations for deep reinforcement learning on robotics
                problems with sparse rewards}},
  journal    = {CoRR},
  volume     = {abs/1707.08817},
  year       = {2017}
}



@inproceedings{agnoistic_system_id_mbrl,
  author    = {Stephane Ross and
               Drew Bagnell},
  title     = {{Agnostic system identification for model-based reinforcement learning}},
  booktitle = {International Conference on Machine Learning},
  year      = {2012}
}


@inproceedings{dagger,
  title     = {{A reduction of imitation learning and structured prediction to no-regret online learning}},
  author    = {Ross, Stephane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle = {International Conference on Artificial Intelligence and Statistics},
  year      = {2011}
}


@inproceedings{offline_mtrl,
  title     = {Conservative Data Sharing for Multi-Task Offline Reinforcement Learning},
  author    = {Tianhe Yu and Aviral Kumar and Yevgen Chebotar and Karol Hausman and Sergey Levine and Chelsea Finn},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2021}
}



@inproceedings{mtrl_knowledge_transfer,
 author = {Xu, Zhiyuan and Wu, Kun and Che, Zhengping and Tang, Jian and Ye, Jieping},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control},
 year = {2020}
}

@inproceedings{
VFS,
title={Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning},
author={Dhruv Shah and Alexander T Toshev and Sergey Levine and Brian Ichter},
booktitle={International Conference on Learning Representations},
year={2022}
}


@inproceedings{say_can,
  title     = {Do As {I} Can, Not As {I} Say: Grounding Language in Robotic Affordances},
  author    = {Brian Ichter and Anthony Brohan and Yevgen Chebotar and Chelsea Finn and Karol Hausman and Alexander Herzog and Daniel Ho and Julian Ibarz and Alex Irpan and Eric Jang and Ryan Julian and Dmitry Kalashnikov and Sergey Levine and Yao Lu and Carolina Parada and Kanishka Rao and Pierre Sermanet and Alexander T Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Mengyuan Yan and Noah Brown and Michael Ahn and Omar Cortes and Nicolas Sievers and Clayton Tan and Sichun Xu and Diego Reyes and Jarek Rettinghouse and Jornell Quiambao and Peter Pastor and Linda Luu and Kuang-Huei Lee and Yuheng Kuang and Sally Jesmonth and Kyle Jeffrey and Rosario Jauregui Ruano and Jasmine Hsu and Keerthana Gopalakrishnan and Byron David and Andy Zeng and Chuyuan Kelly Fu},
  booktitle = {Annual Conference on Robot Learning},
  year      = {2022}
}

@inproceedings{taac,
  title     = {{TAAC}: Temporally Abstract Actor-Critic for Continuous Control},
  author    = {Haonan Yu and Wei Xu and Haichao Zhang},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2021}
}

@inproceedings{MTRL,
  title     = {Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},
  author    = {Tianmin Shu and Caiming Xiong and Richard Socher},
  booktitle = {International Conference on Learning Representations},
  year      = {2018}
}


@inproceedings{policy_distillation,
  author    = {Andrei A. Rusu and Sergio Gomez Colmenarejo and Çaglar Gülçehre and Guillaume Desjardins and James Kirkpatrick and Razvan Pascanu and Volodymyr Mnih and Koray Kavukcuoglu and Raia Hadsell},
  title     = {Policy Distillation},
  year      = {2016},
  booktitle = {International Conference on Learning Representations}
}




@inproceedings{offlne_rl_pretrain,
  title     = {Representation Matters: Offline Pretraining for Sequential Decision Making},
  author    = {Yang, Mengjiao and Nachum, Ofir},
  booktitle = {International Conference on Machine Learning},
  year      = {2021}
}



@inproceedings{pg,
  author               = {Sutton, R. S. and Mcallester, D. and Singh, S. and Mansour, Y.},
  booktitle            = {Advances in Neural Information Processing Systems},
  title                = {Policy gradient methods for reinforcement learning with function approximation},
  year                 = 2000
}





@article{ppo,
  author     = {John Schulman and
                Filip Wolski and
                Prafulla Dhariwal and
                Alec Radford and
                Oleg Klimov},
  title      = {Proximal Policy Optimization Algorithms},
  journal    = {CoRR},
  volume     = {abs/1707.06347},
  year       = {2017}
}



@inproceedings{ddpg,
  author    = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  booktitle = {International Conference on Learning Representations},
  title     = {Continuous control with deep reinforcement learning.},
  year      = 2016
}





@inproceedings{kumar2022should,
  title     = {{Should I} Run Offline Reinforcement Learning or Behavioral Cloning?},
  author    = {Aviral Kumar and Joey Hong and Anikait Singh and Sergey Levine},
  booktitle = {International Conference on Learning Representations},
  year      = {2022}
}


@inproceedings{DT,
  author    = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
  year      = {2021}
}

@inproceedings{TT,
  title     = {Offline Reinforcement Learning as One Big Sequence Modeling Problem},
  author    = {Michael Janner and Qiyang Li and Sergey Levine},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2021}
}



@article{human_level_planning,
  author     = {Pedro A. Tsividis and
                Jo{\~{a}}o Loula and
                Jake Burga and
                Nathan Foss and
                Andres Campero and
                Thomas Pouncy and
                Samuel J. Gershman and
                Joshua B. Tenenbaum},
  title      = {Human-Level Reinforcement Learning through Theory-Based Modeling,
                Exploration, and Planning},
  journal    = {CoRR},
  volume     = {abs/2107.12544},
  year       = {2021},
}


@article{muzero,
  author     = {Julian Schrittwieser and
                Ioannis Antonoglou and
                Thomas Hubert and
                Karen Simonyan and
                Laurent Sifre and
                Simon Schmitt and
                Arthur Guez and
                Edward Lockhart and
                Demis Hassabis and
                Thore Graepel and
                Timothy P. Lillicrap and
                David Silver},
  title      = {Mastering {Atari, Go, Chess and Shogi} by Planning with a Learned Model},
  journal    = {CoRR},
  volume     = {abs/1911.08265},
  year       = {2019}
}


@article{alphago,
  author    = {David Silver and
               Julian Schrittwieser and
               Karen Simonyan and
               Ioannis Antonoglou and
               Aja Huang and
               Arthur Guez and
               Thomas Hubert and
               Lucas Baker and
               Matthew Lai and
               Adrian Bolton and
               Yutian Chen and
               Timothy P. Lillicrap and
               Fan Hui and
               Laurent Sifre and
               George van den Driessche and
               Thore Graepel and
               Demis Hassabis},
  title     = {Mastering the game of {Go} without human knowledge},
  journal   = {Nature},
  volume    = {550},
  number    = {7676},
  pages     = {354--359},
  year      = {2017}
}




@article{tsne,
  author      = {van der Maaten, Laurens and Hinton, Geoffrey E.},
  journal     = {Journal of Machine Learning Research},
  pages       = {2579-2605},
  title       = {Visualizing High-Dimensional Data Using t-{SNE}},
  volume      = 9,
  year        = 2008
}





@inproceedings{BCQ,
  title     = {Off-Policy Deep Reinforcement Learning without Exploration},
  author    = {Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle = {International Conference on Machine Learning},
  year      = {2019}
}

@inproceedings{parrot,
  title     = {Parrot: Data-Driven Behavioral Priors for Reinforcement Learning},
  author    = {Avi Singh and Huihan Liu and Gaoyue Zhou and Albert Yu and Nicholas Rhinehart and Sergey Levine},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}


@inproceedings{imagenet_pretrain,
  title     = {Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning },
  author    = {Zhecheng Yuan and Zhengrong Xue and Bo Yuan and Xueqian Wang and Yi Wu and Yang Gao and Huazhe Xu},
  booktitle = {First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML},
  year      = {2022}
}


@inproceedings{resnet_pretrain,
  title        = {{RRL}: Resnet as representation for Reinforcement Learning},
  author       = {Rutav Shah and Vikash Kumar},
  booktitle    = {International Conference on Machine Learning},
  year         = {2021}
}


@inproceedings{DIAYN,
  title     = {Diversity is All You Need: Learning Skills without a Reward Function},
  author    = {Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
  booktitle = {International Conference on Learning Representations},
  year      = {2019}
}


@inproceedings{TD3,
  title     = {Addressing Function Approximation Error in Actor-Critic Methods},
  author    = {Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle = {International Conference on Machine Learning},
  year      = {2018}
}

@inproceedings{carla,
  title     = {{CARLA}: {An} Open Urban Driving Simulator},
  author    = {Alexey Dosovitskiy and German Ros and Felipe Codevilla and Antonio Lopez and Vladlen Koltun},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
  year      = {2017}
}


@inproceedings{APV,
  title     = {Reinforcement Learning with Action-Free Pre-Training from Videos},
  author    = {Seo, Younggyo and Lee, Kimin and James, Stephen L and Abbeel, Pieter},
  booktitle = {International Conference on Machine Learning},
  year      = {2022}
}



@article{d4rl,
  author     = {Justin Fu and
                Aviral Kumar and
                Ofir Nachum and
                George Tucker and
                Sergey Levine},
  title      = {{D4RL:} Datasets for Deep Data-Driven Reinforcement Learning},
  journal    = {CoRR},
  volume     = {abs/2004.07219},
  year       = {2020}
}


@inproceedings{repr_pre_training,
  author    = {Schwarzer, Max and Rajkumar, Nitarshan and Noukhovitch, Michael and Anand, Ankesh and Charlin, Laurent and Hjelm, R Devon and Bachman, Philip and Courville, Aaron C},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Pretraining Representations for Data-Efficient Reinforcement Learning},
  year      = {2021}
}

@inproceedings{behavior_from_void,
  author    = {Liu, Hao and Abbeel, Pieter},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Behavior From the Void: Unsupervised Active Pre-Training},
  year      = {2021}
}




@inproceedings{bc,
  author    = {Michael Bain and Claude Sammut},
  title     = {A Framework for Behavioural Cloning},
  booktitle = {Machine Intelligence 15},
  year      = {1996}
}

@inproceedings{ALVINN,
  title     = {{ALVINN}: An Autonomous Land Vehicle in a Neural Network},
  author    = {Pomerleau, Dean A.},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {1988}
}



@inproceedings{anti_exploration,
  title        = {Offline Reinforcement Learning as Anti-exploration},
  booktitle      = {AAAI Conference on Artificial Intelligence},
  author       = {Rezaeifar, Shideh and Dadashi, Robert and Vieillard, Nino and Hussenot, Léonard and Bachem, Olivier and Pietquin, Olivier and Geist, Matthieu},
  year         = {2022}
}

@inproceedings{offline_rl_adaptive,
  title     = {Offline {RL} Policies Should Be Trained to be Adaptive},
  author    = {Ghosh, Dibya and Ajay, Anurag and Agrawal, Pulkit and Levine, Sergey},
  booktitle = {International Conference on Machine Learning},
  year      = {2022}
}


@inproceedings{balanced_replay,
  title     = {Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble},
  author    = {Seunghyun Lee and Younggyo Seo and Kimin Lee and Pieter Abbeel and Jinwoo Shin},
  booktitle = {Annual Conference on Robot Learning },
  year      = {2021}
}


@inproceedings{cql,
  author    = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  title     = {Conservative {{Q}-learning} for Offline Reinforcement Learning},
  year      = {2020},
  booktitle = {Advances in Neural Information Processing Systems},
}


@inproceedings{td3bc,
 author = {Fujimoto, Scott and Gu, Shixiang},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {A Minimalist Approach to Offline Reinforcement Learning},
 year = {2021}
}






@inproceedings{data_driven_driving,
  author    = {Rastgoftar, Hossein and Zhang, Bingxin and Atkins, Ella M.},
  booktitle = {Annual American Control Conference},
  title     = {A Data-Driven Approach for Autonomous Motion Planning and Control in Off-Road Driving Scenarios},
  year      = {2018}
}


@article{data_driven_robotics,
  author     = {Serkan Cabi and
                Sergio G{\'{o}}mez Colmenarejo and
                Alexander Novikov and
                Ksenia Konyushkova and
                Scott E. Reed and
                Rae Jeong and
                Konrad Zolna and
                Yusuf Aytar and
                David Budden and
                Mel Vecer{\'{\i}}k and
                Oleg Sushkov and
                David Barker and
                Jonathan Scholz and
                Misha Denil and
                Nando de Freitas and
                Ziyu Wang},
  title      = {A Framework for Data-Driven Robotics},
  journal    = {CoRR},
  volume     = {abs/1909.12200},
  year       = {2019}
}



@article{rl_in_robotics,
  author   = {Jens Kober and J. Andrew Bagnell and Jan Peters},
  title    = {Reinforcement learning in robotics: A survey},
  journal  = {The International Journal of Robotics Research},
  volume   = {32},
  number   = {11},
  pages    = {1238-1274},
  year     = {2013}
}


@inproceedings{mopa,
  title     = {Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments},
  author    = {Yamada, Jun and Lee, Youngwoon and Salhotra, Gautam and Pertsch, Karl and Pflueger, Max and Sukhatme, Gaurav and Lim, Joseph and Englert, Peter},
  booktitle = {Conference on Robot Learning},
  year      = {2021}
}


@inproceedings{wo_offline_eval,
  title     = {Offline {RL} Without Off-Policy Evaluation},
  author    = {David Brandfonbrener and William F Whitney and Rajesh Ranganath and Joan Bruna},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2021}
}

@inproceedings{finetune_nevestop,
  title     = {Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning},
  author    = {Julian, Ryan and Swanson, Benjamin and Sukhatme, Gaurav and Levine, Sergey and Finn, Chelsea and Hausman, Karol},
  booktitle = {Conference on Robot Learning},
  year      = {2021}
}



@inproceedings{finetuning_meta,
  author    = {Mandi, Zhao and Abbeel, Pieter and James, Stephen},
  title     = {On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning},
  booktitle = {arXiv},
  year      = {2022}
}



@inproceedings{weighted_bc,
  title     = {Discriminator-Weighted Offline Imitation Learning from Suboptimal Demonstrations},
  author    = {Xu, Haoran and Zhan, Xianyuan and Yin, Honglei and Qin, Huiling},
  booktitle = {International Conference on Machine Learning},
  year      = {2022}
}



@article{behavior_transfer,
  author     = {V{\'{\i}}ctor Campos and
                Pablo Sprechmann and
                Steven Hansen and
                Andr{\'{e}} Barreto and
                Steven Kapturowski and
                Alex Vitvitskyi and
                Adri{\`{a}} Puigdom{\`{e}}nech Badia and
                Charles Blundell},
  title      = {Beyond Fine-Tuning:
Transferring Behavior in Reinforcement Learning},
  journal    = {CoRR},
  volume     = {abs/2102.13515},
  year       = {2021}
}


@inproceedings{policy_finetuing,
  title     = {Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning},
  author    = {Tengyang Xie and Nan Jiang and Huan Wang and Caiming Xiong and Yu Bai},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2021}
}



@inproceedings{finetune_cv,
  title     = {Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-Tuning},
  author    = {Weifeng Ge and Yizhou Yu},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2017}
}

@inproceedings{imagenet_transfer,
  author    = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
  title     = {Do Better {ImageNet} Models Transfer Better?},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2019}
}





@inproceedings{gpt1,
  title  = {Improving Language Understanding by Generative Pre-Training},
  booktitle = {Tech. Report},
  author = {Alec Radford and Karthik Narasimhan},
  year   = {2018}
}

@article{gpt3,
  author  = {Tom B. Brown and
             Benjamin Mann and
             Nick Ryder and
             Melanie Subbiah and
             Jared Kaplan and
             Prafulla Dhariwal and
             Arvind Neelakantan and
             Pranav Shyam and
             Girish Sastry and
             Amanda Askell and
             Sandhini Agarwal and
             Ariel Herbert{-}Voss and
             Gretchen Krueger and
             Tom Henighan and
             Rewon Child and
             Aditya Ramesh and
             Daniel M. Ziegler and
             Jeffrey Wu and
             Clemens Winter and
             Christopher Hesse and
             Mark Chen and
             Eric Sigler and
             Mateusz Litwin and
             Scott Gray and
             Benjamin Chess and
             Jack Clark and
             Christopher Berner and
             Sam McCandlish and
             Alec Radford and
             Ilya Sutskever and
             Dario Amodei},
  title   = {Language Models are Few-Shot Learners},
  journal = {CoRR},
  volume  = {abs/2005.14165},
  year    = {2020}
}

@article{bert,
  author  = {Jacob Devlin and
             Ming{-}Wei Chang and
             Kenton Lee and
             Kristina Toutanova},
  title   = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
             Understanding},
  journal = {CoRR},
  volume  = {abs/1810.04805},
  year    = {2018}
}


@inproceedings{valueDice,
  title     = {Imitation Learning via Off-Policy Distribution Matching},
  author    = {Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}


@inproceedings{sac,
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author    = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = {International Conference on Machine Learning},
  year      = {2018}
}

@article{behavior_prior,
  author     = {Dhruva Tirumala and
                Alexandre Galashov and
                Hyeonwoo Noh and
                Leonard Hasenclever and
                Razvan Pascanu and
                Jonathan Schwarz and
                Guillaume Desjardins and
                Wojciech Marian Czarnecki and
                Arun Ahuja and
                Yee Whye Teh and
                Nicolas Heess},
  title      = {Behavior Priors for Efficient Reinforcement Learning},
  journal    = {CoRR},
  year       = {2020}
}



@article{mnih2015humanlevel,
  author  = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  journal = {Nature},
  month   = feb,
  number  = 7540,
  pages   = {529--533},
  title   = {Human-level control through deep reinforcement learning},
  volume  = 518,
  year    = 2015
}



@article{offline_rl,
  author    = {Sergey Levine and
               Aviral Kumar and
               George Tucker and
               Justin Fu},
  title     = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives
               on Open Problems},
  journal   = {CoRR},
  volume    = {abs/2005.01643},
  year      = {2020}
}



@InProceedings{UDS,
  title = 	 {How to Leverage Unlabeled Data in Offline Reinforcement Learning},
  author =       {Yu, Tianhe and Kumar, Aviral and Chebotar, Yevgen and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  year = 	 {2022}
}


@inproceedings{DAPG,
    AUTHOR    = {Aravind Rajeswaran AND Vikash Kumar AND Abhishek Gupta AND
                 Giulia Vezzani AND John Schulman AND Emanuel Todorov AND Sergey Levine},
    TITLE     = {Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations},
    BOOKTITLE = {Proceedings of Robotics: Science and Systems},
    YEAR      = {2018},
}

@inproceedings{
iql,
title={Offline Reinforcement Learning with Implicit {{Q}-learning}},
author={Ilya Kostrikov and Ashvin Nair and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2022}
}






@inproceedings{fine_tuning_RL2,
  author = {Mandi, Zhao and Abbeel, Pieter and James, Stephen},
  title = {On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning},
  booktitle = {arXiv},
  year = {2022}
}


@inproceedings{fine_tuning_RL1,
author="Liang, Xiaodan
and Wang, Tairui
and Yang, Luona
and Xing, Eric",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="CIRL: Controllable Imitative Reinforcement Learning for Vision-Based Self-driving",
booktitle="European Conference on Computer Vision",
year="2018"
}



@inproceedings{fine_tuning_RL,
  title = 	 {Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning},
  author =       {Julian, Ryan and Swanson, Benjamin and Sukhatme, Gaurav and Levine, Sergey and Finn, Chelsea and Hausman, Karol},
  booktitle = 	 {Proceedings of the 2020 Conference on Robot Learning},
  pages = 	 {2120--2136},
  year = 	 {2021}
}


@inproceedings{BC_carla,
  title={Exploring the Limitations of Behavior Cloning for Autonomous Driving},
  author={Felipe Codevilla and Eder Santana and Antonio M. L{\'o}pez and Adrien Gaidon},
  booktitle={International Conference on Computer Vision},
  year={2019}
}


@inproceedings{plaid,
title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},
author={Glen Berseth and Cheng Xie and Paul Cernek and Michiel Van de Panne},
booktitle={International Conference on Learning Representations},
year={2018},
}



@inproceedings{jump_start_RL,
  author = {Uchendu, Ikechukwu and Xiao, Ted and Lu, Yao and Zhu, Banghua and Yan, Mengyuan and Simon, Joséphine and Bennice, Matthew and Fu, Chuyuan and Ma, Cong and Jiao, Jiantao and Levine, Sergey and Hausman, Karol},
  title = {Jump-Start Reinforcement Learning},
  booktitle = {arXiv},
  year = {2022}
}




@inproceedings{oneshot_concept,
    title = "Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game",
    author = "Zhang, Haichao  and
      Yu, Haonan  and
      Xu, Wei",
    booktitle = "Annual Meeting of the Association for Computational Linguistics",
    year = "2018"
}


@inproceedings{
progressive_RL,
title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},
author={Glen Berseth and Cheng Xie and Paul Cernek and Michiel Van de Panne},
booktitle={International Conference on Learning Representations},
year={2018}
}


@article{residue_policy_learning,
  author    = {Tom Silver and
               Kelsey R. Allen and
               Josh Tenenbaum and
               Leslie Pack Kaelbling},
  title     = {Residual Policy Learning},
  journal   = {CoRR},
  volume    = {abs/1812.06298},
  year      = {2018}
}


@article{residual_rl,
  author    = {Tobias Johannink and
               Shikhar Bahl and
               Ashvin Nair and
               Jianlan Luo and
               Avinash Kumar and
               Matthias Loskyll and
               Juan Aparicio Ojea and
               Eugen Solowjow and
               Sergey Levine},
  title     = {Residual Reinforcement Learning for Robot Control},
  journal   = {CoRR},
  volume    = {abs/1812.03201},
  year      = {2018}
}

@article{simple_approach,
  author    = {K. R. Zentner and
               Ryan Julian and
               Ujjwal Puri and
               Yulun Zhang and
               Gaurav S. Sukhatme},
  title     = {A Simple Approach to Continual Learning by Transferring Skill Parameters},
  journal   = {CoRR},
  volume    = {abs/2110.10255},
  year      = {2021}
}


@inproceedings{value_function_space,
title={Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning},
author={Dhruv Shah and Alexander T Toshev and Sergey Levine and brian ichter},
booktitle={International Conference on Learning Representations},
year={2022}
}

@article{generalized_policy_update,
author = {André Barreto  and Shaobo Hou  and Diana Borsa  and David Silver  and Doina Precup },
title = {Fast reinforcement learning with generalized policy updates},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {48},
pages = {30079-30087},
year = {2020}
}

@inproceedings{behavior_basis,
title={Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates},
author={Safa Alver and Doina Precup},
booktitle={International Conference on Learning Representations},
year={2022}
}

@article{task_graph,
  author    = {De{-}An Huang and
               Suraj Nair and
               Danfei Xu and
               Yuke Zhu and
               Animesh Garg and
               Li Fei{-}Fei and
               Silvio Savarese and
               Juan Carlos Niebles},
  title     = {Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video
               Demonstration},
  journal   = {CoRR},
  volume    = {abs/1807.03480},
  year      = {2018}
}

@article{primitive_imitation,
  author    = {Corban G. Rivera and
               Katie M. Popek and
               Chace Ashcraft and
               Edward W. Staley and
               Kapil D. Katyal and
               Bart L. Paulhamus},
  title     = {{PICO:} Primitive Imitation for COntrol},
  journal   = {CoRR},
  volume    = {abs/2006.12551},
  year      = {2020}
}

@inproceedings{slot_transformer,
title={Unsupervised Learning of Temporal Abstractions using Slot-based Transformers},
author={Anand Gopalakrishnan and Kazuki Irie and J{\"u}rgen Schmidhuber and Sjoerd van Steenkiste},
booktitle={Deep RL Workshop NeurIPS 2021},
year={2021}
}


@inproceedings{compound_task,
  title = 	 {Learning Compound Tasks without Task-specific Knowledge via Imitation and Self-supervised Learning},
  author =       {Lee, Sang-Hyun and Seo, Seung-Woo},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2020}
}


@inproceedings{skill_trees,
  author    = {George Dimitri Konidaris and
               Scott Kuindersma and
               Andrew G. Barto and
               Roderic A. Grupen},
  title     = {Constructing Skill Trees for Reinforcement Learning Agents from Demonstration
               Trajectories},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2010}
}


@article{recovery_RL,
  author    = {Brijen Thananjeyan and
               Ashwin Balakrishna and
               Suraj Nair and
               Michael Luo and
               Krishnan Srinivasan and
               Minho Hwang and
               Joseph E. Gonzalez and
               Julian Ibarz and
               Chelsea Finn and
               Ken Goldberg},
  title     = {Recovery {RL:} Safe Reinforcement Learning with Learned Recovery Zones},
  journal   = {CoRR},
  volume    = {abs/2010.15920},
  year      = {2020}
}


@article{grounded_demo,
    author = {Scott Niekum and Sarah Osentoski and George Konidaris and Sachin Chitta and Bhaskara Marthi and Andrew G. Barto},
    title ={Learning grounded finite-state representations from unstructured demonstrations},
    journal = {The International Journal of Robotics Research},
    volume = {34},
    number = {2},
    pages = {131-157},
    year = {2015}
}

@inproceedings{TACO,
  author={Kyriacos Shiarlis and Markus Wulfmeier and Sasha Salter and Shimon Whiteson and Ingmar Posner},
  title={TACO: Learning Task Decomposition via Temporal Alignment for Control},
  year={2018},
  booktitle={ICML}
}

@inproceedings{OMPN,
    title={Learning Task Decomposition with Ordered Memory Policy Network},
    author={Yuchen Lu and Yikang Shen and Siyuan Zhou and Aaron Courville and Joshua B. Tenenbaum and Chuang Gan},
    booktitle={International Conference on Learning Representations},
    year={2021}
}





@article{CD_hybrid,
  author    = {Michael Neunert and
               Abbas Abdolmaleki and
               Markus Wulfmeier and
               Thomas Lampe and
               Jost Tobias Springenberg and
               Roland Hafner and
               Francesco Romano and
               Jonas Buchli and
               Nicolas Heess and
               Martin A. Riedmiller},
  title     = {Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics},
  journal   = {CoRR},
  volume    = {abs/2001.00449},
  year      = {2020}
}


@inproceedings{composing_policies,
title={Composing Task-Agnostic Policies with Deep Reinforcement Learning},
author={Ahmed H. Qureshi and Jacob J. Johnson and Yuzhe Qin and Taylor Henderson and Byron Boots and Michael C. Yip},
booktitle={International Conference on Learning Representations},
year={2020}
}

@article{sac_gmm,
  author    = {Iman Nematollahi and
               Erick Rosete{-}Beas and
               Adrian R{\"{o}}fer and
               Tim Welschehold and
               Abhinav Valada and
               Wolfram Burgard},
  title     = {Robot Skill Adaptation via Soft Actor-Critic Gaussian Mixture Models},
  journal   = {CoRR},
  volume    = {abs/2111.13129},
  year      = {2021}
}


@InProceedings{mixture_policy,
  title = 	 {Strength Through Diversity: Robust Behavior Learning via Mixture Policies},
  author =       {Seyde, Tim and Schwarting, Wilko and Gilitschenski, Igor and Wulfmeier, Markus and Rus, Daniela},
  booktitle = 	 { Conference on Robot Learning},
  year = 	 {2022}
}



@article{PoE,
author = {Hinton, Geoffrey E.},
title = {Training Products of Experts by Minimizing Contrastive Divergence},
year = {2002},
volume = {14},
number = {8},
journal = {Neural Comput.},
month = {aug},
pages = {1771–1800},
numpages = {30}
}


@inproceedings{MCP,
  author    = {Xue Bin Peng and
               Michael Chang and
               Grace Zhang and
               Pieter Abbeel and
               Sergey Levine},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {{MCP:} Learning Composable Hierarchical Control with Multiplicative
               Compositional Policies},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019}
}




@inproceedings{compile,
  title={CompILE: Compositional Imitation Learning and Execution},
  author={Kipf, Thomas and Li, Yujia and Dai, Hanjun and Zambaldi, Vinicius and Sanchez-Gonzalez, Alvaro and Grefenstette, Edward and Kohli, Pushmeet and Battaglia, Peter},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}

@article{neural_task_programming,
  author    = {Danfei Xu and
               Suraj Nair and
               Yuke Zhu and
               Julian Gao and
               Animesh Garg and
               Li Fei{-}Fei and
               Silvio Savarese},
  title     = {Neural Task Programming: Learning to Generalize Across Hierarchical
               Tasks},
  journal   = {CoRR},
  volume    = {abs/1710.01813},
  year      = {2017}
}

@article{bottom_up_skill,
  author    = {Yifeng Zhu and
               Peter Stone and
               Yuke Zhu},
  title     = {Bottom-Up Skill Discovery from Unsegmented Demonstrations for Long-Horizon
               Robot Manipulation},
  journal   = {CoRR},
  volume    = {abs/2109.13841},
  year      = {2021}
}

@article{multi-task_skill,
  author    = {Zhanpeng He and
               Matei T. Ciocarlie},
  title     = {Discovering Synergies for Robot Manipulation with Multi-Task Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/2110.01530},
  year      = {2021}
}



@article{AWAC,
  author    = {Ashvin Nair and
               Murtaza Dalal and
               Abhishek Gupta and
               Sergey Levine},
  title     = {{AWAC}: Accelerating Online Reinforcement Learning with Offline Datasets},
  journal   = {CoRR},
  volume    = {abs/2006.09359},
  year      = {2020}
}


@inproceedings{demo_shaping,
author = {Brys, Tim and Harutyunyan, Anna and Suay, Halit Bener and Chernova, Sonia and Taylor, Matthew E. and Now\'{e}, Ann},
title = {Reinforcement Learning from Demonstration through Shaping},
year = {2015},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
series = {IJCAI'15}
}


@article{demo_shaping2,
  author    = {Yuchen Wu and
               Melissa Mozifian and
               Florian Shkurti},
  title     = {Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations
               using Generative Models},
  journal   = {CoRR},
  volume    = {abs/2011.01298},
  year      = {2020}
}



@inproceedings{pathologies_KL,
title={On Pathologies in {KL}-Regularized Reinforcement Learning from Expert Demonstrations},
author={Tim G. J. Rudner and Cong Lu and Michael Osborne and Yarin Gal and Yee Whye Teh},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021}
}


@article{smoothed_policy,
  author    = {Ofir Nachum and
               Mohammad Norouzi and
               George Tucker and
               Dale Schuurmans},
  title     = {Smoothed Action Value Functions for Learning Gaussian Policies},
  journal   = {CoRR},
  volume    = {abs/1803.02348},
  year      = {2018}
}




@article{Distral,
  author    = {Yee Whye Teh and
               Victor Bapst and
               Wojciech Marian Czarnecki and
               John Quan and
               James Kirkpatrick and
               Raia Hadsell and
               Nicolas Heess and
               Razvan Pascanu},
  title     = {Distral: Robust Multitask Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1707.04175},
  year      = {2017}
}


@article{VBD,
  author    = {Xue Bin Peng and
               Angjoo Kanazawa and
               Sam Toyer and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Variational Discriminator Bottleneck: Improving Imitation Learning,
               Inverse RL, and GANs by Constraining Information Flow},
  journal   = {CoRR},
  volume    = {abs/1810.00821},
  year      = {2018}
}






@inproceedings{KL_RL,
title={On Pathologies in {KL}-Regularized Reinforcement Learning from Expert Demonstrations},
author={Tim G. J. Rudner and Cong Lu and Michael Osborne and Yarin Gal and Yee Whye Teh},
booktitle={Advances in Neural Information Processing Systems},
year={2021}
}


@article{munchausen,
  author    = {Nino Vieillard and
               Olivier Pietquin and
               Matthieu Geist},
  title     = {Munchausen Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2007.14430},
  year      = {2020}
}


@inproceedings{
    parameter_space_noise,
    title={Parameter Space Noise for Exploration},
    author={Matthias Plappert and Rein Houthooft and Prafulla Dhariwal and Szymon Sidor and Richard Y. Chen and Xi Chen and Tamim Asfour and Pieter Abbeel and Marcin Andrychowicz},
    booktitle={International Conference on Learning Representations},
    year={2018}
}


@inproceedings{noisy_net,
title={Noisy Networks For Exploration},
author={Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Matteo Hessel and Ian Osband and Alex Graves and Volodymyr Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},
booktitle={International Conference on Learning Representations},
year={2018}
}


@article{feature_prior,
  author    = {Saachi Jain and
               Dimitris Tsipras and
               Aleksander Madry},
  title     = {Combining Diverse Feature Priors},
  journal   = {CoRR},
  volume    = {abs/2110.08220},
  year      = {2021}
}


@inproceedings{RNN_world_model,
 author = {Ha, David and Schmidhuber, J\"{u}rgen},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Recurrent World Models Facilitate Policy Evolution},
 year = {2018}
}


@inproceedings{GPM,
  author    = {Haichao Zhang and
               Wei Xu and
               Haonan Yu},
  title     = {Generative Planning for Temporally Coordinated Exploration in Reinforcement
               Learning},
    booktitle={International Conference on Learning Representations},
  year      = {2022}
}


@article{partial_observability,
  author    = {Stephan Weigand and
               Pascal Klink and
               Jan Peters and
               Joni Pajarinen},
  title     = {Reinforcement Learning using Guided Observability},
  journal   = {CoRR},
  volume    = {abs/2104.10986},
  year      = {2021}
}


# causal confusion: literature
@inproceedings{off_road,
 author = {Muller, Urs and Ben, Jan and Cosatto, Eric and Flepp, Beat and LeCun, Yann},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Off-Road Obstacle Avoidance through End-to-End Learning},
 year = {2005}
}


@inproceedings{LfF,
  title={Learning from Failure: Training Debiased Classifier from Biased Classifier},
  author={Junhyun Nam and Hyuntak Cha and Sungsoo Ahn and Jaeho Lee and Jinwoo Shin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}




@inproceedings{key_frame,
  title = {Keyframe-Focused Visual Imitation Learning},
  author = {Chuan Wen and Jierui Lin and Jianing Qian and Yang Gao 0029 and Dinesh Jayaraman},
  year = {2021},
  booktitle = {International Conference on Machine Learning},
}


@article{functa,
  author    = {Emilien Dupont and
               Hyunjik Kim and
               S. M. Ali Eslami and
               Danilo J. Rezende and
               Dan Rosenbaum},
  title     = {From data to functa: Your data point is a function and you should
               treat it like one},
  journal   = {CoRR},
  volume    = {abs/2201.12204},
  year      = {2022}
}


@article{nn_subspace,
  author    = {Mitchell Wortsman and
               Maxwell Horton and
               Carlos Guestrin and
               Ali Farhadi and
               Mohammad Rastegari},
  title     = {Learning Neural Network Subspaces},
  journal   = {CoRR},
  volume    = {abs/2102.10472},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.10472},
  eprinttype = {arXiv},
  eprint    = {2102.10472},
  timestamp = {Wed, 24 Feb 2021 15:42:45 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-10472.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{nero,
  author    = {Yang Liu and
               Jeremy Bernstein and
               Markus Meister and
               Yisong Yue},
  title     = {Learning by Turning: Neural Architecture Aware Optimisation},
  journal   = {CoRR},
  volume    = {abs/2102.07227},
  year      = {2021}
}



@article{ReduNet,
  author    = {Kwan Ho Ryan Chan and
               Yaodong Yu and
               Chong You and
               Haozhi Qi and
               John Wright and
               Yi Ma},
  title     = {ReduNet: {A} White-box Deep Network from the Principle of Maximizing
               Rate Reduction},
  journal   = {CoRR},
  volume    = {abs/2105.10446},
  year      = {2021}
}

@article{whitening_ssl,
  author    = {Aleksandr Ermolov and
               Aliaksandr Siarohin and
               Enver Sangineto and
               Nicu Sebe},
  title     = {Whitening for Self-Supervised Representation Learning},
  journal   = {CoRR},
  volume    = {abs/2007.06346},
  year      = {2020}
}


@article{barlow_twins,
  author    = {Jure Zbontar and
               Li Jing and
               Ishan Misra and
               Yann LeCun and
               St{\'{e}}phane Deny},
  title     = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  journal   = {CoRR},
  volume    = {abs/2103.03230},
  year      = {2021}
}



@article{shortcut_nlu,
  author    = {Mengnan Du and
               Varun Manjunatha and
               Rajiv Jain and
               Ruchi Deshpande and
               Franck Dernoncourt and
               Jiuxiang Gu and
               Tong Sun and
               Xia Hu},
  title     = {Towards Interpreting and Mitigating Shortcut Learning Behavior of
               {NLU} models},
  journal   = {CoRR},
  volume    = {abs/2103.06922},
  year      = {2021},
}

@article{ESB,
  author    = {Damien Teney and
               Ehsan Abbasnejad and
               Simon Lucey and
               Anton van den Hengel},
  title     = {Evading the Simplicity Bias: Training a Diverse Set of Models Discovers
               Solutions with Superior {OOD} Generalization},
  journal   = {CoRR},
  volume    = {abs/2105.05612},
  year      = {2021},
}


@article{SB,
  author    = {Harshay Shah and
               Kaustav Tamuly and
               Aditi Raghunathan and
               Prateek Jain and
               Praneeth Netrapalli},
  title     = {The Pitfalls of Simplicity Bias in Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2006.07710},
  year      = {2020}
}


@inproceedings{what_shapes_feature,
  title = {What shapes feature representations? Exploring datasets, architectures, and training},
  author = {Katherine L. Hermann and Andrew K. Lampinen},
  year = {2020},
  booktitle = {Advances in Neural Information Processing Systems}
}

@inproceedings{
gradient_starvation,
title={Gradient Starvation: A Learning Proclivity in Neural Networks},
author={Mohammad Pezeshki and S{\'e}kou-Oumar Kaba and Yoshua Bengio and Aaron Courville and Doina Precup and Guillaume Lajoie},
booktitle={Advances in Neural Information Processing Systems},
year={2021}
}



@article{shapely,
  author    = {Marco Ancona and
               Cengiz {\"{O}}ztireli and
               Markus H. Gross},
  title     = {Explaining Deep Neural Networks with a Polynomial Time Algorithm for
               Shapley Values Approximation},
  journal   = {CoRR},
  volume    = {abs/1903.10992},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.10992},
  eprinttype = {arXiv},
  eprint    = {1903.10992},
  timestamp = {Tue, 02 Apr 2019 11:16:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-10992.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{high_confidence_penalty,
  author    = {Gabriel Pereyra and
               George Tucker and
               Jan Chorowski and
               Lukasz Kaiser and
               Geoffrey E. Hinton},
  title     = {Regularizing Neural Networks by Penalizing Confident Output Distributions},
  journal   = {CoRR},
  volume    = {abs/1701.06548},
  year      = {2017}
}


@article{selective_noise_injection,
  author    = {Maximilian Igl and
               Kamil Ciosek and
               Yingzhen Li and
               Sebastian Tschiatschek and
               Cheng Zhang and
               Sam Devlin and
               Katja Hofmann},
  title     = {Generalization in Reinforcement Learning with Selective Noise Injection
               and Information Bottleneck},
  journal   = {CoRR},
  volume    = {abs/1910.12911},
  year      = {2019}
}



@inproceedings{
Brantley2020Disagreement-Regularized,
title={Disagreement-Regularized Imitation Learning},
author={Kiante Brantley and Wen Sun and Mikael Henaff},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgbYyHtwB}
}


@article{plan2explore,
  author    = {Ramanan Sekar and
               Oleh Rybkin and
               Kostas Daniilidis and
               Pieter Abbeel and
               Danijar Hafner and
               Deepak Pathak},
  title     = {Planning to Explore via Self-Supervised World Models},
  journal   = {CoRR},
  volume    = {abs/2005.05960},
  year      = {2020}
}


@article{temporal_noise,
  author    = {Gokul Swamy and
               Sanjiban Choudhury and
               J. Andrew Bagnell and
               Zhiwei Steven Wu},
  title     = {Causal Imitation Learning under Temporally Correlated Noise},
  journal   = {CoRR},
  volume    = {abs/2202.01312},
  year      = {2022}
}


@article{feedback_im,
  author    = {Jonathan C. Spencer and
               Sanjiban Choudhury and
               Arun Venkatraman and
               Brian D. Ziebart and
               J. Andrew Bagnell},
  title     = {Feedback in Imitation Learning: The Three Regimes of Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/2102.02872},
  year      = {2021}
}




@article{obj_reg,
  author    = {Jongjin Park and
               Younggyo Seo and
               Chang Liu and
               Li Zhao and
               Tao Qin and
               Jinwoo Shin and
               Tie{-}Yan Liu},
  title     = {Object-Aware Regularization for Addressing Causal Confusion in Imitation
               Learning},
  journal   = {CoRR},
  volume    = {abs/2110.14118},
  year      = {2021}
}



@article{implicit_affordance,
  author    = {Marin Toromanoff and
               {\'{E}}milie Wirbel and
               Fabien Moutarde},
  title     = {End-to-End Model-Free Reinforcement Learning for Urban Driving using
               Implicit Affordances},
  journal   = {CoRR},
  volume    = {abs/1911.10868},
  year      = {2019}
}


@article{copycat,
  author    = {Chuan Wen and
               Jierui Lin and
               Trevor Darrell and
               Dinesh Jayaraman and
               Yang Gao},
  title     = {Fighting Copycat Agents in Behavioral Cloning from Observation Histories},
  journal   = {CoRR},
  volume    = {abs/2010.14876},
  year      = {2020}
}


@InProceedings{driving_recover,
  title = 	 {Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?},
  author =       {Filos, Angelos and Tigkas, Panagiotis and Mcallister, Rowan and Rhinehart, Nicholas and Levine, Sergey and Gal, Yarin},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2020}
}


@article{sensory_transformer,
  author    = {Yujin Tang and
               David Ha},
  title     = {The Sensory Neuron as a Transformer: Permutation-Invariant Neural
               Networks for Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2109.02869},
  year      = {2021}
}



@article{transfuser,
  author    = {Aditya Prakash and
               Kashyap Chitta and
               Andreas Geiger},
  title     = {Multi-Modal Fusion Transformer for End-to-End Autonomous Driving},
  journal   = {CoRR},
  volume    = {abs/2104.09224},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.09224},
  eprinttype = {arXiv},
  eprint    = {2104.09224},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-09224.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{causal_confusion,
 author = {de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Causal Confusion in Imitation Learning},
 year = {2019}
}



@article{expert_q,
  author    = {Li Meng and
               Anis Yazidi and
               Morten Goodwin and
               Paal Engelstad},
  title     = {Expert {{Q}-learning}: Deep {{Q}-learning} With State Values From Expert Examples},
  journal   = {CoRR},
  volume    = {abs/2106.14642},
  year      = {2021}
}


@article{gap_value_policy,
  author    = {Ofir Nachum and
               Mohammad Norouzi and
               Kelvin Xu and
               Dale Schuurmans},
  title     = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1702.08892},
  year      = {2017}
}


@inproceedings{
implicit_bc,
title={Implicit Behavioral Cloning},
author={Pete Florence and Corey Lynch and Andy Zeng and Oscar A Ramirez and Ayzaan Wahid and Laura Downs and Adrian Wong and Johnny Lee and Igor Mordatch and Jonathan Tompson},
booktitle={5th Annual Conference on Robot Learning },
year={2021}
}



@inproceedings{between_im_irl,
author = {MacGlashan, James and Littman, Michael L.},
title = {Between Imitation and Intention Learning},
year = {2015},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
series = {IJCAI'15}
}




#=============

@inproceedings{ERC,
  title     = {Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification},
  author    = {Benjamin Eysenbach and Sergey Levine and Ruslan Salakhutdinov},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  year      = {2021}
}

@inproceedings{RED,
  title     = {Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation},
  author    = {Wang, Ruohan and Ciliberto, Carlo and Amadori, Pierluigi Vito and Demiris, Yiannis},
  booktitle = {International Conference on Machine Learning},
  year      = {2019}
}


@article{DQfD,
  author     = {Todd Hester and
                Matej Vecer{\'{\i}}k and
                Olivier Pietquin and
                Marc Lanctot and
                Tom Schaul and
                Bilal Piot and
                Andrew Sendonaris and
                Gabriel Dulac{-}Arnold and
                Ian Osband and
                John P. Agapiou and
                Joel Z. Leibo and
                Audrunas Gruslys},
  title      = {Learning from Demonstrations for Real World Reinforcement Learning},
  journal    = {CoRR},
  volume     = {abs/1704.03732},
  year       = {2017}
}

@article{GRI,
  author     = {Rapha{\"{e}}l Chekroun and
                Marin Toromanoff and
                Sascha Hornauer and
                Fabien Moutarde},
  title      = {{GRI:} General Reinforced Imitation and its Application to Vision-Based
                Autonomous Driving},
  journal    = {CoRR},
  volume     = {abs/2111.08575},
  year       = {2021}
}



@inproceedings{CRR,
  author    = {Wang, Ziyu and Novikov, Alexander and Zolna, Konrad and Merel, Josh S and Springenberg, Jost Tobias and Reed, Scott E and Shahriari, Bobak and Siegel, Noah and Gulcehre, Caglar and Heess, Nicolas and de Freitas, Nando},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Critic Regularized Regression},
  year      = {2020}
}


@misc{Cabi2019a,
  author      = {Cabi, Serkan and Colmenarejo, Sergio Gómez and Novikov, Alexander and Konyushkova, Ksenia and Reed, Scott and Jeong, Rae and Żołna, Konrad and Aytar, Yusuf and Budden, David and Vecerik, Mel and Sushkov, Oleg and Barker, David and Scholz, Jonathan and Denil, Misha and de Freitas, Nando and Wang, Ziyu},
  title       = {Scaling data-driven robotics with reward sketching and batch reinforcement learning},
  year        = {2019},
  arxiv       = {https://arxiv.org/abs/1909.12200},
  pdf         = {https://sites.google.com/view/data-driven-robotics/}
}


@misc{offline_reward_relabel,
    author = "Zolna, Konrad and Novikov, Alexander and Konyushkova, Ksenia and Gulcehre, Caglar and Wang, Ziyu and Aytar, Yusuf and Denil, Misha and de Freitas, Nando and Reed, Scott",
    title = "Offline Learning from Demonstrations and Unlabeled Experience",
    year = "2020",
    arxiv = "https://arxiv.org/abs/2011.13885"
}

@misc{wu2020shaping,
  title         = {Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models},
  author        = {Yuchen Wu and Melissa Mozifian and Florian Shkurti},
  year          = {2020},
  eprint        = {2011.01298},
  archiveprefix = {arXiv},
  primaryclass  = {cs.RO}
}


@article{asaf,
  author    = {Paul Barde and
               Julien Roy and
               Wonseok Jeon and
               Joelle Pineau and
               Christopher J. Pal and
               Derek Nowrouzezahrai},
  title     = {Adversarial Soft Advantage Fitting: Imitation Learning without Policy
               Optimization},
  journal   = {CoRR},
  volume    = {abs/2006.13258},
  year      = {2020}
}

# Strictly Batch Imitation Learning by Energy-based Distribution Matching

# joint imitation and RL
@article{im_rl_driving,
  author     = {Haochen Liu and
                Zhiyu Huang and
                Chen Lv},
  title      = {Improved Deep Reinforcement Learning with Expert Demonstrations for
                Urban Autonomous Driving},
  journal    = {CoRR},
  volume     = {abs/2102.09243},
  year       = {2021},
  url        = {https://arxiv.org/abs/2102.09243},
  eprinttype = {arXiv},
  eprint     = {2102.09243},
  timestamp  = {Wed, 24 Feb 2021 15:42:45 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2102-09243.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{reinforced_imitation,
  author     = {Rapha{\"{e}}l Chekroun and
                Marin Toromanoff and
                Sascha Hornauer and
                Fabien Moutarde},
  title      = {{GRI:} General Reinforced Imitation and its Application to Vision-Based
                Autonomous Driving},
  journal    = {CoRR},
  volume     = {abs/2111.08575},
  year       = {2021},
  url        = {https://arxiv.org/abs/2111.08575},
  eprinttype = {arXiv},
  eprint     = {2111.08575},
  timestamp  = {Mon, 22 Nov 2021 16:44:07 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2111-08575.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{reinforced_imitation_eng,
  author     = {Ryoya Ogishima and
                Izumi Karino and
                Yasuo Kuniyoshi},
  title      = {Reinforced Imitation Learning by Free Energy Principle},
  journal    = {CoRR},
  volume     = {abs/2107.11811},
  year       = {2021}
}

@misc{martin2022reward,
  title         = {Reward Relabelling for combined Reinforcement and Imitation Learning on sparse-reward tasks},
  author        = {Jesus Bujalance Martin and Fabien Moutarde},
  year          = {2022},
  eprint        = {2201.03834},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}


@misc{ciosek2021imitation,
  title         = {Imitation Learning by Reinforcement Learning},
  author        = {Kamil Ciosek},
  year          = {2021},
  eprint        = {2108.04763},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}




@article{social_rl,
  author     = {Kamal Ndousse and
                Douglas Eck and
                Sergey Levine and
                Natasha Jaques},
  title      = {Multi-agent Social Reinforcement Learning Improves Generalization},
  journal    = {CoRR},
  volume     = {abs/2010.00581},
  year       = {2020}
}


@inproceedings{teachable_RL,
  title     = {Teachable Reinforcement Learning via Advice Distillation},
  author    = {Olivia Watkins and Abhishek Gupta and Trevor Darrell and Pieter Abbeel and Jacob Andreas},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2021}
}

@article{ranker,
  author     = {Daniel S. Brown and
                Wonjoon Goo and
                Scott Niekum},
  title      = {Ranking-Based Reward Extrapolation without Rankings},
  journal    = {CoRR},
  volume     = {abs/1907.03976},
  year       = {2019}
}


@inproceedings{sqil,
  title     = {{SQIL}: Imitation Learning via Reinforcement Learning with Sparse Rewards},
  author    = {Siddharth Reddy and Anca D. Dragan and Sergey Levine},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@misc{jarrett2021strictly,
  title         = {Strictly Batch Imitation Learning by Energy-based Distribution Matching},
  author        = {Daniel Jarrett and Ioana Bica and Mihaela van der Schaar},
  year          = {2021},
  eprint        = {2006.14154},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@article{ChauffeurNet,
  author     = {Mayank Bansal and
                Alex Krizhevsky and
                Abhijit S. Ogale},
  title      = {ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing
                the Worst},
  journal    = {CoRR},
  volume     = {abs/1812.03079},
  year       = {2018},
  url        = {http://arxiv.org/abs/1812.03079},
  eprinttype = {arXiv},
  eprint     = {1812.03079},
  timestamp  = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1812-03079.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{UrbanDriver,
  author     = {Oliver Scheel and
                Luca Bergamini and
                Maciej Wolczyk and
                Blazej Osinski and
                Peter Ondruska},
  title      = {Urban Driver: Learning to Drive from Real-world Demonstrations Using
                Policy Gradients},
  journal    = {CoRR},
  volume     = {abs/2109.13333},
  year       = {2021}
}

@article{D-REX,
  author     = {Daniel S. Brown and
                Wonjoon Goo and
                Scott Niekum},
  title      = {Ranking-Based Reward Extrapolation without Rankings},
  journal    = {CoRR},
  volume     = {abs/1907.03976},
  year       = {2019},
  url        = {http://arxiv.org/abs/1907.03976},
  eprinttype = {arXiv},
  eprint     = {1907.03976},
  timestamp  = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1907-03976.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}




@article{reward_conditioned_policies,
  author     = {Aviral Kumar and
                Xue Bin Peng and
                Sergey Levine},
  title      = {Reward-Conditioned Policies},
  journal    = {CoRR},
  volume     = {abs/1912.13465},
  year       = {2019}
}




#---------------------
#imitation method

@inproceedings{iqlearn,
  title     = {{IQ}-Learn: Inverse soft-Q Learning for Imitation},
  author    = {Divyansh Garg and Shuvam Chakraborty and Chris Cundy and Jiaming Song and Stefano Ermon},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2021}
}


@inproceedings{learning_by_watching,
  author    = {Zhang, Jimuyang and Ohn-Bar, Eshed},
  title     = {Learning by Watching},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2021}
  }


@inproceedings{fujimoto2021a,
  title     = {A Minimalist Approach to Offline Reinforcement Learning},
  author    = {Scott Fujimoto and Shixiang Gu},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  year      = {2021},
  url       = {https://openreview.net/forum?id=Q32U7dzWXpc}
}



#==== imitation for driving
@article{gail_driving,
  author     = {Gustavo Claudio Karl Couto and
                Eric Aislan Antonelo},
  title      = {Generative Adversarial Imitation Learning for End-to-End Autonomous
                Driving on Urban Environments},
  journal    = {CoRR},
  volume     = {abs/2110.08586},
  year       = {2021}
}


@inproceedings{roundabout,
  author={Ha, Timothy and Lee, Gunmin and Kim, Dohyeong and Oh, Songhwai},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title={Road Graphical Neural Networks for Autonomous Roundabout Driving},
  year={2021}
  }


@article{demo_guided,
  author     = {Karl Pertsch and
                Youngwoon Lee and
                Yue Wu and
                Joseph J. Lim},
  title      = {Demonstration-Guided Reinforcement Learning with Learned Skills},
  journal    = {CoRR},
  volume     = {abs/2107.10253},
  year       = {2021},
  url        = {https://arxiv.org/abs/2107.10253},
  eprinttype = {arXiv},
  eprint     = {2107.10253},
  timestamp  = {Thu, 29 Jul 2021 16:14:15 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2107-10253.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


