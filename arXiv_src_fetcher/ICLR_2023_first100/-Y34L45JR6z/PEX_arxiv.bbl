\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bain \& Sammut(1996)Bain and Sammut]{bc}
Michael Bain and Claude Sammut.
\newblock A framework for behavioural cloning.
\newblock In \emph{Machine Intelligence 15}, 1996.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{CoRR}, abs/2005.14165, 2020.

\bibitem[Cabi et~al.(2019)Cabi, Colmenarejo, Novikov, Konyushkova, Reed, Jeong,
  Zolna, Aytar, Budden, Vecer{\'{\i}}k, Sushkov, Barker, Scholz, Denil,
  de~Freitas, and Wang]{data_driven_robotics}
Serkan Cabi, Sergio~G{\'{o}}mez Colmenarejo, Alexander Novikov, Ksenia
  Konyushkova, Scott~E. Reed, Rae Jeong, Konrad Zolna, Yusuf Aytar, David
  Budden, Mel Vecer{\'{\i}}k, Oleg Sushkov, David Barker, Jonathan Scholz,
  Misha Denil, Nando de~Freitas, and Ziyu Wang.
\newblock A framework for data-driven robotics.
\newblock \emph{CoRR}, abs/1909.12200, 2019.

\bibitem[Campos et~al.(2021)Campos, Sprechmann, Hansen, Barreto, Kapturowski,
  Vitvitskyi, Badia, and Blundell]{behavior_transfer}
V{\'{\i}}ctor Campos, Pablo Sprechmann, Steven Hansen, Andr{\'{e}} Barreto,
  Steven Kapturowski, Alex Vitvitskyi, Adri{\`{a}}~Puigdom{\`{e}}nech Badia,
  and Charles Blundell.
\newblock Beyond fine-tuning: Transferring behavior in reinforcement learning.
\newblock \emph{CoRR}, abs/2102.13515, 2021.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{DT}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Codevilla et~al.(2019)Codevilla, Santana, L{\'o}pez, and
  Gaidon]{BC_carla}
Felipe Codevilla, Eder Santana, Antonio~M. L{\'o}pez, and Adrien Gaidon.
\newblock Exploring the limitations of behavior cloning for autonomous driving.
\newblock In \emph{International Conference on Computer Vision}, 2019.

\bibitem[de~Haan et~al.(2019)de~Haan, Jayaraman, and Levine]{causal_confusion}
Pim de~Haan, Dinesh Jayaraman, and Sergey Levine.
\newblock Causal confusion in imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{CoRR}, abs/1810.04805, 2018.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Gupta, Ibarz, and Levine]{DIAYN}
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock {D4RL:} datasets for deep data-driven reinforcement learning.
\newblock \emph{CoRR}, abs/2004.07219, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{td3bc}
Scott Fujimoto and Shixiang Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and Meger]{TD3}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{BCQ}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Ge \& Yu(2017)Ge and Yu]{finetune_cv}
Weifeng Ge and Yizhou Yu.
\newblock Borrowing treasures from the wealthy: Deep transfer learning through
  selective joint fine-tuning.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2017.

\bibitem[Ghosh et~al.(2022)Ghosh, Ajay, Agrawal, and
  Levine]{offline_rl_adaptive}
Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine.
\newblock Offline {RL} policies should be trained to be adaptive.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{sac}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, Dulac-Arnold, Agapiou, Leibo, and
  Gruslys]{dqn_demo}
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal
  Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, Gabriel
  Dulac-Arnold, John Agapiou, Joel~Z. Leibo, and Audrunas Gruslys.
\newblock Deep {Q}-learning from demonstrations.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{TT}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{rl_in_robotics}
Jens Kober, J.~Andrew Bagnell, and Jan Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Kornblith et~al.(2019)Kornblith, Shlens, and Le]{imagenet_transfer}
Simon Kornblith, Jonathon Shlens, and Quoc~V. Le.
\newblock Do better {ImageNet} models transfer better?
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2019.

\bibitem[Kostrikov et~al.(2022)Kostrikov, Nair, and Levine]{iql}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit {{Q}-learning}.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{cql}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative {{Q}-learning} for offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kumar et~al.(2022)Kumar, Hong, Singh, and Levine]{kumar2022should}
Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine.
\newblock {Should I} run offline reinforcement learning or behavioral cloning?
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Lee et~al.(2021)Lee, Seo, Lee, Abbeel, and Shin]{balanced_replay}
Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin.
\newblock Offline-to-online reinforcement learning via balanced replay and
  pessimistic q-ensemble.
\newblock In \emph{Annual Conference on Robot Learning}, 2021.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{offline_rl}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{CoRR}, abs/2005.01643, 2020.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{ddpg}
Timothy~P. Lillicrap, Jonathan~J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Liu \& Abbeel(2021)Liu and Abbeel]{behavior_from_void}
Hao Liu and Pieter Abbeel.
\newblock Behavior from the void: Unsupervised active pre-training.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Lu et~al.(2022)Lu, Ball, Rudner, Parker-Holder, Osborne, and
  Teh]{Lu22}
Cong Lu, Philip~J. Ball, Tim G.~J. Rudner, Jack Parker-Holder, Michael~A.
  Osborne, and Yee~Whye Teh.
\newblock Challenges and opportunities in offline reinforcement learning from
  visual observations.
\newblock \emph{CoRR}, abs/2206.04779, 2022.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih2015humanlevel}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, February 2015.

\bibitem[Muller et~al.(2005)Muller, Ben, Cosatto, Flepp, and LeCun]{off_road}
Urs Muller, Jan Ben, Eric Cosatto, Beat Flepp, and Yann LeCun.
\newblock Off-road obstacle avoidance through end-to-end learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2005.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{sac_bc}
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter
  Abbeel.
\newblock {Overcoming exploration in reinforcement learning with
  demonstrations}.
\newblock In \emph{{IEEE} International Conference on Robotics and Automation},
  2018.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{AWAC}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock {AWAC}: Accelerating online reinforcement learning with offline
  datasets.
\newblock \emph{CoRR}, abs/2006.09359, 2020.

\bibitem[Pomerleau(1988)]{ALVINN}
Dean~A. Pomerleau.
\newblock {ALVINN}: An autonomous land vehicle in a neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1988.

\bibitem[Radford \& Narasimhan(2018)Radford and Narasimhan]{gpt1}
Alec Radford and Karthik Narasimhan.
\newblock Improving language understanding by generative pre-training.
\newblock In \emph{Tech. Report}, 2018.

\bibitem[Rajeswaran et~al.(2018)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{DAPG}
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John
  Schulman, Emanuel Todorov, and Sergey Levine.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock In \emph{Proceedings of Robotics: Science and Systems}, 2018.

\bibitem[Rastgoftar et~al.(2018)Rastgoftar, Zhang, and
  Atkins]{data_driven_driving}
Hossein Rastgoftar, Bingxin Zhang, and Ella~M. Atkins.
\newblock A data-driven approach for autonomous motion planning and control in
  off-road driving scenarios.
\newblock In \emph{Annual American Control Conference}, 2018.

\bibitem[Rezaeifar et~al.(2022)Rezaeifar, Dadashi, Vieillard, Hussenot, Bachem,
  Pietquin, and Geist]{anti_exploration}
Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Léonard Hussenot, Olivier
  Bachem, Olivier Pietquin, and Matthieu Geist.
\newblock Offline reinforcement learning as anti-exploration.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2022.

\bibitem[Ross \& Bagnell(2012)Ross and Bagnell]{agnoistic_system_id_mbrl}
Stephane Ross and Drew Bagnell.
\newblock {Agnostic system identification for model-based reinforcement
  learning}.
\newblock In \emph{International Conference on Machine Learning}, 2012.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{dagger}
Stephane Ross, Geoffrey Gordon, and Drew Bagnell.
\newblock {A reduction of imitation learning and structured prediction to
  no-regret online learning}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2011.

\bibitem[Rudner et~al.(2021)Rudner, Lu, Osborne, Gal, and Teh]{KL_RL}
Tim G.~J. Rudner, Cong Lu, Michael Osborne, Yarin Gal, and Yee~Whye Teh.
\newblock On pathologies in {KL}-regularized reinforcement learning from expert
  demonstrations.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Rusu et~al.(2016)Rusu, Colmenarejo, Çaglar Gülçehre, Desjardins,
  Kirkpatrick, Pascanu, Mnih, Kavukcuoglu, and Hadsell]{policy_distillation}
Andrei~A. Rusu, Sergio~Gomez Colmenarejo, Çaglar Gülçehre, Guillaume
  Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray
  Kavukcuoglu, and Raia Hadsell.
\newblock Policy distillation.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schrittwieser et~al.(2019)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, Lillicrap, and
  Silver]{muzero}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, Timothy~P. Lillicrap, and David Silver.
\newblock Mastering {Atari, Go, Chess and Shogi} by planning with a learned
  model.
\newblock \emph{CoRR}, abs/1911.08265, 2019.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.

\bibitem[Schwarzer et~al.(2021)Schwarzer, Rajkumar, Noukhovitch, Anand,
  Charlin, Hjelm, Bachman, and Courville]{repr_pre_training}
Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent
  Charlin, R~Devon Hjelm, Philip Bachman, and Aaron~C Courville.
\newblock Pretraining representations for data-efficient reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Seo et~al.(2022)Seo, Lee, James, and Abbeel]{APV}
Younggyo Seo, Kimin Lee, Stephen~L James, and Pieter Abbeel.
\newblock Reinforcement learning with action-free pre-training from videos.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Shah et~al.(2022)Shah, Toshev, Levine, and Ichter]{VFS}
Dhruv Shah, Alexander~T Toshev, Sergey Levine, and Brian Ichter.
\newblock Value function spaces: Skill-centric state abstractions for
  long-horizon reasoning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Shah \& Kumar(2021)Shah and Kumar]{resnet_pretrain}
Rutav Shah and Vikash Kumar.
\newblock {RRL}: Resnet as representation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Shu et~al.(2018)Shu, Xiong, and Socher]{MTRL}
Tianmin Shu, Caiming Xiong, and Richard Socher.
\newblock Hierarchical and interpretable skill acquisition in multi-task
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{alphago}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  Yutian Chen, Timothy~P. Lillicrap, Fan Hui, Laurent Sifre, George van~den
  Driessche, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of {Go} without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Singh et~al.(2021)Singh, Liu, Zhou, Yu, Rhinehart, and Levine]{parrot}
Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey
  Levine.
\newblock Parrot: Data-driven behavioral priors for reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Sutton et~al.(2000)Sutton, Mcallester, Singh, and Mansour]{pg}
R.~S. Sutton, D.~Mcallester, S.~Singh, and Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2000.

\bibitem[Tirumala et~al.(2020)Tirumala, Galashov, Noh, Hasenclever, Pascanu,
  Schwarz, Desjardins, Czarnecki, Ahuja, Teh, and Heess]{behavior_prior}
Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan
  Pascanu, Jonathan Schwarz, Guillaume Desjardins, Wojciech~Marian Czarnecki,
  Arun Ahuja, Yee~Whye Teh, and Nicolas Heess.
\newblock Behavior priors for efficient reinforcement learning.
\newblock \emph{CoRR}, 2020.

\bibitem[Tsividis et~al.(2021)Tsividis, Loula, Burga, Foss, Campero, Pouncy,
  Gershman, and Tenenbaum]{human_level_planning}
Pedro~A. Tsividis, Jo{\~{a}}o Loula, Jake Burga, Nathan Foss, Andres Campero,
  Thomas Pouncy, Samuel~J. Gershman, and Joshua~B. Tenenbaum.
\newblock Human-level reinforcement learning through theory-based modeling,
  exploration, and planning.
\newblock \emph{CoRR}, abs/2107.12544, 2021.

\bibitem[Uchendu et~al.(2022)Uchendu, Xiao, Lu, Zhu, Yan, Simon, Bennice, Fu,
  Ma, Jiao, Levine, and Hausman]{jump_start_RL}
Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Joséphine
  Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, and
  Karol Hausman.
\newblock Jump-start reinforcement learning.
\newblock In \emph{arXiv}, 2022.

\bibitem[van~der Maaten \& Hinton(2008)van~der Maaten and Hinton]{tsne}
Laurens van~der Maaten and Geoffrey~E. Hinton.
\newblock Visualizing high-dimensional data using t-{SNE}.
\newblock \emph{Journal of Machine Learning Research}, 9:\penalty0 2579--2605,
  2008.

\bibitem[Vecer{\'{\i}}k et~al.(2017)Vecer{\'{\i}}k, Hester, Scholz, Wang,
  Pietquin, Piot, Heess, Roth{\"{o}}rl, Lampe, and
  Riedmiller]{use_offline_buffer}
Matej Vecer{\'{\i}}k, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier
  Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth{\"{o}}rl, Thomas Lampe, and
  Martin~A. Riedmiller.
\newblock {Leveraging demonstrations for deep reinforcement learning on
  robotics problems with sparse rewards}.
\newblock \emph{CoRR}, abs/1707.08817, 2017.

\bibitem[Wen et~al.(2020)Wen, Lin, Darrell, Jayaraman, and Gao]{copycat}
Chuan Wen, Jierui Lin, Trevor Darrell, Dinesh Jayaraman, and Yang Gao.
\newblock Fighting copycat agents in behavioral cloning from observation
  histories.
\newblock \emph{CoRR}, abs/2010.14876, 2020.

\bibitem[Xie et~al.(2021)Xie, Jiang, Wang, Xiong, and Bai]{policy_finetuing}
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu~Bai.
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Xu et~al.(2022)Xu, Zhan, Yin, and Qin]{weighted_bc}
Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin.
\newblock Discriminator-weighted offline imitation learning from suboptimal
  demonstrations.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Yang et~al.(2021)Yang, Qi, Cui, and Chen]{yang21}
Chao{-}Han~Huck Yang, Zhengling Qi, Yifan Cui, and Pin{-}Yu Chen.
\newblock Pessimistic model selection for offline deep reinforcement learning.
\newblock \emph{CoRR}, abs/2111.14346, 2021.

\bibitem[Yang \& Nachum(2021)Yang and Nachum]{offlne_rl_pretrain}
Mengjiao Yang and Ofir Nachum.
\newblock Representation matters: Offline pretraining for sequential decision
  making.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Yu et~al.(2021)Yu, Xu, and Zhang]{taac}
Haonan Yu, Wei Xu, and Haichao Zhang.
\newblock {TAAC}: Temporally abstract actor-critic for continuous control.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Yuan et~al.(2022)Yuan, Xue, Yuan, Wang, Wu, Gao, and
  Xu]{imagenet_pretrain}
Zhecheng Yuan, Zhengrong Xue, Bo~Yuan, Xueqian Wang, Yi~Wu, Yang Gao, and
  Huazhe Xu.
\newblock Pre-trained image encoder for generalizable visual reinforcement
  learning.
\newblock In \emph{First Workshop on Pre-training: Perspectives, Pitfalls, and
  Paths Forward at ICML}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Xu, and Yu]{GPM}
Haichao Zhang, Wei Xu, and Haonan Yu.
\newblock Generative planning for temporally coordinated exploration in
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zheng et~al.(2022)Zheng, Henaff, Amos, and Grover]{zheng22}
Qinqing Zheng, Mikael Henaff, Brandon Amos, and Aditya Grover.
\newblock Semi-supervised offline reinforcement learning with action-free
  trajectories.
\newblock \emph{CoRR}, abs/2210.06518, 2022.

\end{thebibliography}
