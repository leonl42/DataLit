\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Adeyemi et~al.(2023)Adeyemi, Oladipo, Pradeep, and Lin}]{adeyemi2023zero}
Mofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep, and Jimmy Lin. 2023.
\newblock Zero-shot cross-lingual reranking with large language models for low-resource languages.
\newblock \emph{arXiv preprint arXiv:2312.16159}.

\bibitem[{Agarwal et~al.(2023)Agarwal, Vieillard, Stanczyk, Ramos, Geist, and Bachem}]{agarwal2023gkd}
Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2023.
\newblock Gkd: Generalized knowledge distillation for auto-regressive sequence models.
\newblock \emph{arXiv preprint arXiv:2306.13649}.

\bibitem[{Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford, Millican, Van Den~Driessche, Lespiau, Damoc, Clark et~al.}]{borgeaud2022improving}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George~Bm Van Den~Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et~al. 2022.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International conference on machine learning}, pages 2206--2240. PMLR.

\bibitem[{Brown et~al.(2023)Brown, Williamson, Anderson, and Lawrence}]{brown-etal-2023-efficient}
Nathan Brown, Ashton Williamson, Tahj Anderson, and Logan Lawrence. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.emnlp-industry.6} {Efficient transformer knowledge distillation: A performance review}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track}, pages 54--65, Singapore. Association for Computational Linguistics.

\bibitem[{Gu et~al.(2023)Gu, Dong, Wei, and Huang}]{gu2023minillm}
Yuxian Gu, Li~Dong, Furu Wei, and Minlie Huang. 2023.
\newblock Minillm: Knowledge distillation of large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang}]{guu2020retrieval}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.
\newblock Retrieval augmented language model pre-training.
\newblock In \emph{International conference on machine learning}, pages 3929--3938. PMLR.

\bibitem[{Hang et~al.(2024)Hang, Yu, and Tan}]{hang2024trumorgpt}
Ching~Nam Hang, Pei-Duo Yu, and Chee~Wei Tan. 2024.
\newblock Trumorgpt: Query optimization and semantic reasoning over networks for automated fact-checking.
\newblock In \emph{2024 58th Annual Conference on Information Sciences and Systems (CISS)}, pages 1--6. IEEE.

\bibitem[{He et~al.(2022)He, Gong, Jin, Qi, Zhang, Jiao, Zhou, Cheng, Yiu, Duan et~al.}]{he2022metric}
Xingwei He, Yeyun Gong, A~Jin, Weizhen Qi, Hang Zhang, Jian Jiao, Bartuer Zhou, Biao Cheng, Siu~Ming Yiu, Nan Duan, et~al. 2022.
\newblock Metric-guided distillation: Distilling knowledge from the metric to ranker and retriever for generative commonsense reasoning.
\newblock \emph{arXiv preprint arXiv:2210.11708}.

\bibitem[{Hinton et~al.(2015)Hinton, Vinyals, and Dean}]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}.

\bibitem[{Ho et~al.(2022)Ho, Schmid, and Yun}]{ho2022large}
Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022.
\newblock Large language models are reasoning teachers.
\newblock \emph{arXiv preprint arXiv:2212.10071}.

\bibitem[{Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna, Lee, and Pfister}]{hsieh2023distilling}
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.
\newblock Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.
\newblock \emph{arXiv preprint arXiv:2305.02301}.

\bibitem[{Izacard et~al.(2022)Izacard, Caron, Hosseini, Riedel, Bojanowski, Joulin, and Grave}]{izacard2022unsupervised}
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022.
\newblock \href {https://arxiv.org/abs/2112.09118} {Unsupervised dense information retrieval with contrastive learning}.
\newblock \emph{Preprint}, arXiv:2112.09118.

\bibitem[{Izacard et~al.(2023)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave}]{izacard2023atlas}
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023.
\newblock Atlas: Few-shot learning with retrieval augmented language models.
\newblock \emph{Journal of Machine Learning Research}, 24(251):1--43.

\bibitem[{Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer}]{joshi-etal-2017-triviaqa}
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.
\newblock \href {https://doi.org/10.18653/v1/P17-1147} {{T}rivia{QA}: A large scale distantly supervised challenge dataset for reading comprehension}.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1601--1611, Vancouver, Canada. Association for Computational Linguistics.

\bibitem[{Khaliq et~al.(2024)Khaliq, Chang, Ma, Pflugfelder, and Mileti{\'c}}]{khaliq2024ragar}
M~Abdul Khaliq, P~Chang, M~Ma, Bernhard Pflugfelder, and F~Mileti{\'c}. 2024.
\newblock Ragar, your falsehood radar: Rag-augmented reasoning for political fact-checking using multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2404.12065}.

\bibitem[{Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, Toutanova, Jones, Kelcey, Chang, Dai, Uszkoreit, Le, and Petrov}]{kwiatkowski-etal-2019-natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew~M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
\newblock \href {https://doi.org/10.1162/tacl_a_00276} {Natural questions: A benchmark for question answering research}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:452--466.

\bibitem[{Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel et~al.}]{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim Rockt{\"a}schel, et~al. 2020.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:9459--9474.

\bibitem[{Li et~al.(2022)Li, Chen, Shen, Chen, Zhang, Li, Wang, Qian, Peng, Mao et~al.}]{li2022explanations}
Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi~Mao, et~al. 2022.
\newblock Explanations from large language models make small reasoners better.
\newblock \emph{arXiv preprint arXiv:2210.06726}.

\bibitem[{Ma et~al.(2023)Ma, Wang, Yang, Wei, and Lin}]{ma2023fine}
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023.
\newblock Fine-tuning llama for multi-stage text retrieval.
\newblock \emph{arXiv preprint arXiv:2310.08319}.

\bibitem[{Menick et~al.(2022)Menick, Trebacz, Mikulik, Aslanides, Song, Chadwick, Glaese, Young, Campbell-Gillingham, Irving, and McAleese}]{menick2022teaching}
Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. 2022.
\newblock \href {https://arxiv.org/abs/2203.11147} {Teaching language models to support answers with verified quotes}.
\newblock \emph{Preprint}, arXiv:2203.11147.

\bibitem[{Min et~al.(2023)Min, Krishna, Lyu, Lewis, tau Yih, Koh, Iyyer, Zettlemoyer, and Hajishirzi}]{min2023factscore}
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang~Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
\newblock \href {https://arxiv.org/abs/2305.14251} {Factscore: Fine-grained atomic evaluation of factual precision in long form text generation}.
\newblock \emph{Preprint}, arXiv:2305.14251.

\bibitem[{Min et~al.(2022)Min, Shi, Lewis, Chen, Yih, Hajishirzi, and Zettlemoyer}]{min2022nonparametric}
Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.
\newblock Nonparametric masked language modeling.
\newblock \emph{arXiv preprint arXiv:2212.01349}.

\bibitem[{Nogueira et~al.(2020)Nogueira, Jiang, and Lin}]{nogueira2020document}
Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.
\newblock Document ranking with a pretrained sequence-to-sequence model.
\newblock \emph{arXiv preprint arXiv:2003.06713}.

\bibitem[{OpenAI(2023)}]{openai2023gpt4}
OpenAI. 2023.
\newblock \href {https://arxiv.org/abs/2303.08774} {Gpt-4 technical report}.
\newblock \emph{Preprint}, arXiv:2303.08774.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:27730--27744.

\bibitem[{Ram et~al.(2023)Ram, Levine, Dalmedigos, Muhlgay, Shashua, Leyton-Brown, and Shoham}]{ram2023context}
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.
\newblock In-context retrieval-augmented language models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 11:1316--1331.

\bibitem[{Rubin and Berant(2023)}]{rubin2023long}
Ohad Rubin and Jonathan Berant. 2023.
\newblock Long-range language modeling with self-retrieval.
\newblock \emph{arXiv preprint arXiv:2306.13421}.

\bibitem[{Shi et~al.(2023)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih}]{shi2023replug}
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.
\newblock Replug: Retrieval-augmented black-box language models.
\newblock \emph{arXiv preprint arXiv:2301.12652}.

\bibitem[{Shuster et~al.(2021)Shuster, Poff, Chen, Kiela, and Weston}]{shuster2021retrieval}
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.
\newblock Retrieval augmentation reduces hallucination in conversation.
\newblock \emph{arXiv preprint arXiv:2104.07567}.

\bibitem[{Siriwardhana et~al.(2023)Siriwardhana, Weerasekera, Wen, Kaluarachchi, Rana, and Nanayakkara}]{siriwardhana2023improving}
Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023.
\newblock Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 11:1--17.

\bibitem[{Sun et~al.(2023)Sun, Yan, Ma, Ren, Yin, and Ren}]{sun2023chatgpt}
Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023.
\newblock Is chatgpt good at search? investigating large language models as re-ranking agent.
\newblock \emph{arXiv preprint arXiv:2304.09542}.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Udagawa et~al.(2023)Udagawa, Trivedi, Merler, and Bhattacharjee}]{udagawa-etal-2023-comparative}
Takuma Udagawa, Aashka Trivedi, Michele Merler, and Bishwaranjan Bhattacharjee. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.emnlp-industry.3} {A comparative analysis of task-agnostic distillation methods for compressing transformer language models}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track}, pages 20--31, Singapore. Association for Computational Linguistics.

\bibitem[{Wang et~al.(2024{\natexlab{a}})Wang, Huang, Deng, Wang, Wang, Wang, Mi, Pan, and Wong}]{wang2024unims}
Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei Wang, Fei Mi, Jeff~Z Pan, and Kam-Fai Wong. 2024{\natexlab{a}}.
\newblock Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems.
\newblock \emph{arXiv preprint arXiv:2401.13256}.

\bibitem[{Wang et~al.(2024{\natexlab{b}})Wang, Ma, Feng, Zhang, Yang, Zhang, Chen, Tang, Chen, Lin et~al.}]{wang2024survey}
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu~Chen, Yankai Lin, et~al. 2024{\natexlab{b}}.
\newblock A survey on large language model based autonomous agents.
\newblock \emph{Frontiers of Computer Science}, 18(6):1--26.

\bibitem[{Wu et~al.(2023)Wu, Irsoy, Lu, Dabravolski, Dredze, Gehrmann, Kambadur, Rosenberg, and Mann}]{wu2023bloomberggpt}
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023.
\newblock Bloomberggpt: A large language model for finance.
\newblock \emph{arXiv preprint arXiv:2303.17564}.

\bibitem[{Xi et~al.(2023)Xi, Chen, Guo, He, Ding, Hong, Zhang, Wang, Jin, Zhou et~al.}]{xi2023rise}
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et~al. 2023.
\newblock The rise and potential of large language model based agents: A survey.
\newblock \emph{arXiv preprint arXiv:2309.07864}.

\bibitem[{Xia et~al.(2008)Xia, Liu, Wang, Zhang, and Li}]{xia2008listwise}
Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008.
\newblock Listwise approach to learning to rank: theory and algorithm.
\newblock In \emph{Proceedings of the 25th international conference on Machine learning}, pages 1192--1199.

\bibitem[{Xu et~al.(2024)Xu, Shi, Yu, Zhuang, Zhu, Wang, Ho, Zhang, and Yang}]{xu2024bmretriever}
Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May~D Wang, Joyce~C Ho, Chao Zhang, and Carl Yang. 2024.
\newblock Bmretriever: Tuning large language models as better biomedical text retrievers.
\newblock \emph{arXiv preprint arXiv:2404.18443}.

\bibitem[{Zhang et~al.(2023{\natexlab{a}})Zhang, Liu, and Zhang}]{zhang2023extractive}
Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2304.04193} {Extractive summarization via chatgpt for faithful summary generation}.
\newblock \emph{Preprint}, arXiv:2304.04193.

\bibitem[{Zhang et~al.(2023{\natexlab{b}})Zhang, Liu, and Zhang}]{zhang2023summit}
Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2305.14835} {Summit: Iterative text summarization via chatgpt}.
\newblock \emph{Preprint}, arXiv:2305.14835.

\bibitem[{Zhang et~al.(2024)Zhang, Patil, Jain, Shen, Zaharia, Stoica, and Gonzalez}]{zhang2024raft}
Tianjun Zhang, Shishir~G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph~E Gonzalez. 2024.
\newblock Raft: Adapting language model to domain specific rag.
\newblock \emph{arXiv preprint arXiv:2403.10131}.

\bibitem[{Zhong et~al.(2022)Zhong, Lei, and Chen}]{zhong2022training}
Zexuan Zhong, Tao Lei, and Danqi Chen. 2022.
\newblock Training language models with memory augmentation.
\newblock \emph{arXiv preprint arXiv:2205.12674}.

\end{thebibliography}
