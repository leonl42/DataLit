\section{Limitation}
This paper proposes a data-efficient distillation scheme using black-box LLMs to train smaller information retrieval models, which prove its effectiveness with training data on the scale of thousands. 
However, we do not evaluate our proposed distillation scheme with larger scales of training data, such as tens of thousands or millions of instances, due to budget limitations on accessing responses from closed-source LLMs and insufficient computational resources to utilize high-quality open-source LLMs like Llama-70B. 
In the future work, we will focus on extending this study to larger-scale training data, using either closed-source or advanced open-source LLMs to further analysis the effectiveness of our proposed distillation scheme.