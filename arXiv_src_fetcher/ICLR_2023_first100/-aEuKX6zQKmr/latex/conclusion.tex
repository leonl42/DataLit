\section{Conclusion}
We propose Intermediate Distillation, a two-stage data-efficient knowledge distillation scheme that uses the remarkable capabilities of black-box LLMs to train an information retrieval model through an intermediate ranker model.
We conduct extensive experiments with advanced LLMs, demonstrating that our method enhances the effectiveness and efficiency of the retriever model performance compared to other supervision signals. 