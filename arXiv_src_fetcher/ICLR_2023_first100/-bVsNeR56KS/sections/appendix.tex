\section{Appendix}




\subsection{Details of Datasets and Hyperparameters }
\label{ap:finetune}

\textcolor{black}{
\noindent{\bf CC-108.} Since Common Crawl~\citep{wenzek2019ccnet} is not a public dataset, so we need to pre-process it by ourselves. Our reserved 108 languages are the union of the languages that XLMR and mBERT support. And we followed the processing method adopted by XLMR~\citep{conneau2019unsupervised} to pre-process CommonCrawl and retain 108 languages.
}

\noindent{\bf \tydi.} ~\citep{zhang-etal-2021-mr}
It aims to evaluate cross-lingual passage retrieval with dense representations. \tydi is a multi-lingual benchmark dataset for mono-lingual query passage retrieval in eleven typologically distinct languages. Given a question in language \textit{L}, the model should retrieve relevant texts in language \textit{L} that can respond to the query. As the original paper suggests, we take MRR@100 and Recall@100 for evaluation.

We mainly follow the open-source codebase DPR~\citep{DPR2020} with minor modifications for multi-lingual models. When fine-tuned on MS MARCO in the zero-shot setting, we use AdamW optimizer with a learning rate of 2e-5. The model is trained for up to 3 epochs with a mini-batch size of 64. When fine-tuning on the NQ dataset, it is up to 40 epochs with a mini-batch size of 128.  When further fine-tuning on \tydi's in-language data, it is 40 epochs, mini-batch size 128, and 1e-5 learning rate. Note all of them mainly follow what the original papers~\citep{zhang-etal-2021-mr,zhang2022towards} suggest. All these experiments are conducted on 8 NVIDIA Tesla A100 GPUs. 

\noindent{\bf \xor.} ~\citep{asai-etal-2021-xor}
XOR QA is proposed for multilingual open-domain QA, and we take its sub-task XOR-Retrieve for our evaluation: given a question in \textit{L} (e.g., Korean), the task is to retrieve English passages that can answer the query.  Following ~\citet{asai-etal-2021-xor}, we calculate the recall by computing the fraction of the questions for which the minimal answer is included in the top n tokens selected, and take R@2kt and R@5kt (kilo-tokens) as the metrics.

Similar to the settings of the previous one, we use AdamW Optimizer with a learning rate of 2e-5. The model is trained up to 40 epochs with a mini-batch size of 128 when fine-tuning on NQ dataset. And when further tuning on XOR's data, the hyper-parameter remains the same. We evaluate all the compared pre-trained models in the same pipelines on 8 NVIDIA Tesla A100 GPUs.

\noindent{\bf Mewsli-X.} ~\citep{ruder-etal-2021-xtreme}
Mewsli-X is built on top of Mewsli~\citep{botha-etal-2020-entity}. We follow the setting of XTREME-R~\citep{ruder-etal-2021-xtreme}, which builds Mewsli-X consisting of 15K mentions in 11 languages: given a mention in context, it is to retrieve the correct target entity description from a candidate pool ranging over 1M candidates across 50 languages. 

We mainly follow the setting of XTREME-R~\citep{hu2020xtreme}. It adopts a 2e-5 learning rate, and it is trained for up to 2 epochs with batch size of 16.  As the original paper suggests, all the evaluations are conducted on NVIDIA Tesla V100 GPU for a fair comparison.

\noindent{\bf LAReQA.} ~\citep{roy-etal-2020-lareqa}
Language Agnostic Retrieval Question Answering is a retrieval task in which each query has target answers in multiple languages, and models require retrieving all correct answers from the candidate pool regardless of language.  Following ~\citet{ruder-etal-2021-xtreme}, we use the LAReQA XQuAD-R dataset which consists of 13,090 questions, each of which has 11 target answers (in 11 different languages) within 13,014 candidate answer sentences. 

Following ~\citet{ruder-etal-2021-xtreme}, we use the LAReQA XQuAD-R dataset which consists of 13,090 questions, each of which has 11 target answers (in 11 different languages) within 13,014 candidate answer sentences. It also follows the setting proposed by XTREME-R~\citep{hu2020xtreme}. It adopts a 2e-5 learning rate and it is trained up to 3 epochs with batch size of 4. All the evaluations are conducted on NVIDIA Tesla V100 GPU.


\subsection{Detailed results across languages}
\label{ap:across}
We show the detailed results for each task across different languages corresponding to Table.\ref{tab:main}. Specifically, the results of \tydi are as shown in Table.\ref{tab:ap1}, \xor in Table.\ref{tab:ap2}, \news in Table.\ref{tab:ap3}, and \lqa in Table.\ref{tab:ap4}. 

\textcolor{black}{
Through the detailed results across languages, there are some findings on multilinguality: (1) MSM can achieve more gains in low-resource language. For example, in zero-shot setting of \tydi (Tab.\ref{tab:ap1}) it improves + 8.1 MRR@100 for SW and + 11.8 for BN compared to XLMR, and for \xor (Tab.\ref{tab:ap2}) + 8.8 R@5k for TE. Though there are limited data in low-resource languages and the model suffers from the curse of multilinguality, our MSM can lead to better transfer to them, benefiting from modeling the sequential sentence relation across languages. (2) The target languages closer to pivot language (i.e. EN in our experiment) usually perform better and achieve more improvements. On \news task (Tab.\ref{tab:ap3}) MSM can improve + 5.3 MAP for language DE while only + 1.3 for UK, for German (DE) is more similar to English in both scripts and language family~\citep{ruder-etal-2021-xtreme}. Similar observations also exist in \lqa (Tab.\ref{tab:ap4}) that MSM performs better and improves more on DE, EL, ES and poorer on ZH and TH. (3) The multi-lingual data lead to better cross-lingual retrieval performance. It can be seen in Tab.\ref{tab:ap1} and Tab.\ref{tab:ap2}, the performance can be further improved after fine-tuning multi-lingual train data. It indicates that the target languages can benefit from the other languages' data, and also shows that fine-tuning on multi-lingual data is necessary if available. It is worth mentioning that in this setting MSM can also achieve more gains, which demonstrates better cross-lingual transfer ability.
}



\begin{table*}[t]
\centering
\scalebox{0.85}{
\begin{tabular}{c|cccccccccccc|c}
\toprule
Method & Metrics & AR & BN & EN & FI & ID & JA & KO & RU & SW & TE & TH & AVG \\ \midrule
\multicolumn{9}{l}{\emph{Cross-lingual zero-shot transfer (models are fine-tuned on English data)}} \\ \midrule
 & MRR@100 & 45.3 & 38.8 & 29.1 & 28.7 & 35.5 & 29.6 & 29.8 & 32.6 & 27.8 & 40.8 & 25.3 & 33.0 \\
\multirow{-2}{*}{mBERT}  & Recall@100 & 77.8 & 84.7 & 73.8 & 65.8 & 72.9 & 68.4 & 59.9 & 71.1 & 56.4 & 76.8 & 59.2 & 69.7 \\  \midrule
& MRR@100 & 43.8 & 41.2 & 29.3 & 33.2 & 45.4 & 27.0 & 33.4 & 32.2 & 35.3 & 44.5 & 49.7 & 37.7 \\ 
\multirow{-2}{*}{XLMR}  & Recall@100 & 77.4 & 81.5 & 68.5 & 72.2 & 81.8 & 61.2 & 66.5 & 66.4 & 63.9 & 75.5 & 85.4 & 72.8 \\ \midrule
 & MRR@100 & 51.6 & 53.0 & 31.6 & 39.4 & 50.5 & 32.0 & 36.8 & 37.2 & 43.4 & 62.6 & 53.5 & 44.7  \\
\multirow{-2}{*}{MSM}  & Recall@100 & 83.0 & 83.8 & 73.9 & 77.9 & 85.7 & 67.5 & 70.3 & 71.4 & 73.0 & 89.8 & 88.2 & 78.6  \\ \midrule
\multicolumn{14}{l}{\emph{Multi-lingual fine-tune (models are fine-tuned on the multi-lingual data)}} \\ \midrule
 & MRR@100 & 66.8 & 63.3 & 50.8 & 55.0 & 55.7 & 47.8 & 43.3 & 47.3 & 60.5 & 85.8 & 57.7 & 57.6 \\
\multirow{-2}{*}{mBERT}  & Recall@100 & 90.3 & 95.0 & 89.0 & 85.8 & 89.3 & 81.6 & 80.2 & 85.6 & 86.7 & 96.3 & 87.2 & 87.9 \\  \midrule
& MRR@100 & 66.4 & 66.0 & 48.5 & 53.7 & 58.3 & 45.4 & 44.3 & 47.4 & 61.4 & 86.2 & 65.7 & 58.5 \\
\multirow{-2}{*}{XLMR}  & Recall@100 & 89.4 & 93.2 & 84.3 & 86.6 & 90.0 & 82.3 & 79.3 & 82.8 & 86.4 & 97.4 & 93.8 & 87.8 \\\midrule
 & MRR@100 & 67.7 & 69.9 & 49.6 & 55.1 & 61.2 & 48.2 & 49.4 & 47.5 & 63.9 & 85.2 & 67.7 & 60.5 \\
\multirow{-2}{*}{MSM} & Recall@100 & 90.5 & 95.9 & 86.5 & 88.0 & 90.1 & 82.0 & 81.6 & 82.5 & 88.9 & 97.6 & 95.0 & 89.0 \\
\bottomrule
\end{tabular}
}
\caption{ \tydi results across languages. We report MRR@100 and Recall@100 on the test sets of Mr. TyDi in the two settings.}
\label{tab:ap1}
\end{table*}



\begin{table*}[t]
\centering
\scalebox{0.85}{
\begin{tabular}{c|cccccccc|c}
\toprule
Method & Metrics & AR & BN & FI & JA & KO & RU & TE & AVG \\ \midrule
\multicolumn{10}{l}{\emph{Cross-lingual zero-shot transfer (models are fine-tuned on English data)}} \\ \midrule
\multirow{2}{*}{mBERT} & R@2k & 31.1 & 26.6 & 38.5 & 32.4 & 38.6 & 24.9 & 29.1 & 31.6 \\
 & R@5k & 44.1 & 36.2 & 48.1 & 41.5 & 48.1 & 38.4 & 39.5 & 42.3 \\ \midrule
\multirow{2}{*}{XLMR} & R@2k & 39.9 & 25.7 & 41.1 & 27.8 & 31.9 & 22.4 & 25.2 & 30.6 \\
 & R@5k & 49.6 & 34.9 & 50.0 & 34.9 & 41.8 & 30.4 & 34.0 & 39.3 \\ \midrule
\multirow{2}{*}{MSM} & R@2k & 47.9 & 32.6 & 44.9 & 24.1 & 35.8 & 25.3 & 34.0 & 34.9 \\
 & R@5k & 58.4 & 41.8 & 51.9 & 36.9 & 44.6 & 37.6 & 42.1 & 44.7 \\ \midrule
\multicolumn{10}{l}{\emph{Multi-lingual fine-tune (models are fine-tuned on the multi-lingual data)}} \\ \midrule
\multirow{2}{*}{mBERT} & R@2k & 51.7 & 52.7 & 52.2 & 41.9 & 51.9 & 47.5 & 40.5 & 48.3 \\
 & R@5k & 58.4 & 61.5 & 58.6 & 50.6 & 63.2 & 54.9 & 51.8 & 57.0 \\ \midrule
\multirow{2}{*}{XLMR} & R@2k & 59.2 & 48.7 & 46.8 & 36.1 & 46.0 & 35.4 & 43.4 & 45.1 \\
 & R@5k & 67.2 & 58.6 & 53.5 & 45.6 & 56.1 & 45.1 & 51.5 & 53.9 \\ \midrule
\multirow{2}{*}{MSM} & R@2k & 61.8 & 55.3 & 51.3 & 36.9 & 50.5 & 41.4 & 43.0 & 48.6 \\
 & R@5k & 68.9 & 63.8 & 59.9 & 48.1 & 56.1 & 52.7 & 51.5 & 57.3 \\ \bottomrule
\end{tabular}
}
\caption{ \xor results across languages. We report R@2k and R@5 metrics on the test sets in the two settings.}
\label{tab:ap2}
\end{table*}



\begin{table*}[t]
\centering
\scalebox{0.85}{
\begin{tabular}{c|ccccccccccc|c}
\toprule
Method & AR & DE & EN & ES & FA & JA & PL & RO & TA & TR & UK & AVG \\ \midrule
\multicolumn{10}{l}{\emph{Cross-lingual zero-shot transfer (models are fine-tuned on English data)}} \\ \midrule
mBERT & 14.2 & 65.5 & 57.4 & 55.7 & 11.3 & 44.8 & 57.2 & 38.2 & 5.8 & 42.7 & 36.2 & 39.0 \\
XLMR & 15.5 & 62.7 & 57.2 & 53.5 & 12.5 & 46.2 & 59.3 & 34.1 & 7.5 & 51.8 & 36.0 & 39.7 \\
MSM & 18.6 & 68.0 & 58.7 & 57.3 & 13.7 & 46.3 & 60.2 & 36.3 & 7.6 & 52.8 & 37.3 & \textbf{41.5} \\ \bottomrule
\end{tabular}
}
\caption{ Mewsli-X results across different input languages. We report the mean average precision@20 (mAP@20) results.}
\label{tab:ap3}
\end{table*}



\begin{table*}[!ht]
\centering
\scalebox{0.85}{
\begin{tabular}{c|ccccccccccc|c}
\toprule
Method & AR & DE & EL & EN & ES & HI & RU & TH & TR & VI & ZH & AVG  \\ \midrule
\multicolumn{10}{l}{\emph{Cross-lingual zero-shot transfer (models are fine-tuned on English data)}} \\ \midrule
mBERT & 20.1 & 32.2 & 19.9 & 34.8 & 33.5 & 15.3 & 29.6 & 7.2 & 23.3 & 27.6 & 26.2 & 24.5 \\
XLMR & 21.4 & 31.7 & 26.7 & 36.7 & 34.3 & 25.2 & 32.3 & 27.8 & 29.0 & 29.1 & 28.5 & 29.3 \\
MSM & 28.0 & 36.1 & 31.9 & 40.6 & 38.3 & 29.4 & 36.0 & 30.2 & 34.3 & 33.2 & 29.7 & \textbf{33.4} \\ \bottomrule
\end{tabular}
}
\caption{ LAReQA results across different question languages. We report the mean average precision@20 (mAP@20) results.}
\label{tab:ap4}
\end{table*}



\subsection{Comparison With Several Existing Methods}
\label{ap:com}

\textcolor{black}{
Table~\ref{app:comparison} shows the comparison of MSM and several existing retrieval approaches. DistilmBERT~\citep{reimers-gurevych-2020-making} distills knowledge from m-USE~\citep{yang2019multilingual} trained on labeled pair data into mBERT. LaBSE~\citep{feng-etal-2022-language} and InfoXLM~\citep{chi-etal-2021-infoxlm} encourage bilingual alignment via a translation ranking loss, and also trained with MLM and TLM tasks~\citep{lample2019cross}. InfoXLM adopts the momentum contrast and LaBSE proposed additive margin softmax for contrastive learning. They all use additional parallel corpora, while our MSM only needs multi-lingual data without relying on any parallel or labeled data.
}

\textcolor{black}{
Among unsupervised pre-trained models, mBERT~\citep{bert2019} and XLMR~\citep{conneau2019unsupervised} are general-purpose multilingual text encoders trained with MLM. XLMR-Long~\citep{Sagen1545786} (or XLMR Longformer) is an XLMR model that has been extended to allow sequence lengths up to 4096 tokens. mContriever~\citep{izacard2021unsupervised} and CCP~\citep{wu2022unsupervised} similarly mine positive pairs by cropping two spans in a document. The former proposes random cropping while the latter utilizes two sentences. However, the quality of the cropped pairs is not guaranteed. In contrast, MSM utilizes a sequence of sentences in a document and models the universal sentence relation across languages via an explicit document encoder, which results in better cross-lingual retrieval capability.
}


\begin{table}[!ht]
    \caption{Comparison with existing approaches. For the training objective, BI means bi-lingual pair alignment task, and CROP means contrastive learning with cropped spans. For the corpora, CC means CommonCrawl, mWiki means multi-lingual data from Wikipedia, and Bi-lingual may include MultiUN, OPUS, WikiMatrix, etc~\citep{chi-etal-2021-infoxlm}, which depends on models.}
    \centering
    \resizebox{0.89\textwidth}{!}{
    \begin{tabular}{l|c|c|c}
    \toprule
        Model & $\#$lg & \makecell[c]{Objective Function} & \makecell[c]{Corpora} \\  \midrule
        DistilmBERT~\citep{reimers-gurevych-2020-making} & 53 & Distillation & Bi-lingual  \\
        LaBSE~\citep{feng-etal-2022-language} & 109 & MLM + TLM + BI & CC + mWiki + Bi-lingual\\
        InfoXLM~\citep{chi-etal-2021-infoxlm} & 94 & MLM + TLM + BI & CC + Bi-lingual\\
        \midrule
        mBERT~\citep{bert2019} & 104 & MLM & mWiki\\
        XLMR~\citep{conneau2019unsupervised} & 100 & MLM & CC\\
        XLMR-Long~\citep{Sagen1545786} & 100 & MLM & CC + Wiki\\
        mContriever~\citep{izacard2021unsupervised} & 29 & CROP & CC + mWiki\\
        CCP~\citep{wu2022unsupervised} & 108 & MLM + CROP & CC\\ \midrule
        MSM & 108 & MLM + MSM & CC \\ \bottomrule
    \end{tabular}
    }
\label{app:comparison}
\end{table}



\subsection{Performance of different model size }
\label{ap:size}

Motivated by the recent progress of giant models, we also increase the model capability. Considering expensive computation, it is initialized with XLMR-large and other settings keep the same as the base model. As shown in Tab.\ref{tab:size}, we report the zero-shot transfer results on \tydi after fine-tuning on NQ data. It clearly shows that as the model capacity increase, the performance on the downstream task can be consistently improved. We also report several strong large-size pre-trained models, including InfoXLM and CCP which are both initialized with XLM-R Large. Compared to them, MSM-Large achieves outperforming results on MRR@100 and comparable results on Recall@100, which further demonstrates the effectiveness of MSM in different model capabilities.



\begin{table*}[t]
\centering
\scalebox{0.72}{
\begin{tabular}{c|cccccccccccc|c}
\toprule
Method & Metrics & AR & BN & EN & FI & ID & JA & KO & RU & SW & TE & TH & AVG \\ \midrule
\multirow{2}{*}{mBERT} &MRR@100 & 28.7 & 33.4 & 28.1 & 24.0 & 32.0 & 24.3 & 21.8 & 29.0 & 18.7 & 14.5 & 19.3 & 24.9 \\
 &Recall@100  & 68.3 & 79.3 & 76.1 & 65.0 & 74.1 & 65.3 & 57.9 & 69.2 & 51.7 & 46.1 & 54.0 & 64.3 \\ \midrule
\multirow{2}{*}{XLMR Base} & MRR@100& 30.8 & 30.2 & 27.1 & 23.5 & 33.5 & 23.5 & 26.6 & 25.7 & 24.0 & 26.6 & 36.3 & 28.0 \\
 &Recall@100  & 72.3 & 78.4 & 74.3 & 67.0 & 79.8 & 66.5 & 64.7 & 65.0 & 57.2 & 72.7 & 84.6 & 71.1 \\ \midrule
\multirow{2}{*}{XLMR Large} & MRR@100 & 36.5 & 37.4 & 27.5 & 31.8 & 39.5 & 29.9 & 30.4 & 30.6 & 27.4 & 34.6 & 40.1 & 33.3 \\
 & Recall@100 & 81.3 & 84.2 & 77.6 & 78.2 & 88.6 & 78.5 & 72.7 & 77.4 & 63.3 & 87.5 & 88.2 & 79.8 \\ \midrule
 \multirow{2}{*}{InfoXLM Large} & MRR@100 & 37.2 & 50.4 & 31.4 & 30.9 & 37.6 & 27.1 & 30.9 & 32.5 & 39.4 & 46.5 & 37.4 & 36.5 \\
 & Recall@100 & 76.2 & 91.0 & 78.3 & 76.0 & 85.2 & 66.9 & 64.4 & 74.4 & 75.0 & 88.9 & 83.4 & 78.2 \\ \midrule
\multirow{2}{*}{CCP Large} & MRR@100 & 42.6 & 45.7 & 35.9 & 37.2 & 46.2 & 37.7 & 34.6 & 36.0 & 39.2 & 47.0 & 48.9 & 41.0 \\
 & Recall@100 & 82.0 & 88.3 & 80.1 & 78.7 & 87.5 & 80.0 & 73.2 & 77.2 & 75.1 & 88.8 & 88.9 & 81.8 \\
  \midrule
\multirow{2}{*}{MSM Base} & MRR@100 & 37.9 & 39.4 & 29.9 & 27.7 & 38.3 & 28.0 & 26.8 & 28.5 & 32.1 & 43.1 & 42.0 & 34.0 \\
 &Recall@100  & 77.3 & 82.9 & 73.6 & 70.5 & 83.5 & 67.9 & 61.8 & 68.6 & 69.9 & 83.5 & 84.9 & 74.9 \\  \midrule
\multirow{2}{*}{MSM Large} & MRR@100 & 45.6 & 55.7 & 33.8 & 39.1 & 46.8 & 40.3 & 38.8 & 37.9 & 41.8 & 46.5 & 50.8 & 43.4 \\
 & Recall@100 & 83.3 & 90.1 & 78.5 & 79.9 & 85.9 & 78.1 & 73.1 & 77.1 & 76.4 & 86.9 & 88.9 & 81.6\\ \bottomrule
\end{tabular}
}
\caption{Cross-lingual zero-shot transfer retrieval performance for different size models. We report MRR@100 and Recall@100 on the test sets of Mr. TyDi after finetuning on NQ dataset.}
\label{tab:size}
\end{table*}




\subsection{Comparison to MT-Based Cross-lingual Retrieval}
\textcolor{black}{
In this section, we compare the model performance to the MT-based (machine translation based) cross-lingual retrieval. As shown in Table.\ref{tab:mt}, we provide four MT-based baselines borrowed from~\citep{asai-etal-2021-xor}, all of which first translate the query to the document language using a translation system and then perform monolingual retrieval. For translation systems, \textit{GMT} means Googleâ€™s online machine translation service and \textit{White-MT} means white-box translation model based on autoregressive transformers. For the monolingual retrieval model, \textit{PATH} means Path Retriever~\citep{asai2019learning}, a graph-based recurrent retrieval approach, and DPR~\citep{DPR2020} is a typical bi-encoder based retriever. Both of them are trained on the human translated queries with the annotated gold paragraph data of XOR Retrieve. 
}

\textcolor{black}{
Results in Table.\ref{tab:mt} clearly shows that MSM can outperform While-box MT-based methods by a large margin, which demonstrates the effectiveness of MSM. Moreover, upgrading the MT system to GMT achieve even better results, due to the superiority
of industrial MT systems (large parallel data, models and pipelines, etc.)~\citep{asai-etal-2021-xor}. These observations indicate that the performance of MT-base methods heavily depends on the quality of MT system. However, GMT is a black-box system so it's difficult to be analyzed. By the way, the MT-based method relies on the two-stage pipeline that first translates and then retrieves, which may lead to cumulative errors. In contrast, our MSM is a universal pre-trained model and can be easily applied in bi-encoder retrievers, which have more advantages in terms of deployments and diagnosis.
}

\begin{table*}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{c|cccccccc|c}
\toprule
Method & Metrics & AR & BN & FI & JA & KO & RU & TE & AVG \\ \midrule
\multirow{2}{*}{GMT + PATH} & R@2k & 59.1 & 58.2 & 60.3 & 50.0 & 50.3 & 54.1 & 58.0 & 58.2 \\
 & R@5k & 63.3 & 78.9 & 64.1 & 52.3 & 54.0 & 56.5 & 62.5 & 61.7 \\ \midrule
\multirow{2}{*}{GMT + DPR} & R@2k & 61.7 & 72.0 & 60.6 & 52.1 & 57.9 & 51.2 & 59.4 & 59.3 \\
 & R@5k & 67.5 & 83.2 & 68.1 & 60.1 & 66.3 & 60.4 & 65.0 & 67.2 \\\midrule
\multirow{2}{*}{White-MT + PATH} & R@2k & 45.0 & 60.9 & 56.6 & 36.7 & 33.8 & 34.7 & 15.7 & 40.5 \\
 & R@5k & 51.6 & 64.8 & 59.5 & 41.7 & 37.6 & 38.1 & 18.1 & 44.5 \\ \midrule
\multirow{2}{*}{White-MT + DPR} & R@2k & 48.3 & 54.4 & 56.7 & 41.8 & 39.4 & 39.6 & 18.7 & 42.7 \\
 & R@5k & 52.5 & 63.2 & 65.9 & 52.1 & 46.5 & 47.3 & 22.7 & 50.0 \\ \midrule
\multirow{2}{*}{XLM-R} & R@2k & 59.2 & 48.7 & 46.8 & 36.1 & 46.0 & 35.4 & 43.4 & 45.1 \\
 & R@5k & 67.2 & 58.6 & 53.5 & 45.6 & 56.1 & 45.1 & 51.5 & 53.9 \\ \midrule
\multirow{2}{*}{MSM} & R@2k & 61.8 & 55.3 & 51.3 & 36.9 & 50.5 & 41.4 & 43.0 & 48.6 \\
 & R@5k & 68.9 & 63.8 & 59.9 & 48.1 & 56.1 & 52.7 & 51.5 & 57.3 \\ \bottomrule
\end{tabular}
}
\caption{ Performance comparison to MT-Based Cross-lingual Retrieval on \xor across languages. We report R@2k and R@5k results.}
\label{tab:mt}
\end{table*}



\subsection{Monolingual Experiment }
\label{ap:monolingual}
We adapt our MSM pre-training method to the monolingual domain, in which we narrow the train corpus and model to English only. We initialize the sentence encoder with ERNIE-2.0-Base model and others adopt the same setting to our multi-lingual experiment. In Table.\ref{ap:mono}, we report the performance on the Natural Question~\citep{kwiatkowski-etal-2019-natural} test set after fine-tuning. It shows that our proposed unsupervised model achieves better performance than advanced baselines including BERT and ERNIE2.0~\citep{sun2020ernie2.0}. It further proves our MSM is a general pre-training method tailored for dense retrieval including monolingual and multi-lingual domains. 


\begin{table*}[t]
\centering
\resizebox{0.58\textwidth}{!}{
    \begin{tabular}{l|cccccc}
    \toprule
    \multirow{1}{*}{Method} 
     & R@5 & R@20 & R@100 & \\ \midrule
    \multicolumn{1}{l|}{BM25~\citep{yang2017anserini}} & - & 59.1 & 73.7  \\
    \multicolumn{1}{l|}{BERT~\citep{DPR2020}} & - & 78.4 & 85.3\\
    \multicolumn{1}{l|}{RoBERTa~\citep{roberta2019}} & 64.9 & 76.8 & 84.7  \\
    \multicolumn{1}{l|}{ERNIE2.0~\citep{sun2020ernie2.0}} & 68.7 & 79.8  & 86.1 \\
    % \multicolumn{1}{l|}{BM25} & - & 59.1 & 73.7  \\
    % \multicolumn{1}{l|}{BERT} & - & 78.4 & 85.3\\
    % \multicolumn{1}{l|}{RoBERTa} & 64.9 & 76.8 & 84.7  \\
    % \multicolumn{1}{l|}{ERNIE2.0} & 68.7 & 79.8  & 86.1 \\
    \midrule
    \multicolumn{1}{l|}{MSM (ours)} & \textbf{69.3}  & \textbf{80.3} & \textbf{86.5}  \\
    \bottomrule
    \end{tabular}
}
\caption{Monolingual Retrieval performance on Natural Question test. The best performing models are marked bold and the results unavailable are left blank.}
\label{ap:mono}
\end{table*}


