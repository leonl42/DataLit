\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Artetxe \& Schwenk(2019)Artetxe and Schwenk]{artetxe2019massively}
Mikel Artetxe and Holger Schwenk.
\newblock Massively multilingual sentence embeddings for zero-shot
  cross-lingual transfer and beyond.
\newblock \emph{TACL}, 2019.

\bibitem[Artetxe et~al.(2020{\natexlab{a}})Artetxe, Ruder, and
  Yogatama]{artetxe-etal-2020-cross}
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
\newblock On the cross-lingual transferability of monolingual representations.
\newblock In \emph{ACL}, 2020{\natexlab{a}}.

\bibitem[Artetxe et~al.(2020{\natexlab{b}})Artetxe, Ruder, Yogatama, Labaka,
  and Agirre]{artetxe2020call}
Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka, and Eneko Agirre.
\newblock A call for more rigor in unsupervised cross-lingual learning.
\newblock In \emph{ACL}, pp.\  7375--7388, 2020{\natexlab{b}}.

\bibitem[Asai et~al.(2019)Asai, Hashimoto, Hajishirzi, Socher, and
  Xiong]{asai2019learning}
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming
  Xiong.
\newblock Learning to retrieve reasoning paths over wikipedia graph for
  question answering.
\newblock \emph{arXiv preprint arXiv:1911.10470}, 2019.

\bibitem[Asai et~al.(2021{\natexlab{a}})Asai, Kasai, Clark, Lee, Choi, and
  Hajishirzi]{asai-etal-2021-xor}
Akari Asai, Jungo Kasai, Jonathan Clark, Kenton Lee, Eunsol Choi, and Hannaneh
  Hajishirzi.
\newblock {XOR} {QA}: Cross-lingual open-retrieval question answering.
\newblock In \emph{NAACL}, 2021{\natexlab{a}}.

\bibitem[Asai et~al.(2021{\natexlab{b}})Asai, Yu, Kasai, and
  Hajishirzi]{asai2021one}
Akari Asai, Xinyan Yu, Jungo Kasai, and Hanna Hajishirzi.
\newblock One question answering model for many languages with cross-lingual
  dense passage retrieval.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 7547--7560, 2021{\natexlab{b}}.

\bibitem[Botha et~al.(2020)Botha, Shan, and Gillick]{botha-etal-2020-entity}
Jan~A. Botha, Zifei Shan, and Daniel Gillick.
\newblock {E}ntity {L}inking in 100 {L}anguages.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Cao et~al.(2022)Cao, Wang, Liang, Gao, Zheng, Ren, and
  Wang]{cao-etal-2022-exploring}
Rui Cao, Yihao Wang, Yuxin Liang, Ling Gao, Jie Zheng, Jie Ren, and Zheng Wang.
\newblock Exploring the impact of negative samples of contrastive learning: A
  case study of sentence embedding.
\newblock In \emph{Findings of ACL}, 2022.

\bibitem[Chang et~al.(2020)Chang, Yu, Chang, Yang, and Kumar]{chang2020pre}
Wei{-}Cheng Chang, Felix~X. Yu, Yin{-}Wen Chang, Yiming Yang, and Sanjiv Kumar.
\newblock Pre-training tasks for embedding-based large-scale retrieval.
\newblock In \emph{ICLR}, 2020.

\bibitem[Chen et~al.(2021)Chen, Xie, and He]{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Chi et~al.(2021)Chi, Dong, Wei, Yang, Singhal, Wang, Song, Mao, Huang,
  and Zhou]{chi-etal-2021-infoxlm}
Zewen Chi, Li~Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song,
  Xian-Ling Mao, Heyan Huang, and Ming Zhou.
\newblock {I}nfo{XLM}: An information-theoretic framework for cross-lingual
  language model pre-training.
\newblock In \emph{NAACL}, 2021.

\bibitem[Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov]{conneau2019unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock \emph{arXiv preprint arXiv:1911.02116}, 2019.

\bibitem[Conneau et~al.(2020)Conneau, Wu, Li, Zettlemoyer, and
  Stoyanov]{conneau-etal-2020-emerging}
Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Emerging cross-lingual structure in pretrained language models.
\newblock In \emph{ACL}, 2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Dong et~al.(2022)Dong, Ni, Bikel, Alfonseca, Wang, Qu, and
  Zitouni]{dong2022exploring}
Zhe Dong, Jianmo Ni, Dan Bikel, Enrique Alfonseca, Yuan Wang, Chen Qu, and Imed
  Zitouni.
\newblock Exploring dual encoder architectures for question answering.
\newblock \emph{arXiv preprint arXiv:2204.07120}, 2022.

\bibitem[Feng et~al.(2022)Feng, Yang, Cer, Arivazhagan, and
  Wang]{feng-etal-2022-language}
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.
\newblock Language-agnostic {BERT} sentence embedding.
\newblock In \emph{ACL}, 2022.

\bibitem[Ferro \& Silvello(2015)Ferro and Silvello]{ferro2015clef}
Nicola Ferro and Gianmaria Silvello.
\newblock Clef 2000-2014: Lessons learnt from ad hoc retrieval.
\newblock In \emph{IIR}, 2015.

\bibitem[Gao \& Callan(2021)Gao and Callan]{gao2021cocondenser}
Luyu Gao and Jamie Callan.
\newblock Unsupervised corpus aware language model pre-training for dense
  passage retrieval.
\newblock \emph{arXiv preprint arXiv:2108.05540}, 2021.

\bibitem[Gao et~al.(2021)Gao, Yao, and Chen]{gao-etal-2021-simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
\newblock {S}im{CSE}: Simple contrastive learning of sentence embeddings.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Giorgi et~al.(2021)Giorgi, Nitski, Wang, and
  Bader]{giorgi-etal-2021-declutr}
John Giorgi, Osvald Nitski, Bo~Wang, and Gary Bader.
\newblock {D}e{CLUTR}: Deep contrastive learning for unsupervised textual
  representations.
\newblock In \emph{ACL}, 2021.

\bibitem[Hu et~al.(2020)Hu, Ruder, Siddhant, Neubig, Firat, and
  Johnson]{hu2020xtreme}
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and
  Melvin Johnson.
\newblock Xtreme: A massively multilingual multi-task benchmark for evaluating
  cross-lingual generalisation.
\newblock In \emph{ICML}, pp.\  4411--4421. PMLR, 2020.

\bibitem[Huang et~al.(2019)Huang, Liang, Duan, Gong, Shou, Jiang, and
  Zhou]{huang2019unicoder}
Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and
  Ming Zhou.
\newblock Unicoder: A universal language encoder by pre-training with multiple
  cross-lingual tasks.
\newblock \emph{arXiv preprint arXiv:1909.00964}, 2019.

\bibitem[Huang et~al.(2022)Huang, Zhai, and Ji]{huang2022concrete}
Kung-Hsiang Huang, ChengXiang Zhai, and Heng Ji.
\newblock Concrete: Improving cross-lingual fact-checking with cross-lingual
  retrieval.
\newblock \emph{arXiv preprint arXiv:2209.02071}, 2022.

\bibitem[Iter et~al.(2020)Iter, Guu, Lansing, and
  Jurafsky]{iter-etal-2020-pretraining}
Dan Iter, Kelvin Guu, Larry Lansing, and Dan Jurafsky.
\newblock Pretraining with contrastive sentence objectives improves discourse
  performance of language models.
\newblock In \emph{ACL}, 2020.

\bibitem[Izacard et~al.(2021{\natexlab{a}})Izacard, Caron, Hosseini, Riedel,
  Bojanowski, Joulin, and Grave]{izacard2021towards}
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr
  Bojanowski, Armand Joulin, and Edouard Grave.
\newblock Towards unsupervised dense information retrieval with contrastive
  learning.
\newblock \emph{arXiv preprint arXiv:2112.09118}, 2021{\natexlab{a}}.

\bibitem[Izacard et~al.(2021{\natexlab{b}})Izacard, Caron, Hosseini, Riedel,
  Bojanowski, Joulin, and Grave]{izacard2021unsupervised}
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr
  Bojanowski, Armand Joulin, and Edouard Grave.
\newblock Unsupervised dense information retrieval with contrastive learning.
\newblock \emph{arXiv preprint arXiv:2112.09118}, 2021{\natexlab{b}}.

\bibitem[Jiang et~al.(2022)Jiang, Liang, Chen, and Duan]{jiang2022xlm}
Xiaoze Jiang, Yaobo Liang, Weizhu Chen, and Nan Duan.
\newblock Xlm-k: Improving cross-lingual language model pre-training with
  multilingual knowledge.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  10840--10848, 2022.

\bibitem[Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen,
  and Yih]{DPR2020}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.~H. Lewis, Ledell Wu,
  Sergey Edunov, Danqi Chen, and Wen{-}tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins,
  Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, Toutanova, Jones, Kelcey,
  Chang, Dai, Uszkoreit, Le, and Petrov]{kwiatkowski-etal-2019-natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
  Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang,
  Andrew~M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: A benchmark for question answering research.
\newblock \emph{TACL}, 2019.

\bibitem[Lample \& Conneau(2019)Lample and Conneau]{lample2019cross}
Guillaume Lample and Alexis Conneau.
\newblock Cross-lingual language model pretraining.
\newblock \emph{arXiv preprint arXiv:1901.07291}, 2019.

\bibitem[Lee et~al.(2019)Lee, Chang, and Toutanova]{lee-etal-2019-latent}
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
\newblock Latent retrieval for weakly supervised open domain question
  answering.
\newblock In \emph{ACL}, 2019.

\bibitem[Li et~al.(2020)Li, Gao, Li, Peng, Li, Zhang, and
  Gao]{li-etal-2020-optimus}
Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, and
  Jianfeng Gao.
\newblock Optimus: Organizing sentences via pre-trained modeling of a latent
  space.
\newblock In \emph{EMNLP}, November 2020.

\bibitem[Li et~al.(2021)Li, Franz, Sultan, Iyer, Lee, and Sil]{li2021learning}
Yulong Li, Martin Franz, Md~Arafat Sultan, Bhavani Iyer, Young-Suk Lee, and
  Avirup Sil.
\newblock Learning cross-lingual ir from an english retriever.
\newblock \emph{arXiv preprint arXiv:2112.08185}, 2021.

\bibitem[Liang et~al.(2020)Liang, Duan, Gong, Wu, Guo, Qi, Gong, Shou, Jiang,
  Cao, Fan, Zhang, Agrawal, Cui, Wei, Bharti, Qiao, Chen, Wu, Liu, Yang,
  Campos, Majumder, and Zhou]{liang-etal-2020-xglue}
Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong,
  Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul
  Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen,
  Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming
  Zhou.
\newblock {XGLUE}: A new benchmark datasetfor cross-lingual pre-training,
  understanding and generation.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Litschko et~al.(2021)Litschko, Vuli{\'c}, Ponzetto, and
  Glava{\v{s}}]{litschko2021evaluating}
Robert Litschko, Ivan Vuli{\'c}, Simone~Paolo Ponzetto, and Goran Glava{\v{s}}.
\newblock Evaluating multilingual text encoders for unsupervised cross-lingual
  retrieval.
\newblock In \emph{ECIR}, pp.\  342--358. Springer, 2021.

\bibitem[Litschko et~al.(2022)Litschko, Vuli{\'c}, and
  Glava{\v{s}}]{litschko2022parameter}
Robert Litschko, Ivan Vuli{\'c}, and Goran Glava{\v{s}}.
\newblock Parameter-efficient neural reranking for cross-lingual and
  multilingual retrieval.
\newblock \emph{arXiv preprint arXiv:2204.02292}, 2022.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{roberta2019}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{CoRR}, abs/1907.11692, 2019.

\bibitem[Liu \& Shao(2022)Liu and Shao]{liu2022retromae}
Zheng Liu and Yingxia Shao.
\newblock Retromae: Pre-training retrieval-oriented transformers via masked
  auto-encoder.
\newblock \emph{arXiv preprint arXiv:2205.12035}, 2022.

\bibitem[Lu et~al.(2021)Lu, He, Xiong, Ke, Malik, Dou, Bennett, Liu, and
  Overwijk]{lu-etal-2021-less}
Shuqi Lu, Di~He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul
  Bennett, Tie-Yan Liu, and Arnold Overwijk.
\newblock Less is more: Pretrain a strong {S}iamese encoder for dense text
  retrieval using a weak decoder.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Ma et~al.(2022)Ma, Guo, Zhang, Fan, and Cheng]{ma2022pre}
Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, and Xueqi Cheng.
\newblock Pre-train a discriminative text encoder for dense retrieval via
  contrastive span prediction.
\newblock \emph{arXiv preprint arXiv:2204.10641}, 2022.

\bibitem[Ma et~al.(2021)Ma, Dou, Xu, Zhang, Jiang, Cao, and Wen]{ma2021pre}
Zhengyi Ma, Zhicheng Dou, Wei Xu, Xinyu Zhang, Hao Jiang, Zhao Cao, and Ji-Rong
  Wen.
\newblock Pre-training for ad-hoc retrieval: hyperlink is also you need.
\newblock In \emph{CIKM}, pp.\  1212--1221, 2021.

\bibitem[Nie(2010)]{nie2010cross}
Jian-Yun Nie.
\newblock Cross-language information retrieval.
\newblock \emph{Synthesis Lectures on Human Language Technologies}, 3\penalty0
  (1):\penalty0 1--125, 2010.

\bibitem[O{\u{g}}uz et~al.(2021)O{\u{g}}uz, Lakhotia, Gupta, Lewis, Karpukhin,
  Piktus, Chen, Riedel, Yih, Gupta, et~al.]{ouguz2021dprpaq}
Barlas O{\u{g}}uz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir
  Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Wen-tau Yih,
  Sonal Gupta, et~al.
\newblock Domain-matched pre-training tasks for dense retrieval.
\newblock \emph{arXiv preprint arXiv:2107.13602}, 2021.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Ouyang et~al.(2021)Ouyang, Wang, Pang, Sun, Tian, Wu, and
  Wang]{ouyang-etal-2021-ernie}
Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu~Sun, Hao Tian, Hua Wu, and Haifeng
  Wang.
\newblock {ERNIE}-{M}: Enhanced multilingual representation by aligning
  cross-lingual semantics with monolingual corpora.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Pfeiffer et~al.(2020)Pfeiffer, Vuli{\'c}, Gurevych, and
  Ruder]{pfeiffer2020mad}
Jonas Pfeiffer, Ivan Vuli{\'c}, Iryna Gurevych, and Sebastian Ruder.
\newblock Mad-x: An adapter-based framework for multi-task cross-lingual
  transfer.
\newblock In \emph{EMNLP}, pp.\  7654--7673, 2020.

\bibitem[Qu et~al.(2021)Qu, Ding, Liu, Liu, Ren, Zhao, Dong, Wu, and
  Wang]{qu-etal-2021-rocketqa}
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne~Xin Zhao, Daxiang
  Dong, Hua Wu, and Haifeng Wang.
\newblock {R}ocket{QA}: An optimized training approach to dense passage
  retrieval for open-domain question answering.
\newblock In \emph{NAACL}, 2021.

\bibitem[Reddy et~al.(2021)Reddy, Yadav, Sultan, Franz, Castelli, Ji, and
  Sil]{reddy2021towards}
Revanth~Gangi Reddy, Vikas Yadav, Md~Arafat Sultan, Martin Franz, Vittorio
  Castelli, Heng Ji, and Avirup Sil.
\newblock Towards robust neural retrieval models with synthetic pre-training.
\newblock \emph{arXiv preprint arXiv:2104.07800}, 2021.

\bibitem[Reimers \& Gurevych(2020)Reimers and
  Gurevych]{reimers-gurevych-2020-making}
Nils Reimers and Iryna Gurevych.
\newblock Making monolingual sentence embeddings multilingual using knowledge
  distillation.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Resnik(1998)]{resnik-1998-parallel}
Philip Resnik.
\newblock Parallel strands: a preliminary investigation into mining the web for
  bilingual text.
\newblock In \emph{Proceedings of the Third Conference of the Association for
  Machine Translation in the Americas: Technical Papers}, 1998.

\bibitem[Rogers et~al.(2020)Rogers, Kovaleva, and Rumshisky]{rogers2020primer}
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
\newblock A primer in bertology: What we know about how bert works.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  2020.

\bibitem[Roy et~al.(2020)Roy, Constant, Al-Rfou, Barua, Phillips, and
  Yang]{roy-etal-2020-lareqa}
Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips, and Yinfei
  Yang.
\newblock {LAR}e{QA}: Language-agnostic answer retrieval from a multilingual
  pool.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, 2020.

\bibitem[Ruder et~al.(2021)Ruder, Constant, Botha, Siddhant, Firat, Fu, Liu,
  Hu, Garrette, Neubig, and Johnson]{ruder-etal-2021-xtreme}
Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan
  Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson.
\newblock {XTREME}-{R}: Towards more challenging and nuanced multilingual
  evaluation.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Sagen(2021)]{Sagen1545786}
Markus Sagen.
\newblock Large-context question answering with cross-lingual transfer.
\newblock Master's thesis, Uppsala University, Department of Information
  Technology, 2021.

\bibitem[Santra et~al.(2021)Santra, Anusha, and
  Goyal]{santra-etal-2021-hierarchical}
Bishal Santra, Potnuru Anusha, and Pawan Goyal.
\newblock Hierarchical transformer for task oriented dialog systems.
\newblock In \emph{NAACL}, June 2021.

\bibitem[Shi et~al.(2021)Shi, Zhang, Bai, and Lin]{shi-etal-2021-cross}
Peng Shi, Rui Zhang, He~Bai, and Jimmy Lin.
\newblock Cross-lingual training of dense retrievers for document retrieval.
\newblock In \emph{Proceedings of the 1st Workshop on Multilingual
  Representation Learning}, 2021.

\bibitem[Sun \& Duh(2020)Sun and Duh]{sun-duh-2020-clirmatrix}
Shuo Sun and Kevin Duh.
\newblock {CLIRM}atrix: A massively large collection of bilingual and
  multilingual datasets for cross-lingual information retrieval.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Sun et~al.(2020)Sun, Wang, Li, Feng, Tian, Wu, and
  Wang]{sun2020ernie2.0}
Yu~Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng
  Wang.
\newblock Ernie 2.0: A continual pre-training framework for language
  understanding.
\newblock In \emph{AAAI}, pp.\  8968--8975, 2020.

\bibitem[Thompson \& Koehn(2020)Thompson and
  Koehn]{thompson-koehn-2020-exploiting}
Brian Thompson and Philipp Koehn.
\newblock Exploiting sentence order in document alignment.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Wei et~al.(2020)Wei, Weng, Hu, Xing, Yu, and Luo]{wei2020learning}
Xiangpeng Wei, Rongxiang Weng, Yue Hu, Luxi Xing, Heng Yu, and Weihua Luo.
\newblock On learning universal representations across languages.
\newblock \emph{arXiv preprint arXiv:2007.15960}, 2020.

\bibitem[Wenzek et~al.(2019)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n,
  Joulin, and Grave]{wenzek2019ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary,
  Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl
  data.
\newblock \emph{arXiv preprint arXiv:1911.00359}, 2019.

\bibitem[Wu et~al.(2022{\natexlab{a}})Wu, Liang, Ren, Shou, Duan, Gong, and
  Jiang]{wu2022unsupervised}
Ning Wu, Yaobo Liang, Houxing Ren, Linjun Shou, Nan Duan, Ming Gong, and Daxin
  Jiang.
\newblock Unsupervised context aware sentence representation pretraining for
  multi-lingual dense retrieval.
\newblock \emph{arXiv preprint arXiv:2206.03281}, 2022{\natexlab{a}}.

\bibitem[Wu et~al.(2022{\natexlab{b}})Wu, Ma, Lin, Lin, Wang, and
  Hu]{wu2022contextual}
Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, and Songlin Hu.
\newblock Contextual mask auto-encoder for dense passage retrieval.
\newblock \emph{arXiv preprint arXiv:2208.07670}, 2022{\natexlab{b}}.

\bibitem[Xiong et~al.(2020)Xiong, Xiong, Li, Tang, Liu, Bennett, Ahmed, and
  Overwijk]{ance2020}
Lee Xiong, Chenyan Xiong, Ye~Li, Kwok{-}Fung Tang, Jialin Liu, Paul Bennett,
  Junaid Ahmed, and Arnold Overwijk.
\newblock Approximate nearest neighbor negative contrastive learning for dense
  text retrieval.
\newblock \emph{CoRR}, abs/2007.00808, 2020.

\bibitem[Yan et~al.(2013)Yan, Guo, Lan, and Cheng]{yan2013biterm}
Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng.
\newblock A biterm topic model for short texts.
\newblock In \emph{Proceedings of the 22nd international conference on World
  Wide Web}, pp.\  1445--1456, 2013.

\bibitem[Yang et~al.(2017)Yang, Fang, and Lin]{yang2017anserini}
Peilin Yang, Hui Fang, and Jimmy Lin.
\newblock Anserini: Enabling the use of lucene for information retrieval
  research.
\newblock In \emph{Proceedings of the 40th International ACM SIGIR Conference
  on Research and Development in Information Retrieval}, pp.\  1253--1256,
  2017.

\bibitem[Yang et~al.(2019)Yang, Cer, Ahmad, Guo, Law, Constant, Abrego, Yuan,
  Tar, Sung, et~al.]{yang2019multilingual}
Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant,
  Gustavo~Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, et~al.
\newblock Multilingual universal sentence encoder for semantic retrieval.
\newblock \emph{arXiv preprint arXiv:1907.04307}, 2019.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Gong, Shen, Li, Lv, Duan, and
  Chen]{zhang2021poolingformer}
Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and
  Weizhu Chen.
\newblock Poolingformer: Long document modeling with pooling attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12437--12446. PMLR, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2019)Zhang, Wei, and Zhou]{zhang-etal-2019-hibert}
Xingxing Zhang, Furu Wei, and Ming Zhou.
\newblock {HIBERT}: Document level pre-training of hierarchical bidirectional
  transformers for document summarization.
\newblock In \emph{ACL}, 2019.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Ma, Shi, and
  Lin]{zhang-etal-2021-mr}
Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin.
\newblock Mr. {T}y{D}i: A multi-lingual benchmark for dense retrieval.
\newblock In \emph{Proceedings of the 1st Workshop on Multilingual
  Representation Learning}, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Ogueji, Ma, and Lin]{zhang2022towards}
Xinyu Zhang, Kelechi Ogueji, Xueguang Ma, and Jimmy Lin.
\newblock Towards best practices for training multilingual dense retrieval
  models.
\newblock \emph{arXiv preprint arXiv:2204.02363}, 2022.

\bibitem[Zhang et~al.(2021{\natexlab{c}})Zhang, He, Liu, Bing, and
  Li]{zhang2021bootstrapped}
Yan Zhang, Ruidan He, Zuozhu Liu, Lidong Bing, and Haizhou Li.
\newblock Bootstrapped unsupervised sentence representation learning.
\newblock In \emph{ACL}, 2021{\natexlab{c}}.

\bibitem[Zhou et~al.(2022)Zhou, Li, Shang, Luo, Zhan, Hu, Zhang, Jiang, Cao,
  Yu, Jiang, Liu, and Chen]{zhou-etal-2022-hyperlink}
Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Lan Luo, Ke~Zhan, Enrui Hu, Xinyu
  Zhang, Hao Jiang, Zhao Cao, Fan Yu, Xin Jiang, Qun Liu, and Lei Chen.
\newblock Hyperlink-induced pre-training for passage retrieval in open-domain
  question answering.
\newblock In \emph{ACL}, 2022.

\end{thebibliography}
