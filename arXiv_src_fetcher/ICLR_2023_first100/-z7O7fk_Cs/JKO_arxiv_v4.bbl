\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdal et~al.(2021)Abdal, Zhu, Mitra, and Wonka]{abdal2021styleflow}
Rameen Abdal, Peihao Zhu, Niloy~J Mitra, and Peter Wonka.
\newblock Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows.
\newblock \emph{ACM Transactions on Graphics (ToG)}, 40\penalty0 (3):\penalty0 1--21, 2021.

\bibitem[Albergo and Vanden-Eijnden(2023)]{albergo2023building}
Michael~Samuel Albergo and Eric Vanden-Eijnden.
\newblock Building normalizing flows with stochastic interpolants.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Alvarez-Melis et~al.(2022)Alvarez-Melis, Schiff, and Mroueh]{alvarez-melis2022optimizing}
David Alvarez-Melis, Yair Schiff, and Youssef Mroueh.
\newblock Optimizing functionals on the space of probabilities with input convex neural networks.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.

\bibitem[Amos et~al.(2017)Amos, Xu, and Kolter]{amos2017input}
Brandon Amos, Lei Xu, and J~Zico Kolter.
\newblock Input convex neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages 146--155. PMLR, 2017.

\bibitem[Arcones and Gine(1992)]{arcones1992bootstrap}
Miguel~A Arcones and Evarist Gine.
\newblock On the bootstrap of u and v statistics.
\newblock \emph{The Annals of Statistics}, pages 655--674, 1992.

\bibitem[Bailey and Telgarsky(2018)]{bailey2018size}
Bolton Bailey and Matus~J Telgarsky.
\newblock Size-noise tradeoffs in generative networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Barron(1993)]{barron1993universal}
Andrew~R Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal function.
\newblock \emph{IEEE Transactions on Information theory}, 39\penalty0 (3):\penalty0 930--945, 1993.

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and Jacobsen]{iResnet}
Jens Behrmann, Will Grathwohl, Ricky~TQ Chen, David Duvenaud, and J{\"o}rn-Henrik Jacobsen.
\newblock Invertible residual networks.
\newblock In \emph{International Conference on Machine Learning}, pages 573--582. PMLR, 2019.

\bibitem[Block et~al.(2020)Block, Mroueh, and Rakhlin]{block2020generative}
Adam Block, Youssef Mroueh, and Alexander Rakhlin.
\newblock Generative modeling with denoising auto-encoders and langevin sampling.
\newblock \emph{arXiv preprint arXiv:2002.00107}, 2020.

\bibitem[Boffi and Vanden-Eijnden(2023)]{boffi2023probability}
Nicholas~M Boffi and Eric Vanden-Eijnden.
\newblock Probability flow solution of the fokker--planck equation.
\newblock \emph{Machine Learning: Science and Technology}, 4\penalty0 (3):\penalty0 035012, 2023.

\bibitem[Bolley et~al.(2012)Bolley, Gentil, and Guillin]{bolley2012convergence}
Fran{\c{c}}ois Bolley, Ivan Gentil, and Arnaud Guillin.
\newblock Convergence to equilibrium in wasserstein distance for fokker--planck equations.
\newblock \emph{Journal of Functional Analysis}, 263\penalty0 (8):\penalty0 2430--2457, 2012.

\bibitem[Bunne et~al.(2022)Bunne, Papaxanthos, Krause, and Cuturi]{bunne2022proximal}
Charlotte Bunne, Laetitia Papaxanthos, Andreas Krause, and Marco Cuturi.
\newblock Proximal optimal transport modeling of population dynamics.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 6511--6528. PMLR, 2022.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{chen2018neural}
Ricky~TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chen et~al.(2019)Chen, Behrmann, Duvenaud, and Jacobsen]{ResFlow}
Ricky~TQ Chen, Jens Behrmann, David~K Duvenaud, and J{\"o}rn-Henrik Jacobsen.
\newblock Residual flows for invertible generative modeling.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Cheng et~al.(2023)Cheng, Lu, Tan, and Xie]{cheng2023convergence}
Xiuyuan Cheng, Jianfeng Lu, Yixin Tan, and Yao Xie.
\newblock Convergence of flow-based generative models via proximal gradient descent in wasserstein space.
\newblock \emph{arXiv preprint arXiv:2310.17582}, 2023.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and Vandergheynst]{Chebnet}
Micha{\"e}l Defferrard, Xavier Bresson, and Pierre Vandergheynst.
\newblock Convolutional neural networks on graphs with fast localized spectral filtering.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Degond and Mas-Gallic(1989)]{degond1989weighted}
Pierre Degond and S~Mas-Gallic.
\newblock The weighted particle method for convection-diffusion equations. i. the case of an isotropic viscosity.
\newblock \emph{Mathematics of computation}, 53\penalty0 (188):\penalty0 485--507, 1989.

\bibitem[Degond and Mustieles(1990)]{degond1990deterministic}
Pierre Degond and Francisco-Jos{\'e} Mustieles.
\newblock A deterministic approximation of diffusion equations using particles.
\newblock \emph{SIAM Journal on Scientific and Statistical Computing}, 11\penalty0 (2):\penalty0 293--310, 1990.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem[Deng(2012)]{deng2012mnist}
Li~Deng.
\newblock The mnist database of handwritten digit images for machine learning research.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0 141--142, 2012.

\bibitem[Dinh et~al.(2015)Dinh, Krueger, and Bengio]{dinh2015nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock {NICE:} non-linear independent components estimation.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings}, 2015.

\bibitem[Dinh et~al.(2017)Dinh, Sohl{-}Dickstein, and Bengio]{RNVP}
Laurent Dinh, Jascha Sohl{-}Dickstein, and Samy Bengio.
\newblock Density estimation using real {NVP}.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}. OpenReview.net, 2017.

\bibitem[Esser et~al.(2021)Esser, Rombach, and Ommer]{esser2021taming}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 12873--12883, 2021.

\bibitem[Fan et~al.(2022)Fan, Zhang, Taghvaei, and Chen]{fan2022variational}
Jiaojiao Fan, Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin Chen.
\newblock Variational {W}asserstein gradient flow.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning}, pages 6185--6215. PMLR, 2022.

\bibitem[Fey and Lenssen(2019)]{Fey/Lenssen/2019}
Matthias Fey and Jan~E. Lenssen.
\newblock Fast graph representation learning with {PyTorch Geometric}.
\newblock In \emph{ICLR Workshop on Representation Learning on Graphs and Manifolds}, 2019.

\bibitem[Finlay et~al.(2020)Finlay, Jacobsen, Nurbekyan, and Oberman]{finlay2020train}
Chris Finlay, J{\"o}rn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman.
\newblock How to train your neural ode: the world of jacobian and kinetic regularization.
\newblock In \emph{International conference on machine learning}, pages 3154--3164. PMLR, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{GAN}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron~C. Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{NIPS}, 2014.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Chen, Bettencourt, and Duvenaud]{FFJORD}
Will Grathwohl, Ricky T.~Q. Chen, Jesse Bettencourt, and David Duvenaud.
\newblock Scalable reversible generative models with free-form continuous dynamics.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Gretton et~al.(2012{\natexlab{a}})Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and Smola]{Gretton2012AKT}
Arthur Gretton, Karsten~M. Borgwardt, Malte~J. Rasch, Bernhard Sch{\"o}lkopf, and Alex Smola.
\newblock A kernel two-sample test.
\newblock \emph{J. Mach. Learn. Res.}, 13:\penalty0 723--773, 2012{\natexlab{a}}.

\bibitem[Gretton et~al.(2012{\natexlab{b}})Gretton, Sejdinovic, Strathmann, Balakrishnan, Pontil, Fukumizu, and Sriperumbudur]{gretton2012optimal}
Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, and Bharath~K Sriperumbudur.
\newblock Optimal kernel choice for large-scale two-sample tests.
\newblock \emph{Advances in neural information processing systems}, 25, 2012{\natexlab{b}}.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and Courville]{WassersteinGAN}
Ishaan Gulrajani, Faruk Ahmed, Mart{\'i}n Arjovsky, Vincent Dumoulin, and Aaron~C. Courville.
\newblock Improved training of wasserstein gans.
\newblock In \emph{NIPS}, 2017.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Huang et~al.(2021)Huang, Lim, and Courville]{huang2021variational}
Chin-Wei Huang, Jae~Hyun Lim, and Aaron~C Courville.
\newblock A variational perspective on diffusion-based generative models and score matching.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 22863--22876, 2021.

\bibitem[Huang et~al.(2023)Huang, Yu, Chen, and Lai]{huang2023bridging}
Han Huang, Jiajia Yu, Jie Chen, and Rongjie Lai.
\newblock Bridging mean-field games and normalizing flows with trajectory regularization.
\newblock \emph{Journal of Computational Physics}, page 112155, 2023.

\bibitem[Hutchinson(1989)]{Hutchinson1989ASE}
Michael~F. Hutchinson.
\newblock A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines.
\newblock \emph{Communications in Statistics - Simulation and Computation}, 18:\penalty0 1059--1076, 1989.

\bibitem[Isola et~al.(2017)Isola, Zhu, Zhou, and Efros]{CGAN}
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei~A. Efros.
\newblock Image-to-image translation with conditional adversarial networks.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 5967--5976, 2017.

\bibitem[Johnson and Zhang(2019)]{johnson2019framework}
Rie Johnson and Tong Zhang.
\newblock A framework of composite functional gradient methods for generative adversarial models.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 43\penalty0 (1):\penalty0 17--32, 2019.

\bibitem[Jordan et~al.(1998)Jordan, Kinderlehrer, and Otto]{jordan1998variational}
Richard Jordan, David Kinderlehrer, and Felix Otto.
\newblock The variational formulation of the fokker--planck equation.
\newblock \emph{SIAM journal on mathematical analysis}, 29\penalty0 (1):\penalty0 1--17, 1998.

\bibitem[Kingma and Ba(2015)]{Kingma2015AdamAM}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem[Kingma and Welling(2014)]{VAE}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings}, 2014.

\bibitem[Kingma and Welling(2019)]{VAE_review}
Diederik~P. Kingma and Max Welling.
\newblock An introduction to variational autoencoders.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 12\penalty0 (4):\penalty0 307--392, 2019.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Kobyzev et~al.(2020)Kobyzev, Prince, and Brubaker]{nflow_review}
Ivan Kobyzev, Simon~JD Prince, and Marcus~A Brubaker.
\newblock Normalizing flows: An introduction and review of current methods.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 43\penalty0 (11):\penalty0 3964--3979, 2020.

\bibitem[Korotin et~al.(2021)Korotin, Egiazarian, Asadulaev, Safin, and Burnaev]{korotin2021wasserstein}
Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev.
\newblock Wasserstein-2 generative networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~0, University of Toronto, Toronto, Ontario, 2009.

\bibitem[Lee et~al.(2017)Lee, Ge, Ma, Risteski, and Arora]{lee2017ability}
Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora.
\newblock On the ability of neural nets to express distributions.
\newblock In \emph{Conference on Learning Theory}, pages 1271--1296. PMLR, 2017.

\bibitem[Lipman et~al.(2023)Lipman, Chen, Ben-Hamu, Nickel, and Le]{lipman2023flow}
Yaron Lipman, Ricky T.~Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le.
\newblock Flow matching for generative modeling.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Liu et~al.(2019)Liu, Kumar, Ba, Kiros, and Swersky]{liu2019graph}
Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky.
\newblock Graph normalizing flows.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Liu and Wang(2016)]{liu2016stein}
Qiang Liu and Dilin Wang.
\newblock Stein variational gradient descent: A general purpose bayesian inference algorithm.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Liutkus et~al.(2019)Liutkus, Simsekli, Majewski, Durmus, and St{\"o}ter]{liutkus2019sliced}
Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert St{\"o}ter.
\newblock Sliced-wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions.
\newblock In \emph{International Conference on Machine Learning}, pages 4104--4113. PMLR, 2019.

\bibitem[Lu and Lu(2020)]{lu2020universal}
Yulong Lu and Jianfeng Lu.
\newblock A universal approximation theorem of deep neural networks for expressing probability distributions.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 3094--3105, 2020.

\bibitem[Lucas et~al.(2019)Lucas, Tucker, Grosse, and Norouzi]{Lucas2019UnderstandingPC}
James Lucas, G.~Tucker, Roger~B. Grosse, and Mohammad Norouzi.
\newblock Understanding posterior collapse in generative latent variable models.
\newblock In \emph{DGS@ICLR}, 2019.

\bibitem[Maoutsa et~al.(2020)Maoutsa, Reich, and Opper]{maoutsa2020interacting}
Dimitra Maoutsa, Sebastian Reich, and Manfred Opper.
\newblock Interacting particle solutions of fokker--planck equations through gradient--log--density estimation.
\newblock \emph{Entropy}, 22\penalty0 (8):\penalty0 802, 2020.

\bibitem[Mathieu and Nickel(2020)]{mathieu2020riemannian}
Emile Mathieu and Maximilian Nickel.
\newblock Riemannian continuous normalizing flows.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 2503--2515, 2020.

\bibitem[Mokrov et~al.(2021)Mokrov, Korotin, Li, Genevay, Solomon, and Burnaev]{mokrov2021large}
Petr Mokrov, Alexander Korotin, Lingxiao Li, Aude Genevay, Justin~M Solomon, and Evgeny Burnaev.
\newblock Large-scale wasserstein gradient flows.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 15243--15256, 2021.

\bibitem[Onken et~al.(2021)Onken, Wu~Fung, Li, and Ruthotto]{OT-Flow}
Derek Onken, S~Wu~Fung, Xingjian Li, and Lars Ruthotto.
\newblock Ot-flow: Fast and accurate continuous normalizing flows via optimal transport.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, 2021.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and Murray]{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems 32}, pages 8024--8035. Curran Associates, Inc., 2019.

\bibitem[Perekrestenko et~al.(2020)Perekrestenko, M{\"u}ller, and B{\"o}lcskei]{perekrestenko2020constructive}
Dmytro Perekrestenko, Stephan M{\"u}ller, and Helmut B{\"o}lcskei.
\newblock Constructive universal high-dimensional distribution generation through deep relu networks.
\newblock In \emph{International Conference on Machine Learning}, pages 7610--7619. PMLR, 2020.

\bibitem[Perekrestenko et~al.(2021)Perekrestenko, Eberhard, and B{\"o}lcskei]{perekrestenko2021high}
Dmytro Perekrestenko, L{\'e}andre Eberhard, and Helmut B{\"o}lcskei.
\newblock High-dimensional distribution generation through deep neural networks.
\newblock \emph{Partial Differential Equations and Applications}, 2\penalty0 (5):\penalty0 1--44, 2021.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Rout et~al.(2022)Rout, Korotin, and Burnaev]{rout2022generative}
Litu Rout, Alexander Korotin, and Evgeny Burnaev.
\newblock Generative modeling with optimal transport maps.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Ruthotto et~al.(2020)Ruthotto, Osher, Li, Nurbekyan, and Fung]{ruthotto2020machine}
Lars Ruthotto, Stanley~J Osher, Wuchen Li, Levon Nurbekyan, and Samy~Wu Fung.
\newblock A machine learning framework for solving high-dimensional mean field game and mean field control problems.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (17):\penalty0 9183--9193, 2020.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford, Chen, and Chen]{Salimans2016ImprovedTF}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi~Chen, and Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~29. Curran Associates, Inc., 2016.

\bibitem[Schrab et~al.(2023)Schrab, Kim, Albert, Laurent, Guedj, and Gretton]{schrab2023MMD}
Antonin Schrab, Ilmun Kim, MÃ©lisande Albert, BÃ©atrice Laurent, Benjamin Guedj, and Arthur Gretton.
\newblock Mmd aggregated two-sample test.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (194):\penalty0 1--81, 2023.

\bibitem[Shen et~al.(2022)Shen, Wang, Kale, Ribeiro, Karbasi, and Hassani]{shen2022self}
Zebang Shen, Zhenfu Wang, Satyen Kale, Alejandro Ribeiro, Amin Karbasi, and Hamed Hassani.
\newblock Self-consistency of the fokker planck equation.
\newblock In \emph{Conference on Learning Theory}, pages 817--841. PMLR, 2022.

\bibitem[Sideris(2013)]{Sideris2013OrdinaryDE}
Thomas~C Sideris.
\newblock \emph{Ordinary differential equations and dynamical systems}, volume~2.
\newblock Springer, 2013.

\bibitem[Simonyan and Zisserman(2015)]{Simonyan15}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2021score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Stein(1972)]{stein1972bound}
Charles Stein.
\newblock A bound for the error in the normal approximation to the distribution of a sum of dependent random variables.
\newblock In \emph{Proceedings of the sixth Berkeley symposium on mathematical statistics and probability, volume 2: Probability theory}, volume~6, pages 583--603. University of California Press, 1972.

\bibitem[Sutherland et~al.(2017)Sutherland, Tung, Strathmann, De, Ramdas, Smola, and Gretton]{sutherland2017generative}
Danica~J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and Arthur Gretton.
\newblock Generative models and model criticism via optimized maximum mean discrepancy.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Tabak and Vanden-Eijnden(2010)]{tabak2010density}
Esteban~G Tabak and Eric Vanden-Eijnden.
\newblock Density estimation by dual ascent of the log-likelihood.
\newblock \emph{Communications in Mathematical Sciences}, 8\penalty0 (1):\penalty0 217--233, 2010.

\bibitem[Tzen and Raginsky(2019{\natexlab{a}})]{tzen2019neural}
Belinda Tzen and Maxim Raginsky.
\newblock Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit.
\newblock \emph{arXiv preprint arXiv:1905.09883}, 2019{\natexlab{a}}.

\bibitem[Tzen and Raginsky(2019{\natexlab{b}})]{tzen2019theoretical}
Belinda Tzen and Maxim Raginsky.
\newblock Theoretical guarantees for sampling and inference in generative models with latent diffusions.
\newblock In \emph{Conference on Learning Theory}, pages 3084--3114. PMLR, 2019{\natexlab{b}}.

\bibitem[Vidal et~al.(2023)Vidal, Wu~Fung, Tenorio, Osher, and Nurbekyan]{vidal2023taming}
Alexander Vidal, Samy Wu~Fung, Luis Tenorio, Stanley Osher, and Levon Nurbekyan.
\newblock Taming hyperparameter tuning in continuous normalizing flows using the jko scheme.
\newblock \emph{Scientific Reports}, 13\penalty0 (1):\penalty0 4501, 2023.

\bibitem[Xu et~al.(2022)Xu, Cheng, and Xie]{xu2022invertible}
Chen Xu, Xiuyuan Cheng, and Yao Xie.
\newblock Invertible neural networks for graph prediction.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 3\penalty0 (3):\penalty0 454--467, 2022.

\bibitem[Yarotsky(2017)]{yarotsky2017error}
Dmitry Yarotsky.
\newblock Error bounds for approximations with deep relu networks.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Wang, et~al.]{zhang2018monge}
Linfeng Zhang, Lei Wang, et~al.
\newblock Monge-{A}mp\'ere flow for generative modeling.
\newblock \emph{arXiv preprint arXiv:1809.10188}, 2018.

\bibitem[Zhang and Chen(2021)]{zhang2021diffusion}
Qinsheng Zhang and Yongxin Chen.
\newblock Diffusion normalizing flow.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 16280--16291, 2021.

\end{thebibliography}
