\section{Conclusion}
\label{main:sec:conclusion}
In this paper, we proposed a novel extension of \glspl{np} by taking a new approach to model the functional uncertainty for \glspl{np}.
The proposed model \gls{mpnp} utilizes the martingale posterior distribution~\citep{fong2021martingale}, 
where the functional uncertainty is driven from the uncertainty of future data generated from the joint predictive.
We present a simple architecture satisfying the theoretical requirements of the martingale posterior, and propose a training scheme to properly train it. We empirically validate \glspl{mpnp} on various tasks, where our method consistently outperforms the baselines.

\paragraph{Limitation}
As we presented in the \textbf{Predator-Prey Model} experiments in \cref{main:subsec:predator-prey}, 
our method did not significantly outperform baselines under model-data mismatch. This was also higlighted in \citet{fong2021martingale}: model-data mismatch under the martingale posterior framework remains an open problem.
Our method with direct input generation also performed poorly, as we found it difficult to prevent models from generating meaningless inputs that are ignored by the decoders. We present more details on unsuccessful attempts for direct input generation in \cref{app:sec:directly_generating_input}.

