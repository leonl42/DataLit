%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{main:sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We provide extensive experimental results to show how \gls{mpnp} and \gls{mpanp} effectively increase performance upon the following baselines: \gls{cnp}, \gls{np}, \gls{bnp}, \gls{canp}, \gls{anp}, and \gls{banp}. All models except deterministic models (i.e., \gls{cnp} and \gls{canp}) use the same number of samples; $K=5$ for the image completion task and $K=10$ for the others. Refer to~\cref{app:sec:architectures,app:sec:details} for more detailed experimental setup including model architectures, dataset and evaluation metrics.

% In this section, we carry out various experiments to show how \gls{mpnp} effectly increase performance.
% We compare our model (\gls{mpnp} and \gls{mpanp}) with various baseline \gls{npf} models(\gls{cnp}, \gls{np}, \gls{bnp}, \gls{neubnp}, \gls{canp}, \gls{anp}, \gls{banp} and \gls{neubanp}).
% We used same number of samples($K=5$ for image completion task and $K=10$ for the others) for all models except deterministic models \gls{cnp} and \gls{canp}.
% For detailed experimental setup including model architectures and dataset, please refer to \cref{app:sec:architectures,app:sec:details}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{1D Regression}
\label{main:sec:experiments:1dregression}

In this section, we conducted 1D regression experiments following \citet{kim2018attentive} and \citet{lee2020bootstrapping}. 
In this experiments, the dataset curves are generated from \gls{gp} with 4 different settings: \romannumeral1) RBF kernels, \romannumeral2) Mat\'ern 5/2 kernels, \romannumeral3) Periodic kernels, and \romannumeral4) RBF kernels with Student's $t$ noise.

\paragraph{Infinite Training Dataset}
\label{main:subsec:infinite_training}
\begin{figure}[t]
    \centering
    \includegraphics[width = 0.49\textwidth]{figure/main_vis_1.pdf}
    \includegraphics[width = 0.49\textwidth]{figure/main_vis_2.pdf}
    % \includegraphics[width = 0.32\textwidth]{figure/data plot 9.pdf}
    % \includegraphics[width = 0.32\textwidth]{figure/sample plot 9.pdf}
    % \includegraphics[width = 0.32\textwidth]{figure/posterior plot 9.pdf}
    \caption{Posterior samples of \gls{mpanp} for 1D regression task with RBF kernel. The black dashed line is the true function sampled from \gls{gp} with RBF kernel, and the black dots are context points. We visualized decoded mean and standard deviation with colored lines and areas. (Left) \Gls{mpanp} posterior predictions using the combined features of the original contexts and the generated pseudo contexts. (Right) Predictions using only the generated pseudo contexts without the original contexts. The pseudo contexts are decoded into reasonable functions, especially with high uncertainty for the region without context points.}
    % \caption{\color{blue}It shows posterior samples of \gls{mpanp} with context data and generated pseudo context data in 1D regression task with RBF kernel. Here, the black dashed line is a function sampled from \gls{gp} with RBF kernel and the black dots are context points. (Left) Overlaid posterior predictions with the combined feature from the contexts and generated pseudo contexts. Colored lines and areas represent decoded mean and std. (Right) In order to show the quality of generated pseudo context dataset, we visualize the posterior predictions with feature only from the generated pseudo contexts.}
    % \caption{Generated pseudo context data and posterior samples of MPANP in 1d regression task. (Left)
    % We train an autoencoder that can decode the representations $R_c$ into the points in the input space, and visualize the decoded inputs of the generated pseudo-contexts using the autoencoder. Red crosses are the real context data points, and the points sharing the same colors correspond to a set of pseudo context. (Right) Overlaid posterior predictions from the generated pseudo contexts.} 
    \label{fig:feature_method}
\end{figure}

% \begin{table}[t]
%     \caption{Context and target log likelihood values on data sampled from Gaussian Processes with various kernel. In the last column $t$-noise data generated by adding noise sampled from student $t$ distribution to Gaussian Processes with RBF Kernel. Performances are measured over 4 seeds.}
%     \label{tab:table_1d}
%     \centering
%     \scriptsize
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lrrrrrrrrrr}
%     \toprule
%          Dataset             &               & RBF           &               &Matern         &               & Periodic       &               &t-noise \\
%         \midrule                  
%                              & context       & target        & context       & target        & context       & target         & context       & target  \\
%                 \midrule
%  CNP                 & $0.971\pm{0.004}$ & $0.266\pm{0.007}$ & $0.855\pm{0.001}$ & $0.105\pm{0.008}$ &$-0.151\pm{0.020}$ & $-0.722\pm{0.010}$ &$-0.005\pm{0.054}$ & $-0.583\pm{0.009}$ \\
%  BNP                 & $0.978\pm{0.018}$ & $0.307\pm{0.010}$ & $0.880\pm{0.006}$ & $0.153\pm{0.007}$ &$-0.086\pm{0.016}$ & $-0.703\pm{0.003}$ &$ 0.000\pm{0.059}$ & $-0.434\pm{0.012}$  \\
%  NeuBNP              & $0.903\pm{0.005}$ & $0.248\pm{0.006}$ & $0.779\pm{0.012}$ & $0.091\pm{0.005}$ &$-0.084\pm{0.010}$ & $-0.691\pm{0.013}$ &$ 0.024\pm{0.081}$ & $-0.487\pm{0.043}$  \\+

%  Ours                & $\textbf{1.102}\pm{0.008}$ & $\textbf{0.446}\pm{0.007}$ & $\textbf{1.021}\pm{0.004}$ & $\textbf{0.264}\pm{0.008}$ &$ \textbf{0.174}\pm{0.026}$ & $\textbf{-0.676}\pm{0.007}$ &$ \textbf{0.205}\pm{0.047}$ & $\textbf{-0.364}\pm{0.008}$  \\
%         \midrule
%  AttnCNP              & $1.372\pm{0.002}$ & $0.575\pm{0.021}$ & $1.371\pm{0.004}$ & $0.396\pm{0.004}$ & $\textbf{1.229}\pm{0.028}$ & $-0.711\pm{0.015}$ & $0.415\pm{0.031}$ & $-0.783\pm{0.046}$ \\
%  AttnBNP              & $1.372\pm{0.001}$ & $0.644\pm{0.008}$ & $1.372\pm{0.001}$ & $0.462\pm{0.010}$ & $0.776\pm{0.001}$ & $-0.876\pm{0.034}$ & $0.813\pm{0.023}$ & $-0.489\pm{0.027}$  \\
%  AttnNeuBNP           & $1.353\pm{0.003}$ & $0.601\pm{0.002}$ & $1.341\pm{0.003}$ & $0.417\pm{0.011}$ & $0.626\pm{0.004}$ & $-0.744\pm{0.007}$ & $0.748\pm{0.039}$ & $\textbf{-0.391}\pm{0.012}$  \\
%  Ours                 & $\textbf{1.375}\pm{0.000}$ & $\textbf{0.686}\pm{0.003}$ & $\textbf{1.374}\pm{0.000}$ & $\textbf{0.487}\pm{0.002}$ & $1.168\pm{0.036}$ & $\textbf{-0.662}\pm{0.029}$ & $\textbf{0.887}\pm{0.055}$ & $-0.476\pm{0.031}$  \\
%         \bottomrule
%     \end{tabular}}
% \end{table}

% \input{table/table_gp_inf}
\input{table/main_gp_inf}

% \begin{figure}[t]
%     \centering
%     % \includegraphics[width=0.24\textwidth]{figure/gp_finite/rbf_ctx_cnps.pdf}
%     % \includegraphics[width=0.24\textwidth]{figure/gp_finite/rbf_tar_cnps.pdf}
%     % \includegraphics[width=0.24\textwidth]{figure/gp_finite/rbf_ctx_canps.pdf}
%     % \includegraphics[width=0.24\textwidth]{figure/gp_finite/rbf_tar_canps.pdf}
%     \includegraphics[width=\textwidth]{figure/finite.pdf}
%     \caption{Context and target log likelihood values of 1D regression experiments with RBF kernel. 
%     $\times n$ in the $x$ axis denotes how many times training data is used based on the number of training data of 500 epochs.
%     The first and the second figure are context and target log likelihood for \gls{cnp} variation models, respectively. The third and the forth are context and target log likelihood for \gls{canp} variation models, respectively.
% }
%     \label{fig:figure_gp_finite}
% \end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figure/main_gp_finite_np.pdf}
    \includegraphics[width=0.95\textwidth]{figure/main_gp_finite_anp.pdf}
    \caption{Test target log-likelihood values with varying the number of train data for 1D regression tasks on RBF, Matern, Periodic, and $t$-noise. Here, x-axis denotes how many examples are used for training, i.e., $\times1$, $\times2$, and $\times5$ respectively denote $51200$, $102400$, and $256000$ train examples.}
    \label{figure/main_gp_finite}
\end{figure}

Previous works~\citep{garnelo2018neural,kim2018attentive,le2018empirical} assumed that there exists a \gls{gp} curve generator that can provide virtually infinite amount of tasks for training. We first follow this setup, training all models for 100,000 steps where a new task is generated from each training step. We compare the models by picking checkpoints achieving the lowest validation loss. \cref{table/main_gp_inf} clearly shows that our model outperforms the other models in most cases. This results show that our model well captures the functional uncertainty compared to the other methods.
In~\cref{app:sec:additional_experiments}, we also report the comparison with the baselines with increased number of parameters to match the additional number of parameters introduced for the generator in our model, where ours still significantly outperforms the baselines.

% We also report the performances of our model generating pseudo contexts directly in the input space, denoted as DirectNP and DirectANP in \cref{table/main_gp_inf}. Unfortunately, DirectNP and DirectANP perform worse then ours when generating representations, presumably due to the difficulty of generating inputs that should be processed with nonlinear encoders and decoders. See \cref{app:sec:directly_generating_input} for the model architectures and the samples generated from DirectNP and DirectANP. In \cref{fig:feature_method}, we present the generated pseudo context data and posterior samples from trained \gls{mpanp} model. 
% In \cref{app:sec:additional_experiments}, we also report the comparison with the baselines with increased number of parameters to match the additional number of parameters introduced for the generator in our model, where ours still significantly outperforms the baselines.

\paragraph{Finite Training Dataset}
We also compare the models on more realistic setting assuming a finite amount of training tasks. Specifically, we first configured the finite training dataset consisting of $\{51200, 102400, 256000\}$ examples at the start of the training, instead of generating new tasks for each training step. We then trained all models with the same 100,000 training iterations in order to train the models with the same training budget as in the infinite training dataset situation. \cref{figure/main_gp_finite} clearly shows that our model consistently outperforms other models in terms of the target log-likelihood even when the training dataset is finite. This indicates that \glspl{mpnp} effectively learn a predictive distribution of unseen dataset from a given dataset with small number of tasks. Refer to~\cref{app:sec:additional_experiments} for more detailed results.

% Now we compare the models on more realistic setting assuming a finite amount of training tasks.
% In order to train the models with the same training budget as in the infinite training dataset situation, we first configured the training dataset so that 500 epochs become 100000 steps.
% We then gradually increase the number of data so that 100000 steps become 250, 125, and 100 epochs, respectively.
% \cref{fig:figure_gp_finite} shows that our model consistently outperforms other models for almost all dataset size in \gls{gp} with RBF kernel. This shows that \glspl{mpnp} effectively learn a predictive distribution of unseen dataset from a given dataset with small number of tasks. Please refer to \cref{app:sec:additional_experiments} for the results for the kernels other than RBF for which the results are similar as in RBF kernel.

% \paragraph{Robustness Across Kernels}
% \input{table/table_gp_robust}
% \citet{lee2020bootstrapping} conducted model-data mismatch experiments in 1D regression experiments by training the models with RBF kernels and evaluating them on the tasks generated from \glspl{gp} with kernels other than RBF.
% Following \citet{lee2020bootstrapping}, we first train a model with infinite \gls{gp} data with RBF kernel,
%  and measure the log-likelihoods on the test datasets generated from other kernels.
% As one can see from \cref{tab:table_gp_robust}, \glspl{mpnp} outperform baselines on RBF and Matern kernel (which is relatively simlar to RBF kernel compared to the others), but perform worse than bootstrap based method for the other kernels which are particularly designed to be robust under such settings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Image Completion}
\label{main:sec:experiments:imagecompletion}

\input{table/main_image}
Next we conducted 2D image completion tasks for three different datasets, i.e., MNIST, SVHN, and CelebA.
For training, we uniformly sample the number of context pixels $|c|\in\{3,...,197\}$ and the number of target pixels $|t|\in\{3,...,200-|c|\}$ from an image. For evaluation, we uniformly sample the number of context pixels $|c|\in\{3,...,197\}$ and set all the remaining pixels as the targets. \cref{table/main_image} clearly demonstrates that our model outperforms the baselines over all three datasets, demonstrating the effectiveness of our method for high-dimensional image data. See~\cref{app:sec:additional_experiments} for the visualizations of completed images along with the uncertainties in terms of predictive variances, and~\cref{app:sec:details} for the detailed training setup.

% Next we conducted 2D image completion tasks for 3 different datasets (MNIST, SVHN, CelebA).
% We trained all models for 100 epochs. For each training task, we uniformly sample the number of contexts $|c|\in \{3,...,197\}$ and number of targets $|t|\in \{3,...,200-|c|\}$ from an image. For evaluation, for a test image,
% we first uniformly sample the number of contexts $|c|\in \{3, \dots, 197\}$, and set all the remaining points as the targets. \cref{table/main_image} shows that our model outperforms the baselines over all three datasets, demonstrating the effectiveness of our method for high-dimensional image data. Please refer to \cref{app:sec:additional_experiments} for the visualizations of completed images along with the uncertainties in terms of predictive variances.

% \begin{table}[t]
%     \caption{Image completion task for MNIST and CIFAR10 dataset. We use $n_c \in\{3,...,197\}$ context points and $n_t \in \{3,\ldots, 200-n_c\}$ target points.\Todo{Performances are measured over 4 seeds.}}
%     \label{tab:table_2d}
%     \centering
%     \scriptsize
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lrrrr}
%     \toprule
%  Dataset             &               & MNIST         &               & CIFAR10    \\    
% \midrule                  
%                      & context       & target        & context       & target     \\  
% \midrule
% AttnCNP              & $1.366\pm{0}$ & $0.675\pm{0}$ & $\textbf{4.135}\pm{0}$ & $2.223\pm{0}$\\
% AttnBNP              & $0.725\pm{0}$ & $0.662\pm{0}$ & $\textbf{4.135}\pm{0}$ & $2.370\pm{0}$\\
% AttnNeuBNP           & $1.225\pm{0}$ & $0.767\pm{0}$ & $3.842\pm{0}$ & $2.488\pm{0}$\\
% Ours                 & $\textbf{1.371}\pm{0}$ & $\textbf{0.899}\pm{0}$ & $4.126\pm{0}$ & $\textbf{2.623}\pm{0}$\\
%         \bottomrule
%     \end{tabular}
    
%     \begin{tabular}{lrrrr}
%     \toprule
% Dataset              &               & MNIST         &               & CIFAR10   \\     
% \midrule                  
%                      & context       & target        & context       & target     \\  
% \midrule
% CNP                  & $1.113\pm{0}$ & $0.761\pm{0}$ & $2.725\pm{0}$ & $1.809\pm{0}$\\
% BNP                  & $1.059\pm{0}$ & $0.838\pm{0}$ & $2.734\pm{0}$ & $2.215\pm{0}$\\
% NeuBNP               & $1.076\pm{0}$ & $0.822\pm{0}$ & $2.857\pm{0}$ & $2.231\pm{0}$\\
% Ours                 & $\textbf{1.142}\pm{0}$ & $\textbf{0.923}\pm{0}$ & $\textbf{3.041}\pm{0}$ & $\textbf{2.496}\pm{0}$\\        \bottomrule
%     \end{tabular}}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayesian Optimization}
\label{main:sec:experiments:bo}
Using pre-trained models with RBF kernels in~\cref{main:subsec:infinite_training} Infinite Training Dataset experiments, we conducted Bayesian optimization~\citep{brochu2010tutorial} for two benchmark functions~\citep{gramacy2012cases,sobester2008engineering}. As a performance measurement, we use best simple regret, which measures the difference between the current best value and the global optimum value. \cref{figure/main_bo_gram} depicts the normalized regret and the cumulative normalized regret averaged over 100 trials of the~\citet{gramacy2012cases} function.
Here, we also consider a \gls{gp} variant with RBF kernel, tuned by pre-training~\citep{wang2021automatic}.
It clearly demonstrates that our model shows the best performance among \glspl{np} for both the normalized regret and the cumulative normalized regret. \cref{app:sec:additional_experiments:bo} provides the results for the~\citet{sobester2008engineering} function and \cref{app:sec:details:bo} provides detailed experimental setups.


\begin{figure}
\centering
\includegraphics[width=0.49\linewidth]{figure/main_bo_gram_np}
\includegraphics[width=0.49\linewidth]{figure/main_bo_gram_anp}
\caption{Results for Bayesian optimization on \citet{gramacy2012cases} function; we measured normalized simple regret and its cumulative value for a iteration. All models are pre-trained on 1D regression task generated with RBF kernel (cf.~\cref{main:sec:experiments:1dregression}) and evaluated on the benchmark function for Bayesian optimization.}
\label{figure/main_bo_gram}
\end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\textwidth]{figure/BO.png}
%     \caption{Results for Bayesian optimization with RBF kernel. Here we used 50 iterations for each model and report the means and standard deviations of minimum simple regret and cumulative simple regret for each iteration. The first and the second figure present the minimum simple regret for the models. The third and the forth figure present the cumulative simple regret for the models.}
%     \label{fig:figure_BO}
% \end{figure}
% We conducted Bayesian optimization~\citep{brochu2010tutorial} with tasks sampled from a \gls{gp} prior with RBF kernels. 
% We used pretrained models with RBF kernels in \cref{main:subsec:infinite_training} Infinite Training Dataset experiments.
% In Bayesian optimization, we use best simple regret, which measures the difference between current best value and the global optimum value, as a performance measurement.
% Following \citet{lee2020bootstrapping}, we evaluate the best simple regret and the cumulative best simple regret for 100 tasks.
% We used same fixed tasks and normalized initial minimum simple regret for the fare comparison among various models.
% In \cref{fig:figure_BO}, one can see that our model shows the best performance for both the best simple regret and especially on the cumulative best simple regret.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Predator-Prey Model}
\label{main:subsec:predator-prey}
Following~\citet{lee2020bootstrapping}, we conducted the predator-prey population regression experiments. We first trained the models using the simulation datasets which are generated from a Lotka-Volterra model~\citep{wilkinson2018stochastic} with the simulation settings followed by~\citet{lee2020bootstrapping}. Then tested on the generated simulation test dataset and real-world dataset which is called Hudson's Bay hare-lynx data. As mentioned in~\citet{lee2020bootstrapping}, the real-world dataset shows different tendency from generated simulation datasets, so we can treat this experiment as model-data mismatch experiments. In~\cref{table/main_lv}, we can see the \glspl{mpnp} outperform the other baselines for the test simulation datasets but underperforms in the real-world dataset compare to other baselines. This shows that model-data mismatch is an open problem for the \glspl{mpnp}.

\input{table/main_lv}

