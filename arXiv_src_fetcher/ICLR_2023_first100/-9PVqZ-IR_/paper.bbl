\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
J.~Bradbury, R.~Frostig, P.~Hawkins, M.~J. Johnson, C.~Leary, D.~Maclaurin,
  G.~Necula, A.~Paszke, J.~Vander{P}las, S.~Wanderman-{M}ilne, and Q.~Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brochu et~al.(2010)Brochu, Cora, and De~Freitas]{brochu2010tutorial}
E.~Brochu, V.~M. Cora, and N.~De~Freitas.
\newblock A tutorial on bayesian optimization of expensive cost functions, with
  application to active user modeling and hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1012.2599}, 2010.

\bibitem[De~Finetti(1937)]{de1937prevision}
B.~De~Finetti.
\newblock La pr{\'e}vision: ses lois logiques, ses sources subjectives.
\newblock \emph{Annales de l'institut Henri Poincar{\'e}}, 7\penalty0
  (1):\penalty0 1--68, 1937.

\bibitem[Efron(1992)]{efron1992bootstrap}
B.~Efron.
\newblock Bootstrap methods: another look at the jackknife.
\newblock In \emph{Breakthroughs in statistics}, pages 569--593. Springer,
  1992.

\bibitem[Fong et~al.(2021)Fong, Holmes, and Walker]{fong2021martingale}
E.~Fong, C.~Holmes, and S.~G. Walker.
\newblock Martingale posterior distributions.
\newblock \emph{arXiv preprint arXiv:2103.15671}, 2021.

\bibitem[Foong et~al.(2020)Foong, Bruinsma, Gordon, Dubois, Requeima, and
  Turner]{foong2020meta}
A.~Y.~K. Foong, W.~P. Bruinsma, J.~Gordon, Y.~Dubois, J.~Requeima, and R.~E.
  Turner.
\newblock Meta-learning stationary stochastic process prediction with
  convolutional neural processes.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS 2020)}, 2020.

\bibitem[Forrester et~al.(2008)Forrester, Sobester, and
  Keane]{sobester2008engineering}
A.~Forrester, A.~Sobester, and A.~Keane.
\newblock \emph{Engineering design via surrogate modelling: a practical guide}.
\newblock Wiley, 2008.

\bibitem[Garnelo et~al.(2018{\natexlab{a}})Garnelo, Rosenbaum, Maddison,
  Ramalho, Saxton, Shanahan, Teh, Rezende, and Eslami]{garnelo2018conditional}
M.~Garnelo, D.~Rosenbaum, C.~J. Maddison, T.~Ramalho, D.~Saxton, M.~Shanahan,
  Y.~W. Teh, D.~J. Rezende, and S.~M.~A. Eslami.
\newblock Conditional neural processes.
\newblock In \emph{Proceedings of The 35th International Conference on Machine
  Learning (ICML 2018)}, 2018{\natexlab{a}}.

\bibitem[Garnelo et~al.(2018{\natexlab{b}})Garnelo, Schwarz, Rosenbaum, Viola,
  Rezende, Eslami, and Teh]{garnelo2018neural}
M.~Garnelo, J.~Schwarz, D.~Rosenbaum, F.~Viola, D.~J. Rezende, S.~M.~A. Eslami,
  and Y.~W. Teh.
\newblock Neural processes.
\newblock \emph{ICML Workshop on Theoretical Foundations and Applications of
  Deep Generative Models}, 2018{\natexlab{b}}.

\bibitem[Gordon et~al.(2020)Gordon, Bruinsma, Foong, Requeima, Dubois, and
  Turner]{gordon2020convolutional}
J.~Gordon, W.~P. Bruinsma, A.~Y.~K. Foong, J.~Requeima, Y.~Dubois, and R.~E.
  Turner.
\newblock Convolutional conditional neural processes.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Gramacy and Lee(2012)]{gramacy2012cases}
R.~B. Gramacy and H.~K. Lee.
\newblock Cases for the nugget in modeling computer experiments.
\newblock \emph{Statistics and Computing}, 22\penalty0 (3):\penalty0 713--722,
  2012.

\bibitem[Heek et~al.(2020)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{flax2020github}
J.~Heek, A.~Levskaya, A.~Oliver, M.~Ritter, B.~Rondepierre, A.~Steiner, and
  M.~van {Z}ee.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Hessel et~al.(2020)Hessel, Budden, Viola, Rosca, Sezener, and
  Hennigan]{optax2020github}
M.~Hessel, D.~Budden, F.~Viola, M.~Rosca, E.~Sezener, and T.~Hennigan.
\newblock Optax: composable gradient transformation and optimisation, in jax!,
  2020.
\newblock URL \url{http://github.com/deepmind/optax}.

\bibitem[Kim et~al.(2018)Kim, Mnih, Schwarz, Garnelo, Eslami, Rosenbaum, and
  Oriol]{kim2018attentive}
H.~Kim, A.~Mnih, J.~Schwarz, M.~Garnelo, S.~M.~A. Eslami, D.~Rosenbaum, and
  V.~Oriol.
\newblock Attentive neural processes.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Kingma and Ba(2015)]{kingma2015adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Le et~al.(2018)Le, Kim, Garnelo, Rosenbaum, Schwarz, and
  Teh]{le2018empirical}
T.~A. Le, H.~Kim, M.~Garnelo, D.~Rosenbaum, J.~Schwarz, and Y.~W. Teh.
\newblock Empirical evaluation of neural process objectives.
\newblock In \emph{NeurIPS workshop on Bayesian Deep Learning}, page~71, 2018.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Lee et~al.(2019)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2019set}
J.~Lee, Y.~Lee, J.~Kim, A.~Kosiorek, S.~Choi, and Y.~W. Teh.
\newblock Set transformer: A framework for attention-based
  permutation-invariant neural networks.
\newblock In \emph{Proceedings of The 36th International Conference on Machine
  Learning (ICML 2019)}, 2019.

\bibitem[Lee et~al.(2020)Lee, Lee, Kim, Yang, Hwang, and
  Teh]{lee2020bootstrapping}
J.~Lee, Y.~Lee, J.~Kim, E.~Yang, S.~J. Hwang, and Y.~W. Teh.
\newblock Bootstrapping neural processes.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS 2020)}, 2020.

\bibitem[Lee et~al.(2022)Lee, Park, Jang, Lee, Cho, Shin, and
  Lim]{lee2022neural}
M.~Lee, J.~Park, S.~Jang, C.~Lee, H.~Cho, M.~Shin, and S.~Lim.
\newblock Neural bootstrapping attention for neural processes.
\newblock \emph{Under Review for International Conference on Learning
  Representations (ICLR)}, 2022.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Z.~Liu, P.~Luo, X.~Wang, and X.~Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, 2015.
\newblock URL \url{http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html}.

\bibitem[Louizos et~al.(2019)Louizos, Shi, Schutte, and
  Welling]{louizos2019functional}
C.~Louizos, X.~Shi, K.~Schutte, and M.~Welling.
\newblock The functional neural process.
\newblock In \emph{Advances in Neural Information Processing Systems 32
  (NeurIPS 2019)}, 2019.

\bibitem[Mohamed and Lakshminarayanan(2016)]{mohamed2016learning}
S.~Mohamed and B.~Lakshminarayanan.
\newblock Learning in implicit generative models.
\newblock \emph{arxiv:1610.03483}, 2016.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning 2011}, 2011.
\newblock URL \url{http://ufldl.stanford.edu/housenumbers/}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems 30 (NIPS
  2017)}, 2017.

\bibitem[Wang et~al.(2022)Wang, Dahl, Swersky, Lee, Mariet, Nado, Gilmer,
  Snoek, and Ghahramani]{wang2021automatic}
Z.~Wang, G.~E. Dahl, K.~Swersky, C.~Lee, Z.~Mariet, Z.~Nado, J.~Gilmer,
  J.~Snoek, and Z.~Ghahramani.
\newblock Pre-trained gaussian processes for bayesian optimization.
\newblock \emph{arXiv preprint arXiv:2109.08215}, 2022.

\bibitem[Wilkinson(2018)]{wilkinson2018stochastic}
D.~J. Wilkinson.
\newblock \emph{Stochastic modelling for systems biology}.
\newblock Chapman and Hall/CRC, 2018.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov,
  and Smola]{zaheer2017deep}
M.~Zaheer, S.~Kottur, S.~Ravanbakhsh, B.~Poczos, R.~R. Salakhutdinov, and A.~J.
  Smola.
\newblock Deep sets.
\newblock In \emph{Advances in Neural Information Processing Systems 30 (NIPS
  2017)}, 2017.

\end{thebibliography}
