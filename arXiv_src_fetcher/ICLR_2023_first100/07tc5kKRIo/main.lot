\contentsline {table}{\numberline {1}{\ignorespaces Pearson correlation coefficients between the accuracy of classes and the semantic scales $S$ with different $\alpha $. $N$ denotes the number of samples, and $S'$ represents the semantic scale without considering inter-class interference. $E_n$ denotes the number of effective samples.}}{5}{table.1}%
\contentsline {table}{\numberline {2}{\ignorespaces Top-1 Acc(\%) on ImageNet-LT and iNaturalist2018. We use ResNext-50 \cite {paper81} on ImageNet-LT and ResNet-50 \cite {paper32} on iNaturalist2018 as the network backbone for all methods. And we conduct model training with the SGD optimizer based on batch size 256 (for ImageNet-LT) / 512 (for iNaturalist), momentum 0.9, weight decay factor 0.0005, and learning rate 0.1 (linear LR decay).}}{8}{table.2}%
\contentsline {table}{\numberline {3}{\ignorespaces Comparison on ImageNet and CIFAR-100. On ImageNet, we use random clipping, mixup \cite {paper76}, and cutmix \cite {paper77} to augment the training data, and all models are optimized by Adam with batch size of 512, learning rate of 0.05, momentum of 0.9, and weight decay factor of 0.0005. On CIFAR-100, we set the batch size to 64 and augment the training data using random clipping, mixup, and cutmix. An Adam optimizer with learning rate of 0.1 (linear decay), momentum of 0.9, and weight decay factor of 0.005 is used to train all networks.}}{9}{table.3}%
\contentsline {table}{\numberline {4}{\ignorespaces Results on CUB-2011 and Cars196. We evaluate the model performance with Recall@K \cite {paper83} and Normalized Mutual Information (NMI) \cite {paper84}.}}{9}{table.4}%
\contentsline {table}{\numberline {5}{\ignorespaces Results on CIFAR-100-LT. The imbalance factor of a dataset is deÔ¨Åned as the value of the number of training samples in the largest class divided by that in the smallest class.}}{10}{table.5}%
\contentsline {table}{\numberline {6}{\ignorespaces Evaluation on MSCOCO-GLT.}}{10}{table.6}%
\contentsline {table}{\numberline {7}{\ignorespaces The sample-balanced sub-datasets with a total of $31$. Among them, $13$ sub-datasets are from CIFAR-10, $9$ sub-datasets are from CIFAR-100, and the rest are from Mini-ImageNet. The test set remains the original test set. $C$ denotes the total number of classes in the original dataset and $m$ is the number of samples per class in the sub-dataset.}}{25}{table.7}%
\contentsline {table}{\numberline {8}{\ignorespaces The two long-tailed MNIST datasets resampled from MNIST.}}{26}{table.8}%
\contentsline {table}{\numberline {9}{\ignorespaces Comparison on long-tailed Cars196.}}{27}{table.9}%
\contentsline {table}{\numberline {10}{\ignorespaces Comparison on Mini-ImageNet and CIFAR-100.}}{28}{table.10}%
\contentsline {table}{\numberline {11}{\ignorespaces Comparison of DSB-ST and SoftTriple in terms of memory consumption and training speed. The speed is measured by the average number of iterations per second. The additional video memory consumption due to our method is almost negligible.}}{35}{table.11}%
\contentsline {table}{\numberline {12}{\ignorespaces \textbf {Results of matching parent class for each child class}, where the Ratio of semantic scales denotes the ratio of the semantic scales of the parent class after mixing to before mixing, Predicted parent class means the parent class we matched for the child class, and Real parent class denotes the real parent class corresponding to the child class.}}{43}{table.12}%
