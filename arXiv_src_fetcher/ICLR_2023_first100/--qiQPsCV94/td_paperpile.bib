@ARTICLE{Tipping1999-vv,
  title    = "Mixtures of probabilistic principal component analyzers",
  author   = "Tipping, M E and Bishop, C M",
  abstract = "Principal component analysis (PCA) is one of the most popular
              techniques for processing, compressing, and visualizing data,
              although its effectiveness is limited by its global linearity.
              While nonlinear variants of PCA have been proposed, an
              alternative paradigm is to capture data complexity by a
              combination of local linear PCA projections. However,
              conventional PCA does not correspond to a probability density,
              and so there is no unique way to combine PCA models. Therefore,
              previous attempts to formulate mixture models for PCA have been
              ad hoc to some extent. In this article, PCA is formulated within
              a maximum likelihood framework, based on a specific form of
              gaussian latent variable model. This leads to a well-defined
              mixture model for probabilistic principal component analyzers,
              whose parameters can be determined using an
              expectation-maximization algorithm. We discuss the advantages of
              this model in the context of clustering, density modeling, and
              local dimensionality reduction, and we demonstrate its
              application to image compression and handwritten digit
              recognition.",
  journal  = "Neural Comput.",
  volume   =  11,
  number   =  2,
  pages    = "443--482",
  month    =  feb,
  year     =  1999,
  language = "en"
}

@ARTICLE{Vidal2005-ah,
  title    = "Generalized principal component analysis ({GPCA})",
  author   = "Vidal, Ren{\'e} and Ma, Yi and Sastry, Shankar",
  abstract = "This paper presents an algebro-geometric solution to the problem
              of segmenting an unknown number of subspaces of unknown and
              varying dimensions from sample data points. We represent the
              subspaces with a set of homogeneous polynomials whose degree is
              the number of subspaces and whose derivatives at a data point
              give normal vectors to the subspace passing through the point.
              When the number of subspaces is known, we show that these
              polynomials can be estimated linearly from data; hence, subspace
              segmentation is reduced to classifying one point per subspace. We
              select these points optimally from the data set by minimizing
              certain distance function, thus dealing automatically with
              moderate noise in the data. A basis for the complement of each
              subspace is then recovered by applying standard PCA to the
              collection of derivatives (normal vectors). Extensions of GPCA
              that deal with data in a high-dimensional space and with an
              unknown number of subspaces are also presented. Our experiments
              on low-dimensional data show that GPCA outperforms existing
              algebraic algorithms based on polynomial factorization and
              provides a good initialization to iterative techniques such as
              K-subspaces and Expectation Maximization. We also present
              applications of GPCA to computer vision problems such as face
              clustering, temporal video segmentation, and 3D motion
              segmentation from point correspondences in multiple affine views.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  27,
  number   =  12,
  pages    = "1945--1959",
  month    =  dec,
  year     =  2005,
  language = "en"
}

@MISC{Tipping_undated-ua,
  title        = "Mixtures of probabilistic principal component analysers",
  author       = "Tipping, Michael E and Bishop, Christopher M",
  howpublished = "\url{http://www.miketipping.com/papers/met-mppca.pdf}",
  note         = "Accessed: 2023-8-3"
}

@ARTICLE{Leonardis2002-jm,
  title    = "Multiple eigenspaces",
  author   = "Leonardis, Ale{\v s} and Bischof, Horst and Maver, Jasna",
  abstract = "In this paper, we propose a novel self-organizing framework to
              construct multiple, low-dimensional eigenspaces from a set of
              training images. Grouping of images is systematically and
              robustly performed via eigenspace-growing in terms of
              low-dimensional eigenspaces. To further increase the robustness,
              the eigenspace-growing is initiated independently with many small
              groups of images---seeds. All these grown eigenspaces are treated
              as hypotheses that are subject to a selection procedure
              eigenspace-selection, based on the MDL principle, which selects
              the final resulting set of eigenspaces as an efficient
              representation of the training set, taking into account the
              number of images encompassed by the eigenspaces, the dimensions
              of the eigenspaces, and their corresponding residual errors. We
              have tested the proposed method on a number of standard image
              sets, and the significance of the approach with respect to the
              recognition rate has been demonstrated.",
  journal  = "Pattern Recognit.",
  volume   =  35,
  number   =  11,
  pages    = "2613--2627",
  month    =  nov,
  year     =  2002,
  keywords = "Multiple eigenspaces; Appearance-based object representation;
              Principal component analysis (PCA); Visual learning; Image
              grouping; Appearance-based object recognition; Dimensionality
              reduction; View-based navigation map"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chen2018-qc,
  title    = "Active Orthogonal Matching Pursuit for Sparse Subspace Clustering",
  author   = "Chen, Yanxi and Li, Gen and Gu, Yuantao",
  abstract = "Sparse subspace clustering (SSC) is a state-of-the-art method for
              clustering high-dimensional data points lying in a union of
              low-dimensional subspaces. However, while ℓ1 optimization-based
              SSC algorithms suffer from high computational complexity, other
              variants of SSC, such as orthogonal-matching-pursuit-based SSC
              (OMP-SSC), lose clustering accuracy in pursuit of improving time
              efficiency. In this letter, we propose a novel active OMP-SSC,
              which improves clustering accuracy of OMP-SSC by adaptively
              updating data points and randomly dropping data points in the OMP
              process, while still enjoying the low computational complexity of
              greedy pursuit algorithms. We provide heuristic analysis of our
              approach and explain how these two active steps achieve a better
              tradeoff between connectivity and separation. Numerical results
              on both synthetic data and real-world data validate our analyses
              and show the advantages of the proposed active algorithm.",
  journal  = "IEEE Signal Process. Lett.",
  volume   =  25,
  number   =  2,
  pages    = "164--168",
  month    =  feb,
  year     =  2018,
  keywords = "Clustering algorithms;Matching pursuit algorithms;Signal
              processing algorithms;Algorithm design and
              analysis;Dictionaries;Indexes;Optimization;Active
              algorithm;connectivity;orthogonal matching pursuit (OMP);sparse
              subspace clustering (SSC);subspace detection property (SDP)"
}

@ARTICLE{Zhu2021-ki,
  title   = "A geometric analysis of neural collapse with unconstrained
             features",
  author  = "Zhu, Zhihui and Ding, Tianyu and Zhou, Jinxin and Li, Xiao and
             You, Chong and Sulam, Jeremias and Qu, Qing",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  34,
  pages   = "29820--29834",
  year    =  2021
}

@ARTICLE{Mixon2020-oe,
  title         = "Neural collapse with unconstrained features",
  author        = "Mixon, Dustin G and Parshall, Hans and Pi, Jianzong",
  abstract      = "Neural collapse is an emergent phenomenon in deep learning
                   that was recently discovered by Papyan, Han and Donoho. We
                   propose a simple ``unconstrained features model'' in which
                   neural collapse also emerges empirically. By studying this
                   model, we provide some explanation for the emergence of
                   neural collapse in terms of the landscape of empirical risk.",
  month         =  nov,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2011.11619"
}

@INPROCEEDINGS{Zhou2022-ob,
  title     = "Are All Losses Created Equal: A Neural Collapse Perspective",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Zhou, Jinxin and You, Chong and Li, Xiao and Liu, Kangning and
               Liu, Sheng and Qu, Qing and Zhu, Zhihui",
  editor    = "Oh, Alice H and Agarwal, Alekh and Belgrave, Danielle and Cho,
               Kyunghyun",
  year      =  2022
}

@ARTICLE{Peng2023-wc,
  title         = "The Ideal Continual Learner: An agent that never forgets",
  author        = "Peng, Liangzu and Giampouras, Paris V and Vidal, Ren{\'e}",
  abstract      = "The goal of continual learning is to find a model that
                   solves multiple learning tasks which are presented
                   sequentially to the learner. A key challenge in this setting
                   is that the learner may forget how to solve a previous task
                   when learning a new task, a phenomenon known as catastrophic
                   forgetting. To address this challenge, many practical
                   methods have been proposed, including memory-based,
                   regularization-based, and expansion-based methods. However,
                   a rigorous theoretical understanding of these methods
                   remains elusive. This paper aims to bridge this gap between
                   theory and practice by proposing a new continual learning
                   framework called Ideal Continual Learner (ICL), which is
                   guaranteed to avoid catastrophic forgetting by construction.
                   We show that ICL unifies multiple well-established continual
                   learning methods and gives new theoretical insights into the
                   strengths and weaknesses of these methods. We also derive
                   generalization bounds for ICL which allow us to
                   theoretically quantify how rehearsal affects generalization.
                   Finally, we connect ICL to several classic subjects and
                   research topics of modern interest, which allows us to make
                   historical remarks and inspire future directions.",
  month         =  apr,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2305.00316"
}

@ARTICLE{Wang2021-ut,
  title         = "Linear Convergence of a Proximal Alternating Minimization
                   Method with Extrapolation for {$\ell_1$-Norm} Principal
                   Component Analysis",
  author        = "Wang, Peng and Liu, Huikang and So, Anthony Man-Cho",
  abstract      = "A popular robust alternative of the classic principal
                   component analysis (PCA) is the $\ell_1$-norm PCA (L1-PCA),
                   which aims to find a subspace that captures the most
                   variation in a dataset as measured by the $\ell_1$-norm.
                   L1-PCA has shown great promise in alleviating the effect of
                   outliers in data analytic applications. However, it gives
                   rise to a challenging non-smooth non-convex optimization
                   problem, for which existing algorithms are either not
                   scalable or lack strong theoretical guarantees on their
                   convergence behavior. In this paper, we propose a proximal
                   alternating minimization method with extrapolation (PAMe)
                   for solving a two-block reformulation of the L1-PCA problem.
                   We then show that for both the L1-PCA problem and its
                   two-block reformulation, the Kurdyka-\textbackslashL
                   ojasiewicz exponent at any of the limiting critical points
                   is $1/2$. This allows us to establish the linear convergence
                   of the sequence of iterates generated by PAMe and to
                   determine the criticality of the limit of the sequence with
                   respect to both the L1-PCA problem and its two-block
                   reformulation. To complement our theoretical development, we
                   show via numerical experiments on both synthetic and
                   real-world datasets that PAMe is competitive with a host of
                   existing methods. Our results not only significantly advance
                   the convergence theory of iterative methods for L1-PCA but
                   also demonstrate the potential of our proposed method in
                   applications.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2107.07107"
}

@ARTICLE{Shwartz-Ziv2023-vh,
  title         = "To Compress or Not to Compress- {Self-Supervised} Learning
                   and Information Theory: A Review",
  author        = "Shwartz-Ziv, Ravid and LeCun, Yann",
  abstract      = "Deep neural networks have demonstrated remarkable
                   performance in supervised learning tasks but require large
                   amounts of labeled data. Self-supervised learning offers an
                   alternative paradigm, enabling the model to learn from data
                   without explicit labels. Information theory has been
                   instrumental in understanding and optimizing deep neural
                   networks. Specifically, the information bottleneck principle
                   has been applied to optimize the trade-off between
                   compression and relevant information preservation in
                   supervised settings. However, the optimal information
                   objective in self-supervised learning remains unclear. In
                   this paper, we review various approaches to self-supervised
                   learning from an information-theoretic standpoint and
                   present a unified framework that formalizes the
                   self-supervised information-theoretic learning problem. We
                   integrate existing research into a coherent framework,
                   examine recent self-supervised methods, and identify
                   research opportunities and challenges. Moreover, we discuss
                   empirical measurement of information-theoretic quantities
                   and their estimators. This paper offers a comprehensive
                   review of the intersection between information theory,
                   self-supervised learning, and deep neural networks.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2304.09355"
}

@ARTICLE{Bengio2013-ps,
  title    = "Representation learning: a review and new perspectives",
  author   = "Bengio, Yoshua and Courville, Aaron and Vincent, Pascal",
  abstract = "The success of machine learning algorithms generally depends on
              data representation, and we hypothesize that this is because
              different representations can entangle and hide more or less the
              different explanatory factors of variation behind the data.
              Although specific domain knowledge can be used to help design
              representations, learning with generic priors can also be used,
              and the quest for AI is motivating the design of more powerful
              representation-learning algorithms implementing such priors. This
              paper reviews recent work in the area of unsupervised feature
              learning and deep learning, covering advances in probabilistic
              models, autoencoders, manifold learning, and deep networks. This
              motivates longer term unanswered questions about the appropriate
              objectives for learning good representations, for computing
              representations (i.e., inference), and the geometrical
              connections between representation learning, density estimation,
              and manifold learning.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  35,
  number   =  8,
  pages    = "1798--1828",
  month    =  aug,
  year     =  2013,
  language = "en"
}

@ARTICLE{Shaham2018-rn,
  title         = "{SpectralNet}: Spectral Clustering using Deep Neural
                   Networks",
  author        = "Shaham, Uri and Stanton, Kelly and Li, Henry and Nadler,
                   Boaz and Basri, Ronen and Kluger, Yuval",
  abstract      = "Spectral clustering is a leading and popular technique in
                   unsupervised data analysis. Two of its major limitations are
                   scalability and generalization of the spectral embedding
                   (i.e., out-of-sample-extension). In this paper we introduce
                   a deep learning approach to spectral clustering that
                   overcomes the above shortcomings. Our network, which we call
                   SpectralNet, learns a map that embeds input data points into
                   the eigenspace of their associated graph Laplacian matrix
                   and subsequently clusters them. We train SpectralNet using a
                   procedure that involves constrained stochastic optimization.
                   Stochastic optimization allows it to scale to large
                   datasets, while the constraints, which are implemented using
                   a special-purpose output layer, allow us to keep the network
                   output orthogonal. Moreover, the map learned by SpectralNet
                   naturally generalizes the spectral embedding to unseen data
                   points. To further improve the quality of the clustering, we
                   replace the standard pairwise Gaussian affinities with
                   affinities leaned from unlabeled data using a Siamese
                   network. Additional improvement can be achieved by applying
                   the network to code representations produced, e.g., by
                   standard autoencoders. Our end-to-end learning procedure is
                   fully unsupervised. In addition, we apply VC dimension
                   theory to derive a lower bound on the size of SpectralNet.
                   State-of-the-art clustering results are reported on the
                   Reuters dataset. Our implementation is publicly available at
                   https://github.com/kstant0725/SpectralNet .",
  month         =  jan,
  year          =  2018,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1801.01587"
}

@INPROCEEDINGS{You2016-rt,
  title     = "A divide-and-conquer framework for large-scale subspace
               clustering",
  booktitle = "2016 50th Asilomar Conference on Signals, Systems and Computers",
  author    = "You, Chong and Donnat, Claire and Robinson, Daniel P and Vidal,
               Ren{\'e}",
  abstract  = "Given data that lies in a union of low-dimensional subspaces,
               the problem of subspace clustering aims to learn - in an
               unsupervised manner - the membership of the data to their
               respective subspaces. State-of-the-art subspace clustering
               methods typically adopt a two-step procedure. In the first step,
               an affinity measure among data points is constructed, usually by
               exploiting some form of data self-representation. In the second
               step, spectral clustering is applied to the affinity measure to
               find the membership of the data to their respective subspaces.
               While such methods are broadly applicable to mid-size datasets
               with 10,000 data points in 10,000 variables, they cannot be
               directly applied to large-scale datasets. This paper proposes a
               divide-and-conquer framework for large-scale subspace
               clustering. The data is first divided into chunks and subspace
               clustering is applied to each chunk. After removing potential
               outliers from each cluster, a new cross-representation measure
               for the similarity between subspaces is used to merge clusters
               from different chunks that correspond to the same subspace. A
               self-representation method is then used to assign outliers to
               clusters. We evaluate the proposed strategy on synthetic
               large-scale dataset with 1,000,000 data points, as well as on
               the MNIST database, which contains 70,000 images of handwritten
               digits. The numerical results highlight the scalability of our
               approach.",
  pages     = "1014--1018",
  month     =  nov,
  year      =  2016,
  keywords  = "Clustering algorithms;Clustering methods;Robustness;Matching
               pursuit algorithms;Dictionaries;Principal component
               analysis;Matrix decomposition"
}

@ARTICLE{Bruna2011-hx,
  title         = "Classification with Invariant Scattering Representations",
  author        = "Bruna, Joan and Mallat, St{\'e}phane",
  abstract      = "A scattering transform defines a signal representation which
                   is invariant to translations and Lipschitz continuous
                   relatively to deformations. It is implemented with a
                   non-linear convolution network that iterates over wavelet
                   and modulus operators. Lipschitz continuity locally
                   linearizes deformations. Complex classes of signals and
                   textures can be modeled with low-dimensional affine spaces,
                   computed with a PCA in the scattering domain. Classification
                   is performed with a penalized model selection. State of the
                   art results are obtained for handwritten digit recognition
                   over small training sets, and for texture classification.",
  month         =  dec,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1112.1120"
}

@ARTICLE{Kreisler2023-mm,
  title         = "Gradient Descent Monotonically Decreases the Sharpness of
                   Gradient Flow Solutions in Scalar Networks and Beyond",
  author        = "Kreisler, Itai and Nacson, Mor Shpigel and Soudry, Daniel
                   and Carmon, Yair",
  abstract      = "Recent research shows that when Gradient Descent (GD) is
                   applied to neural networks, the loss almost never decreases
                   monotonically. Instead, the loss oscillates as gradient
                   descent converges to its ''Edge of Stability'' (EoS). Here,
                   we find a quantity that does decrease monotonically
                   throughout GD training: the sharpness attained by the
                   gradient flow solution (GFS)-the solution that would be
                   obtained if, from now until convergence, we train with an
                   infinitesimal step size. Theoretically, we analyze scalar
                   neural networks with the squared loss, perhaps the simplest
                   setting where the EoS phenomena still occur. In this model,
                   we prove that the GFS sharpness decreases monotonically.
                   Using this result, we characterize settings where GD
                   provably converges to the EoS in scalar networks.
                   Empirically, we show that GD monotonically decreases the GFS
                   sharpness in a squared regression model as well as practical
                   neural network architectures.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2305.13064"
}

@INPROCEEDINGS{Zhong2021-gl,
  title           = "Graph Contrastive Clustering",
  booktitle       = "2021 {IEEE/CVF} International Conference on Computer
                     Vision ({ICCV})",
  author          = "Zhong, Huasong and Wu, Jianlong and Chen, Chong and Huang,
                     Jianqiang and Deng, Minghua and Nie, Liqiang and Lin,
                     Zhouchen and Hua, Xian-Sheng",
  publisher       = "IEEE",
  institution     = "Github",
  month           =  oct,
  year            =  2021,
  language        = "en",
  conference      = "2021 IEEE/CVF International Conference on Computer Vision
                     (ICCV)",
  location        = "Montreal, QC, Canada"
}

@INPROCEEDINGS{Dang2021-no,
  title           = "Nearest neighbor matching for deep clustering",
  booktitle       = "2021 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "Dang, Zhiyuan and Deng, Cheng and Yang, Xu and Wei, Kun
                     and Huang, Heng",
  abstract        = "The PyTorch official implementation of the CVPR2021 Poster
                     Paper NNM: Nearest Neighbor Matching for Deep Clustering.
                     - GitHub - ZhiyuanDang/NNM: The PyTorch official
                     implementation of the CVPR2021 Poster Paper NNM: Nearest
                     Neighbor Matching for Deep Clustering.",
  publisher       = "IEEE",
  institution     = "Github",
  month           =  jun,
  year            =  2021,
  language        = "en",
  conference      = "2021 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Nashville, TN, USA"
}

@ARTICLE{Chen2020-ws,
  title         = "Improved Baselines with Momentum Contrastive Learning",
  author        = "Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He,
                   Kaiming",
  abstract      = "Contrastive unsupervised learning has recently shown
                   encouraging progress, e.g., in Momentum Contrast (MoCo) and
                   SimCLR. In this note, we verify the effectiveness of two of
                   SimCLR's design improvements by implementing them in the
                   MoCo framework. With simple modifications to MoCo---namely,
                   using an MLP projection head and more data augmentation---we
                   establish stronger baselines that outperform SimCLR and do
                   not require large training batches. We hope this will make
                   state-of-the-art unsupervised learning research more
                   accessible. Code will be made public.",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2003.04297"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wang2015-vr,
  title    = "A {Global/Local} Affinity Graph for Image Segmentation",
  author   = "Wang, Xiaofang and Tang, Yuxing and Masnou, Simon and Chen,
              Liming",
  abstract = "Construction of a reliable graph capturing perceptual grouping
              cues of an image is fundamental for graph-cut based image
              segmentation methods. In this paper, we propose a novel sparse
              global/local affinity graph over superpixels of an input image to
              capture both short- and long-range grouping cues, and thereby
              enabling perceptual grouping laws, including proximity,
              similarity, continuity, and to enter in action through a suitable
              graph-cut algorithm. Moreover, we also evaluate three major
              visual features, namely, color, texture, and shape, for their
              effectiveness in perceptual segmentation and propose a simple
              graph fusion scheme to implement some recent findings from
              psychophysics, which suggest combining these visual features with
              different emphases for perceptual grouping. In particular, an
              input image is first oversegmented into superpixels at different
              scales. We postulate a gravitation law based on empirical
              observations and divide superpixels adaptively into small-,
              medium-, and large-sized sets. Global grouping is achieved using
              medium-sized superpixels through a sparse representation of
              superpixels' features by solving a ℓ0-minimization problem, and
              thereby enabling continuity or propagation of local smoothness
              over long-range connections. Small- and large-sized superpixels
              are then used to achieve local smoothness through an adjacent
              graph in a given feature space, and thus implementing perceptual
              laws, for example, similarity and proximity. Finally, a bipartite
              graph is also introduced to enable propagation of grouping cues
              between superpixels of different scales. Extensive experiments
              are carried out on the Berkeley segmentation database in
              comparison with several state-of-the-art graph constructions. The
              results show the effectiveness of the proposed approach, which
              outperforms state-of-the-art graphs using four different
              objective criteria, namely, the probabilistic rand index, the
              variation of information, the global consistency error, and the
              boundary displacement error.",
  journal  = "IEEE Trans. Image Process.",
  volume   =  24,
  number   =  4,
  pages    = "1399--1411",
  month    =  apr,
  year     =  2015,
  keywords = "Image segmentation;Visualization;Image color
              analysis;Reliability;Shape;Bipartite graph;Materials;Image
              segmentation;graph construction;sparse representation;normalized
              cut;superpixels"
}

@ARTICLE{Liu2020-mg,
  title    = "Multimodal {MRI} Brain Tumor Image Segmentation Using Sparse
              Subspace Clustering Algorithm",
  author   = "Liu, Li and Kuang, Liang and Ji, Yunfeng",
  abstract = "Brain tumors are one of the most deadly diseases with a high
              mortality rate. The shape and size of the tumor are random during
              the growth process. Brain tumor segmentation is a brain tumor
              assisted diagnosis technology that separates different brain
              tumor structures such as edema and active and tumor necrosis
              tissues from normal brain tissue. Magnetic resonance imaging
              (MRI) technology has the advantages of no radiation impact on the
              human body, good imaging effect on structural tissues, and an
              ability to realize tomographic imaging of any orientation.
              Therefore, doctors often use MRI brain tumor images to analyze
              and process brain tumors. In these images, the tumor structure is
              only characterized by grayscale changes, and the developed images
              obtained by different equipment and different conditions may also
              be different. This makes it difficult for traditional image
              segmentation methods to deal well with the segmentation of brain
              tumor images. Considering that the traditional single-mode MRI
              brain tumor images contain incomplete brain tumor information, it
              is difficult to segment the single-mode brain tumor images to
              meet clinical needs. In this paper, a sparse subspace clustering
              (SSC) algorithm is introduced to process the diagnosis of
              multimodal MRI brain tumor images. In the absence of added noise,
              the proposed algorithm has better advantages than traditional
              methods. Compared with the top 15 in the Brats 2015 competition,
              the accuracy is not much different, being basically stable
              between 10 and 15. In order to verify the noise resistance of the
              proposed algorithm, this paper adds 5\%, 10\%, 15\%, and 20\%
              Gaussian noise to the test image. Experimental results show that
              the proposed algorithm has better noise immunity than a
              comparable algorithm.",
  journal  = "Comput. Math. Methods Med.",
  volume   =  2020,
  pages    = "8620403",
  month    =  jul,
  year     =  2020,
  language = "en"
}

@INPROCEEDINGS{Li2015-wi,
  title           = "Temporal subspace clustering for human motion segmentation",
  booktitle       = "2015 {IEEE} International Conference on Computer Vision
                     ({ICCV})",
  author          = "Li, Sheng and Li, Kang and Fu, Yun",
  publisher       = "IEEE",
  month           =  dec,
  year            =  2015,
  conference      = "2015 IEEE International Conference on Computer Vision
                     (ICCV)",
  location        = "Santiago, Chile"
}

@INPROCEEDINGS{Tierney2014-og,
  title           = "Subspace clustering for sequential data",
  booktitle       = "2014 {IEEE} Conference on Computer Vision and Pattern
                     Recognition",
  author          = "Tierney, Stephen and Gao, Junbin and Guo, Yi",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2014,
  conference      = "2014 IEEE Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Columbus, OH, USA"
}

@INPROCEEDINGS{Paoletti2021-qh,
  title     = "Subspace Clustering for Action Recognition with Covariance
               Representations and Temporal Pruning",
  booktitle = "2020 25th International Conference on Pattern Recognition
               ({ICPR})",
  author    = "Paoletti, Giancarlo and Cavazza, Jacopo and Beyan, Cigdem and
               Del Bue, Alessio",
  abstract  = "This paper tackles the problem of human action recognition,
               defined as classifying which action is displayed in a trimmed
               sequence, from skeletal data. Albeit state-of-the-art approaches
               designed for this application are all supervised, in this paper
               we pursue a more challenging direction: solving the problem with
               unsupervised learning. To this end, we propose a novel subspace
               clustering method, which exploits covariance matrix to enhance
               the action's discriminability and a timestamp pruning approach
               that allow us to better handle the temporal dimension of the
               data. Through a broad experimental validation, we show that our
               computational pipeline surpasses existing unsupervised
               approaches but also can result in favorable performances as
               compared to the supervised methods. The code is available here:
               https://github.com/IIT-PAVIS/subspace-clustering-action-recognition.",
  pages     = "6035--6042",
  month     =  jan,
  year      =  2021,
  keywords  = "Protocols;Laplace equations;Data analysis;Clustering
               methods;Pipelines;Supervised learning;Machine learning"
}

@INPROCEEDINGS{Gholami2017-nz,
  title           = "Probabilistic temporal subspace clustering",
  booktitle       = "2017 {IEEE} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "Gholami, Behnam and Pavlovic, Vladimir",
  publisher       = "IEEE",
  month           =  jul,
  year            =  2017,
  conference      = "2017 IEEE Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Honolulu, HI"
}

@ARTICLE{Wu2016-yv,
  title    = "Ordered Subspace Clustering With {Block-Diagonal} Priors",
  author   = "Wu, Fei and Hu, Yongli and Gao, Junbin and Sun, Yanfeng and Yin,
              Baocai",
  abstract = "Many application scenarios involve sequential data, but most
              existing clustering methods do not well utilize the order
              information embedded in sequential data. In this paper, we study
              the subspace clustering problem for sequential data and propose a
              new clustering method, namely ordered sparse clustering with
              block-diagonal prior (BD-OSC). Instead of using the sparse
              normalizer in existing sparse subspace clustering methods, a
              quadratic normalizer for the data sparse representation is
              adopted to model the correlation among the data sparse
              coefficients. Additionally, a block-diagonal prior for the
              spectral clustering affinity matrix is integrated with the model
              to improve clustering accuracy. To solve the proposed BD-OSC
              model, which is a complex optimization problem with quadratic
              normalizer and block-diagonal prior constraint, an efficient
              algorithm is proposed. We test the proposed clustering method on
              several types of databases, such as synthetic subspace data set,
              human face database, video scene clips, motion tracks, and
              dynamic 3-D face expression sequences. The experiments show that
              the proposed method outperforms state-of-the-art subspace
              clustering methods.",
  journal  = "IEEE Trans Cybern",
  volume   =  46,
  number   =  12,
  pages    = "3209--3219",
  month    =  dec,
  year     =  2016,
  language = "en"
}

@ARTICLE{Jin2022-me,
  title   = "Expectation-maximization contrastive learning for compact
             video-and-language representations",
  author  = "Jin, Peng and Huang, Jinfa and Liu, Fenglin and Wu, Xian and Ge,
             Shen and Song, Guoli and Clifton, David and Chen, Jie",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  35,
  pages   = "30291--30306",
  year    =  2022
}

@INPROCEEDINGS{Ho2003-uk,
  title     = "Clustering appearances of objects under varying illumination
               conditions",
  booktitle = "2003 {IEEE} Computer Society Conference on Computer Vision and
               Pattern Recognition, 2003. Proceedings.",
  author    = "Ho, J and Yang, Ming-Husang and Lim, Jongwoo and Lee, Kuang-Chih
               and Kriegman, D",
  abstract  = "We introduce two appearance-based methods for clustering a set
               of images of 3D (three-dimensional) objects, acquired under
               varying illumination conditions, into disjoint subsets
               corresponding to individual objects. The first algorithm is
               based on the concept of illumination cones. According to the
               theory, the clustering problem is equivalent to finding convex
               polyhedral cones in the high-dimensional image space. To
               efficiently determine the conic structures hidden in the image
               data, we introduce the concept of conic affinity, which measures
               the likelihood of a pair of images belonging to the same
               underlying polyhedral cone. For the second method, we introduce
               another affinity measure based on image gradient comparisons.
               The algorithm operates directly on the image gradients by
               comparing the magnitudes and orientations of the image gradient
               at each pixel. Both methods have clear geometric motivations,
               and they operate directly on the images without the need for
               feature extraction or computation of pixel statistics. We
               demonstrate experimentally that both algorithms are surprisingly
               effective in clustering images acquired under varying
               illumination conditions with two large, well-known image data
               sets.",
  volume    =  1,
  pages     = "I--I",
  month     =  jun,
  year      =  2003,
  keywords  = "Lighting;Clustering algorithms;Computer vision;Computer
               science;Pixel;Image databases;Feature
               extraction;Statistics;Image converters;Cameras"
}

@ARTICLE{Georghiades2001-xm,
  title    = "From few to many: illumination cone models for face recognition
              under variable lighting and pose",
  author   = "Georghiades, A S and Belhumeur, P N and Kriegman, D J",
  abstract = "We present a generative appearance-based method for recognizing
              human faces under variation in lighting and viewpoint. Our method
              exploits the fact that the set of images of an object in fixed
              pose, but under all possible illumination conditions, is a convex
              cone in the space of images. Using a small number of training
              images of each face taken with different lighting directions, the
              shape and albedo of the face can be reconstructed. In turn, this
              reconstruction serves as a generative model that can be used to
              render (or synthesize) images of the face under novel poses and
              illumination conditions. The pose space is then sampled and, for
              each pose, the corresponding illumination cone is approximated by
              a low-dimensional linear subspace whose basis vectors are
              estimated using the generative model. Our recognition algorithm
              assigns to a test image the identity of the closest approximated
              illumination cone. Test results show that the method performs
              almost without error, except on the most extreme lighting
              directions.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  23,
  number   =  6,
  pages    = "643--660",
  month    =  jun,
  year     =  2001,
  keywords = "Lighting;Face recognition;Image
              reconstruction;Testing;Humans;Shape;Rendering (computer
              graphics);Vectors;Image recognition;Performance evaluation"
}

@INPROCEEDINGS{Wang2013-nw,
  title     = "Sparse Subspace Denoising for Image Manifolds",
  booktitle = "2013 {IEEE} Conference on Computer Vision and Pattern
               Recognition",
  author    = "Wang, Bo and Tu, Zhuowen",
  abstract  = "With the increasing availability of high dimensional data and
               demand in sophisticated data analysis algorithms, manifold
               learning becomes a critical technique to perform dimensionality
               reduction, unraveling the intrinsic data structure. The
               real-world data however often come with noises and outliers,
               seldom, all the data live in a single linear subspace. Inspired
               by the recent advances in sparse subspace learning and
               diffusion-based approaches, we propose a new manifold denoising
               algorithm in which data neighborhoods are adaptively inferred
               via sparse subspace reconstruction, we then derive a new
               formulation to perform denoising to the original data.
               Experiments carried out on both toy and real applications
               demonstrate the effectiveness of our method, it is insensitive
               to parameter tuning and we show significant improvement over the
               competing algorithms.",
  pages     = "468--475",
  month     =  jun,
  year      =  2013,
  keywords  = "Manifolds;Noise reduction;Noise;Algorithm design and
               analysis;Clustering algorithms;Sparse matrices;Principal
               component analysis"
}

@INPROCEEDINGS{Lopes2020-vr,
  title           = "Manifold learning-based clustering approach applied to
                     anomaly detection in surveillance videos",
  booktitle       = "Proceedings of the 15th International Joint Conference on
                     Computer Vision, Imaging and Computer Graphics Theory and
                     Applications",
  author          = "Lopes, Leonardo and Valem, Lucas and Pedronette, Daniel
                     and Guilherme, Ivan and Papa, Jo{\~a}o and Santana, Marcos
                     and Colombo, Danilo",
  publisher       = "SCITEPRESS - Science and Technology Publications",
  year            =  2020,
  conference      = "15th International Conference on Computer Vision Theory
                     and Applications",
  location        = "Valletta, Malta"
}

@INPROCEEDINGS{Yankov2006-ex,
  title           = "Manifold clustering of shapes",
  booktitle       = "Sixth International Conference on Data Mining ({ICDM'06})",
  author          = "Yankov, Dragomir and Keogh, Eamonn",
  abstract        = "Shape clustering can significantly facilitate the
                     automatic labeling of objects present in image
                     collections. For example, it could outline the existing
                     groups of pathological cells in a bank of cyto-images; the
                     groups of species on photographs collected from certain
                     aerials; or the groups of objects observed on surveillance
                     scenes from an office building. Here we demonstrate that a
                     nonlinear projection algorithm such as Isomap can attract
                     together shapes of similar objects, suggesting the
                     existence of isometry between the shape space and a low
                     dimensional nonlinear embedding. Whenever there is a
                     relatively small amount of noise in the data, the
                     projection forms compact, convex clusters that can easily
                     be learned by a subsequent partitioning scheme. We further
                     propose a modification of the Isomap projection based on
                     the concept of degree-bounded minimum spanning trees. The
                     proposed approach is demonstrated to move apart bridged
                     clusters and to alleviate the effect of noise in the data.",
  publisher       = "IEEE",
  month           =  dec,
  year            =  2006,
  conference      = "Sixth International Conference on Data Mining (ICDM'06)",
  location        = "Hong Kong, China"
}

@ARTICLE{noauthor_undated-jt,
  title = "meila05a.pdf"
}

@ARTICLE{Dhillon2007-kg,
  title    = "Weighted graph cuts without eigenvectors a multilevel approach",
  author   = "Dhillon, Inderjit S and Guan, Yuqiang and Kulis, Brian",
  abstract = "A variety of clustering algorithms have recently been proposed to
              handle data that is not linearly separable; spectral clustering
              and kernel k-means are two of the main methods. In this paper, we
              discuss an equivalence between the objective functions used in
              these seemingly different methods--in particular, a general
              weighted kernel k-means objective is mathematically equivalent to
              a weighted graph clustering objective. We exploit this
              equivalence to develop a fast, high-quality multilevel algorithm
              that directly optimizes various weighted graph clustering
              objectives, such as the popular ratio cut, normalized cut, and
              ratio association criteria. This eliminates the need for any
              eigenvector computation for graph clustering problems, which can
              be prohibitive for very large graphs. Previous multilevel graph
              partitioning methods, such as Metis, have suffered from the
              restriction of equal-sized clusters; our multilevel algorithm
              removes this restriction by using kernel k-means to optimize
              weighted graph cuts. Experimental results show that our
              multilevel algorithm outperforms a state-of-the-art spectral
              clustering algorithm in terms of speed, memory usage, and
              quality. We demonstrate that our algorithm is applicable to
              large-scale clustering tasks such as image segmentation, social
              network analysis and gene network analysis.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  29,
  number   =  11,
  pages    = "1944--1957",
  month    =  nov,
  year     =  2007,
  language = "en"
}

@ARTICLE{Wainwright2008-wg,
  title     = "Graphical Models, Exponential Families, and Variational
               Inference",
  author    = "Wainwright, Martin J and Jordan, Michael I",
  journal   = "Foundations and Trends\textregistered{} in Machine Learning",
  publisher = "Now Publishers",
  volume    =  1,
  number    = "1--2",
  pages     = "1--305",
  year      =  2008,
  keywords  = "Graphical models"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Ovsjanikov_undated-te,
  title        = "Functional maps: A flexible representation of maps between
                  shapes",
  author       = "Ovsjanikov, Maks and Ben-Chen, Mirela and Solomon, Justin and
                  Butscher, Adrian and Guibas, Leonidas and Lix, † and
                  Polytechnique, {\'E}cole",
  abstract     = "Figure 1: Horse algebra: the functional representation and
                  map inference algorithm allow us to go beyond point-to-point
                  maps. The source shape (top left corner) was mapped to the
                  target shape (left) by posing descriptor-based functional
                  constraints which do not disambiguate symmetries (i.e.
                  without landmark constraints). By further adding
                  correspondence constraints, we obtain a near isometric map
                  which reverses orientation, mapping left to right (center).
                  The representation allows for algebraic operations on shape
                  maps, so we can subtract this map from the ambivalent map, to
                  retrieve the orientation preserving near-isometry (right).
                  Each column shows the first 20x20 block of the functional map
                  representation (bottom), and the action of the map by
                  transferring colors from the source shape to the target shape
                  (top).",
  howpublished = "\url{https://people.csail.mit.edu/jsolomon/assets/fmaps.pdf}",
  note         = "Accessed: 2022-7-12"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{noauthor_2016-qr,
  title     = "A fast projected fixed-point algorithm for large graph matching",
  abstract  = "We propose a fast algorithm for approximate matching of large
               graphs. Previous graph matching algorithms suffer from high
               computational complexity and…",
  journal   = "Pattern Recognit.",
  publisher = "Pergamon",
  volume    =  60,
  pages     = "971--982",
  month     =  dec,
  year      =  2016
}

@ARTICLE{Bauschke1993-zp,
  title     = "On the convergence of von Neumann's alternating projection
               algorithm for two sets",
  author    = "Bauschke, H H and Borwein, J M",
  abstract  = "We give several unifying results, interpretations, and examples
               regarding the convergence of the von Neumann alternating
               projection algorithm for two arbitrary closed convex nonempty
               subsets of a Hilbert space. Our research is formulated within
               the framework of Fej{\'e}r monotonicity, convex and set-valued
               analysis. We also discuss the case of finitely many sets.",
  journal   = "Set-valued anal.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  1,
  number    =  2,
  pages     = "185--212",
  year      =  1993,
  language  = "en"
}

@ARTICLE{Makarenkov2008-gr,
  title         = "Lipschitz perturbations of differentiable implicit functions",
  author        = "Makarenkov, Oleg",
  abstract      = "Let $y=f(x)$ be a continuously differentiable implicit
                   function solving the equation $F(x,y)=0$ with continuously
                   differentiable $F.$ In this paper we show that if $F_\eps$
                   is a Lipschitz function such that the Lipschitz constant of
                   $F_\eps-F$ goes to 0 as $\eps\to 0$ then the equation
                   $F_\eps(x,y)=0$ has a Lipschitz solution $y=f_\eps(x)$ such
                   that the Lipschitz constant of $f_\eps-f$ goes to 0 as
                   $\eps\to 0$ either. As an application we evaluate the length
                   of time intervals where the right hand parts of some
                   nonautonomous discontinuous systems of ODEs are continuously
                   differentiable with respect to state variables. The latter
                   is done as a preparatory step toward generalizing the second
                   Bogolyubov's theorem for discontinuous systems.",
  month         =  mar,
  year          =  2008,
  archivePrefix = "arXiv",
  primaryClass  = "math.FA",
  eprint        = "0803.1006"
}

@ARTICLE{Zhou2018-gg,
  title         = "On the Fenchel Duality between Strong Convexity and
                   Lipschitz Continuous Gradient",
  author        = "Zhou, Xingyu",
  abstract      = "We provide a simple proof for the Fenchel duality between
                   strong convexity and Lipschitz continuous gradient. To this
                   end, we first establish equivalent conditions of convexity
                   for a general function that may not be differentiable. By
                   utilizing these equivalent conditions, we can directly
                   obtain equivalent conditions for strong convexity and
                   Lipschitz continuous gradient. Based on these results, we
                   can easily prove Fenchel duality. Beside this main result,
                   we also identify several conditions that are implied by
                   strong convexity or Lipschitz continuous gradient, but are
                   not necessarily equivalent to them. This means that these
                   conditions are more general than strong convexity or
                   Lipschitz continuous gradient themselves.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1803.06573"
}

@ARTICLE{Villar2021-cx,
  title         = "Scalars are universal: Equivariant machine learning,
                   structured like classical physics",
  author        = "Villar, Soledad and Hogg, David W and Storey-Fisher, Kate
                   and Yao, Weichi and Blum-Smith, Ben",
  abstract      = "There has been enormous progress in the last few years in
                   designing neural networks that respect the fundamental
                   symmetries and coordinate freedoms of physical law. Some of
                   these frameworks make use of irreducible representations,
                   some make use of high-order tensor objects, and some apply
                   symmetry-enforcing constraints. Different physical laws obey
                   different combinations of fundamental symmetries, but a
                   large fraction (possibly all) of classical physics is
                   equivariant to translation, rotation, reflection (parity),
                   boost (relativity), and permutations. Here we show that it
                   is simple to parameterize universally approximating
                   polynomial functions that are equivariant under these
                   symmetries, or under the Euclidean, Lorentz, and
                   Poincar\textbackslash'e groups, at any dimensionality $d$.
                   The key observation is that nonlinear O($d$)-equivariant
                   (and related-group-equivariant) functions can be universally
                   expressed in terms of a lightweight collection of scalars --
                   scalar products and scalar contractions of the scalar,
                   vector, and tensor inputs. We complement our theory with
                   numerical examples that show that the scalar-based method is
                   simple, efficient, and scalable.",
  month         =  jun,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.06610"
}

@ARTICLE{Li2023-qv,
  title         = "A Theoretical Understanding of Shallow Vision Transformers:
                   Learning, Generalization, and Sample Complexity",
  author        = "Li, Hongkang and Wang, Meng and Liu, Sijia and Chen, Pin-Yu",
  abstract      = "Vision Transformers (ViTs) with self-attention modules have
                   recently achieved great empirical success in many vision
                   tasks. Due to non-convex interactions across layers,
                   however, theoretical learning and generalization analysis is
                   mostly elusive. Based on a data model characterizing both
                   label-relevant and label-irrelevant tokens, this paper
                   provides the first theoretical analysis of training a
                   shallow ViT, i.e., one self-attention layer followed by a
                   two-layer perceptron, for a classification task. We
                   characterize the sample complexity to achieve a zero
                   generalization error. Our sample complexity bound is
                   positively correlated with the inverse of the fraction of
                   label-relevant tokens, the token noise level, and the
                   initial model error. We also prove that a training process
                   using stochastic gradient descent (SGD) leads to a sparse
                   attention map, which is a formal verification of the general
                   intuition about the success of attention. Moreover, this
                   paper indicates that a proper token sparsification can
                   improve the test performance by removing label-irrelevant
                   and/or noisy tokens, including spurious correlations.
                   Empirical experiments on synthetic data and CIFAR-10 dataset
                   justify our theoretical results and generalize to deeper
                   ViTs.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2302.06015"
}

@ARTICLE{Davis2018-wj,
  title         = "Stochastic subgradient method converges at the rate
                   {$O(k^{-1/4})$} on weakly convex functions",
  author        = "Davis, Damek and Drusvyatskiy, Dmitriy",
  abstract      = "We prove that the proximal stochastic subgradient method,
                   applied to a weakly convex problem, drives the gradient of
                   the Moreau envelope to zero at the rate $O(k^\{-1/4\})$. As
                   a consequence, we resolve an open question on the
                   convergence rate of the proximal stochastic gradient method
                   for minimizing the sum of a smooth nonconvex function and a
                   convex proximable function.",
  month         =  feb,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1802.02988"
}

@ARTICLE{Hartley_undated-dr,
  title  = "The Multibody Trifocal Tensor: Motion Segmentation from 3
            Perspective Views",
  author = "Hartley, Richard and Vidal, Rene"
}

@MISC{noauthor_undated-jx,
  title = "{SimilarityClusteringSupplementary-annotated.pdf}"
}

@ARTICLE{Li2022-vq,
  title     = "Neural Manifold Clustering and Embedding",
  author    = "Li, Zengyi and Chen, Yubei and LeCun, Yann and Sommer, Friedrich
               T",
  abstract  = "Given a union of non-linear manifolds, non-linear subspace
               clustering or manifold clustering aims to cluster data points
               based on manifold structures and also learn to parameterize each
               manifold as a linear subspace in a feature space. Deep neural
               networks have the potential to achieve this goal under highly
               non-linear settings given their large capacity and flexibility.
               We argue that achieving manifold clustering with neural networks
               requires two essential ingredients: a domain-specific constraint
               that ensures the identification of the manifolds, and a learning
               algorithm for embedding each manifold to a linear subspace in
               the feature space. This work shows that many constraints can be
               implemented by data augmentation. For subspace feature learning,
               Maximum Coding Rate Reduction (MCR$^2$) objective can be used.
               Putting them together yields \{\textbackslashem Neural Manifold
               Clustering and Embedding\} (NMCE), a novel method for general
               purpose manifold clustering, which significantly outperforms
               autoencoder-based deep subspace clustering. Further, on more
               challenging natural image datasets, NMCE can also outperform
               other algorithms specifically designed for clustering.
               Qualitatively, we demonstrate that NMCE learns a meaningful and
               interpretable feature space. As the formulation of NMCE is
               closely related to several important Self-supervised learning
               (SSL) methods, we believe this work can help us build a deeper
               understanding on SSL representation learning.",
  journal   = "arXiv [cs.LG]",
  month     =  jan,
  year      =  2022,
  copyright = "http://creativecommons.org/licenses/by/4.0/"
}

@MISC{noauthor_undated-iw,
  title = "Unknown - Unknown - The problem of degeneracy in structure and
           motion recovery from uncalibrated image sequences-annotated.pdf"
}

@ARTICLE{Heinrich_undated-jx,
  title  = "Robust Estimation of the Trifocal Tensor: A Comparative Performance
            Evaluation",
  author = "Heinrich, Stuart B and Snyder, Wesley E"
}

@MISC{noauthor_undated-vg,
  title = "Olsson - 2016 - {Non-Sequential} Structure from Motion
           {Non-Sequential} Structure from Motion-annotated.pdf"
}

@MISC{noauthor_undated-uv,
  title = "{Thompson-Brenner} - 2011 - Introduction to the special section
           Contextualizing significance testing in clinical
           trials-annotated.pdf"
}

@MISC{noauthor_undated-uf,
  title = "Fitzgibbon, Zisserman - 1998 - Automatic camera recovery for closed
           or open image sequences-annotated.pdf"
}

@ARTICLE{Goh_undated-lh,
  title  = "Segmenting Motions of Different Types by Unsupervised Manifold
            Clustering",
  author = "Goh, Alvina and Vidal, Rene"
}

@ARTICLE{Rodehorst_undated-gs,
  title  = "{EVALUATION} {OF} {THE} {METRIC} {TRIFOCAL} {TENSOR} {FOR}
            {RELATIVE} {THREE-VIEW} {ORIENTATION}",
  author = "Rodehorst, V"
}

@ARTICLE{Sugimoto2000-rz,
  title   = "A Linear Algorithm for Computing the Homography from Conics in
             Correspondence",
  author  = "Sugimoto, Akihiro",
  journal = "J. Math. Imaging Vis.",
  volume  =  13,
  pages   = "115--130",
  year    =  2000
}

@ARTICLE{Ranftl_undated-fq,
  title  = "Deep Fundamental Matrix Estimation",
  author = "Ranftl, Rene and Koltun, Vladlen"
}

@ARTICLE{Rontsis_undated-kk,
  title  = "Optimal Approximation of Doubly Stochastic Matrices",
  author = "Rontsis, Nikitas and Goulart, Paul J"
}

@ARTICLE{Zelnik-Manor_undated-gm,
  title  = "{Self-Tuning} Spectral Clustering",
  author = "Zelnik-Manor, Lihi and Perona, Pietro"
}

@ARTICLE{Zhou_undated-zz,
  title  = "Robust {Plane-Based} Structure From Motion",
  author = "Zhou, Zihan and Jin, Hailin and Ma, Yi"
}

@ARTICLE{Ding_undated-qh,
  title  = "A Convex Optimization Approach to Robust Fundamental Matrix",
  author = "Ding, Tianjiao"
}

@ARTICLE{Bian_undated-jn,
  title  = "An Evaluation of Feature Matchers for Fundamental Matrix Estimation",
  author = "Bian, Jia-Wang"
}

@MISC{noauthor_undated-ik,
  title = "Unknown - 2011 - {SPECTRAL} {THEORY} {OF} {GRAPHS} {BASED} {ON}
           {THE} {SIGNLESS} {LAPLACIAN-annotated.pdf}"
}

@ARTICLE{Von_Luxburg2007-xu,
  title     = "A Tutorial on Spectral Clustering",
  author    = "von Luxburg, Ulrike",
  journal   = "Statistics and computing",
  publisher = "Springer",
  volume    =  17,
  number    =  4,
  pages     = "395--416",
  year      =  2007
}

@ARTICLE{Aizawa_undated-se,
  title  = "Fast and Robust Estimation for {Unit-Norm} Constrained Linear
            Fitting Problems",
  author = "Aizawa, Daiki Ikami Toshihiko Yamasaki"
}

@MISC{noauthor_undated-qh,
  title = "Wang et al. - 2015 - Self Scaled Regularized Robust
           Regression-annotated.pdf"
}

@ARTICLE{Shashua_undated-mb,
  title  = "{'IFilinearity} 'lhree Yerspect ive views and its {AsSOCiated}
            xensor",
  author = "Shashua', Amnon and Werman, Michael"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Serych_undated-pf,
  title  = "Fast L-based {RANSAC} for homography estimation",
  author = "Serych, Jonas and Matas, Jiˇri and Drbohlav, Ondˇrej"
}

@ARTICLE{Schonberger_undated-vy,
  title  = "{Structure-from-Motion} Revisited",
  author = "Schonberger, Johannes L and Frahm, Jan-Michael"
}

@MISC{noauthor_undated-gr,
  title = "Hartley, Sturm - Unknown - Triangulation-annotated.pdf"
}

@ARTICLE{Crd_undated-nf,
  title  = "Lines and Points in Three Views and the Trifocal Tensor",
  author = "Crd, G E"
}

@ARTICLE{Shashua_undated-hh,
  title  = "Dynamic {P} to {P} Alignment",
  author = "Shashua, Amnon and Wolf, Lior"
}

@MISC{noauthor_undated-zp,
  title = "Nist - 2003 - An Efficient Solution to the {Five-Point} Relative
           Pose Problem 3 . The {Five-Point} Algorithm-annotated.pdf"
}

@ARTICLE{Canterakis_undated-xk,
  title  = "A Minimal Set of Constraints for the Trifocal Tensor",
  author = "Canterakis, Nikos"
}

@ARTICLE{Rahmani_undated-pm,
  title  = "Outlier Detection and Robust {PCA} Using a Convex Measure of
            Innovation",
  author = "Rahmani, Mostafa and Li, Ping"
}

@ARTICLE{Kalofolias_undated-ov,
  title  = "How to learn a graph from smooth signals",
  author = "Kalofolias, Vassilis"
}

@ARTICLE{Trager_undated-jv,
  title  = "In Defense of Relative {Multi-View} Geometry",
  author = "Trager, Matthew and Ponce, Jean"
}

@MISC{noauthor_undated-ck,
  title = "Chojnacki et al. - 2003 - Revisiting Hartley's Normalized
           {Eight-Point} Algorithm-annotated.pdf"
}

@ARTICLE{Zhou_undated-hx,
  title  = "Deep Adversarial Subspace Clustering",
  author = "Zhou, Pan and Hou, Yunqing and Feng, Jiashi"
}

@MISC{noauthor_undated-sj,
  title = "Song, Lichtenberg, Xiao - 2015 - {SUN} {RGB-D} A {RGB-D} scene
           understanding benchmark suite-annotated.pdf"
}

@ARTICLE{Wright_undated-ok,
  title  = "Classification via Minimum Incremental Coding Length ({MICL})",
  author = "Wright, John and Ma, Yi"
}

@ARTICLE{Martinec_undated-jt,
  title  = "Robust Rotation and Translation Estimation in Multiview
            Reconstruction",
  author = "Martinec, Daniel and Pajdla, Tomas"
}

@MISC{noauthor_undated-jn,
  title = "Mudigonda, Jawahar, Narayanan - 2004 - Geometric structure
           computation from conics-annotated.pdf"
}

@ARTICLE{Qadir_undated-tm,
  title  = "A {Line-Point} Unified Solution to Relative Camera Pose Estimation",
  author = "Qadir, Ashraf and Neubert, Jeremiah"
}

@ARTICLE{Jain_undated-sg,
  title  = "Homography Estimation from Planar Contours",
  author = "Jain, Paresh Kumar and Jawahar, C V"
}

@ARTICLE{Vidal_undated-ns,
  title  = "Structure from Planar Motions with Small Baselines",
  author = "Vidal, Ren e and Oliensis, John"
}

@MISC{noauthor_undated-az,
  title = "Torr - 1998 - Geometric motion segmentation and model
           selection-annotated.pdf"
}

@ARTICLE{Nie_undated-nb,
  title  = "Subspace Clustering via New {Low-Rank} Model with Discrete Group
            Structure Constraint",
  author = "Nie, Feiping and Huang, Heng"
}

@ARTICLE{Julia_undated-ps,
  title  = "A Critical Review of the Trifocal Tensor Estimation",
  author = "Juli{\`a}, Laura and Monasse, Pascal"
}

@ARTICLE{Irani_undated-kp,
  title  = "Parallax Geometry of Pairs of Points for {D} Scene Analysis",
  author = "Irani, Michal and Anandan, P"
}

@ARTICLE{Johnson_undated-dt,
  title  = "Accelerating Stochastic Gradient Descent using Predictive Variance
            Reduction",
  author = "Johnson, Rie and Zhang, Tong"
}

@MISC{noauthor_undated-dn,
  title = "Shum, Szeliski - 1998 - Construction and refinement of panoramic
           mosaics with global and local alignment-annotated.pdf"
}

@MISC{noauthor_undated-re,
  title = "Zhang, Hanson - 1996 - {3D} Reconstruction Based on Homography
           Mapping-annotated.pdf"
}

@ARTICLE{Ng_undated-hf,
  title  = "{CS294A} Lecture notes",
  author = "Ng, Andrew"
}

@ARTICLE{Mayer_undated-er,
  title  = "{ESTIMATION} {OF} {AND} {VIEW} {SYNTHESIS} {WITH} {THE} {TRIFOCAL}
            {TENSOR}",
  author = "Mayer, Helmut"
}

@ARTICLE{Malis_undated-bx,
  title  = "Deeper understanding of the homography decomposition for
            vision-based control",
  author = "Malis, Ezio and Vargas, Manuel"
}

@ARTICLE{Zhang_undated-cp,
  title  = "{Low-Rank-Sparse} Subspace Representation for Robust Regression",
  author = "Zhang, Yongqiang and Shi, Daming"
}

@ARTICLE{Tsakiris_undated-ay,
  title  = "Hyperplane Clustering Via Dual Principal Component Pursuit",
  author = "Tsakiris, Manolis C and Vidal, Rene"
}

@ARTICLE{Arandjelovic_undated-hp,
  title  = "Three things everyone should know to improve object retrieval",
  author = "Arandjelovic, Relja and Zisserman, Andrew"
}

@ARTICLE{Yang_undated-yv,
  title  = "{Low-Rank} Doubly Stochastic Matrix Decomposition for Cluster
            Analysis",
  author = "Yang, Zhirong"
}

@ARTICLE{Shashua_undated-dp,
  title  = "{HomographyTensors}: On Algebraic Entities That Represent Three
            Views of Static or Moving Planar Points",
  author = "Shashua, Amnon and Wolf, Lior"
}

@ARTICLE{Boyd_undated-tl,
  title  = "Additional Exercises for Convex Optimization",
  author = "Boyd, Stephen and Vandenberghe, Lieven"
}

@ARTICLE{Author_undated-yd,
  title  = "A critique of self-expressive deep subspace clustering",
  author = "Author, Anonymous"
}

@INPROCEEDINGS{Edelman2022-vr,
  title     = "Inductive Biases and Variable Creation in {Self-Attention}
               Mechanisms",
  booktitle = "Proceedings of the 39th International Conference on Machine
               Learning",
  author    = "Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang,
               Cyril",
  editor    = "Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and
               Szepesvari, Csaba and Niu, Gang and Sabato, Sivan",
  abstract  = "Self-attention, an architectural motif designed to model
               long-range interactions in sequential data, has driven numerous
               recent breakthroughs in natural language processing and beyond.
               This work provides a theoretical analysis of the inductive
               biases of self-attention modules. Our focus is to rigorously
               establish which functions and long-range dependencies
               self-attention blocks prefer to represent. Our main result shows
               that bounded-norm Transformer networks ``create sparse
               variables'': a single self-attention head can represent a sparse
               function of the input sequence, with sample complexity scaling
               only logarithmically with the context length. To support our
               analysis, we present synthetic experiments to probe the sample
               complexity of learning sparse Boolean functions with
               Transformers.",
  publisher = "PMLR",
  volume    =  162,
  pages     = "5793--5831",
  series    = "Proceedings of Machine Learning Research",
  year      =  2022
}

@ARTICLE{Covert2023-xl,
  title         = "Learning to Maximize Mutual Information for Dynamic Feature
                   Selection",
  author        = "Covert, Ian and Qiu, Wei and Lu, Mingyu and Kim, Nayoon and
                   White, Nathan and Lee, Su-In",
  abstract      = "Feature selection helps reduce data acquisition costs in ML,
                   but the standard approach is to train models with static
                   feature subsets. Here, we consider the dynamic feature
                   selection (DFS) problem where a model sequentially queries
                   features based on the presently available information. DFS
                   is often addressed with reinforcement learning (RL), but we
                   explore a simpler approach of greedily selecting features
                   based on their conditional mutual information. This method
                   is theoretically appealing but requires oracle access to the
                   data distribution, so we develop a learning approach based
                   on amortized optimization. The proposed method is shown to
                   recover the greedy policy when trained to optimality and
                   outperforms numerous existing feature selection methods in
                   our experiments, thus validating it as a simple but powerful
                   approach for this problem.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2301.00557"
}

@ARTICLE{Authors_undated-eu,
  title  = "{LEARNING} {SPARSE} {AND} {LOW-RANK} {PRIORS} {FOR} {IM-} {AGE}
            {RECOVERY} {VIA} {ITERATIVE} {REWEIGHTED} {LEAST} {SQUARES}
            {MINIMIZATION}",
  author = "Authors, Anonymous"
}

@ARTICLE{Yang2019-lf,
  title         = "Unsupervised Moving Object Detection via Contextual
                   Information Separation",
  author        = "Yang, Yanchao and Loquercio, Antonio and Scaramuzza, Davide
                   and Soatto, Stefano",
  abstract      = "We propose an adversarial contextual model for detecting
                   moving objects in images. A deep neural network is trained
                   to predict the optical flow in a region using information
                   from everywhere else but that region (context), while
                   another network attempts to make such context as
                   uninformative as possible. The result is a model where
                   hypotheses naturally compete with no need for explicit
                   regularization or hyper-parameter tuning. Although our
                   method requires no supervision whatsoever, it outperforms
                   several methods that are pre-trained on large annotated
                   datasets. Our model can be thought of as a generalization of
                   classical variational generative region-based segmentation,
                   but in a way that avoids explicit regularization or solution
                   of partial differential equations at run-time.",
  month         =  jan,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1901.03360"
}

@ARTICLE{Schwab2012-hx,
  title    = "Estimation of non-negative {ODFs} using the eigenvalue
              distribution of spherical functions",
  author   = "Schwab, Evan and Afsari, Bijan and Vidal, Ren{\'e}",
  abstract = "Current methods in high angular resolution diffusion imaging
              (HARDI) estimate the probability density function of water
              diffusion as a continuous-valued orientation distribution
              function (ODF) on the sphere. However, such methods could produce
              an ODF with negative values, because they enforce non-negativity
              only at finitely many directions. In this paper, we propose to
              enforce non-negativity on the continuous domain by enforcing the
              positive semi-definiteness of Toeplitz-like matrices constructed
              from the spherical harmonic representation of the ODF. We study
              the distribution of the eigenvalues of these matrices and use it
              to derive an iterative semi-definite program that enforces
              non-negativity on the continuous domain. We illustrate the
              performance of our method and compare it to the state-of-the-art
              with experiments on synthetic and real data.",
  journal  = "Med. Image Comput. Comput. Assist. Interv.",
  volume   =  15,
  number   = "Pt 2",
  pages    = "322--330",
  year     =  2012,
  language = "en"
}

@ARTICLE{Peng_undated-bb,
  title  = "{RES-PCA}: A Scalable Approach to Recovering Low-rank Matrices",
  author = "Peng, Chong and Chen, Chenglizhao and Kang, Zhao and Li, Jianbo and
            Cheng, Qiang"
}

@ARTICLE{Zietlow2021-oe,
  title         = "Demystifying Inductive Biases for {$\beta$-VAE} Based
                   Architectures",
  author        = "Zietlow, Dominik and Rolinek, Michal and Martius, Georg",
  abstract      = "The performance of $\beta$-Variational-Autoencoders
                   ($\beta$-VAEs) and their variants on learning semantically
                   meaningful, disentangled representations is unparalleled. On
                   the other hand, there are theoretical arguments suggesting
                   the impossibility of unsupervised disentanglement. In this
                   work, we shed light on the inductive bias responsible for
                   the success of VAE-based architectures. We show that in
                   classical datasets the structure of variance, induced by the
                   generating factors, is conveniently aligned with the latent
                   directions fostered by the VAE objective. This builds the
                   pivotal bias on which the disentangling abilities of VAEs
                   rely. By small, elaborate perturbations of existing
                   datasets, we hide the convenient correlation structure that
                   is easily exploited by a variety of architectures. To
                   demonstrate this, we construct modified versions of standard
                   datasets in which (i) the generative factors are perfectly
                   preserved; (ii) each image undergoes a mild transformation
                   causing a small change of variance; (iii) the leading
                   \textbackslashtextbf\{VAE-based disentanglement
                   architectures fail to produce disentangled representations
                   whilst the performance of a non-variational method remains
                   unchanged\}. The construction of our modifications is
                   nontrivial and relies on recent progress on mechanistic
                   understanding of $\beta$-VAEs and their connection to PCA.
                   We strengthen that connection by providing additional
                   insights that are of stand-alone interest.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2102.06822"
}

@ARTICLE{Ressl_undated-js,
  title  = "A {MINIMAL} {SET} {OF} {CONSTRAINTS} {AND} A {MINIMAL}
            {PARAMETERIZATION} {FOR} {THE} {TRIFOCAL} {TENSOR}",
  author = "Ressl, C"
}

@ARTICLE{Du2019-lw,
  title     = "Existence and Uniqueness of Zeros for {Vector-Valued} Functions
               with {K-Adjustability} Convexity and Their Applications",
  author    = "Du, Wei-Shih",
  abstract  = "In this paper, we introduce the new concepts of K-adjustability
               convexity and strictly K-adjustability convexity which
               respectively generalize and extend the concepts of K-convexity
               and strictly K-convexity. We establish some new existence and
               uniqueness theorems of zeros for vector-valued functions with
               K-adjustability convexity. As their applications, we obtain
               existence theorems for the minimization problem and fixed point
               problem which are original and quite different from the known
               results in the existing literature.",
  journal   = "Sci. China Ser. A Math.",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  7,
  number    =  9,
  pages     = "809",
  month     =  sep,
  year      =  2019,
  language  = "en"
}

@INPROCEEDINGS{Pimentel-Alarcon2016-jl,
  title     = "Group-sparse subspace clustering with missing data",
  booktitle = "2016 {IEEE} Statistical Signal Processing Workshop ({SSP})",
  author    = "Pimentel-Alarc{\'o}n, D and Balzano, L and Marcia, R and Nowak,
               R and Willett, R",
  abstract  = "This paper explores algorithms for subspace clustering with
               missing data. In many high-dimensional data analysis settings,
               data points Lie in or near a union of subspaces. Subspace
               clustering is the process of estimating these subspaces and
               assigning each data point to one of them. However, in many
               modern applications the data are severely corrupted by missing
               values. This paper describes two novel methods for subspace
               clustering with missing data: (a) group-sparse sub-space
               clustering (GSSC), which is based on group-sparsity and
               alternating minimization, and (b) mixture subspace clustering
               (MSC), which models each data point as a convex combination of
               its projections onto all subspaces in the union. Both of these
               algorithms are shown to converge to a local minimum, and
               experimental results show that they outperform the previous
               state-of-the-art, with GSSC yielding the highest overall
               clustering accuracy.",
  pages     = "1--5",
  month     =  jun,
  year      =  2016,
  keywords  = "Clustering algorithms;Signal processing
               algorithms;Minimization;Data models;Signal
               processing;Conferences;Optimization;Low-rank matrix
               completion;low-dimensional models;lasso;sparsity;subspace
               clustering;missing data;alternating optimization;compressed
               sensing"
}

@ARTICLE{Brown_undated-fe,
  title  = "Recognising Panoramas",
  author = "Brown, M and Lowe, D G"
}

@ARTICLE{Tron_undated-lt,
  title  = "A Benchmark for the Comparison of {3-D} Motion Segmentation
            Algorithms",
  author = "Tron, Roberto and Vidal, Rene"
}

@MISC{noauthor_undated-vv,
  title = "Rublee et al. - 2011 - {ORB} An efficient alternative to {SIFT} or
           {SURF-annotated.pdf}"
}

@ARTICLE{Peck_undated-bt,
  title  = "Robustness of Classifiers to Adversarial Perturbations",
  author = "Peck, Jonathan"
}

@ARTICLE{Kaba2019-lf,
  title         = "What is the Largest Sparsity Pattern that Can Be Recovered
                   by 1-Norm Minimization?",
  author        = "Kaba, Mustafa D and Zhao, Mengnan and Vidal, Rene and
                   Robinson, Daniel P and Mallada, Enrique",
  abstract      = "Much of the existing literature in sparse recovery is
                   concerned with the following question: given a sparsity
                   pattern and a corresponding regularizer, derive conditions
                   on the dictionary under which exact recovery is possible. In
                   this paper, we study the opposite question: given a
                   dictionary and the 1-norm regularizer, find the largest
                   sparsity pattern that can be recovered. We show that such a
                   pattern is described by a mathematical object called a
                   ``maximum abstract simplicial complex'', and provide two
                   different characterizations of this object: one based on
                   extreme points and the other based on vectors of minimal
                   support. In addition, we show how this new framework is
                   useful in the study of sparse recovery problems when the
                   dictionary takes the form of a graph incidence matrix or a
                   partial discrete Fourier transform. In case of incidence
                   matrices, we show that the largest sparsity pattern that can
                   be recovered is determined by the set of simple cycles of
                   the graph. As a byproduct, we show that standard sparse
                   recovery can be certified in polynomial time, although this
                   is known to be NP-hard for general matrices. In the case of
                   the partial discrete Fourier transform, our characterization
                   of the largest sparsity pattern that can be recovered
                   requires the unknown signal to be real and its dimension to
                   be a prime number.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "eess.SP",
  eprint        = "1910.05652"
}

@MISC{Arora_undated-xm,
  title        = "Clustering by left-stochastic matrix factorization",
  author       = "Arora, Raman and Gupta, Maya R and Kapila, Amol and Fazel,
                  Maryam",
  abstract     = "We propose clustering samples given their pairwise
                  similarities by factorizing the similarity matrix into the
                  product of a cluster probability matrix and its transpose. We
                  propose a rotation-based algorithm to compute this
                  left-stochastic decomposition (LSD). Theoretical results link
                  the LSD clustering method to a soft kernel k-means
                  clustering, give conditions for when the factorization and
                  clustering are unique, and provide error bounds. Experimental
                  results on simulated and real similarity datasets show that
                  the proposed method reliably provides accurate clusterings.",
  howpublished = "\url{https://icml.cc/Conferences/2011/papers/426_icmlpaper.pdf}",
  note         = "Accessed: 2022-7-23"
}

@ARTICLE{Hyvarinen_undated-hc,
  title    = "Estimation of {Non-Normalized} Statistical Models by Score
              Matching",
  author   = "{Hyvarinen}",
  journal  = "J. Mach. Learn. Res.",
  keywords = "Pointers"
}

@MISC{noauthor_undated-ys,
  title = "Girshick - 2015 - Fast {R-CNN-annotated.pdf}"
}

@ARTICLE{Tron_undated-jf,
  title  = "A Benchmark for the Comparison of {3-D} Motion Segmentation
            Algorithms",
  author = "Tron, Roberto and Vidal, Rene"
}

@MISC{noauthor_undated-xw,
  title = "Vision, Pair - 2013 - Degenerate Configurations for Fundamental
           Matrix Estimation A Note on Oriented Epipolar
           Constraint-annotated.pdf"
}

@ARTICLE{Werner_undated-tr,
  title  = "Model Selection for Automated Architectural Reconstruction from
            Multiple Views",
  author = "Werner, Tomas and Zisserman, Andrew"
}

@ARTICLE{Dubrofsky_undated-vn,
  title  = "Homography Estimation",
  author = "Dubrofsky, Elan"
}

@ARTICLE{Barazzetti_undated-rn,
  title  = "Commission {V} Symposium, Newcastle upon Tyne, {UK}. 2010",
  author = "Barazzetti, L and Remondino, F and Scaioni, M"
}

@ARTICLE{Liebowitz_undated-fy,
  title  = "Metric Rectification for Perspective Images of Planes",
  author = "Liebowitz, David and {Andrew Zisserman Robotics Research Group}"
}

@ARTICLE{Oskarsson_undated-ul,
  title  = "Efficient Solvers for Minimal Problems by Syzygy-based Reduction",
  author = "Oskarsson, Viktor Larsson Kalle Astrom"
}

@ARTICLE{Li_undated-rh,
  title  = "{THE} {TRIFOCAL} {TENSOR} {AND} {ITS} {APPLICATIONS} {IN}
            {AUGMENTED} {REALITY}",
  author = "Li, Jia"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{noauthor_2006-hk,
  title     = "Diffusion maps",
  abstract  = "In this paper, we provide a framework based upon diffusion
               processes for finding meaningful geometric descriptions of data
               sets. We show that eigenfun…",
  journal   = "Appl. Comput. Harmon. Anal.",
  publisher = "Academic Press",
  volume    =  21,
  number    =  1,
  pages     = "5--30",
  month     =  jul,
  year      =  2006
}

@ARTICLE{Psenka_undated-ja,
  title  = "Representation Learning Through Manifold Flattening and
            Reconstruction",
  author = "Psenka, Michael"
}

@ARTICLE{Zantedeschi2023-ny,
  title         = "{DAG} Learning on the Permutahedron",
  author        = "Zantedeschi, Valentina and Franceschi, Luca and Kaddour,
                   Jean and Kusner, Matt J and Niculae, Vlad",
  abstract      = "We propose a continuous optimization framework for
                   discovering a latent directed acyclic graph (DAG) from
                   observational data. Our approach optimizes over the polytope
                   of permutation vectors, the so-called Permutahedron, to
                   learn a topological ordering. Edges can be optimized
                   jointly, or learned conditional on the ordering via a
                   non-differentiable subroutine. Compared to existing
                   continuous optimization approaches our formulation has a
                   number of advantages including: 1. validity: optimizes over
                   exact DAGs as opposed to other relaxations optimizing
                   approximate DAGs; 2. modularity: accommodates any
                   edge-optimization procedure, edge structural
                   parameterization, and optimization loss; 3. end-to-end:
                   either alternately iterates between node-ordering and
                   edge-optimization, or optimizes them jointly. We
                   demonstrate, on real-world data problems in
                   protein-signaling and transcriptional network discovery,
                   that our approach lies on the Pareto frontier of two key
                   metrics, the SID and SHD.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2301.11898"
}

@ARTICLE{Paffenholz2013-iq,
  title         = "Faces of Birkhoff Polytopes",
  author        = "Paffenholz, Andreas",
  abstract      = "The Birkhoff polytope B(n) is the convex hull of all (n x n)
                   permutation matrices, i.e., matrices where precisely one
                   entry in each row and column is one, and zeros at all other
                   places. This is a widely studied polytope with various
                   applications throughout mathematics. In this paper we study
                   combinatorial types L of faces of a Birkhoff polytope. The
                   Birkhoff dimension bd(L) of L is the smallest n such that
                   B(n) has a face with combinatorial type L. By a result of
                   Billera and Sarangarajan, a combinatorial type L of a
                   d-dimensional face appears in some B(k) for k less or equal
                   to 2d, so bd(L) is at most d. We will characterize those
                   types whose Birkhoff dimension is at least 2d-3, and we
                   prove that any type whose Birkhoff dimension is at least d
                   is either a product or a wedge over some lower dimensional
                   face. Further, we computationally classify all d-dimensional
                   combinatorial types for d between 2 and 8.",
  month         =  apr,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "math.CO",
  eprint        = "1304.3948"
}

@ARTICLE{Han2020-jn,
  title    = "Unsupervised semantic aggregation and deformable template
              matching for semi-supervised learning",
  author   = "Han, Tao and Gao, Junyu and Yuan, Yuan and Wang, Qi",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  33,
  pages    = "9972--9982",
  year     =  2020
}

@ARTICLE{Gross2009-el,
  title         = "Recovering low-rank matrices from few coefficients in any
                   basis",
  author        = "Gross, David",
  abstract      = "We present novel techniques for analyzing the problem of
                   low-rank matrix recovery. The methods are both considerably
                   simpler and more general than previous approaches. It is
                   shown that an unknown (n x n) matrix of rank r can be
                   efficiently reconstructed from only O(n r nu log^2 n)
                   randomly sampled expansion coefficients with respect to any
                   given matrix basis. The number nu quantifies the ``degree of
                   incoherence'' between the unknown matrix and the basis.
                   Existing work concentrated mostly on the problem of ``matrix
                   completion'' where one aims to recover a low-rank matrix
                   from randomly selected matrix elements. Our result covers
                   this situation as a special case. The proof consists of a
                   series of relatively elementary steps, which stands in
                   contrast to the highly involved methods previously employed
                   to obtain comparable results. In cases where bounds had been
                   known before, our estimates are slightly tighter. We discuss
                   operator bases which are incoherent to all low-rank matrices
                   simultaneously. For these bases, we show that O(n r nu log
                   n) randomly sampled expansion coefficients suffice to
                   recover any low-rank matrix with high probability. The
                   latter bound is tight up to multiplicative constants.",
  month         =  oct,
  year          =  2009,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IT",
  eprint        = "0910.1879"
}

@INPROCEEDINGS{Wang2022-rt,
  title     = "Convergence and Recovery Guarantees of the {K-Subspaces} Method
               for Subspace Clustering",
  booktitle = "the 39th International Conference on Machine Learning",
  author    = "Wang, Peng and Liu, Huikang and So, Anthony Man-Cho and Balzano,
               Laura",
  editor    = "Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and
               Szepesvari, Csaba and Niu, Gang and Sabato, Sivan",
  abstract  = "The K-subspaces (KSS) method is a generalization of the K-means
               method for subspace clustering. In this work, we present local
               convergence analysis and a recovery guarantee for KSS, assuming
               data are generated by the semi-random union of subspaces model,
               where $N$ points are randomly sampled from $K \ge 2$ overlapping
               subspaces. We show that if the initial assignment of the KSS
               method lies within a neighborhood of a true clustering, it
               converges at a superlinear rate and finds the correct clustering
               within $\Theta(\log\log N)$ iterations with high probability.
               Moreover, we propose a thresholding inner-product based spectral
               method for initialization and prove that it produces a point in
               this neighborhood. We also present numerical results of the
               studied method to support our theoretical developments.",
  publisher = "PMLR",
  volume    =  162,
  pages     = "22884--22918",
  series    = "Proceedings of Machine Learning Research",
  year      =  2022
}

@ARTICLE{Hager2016-sx,
  title     = "Projection onto a Polyhedron that Exploits Sparsity",
  author    = "Hager, William W and Zhang, Hongchao",
  journal   = "SIAM J. Optim.",
  publisher = "Society for Industrial \& Applied Mathematics (SIAM)",
  volume    =  26,
  number    =  3,
  pages     = "1773--1798",
  month     =  jan,
  year      =  2016
}

@INPROCEEDINGS{Zhang2021-sx,
  title     = "Learning a self-Expressive Network for subspace clustering",
  booktitle = "{IEEE} Conference on Computer Vision and Pattern Recognition",
  author    = "Zhang, Shangzhi and You, Chong and Vidal, Ren{\'e} and Li,
               Chun-Guang",
  abstract  = "State-of-the-art subspace clustering methods are based on
               self-expressive model, which represents each data point as a
               linear combination of other data points. However, such methods
               are designed for a finite sample dataset and lack the ability to
               generalize to out-of-sample data. Moreover, since the number of
               self-expressive coefficients grows quadratically with the number
               of data points, their ability to handle large-scale datasets is
               often limited. In this paper, we propose a novel framework for
               subspace clustering, termed Self-Expressive Network (SENet),
               which employs a properly designed neural network to learn a
               self-expressive representation of the data. We show that our
               SENet can not only learn the self-expressive coefficients with
               desired properties on the training data, but also handle
               out-of-sample data. Besides, we show that SENet can also be
               leveraged to perform subspace clustering on large-scale
               datasets. Extensive experiments conducted on synthetic data and
               real world benchmark data validate the effectiveness of the
               proposed method. In particular, SENet yields highly competitive
               performance on MNIST, Fashion MNIST and Extended MNIST and
               state-of-the-art performance on CIFAR-10. The code is available
               at https://github.com/zhangsz1998/Self-Expressive-Network.",
  month     =  oct,
  year      =  2021,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/"
}

@ARTICLE{Strehl2002-wx,
  title   = "Cluster Ensembles -- A Knowledge Reuse Framework for Combining
             Multiple Partitions",
  author  = "{Strehl} and {Ghosh}",
  journal = "Journal of machine learning research",
  year    =  2002
}

@ARTICLE{Chen2020-zj,
  title    = "Big self-supervised models are strong semi-supervised learners",
  author   = "Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi,
              Mohammad and Hinton, Geoffrey E",
  journal  = "Advances in neural information processing systems",
  volume   =  33,
  pages    = "22243--22255",
  year     =  2020,
  language = "en"
}

@ARTICLE{Lee_undated-cm,
  title  = "Membership Representation for Detecting Block-diagonal Structure in
            Low-rank or Sparse Subspace Clustering",
  author = "Lee, Minsik and Lee, Jieun and Lee, Hyeogjin and Kwak, Nojun"
}

@ARTICLE{Yu_undated-vd,
  title  = "Recursive Camera Motion Estimation with Trif ocal Tensor",
  author = "Yu, Ying Kin and Wong, Kin Hong and Chang, Michael Ming Yuen and
            Or, Siu Hang"
}

@ARTICLE{Fan_undated-jv,
  title  = "Geometric Estimation via Robust Subspace Recovery",
  author = "Fan, Aoxiang and Jiang, Xingyu and Wang, Yang and Jiang, Junjun and
            Ma, Jiayi"
}

@ARTICLE{Feng_undated-yy,
  title  = "Complementarity Formulations of `-norm Optimization Problems",
  author = "Feng, Mingbin and Mitchell, John E and Pang, Jong-Shi and Shen, Xin
            and Wachter, Andreas"
}

@MISC{Hartley_undated-tx,
  title   = "The multibody trifocal tensor: motion segmentation from 3
             perspective views",
  author  = "Hartley, R and Vidal, R",
  journal = "Proceedings of the 2004 IEEE Computer Society Conference on
             Computer Vision and Pattern Recognition, 2004. CVPR 2004."
}

@ARTICLE{Trevor_undated-xx,
  title  = "Efficient Organized Point Cloud Segmentation with Connected
            Components",
  author = "Trevor, Alexander J B and Gedikli, Suat and Rusu, Radu B and
            Christensen, Henrik I"
}

@MISC{Liu_undated-or,
  title       = "liuzhuang13 - Overview",
  author      = "Liu, Zhuang",
  abstract    = "PhD student at UC Berkeley. liuzhuang13 has 15 repositories
                 available. Follow their code on GitHub.",
  institution = "Github",
  language    = "en"
}

@MISC{Mayer2003-ti,
  title   = "Robust Orientation, Calibration, and Disparity Estimation of Image
             Triplets",
  author  = "Mayer, Helmut",
  journal = "Lecture Notes in Computer Science",
  pages   = "281--288",
  year    =  2003
}

@MISC{Decker2008-gx,
  title   = "Dealing with degeneracy in essential matrix estimation",
  author  = "Decker, Peter and Paulus, Dietrich and Feldmann, Tobias",
  journal = "2008 15th IEEE International Conference on Image Processing",
  year    =  2008
}

@ARTICLE{Khropov_undated-xd,
  title  = "Reconstruction of projective and metric cameras for image triplets",
  author = "Khropov, Andrey and Shokurov, Anton and Lempitskiy, Victor and
            Ivanov, Denis"
}

@ARTICLE{Fang2016-er,
  title         = "Vision-aided Localization and Navigation Based on Trifocal
                   Tensor",
  author        = "Fang, Qiang",
  abstract      = "In this paper, a novel method for vision-aided navigation
                   based on trifocal tensor is presented. The main goal of the
                   proposed method is to provide position estimation in
                   GPS-denied environments for vehicles equipped with a
                   standard inertial navigation systems(INS) and a single
                   camera only. We treat the trifocal tensor as the measurement
                   model, being only concerned about the vehicle state and do
                   not estimate the the position of the tracked landmarks. The
                   performance of the proposed method is demonstrated using
                   simulation and experimental data.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1611.03538"
}

@MISC{Zeng_undated-fy,
  title       = "{E3Outlier}: Effective End-to-end Unsupervised Outlier
                 Detection via Inlier Priority of Discriminative Network",
  author      = "Zeng, Yijie",
  abstract    = "Effective End-to-end Unsupervised Outlier Detection via Inlier
                 Priority of Discriminative Network - demonzyj56/E3Outlier:
                 Effective End-to-end Unsupervised Outlier Detection via Inlier
                 Priority of Discriminative Network",
  institution = "Github",
  language    = "en"
}

@MISC{Jiang2015-ow,
  title   = "Direct structure estimation for {3D} reconstruction",
  author  = "Jiang, Nianjuan and Lin, Wen-Yan and Do, Minh N and Lu, Jiangbo",
  journal = "2015 IEEE Conference on Computer Vision and Pattern Recognition
             (CVPR)",
  year    =  2015
}

@INPROCEEDINGS{Zhang2019-yp,
  title     = "{Self-Supervised} Convolutional Subspace Clustering Network",
  booktitle = "{IEEE/CVF} conference on computer vision and pattern recognition",
  author    = "Zhang, Junjian and Li, Chun-Guang and You, Chong and Qi,
               Xianbiao and Zhang, Honggang and Guo, Jun and Lin, Zhouchen",
  year      =  2019
}

@ARTICLE{Okuma_undated-at,
  title  = "{AUTOMATIC} {RECTIFICATION} {OF} {LONG} {IMAGE} {SEQUENCES}",
  author = "Okuma, Kenji and Little, James J and Lowe, David G"
}

@ARTICLE{Liu_undated-we,
  title  = "{PlaneNet}: Piece-wise Planar Reconstruction from a Single {RGB}
            Image",
  author = "Liu, Chen and Yang, Jimei and Ceylan, Duygu and Yumer, Ersin and
            Furukawa, Yasutaka"
}

@MISC{Grant2013-br,
  title   = "Finding planes in {LiDAR} point clouds for real-time registration",
  author  = "Grant, W Shane and Shane Grant, W and Voorhies, Randolph C and
             Itti, Laurent",
  journal = "2013 IEEE/RSJ International Conference on Intelligent Robots and
             Systems",
  year    =  2013
}

@MISC{Pollefeys2002-kf,
  title   = "Surviving Dominant Planes in Uncalibrated Structure and Motion
             Recovery",
  author  = "Pollefeys, Marc and Verbiest, Frank and Van Gool, Luc",
  journal = "Computer Vision --- ECCV 2002",
  pages   = "837--851",
  year    =  2002
}

@ARTICLE{Sinkhorn1964-zd,
  title     = "A Relationship Between Arbitrary Positive Matrices and Doubly
               Stochastic Matrices",
  author    = "Sinkhorn, Richard",
  journal   = "Ann. Math. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  35,
  number    =  2,
  pages     = "876--879",
  year      =  1964
}

@ARTICLE{Dai_undated-ec,
  title  = "Discriminative Embeddings of Latent Variable Models for Structured
            Data",
  author = "Dai, Hanjun and Dai, Bo and Song, Le"
}

@ARTICLE{Grundmann_undated-at,
  title  = "{Calibration-Free} Rolling Shutter Removal",
  author = "Grundmann, Matthias and Kwatra, Vivek and Castro, Daniel and Essa,
            Irfan"
}

@ARTICLE{Luo_undated-zg,
  title  = "Forging The Graphs: A Low Rank and Positive Semidefinite Graph
            Learning Approach",
  author = "Luo, Dijun and Ding, Chris and Huang, Heng and Nie, Feiping"
}

@MISC{Moulon2013-vj,
  title   = "Adaptive Structure from Motion with a Contrario Model Estimation",
  author  = "Moulon, Pierre and Monasse, Pascal and Marlet, Renaud",
  journal = "Computer Vision -- ACCV 2012",
  pages   = "257--270",
  year    =  2013
}

@ARTICLE{Esteves_undated-ec,
  title  = "Equivariant {Multi-View} Networks",
  author = "Esteves, Carlos and Xu, Yinshuang and Allen-Blanchette, Christine
            and Daniilidis, Kostas"
}

@ARTICLE{Kuang_undated-dp,
  title  = "Revisiting Trifocal Tensor Estimation using Lines",
  author = "Kuang, Yubin and Oskarsson, Magnus and Astrom, Kalle"
}

@MISC{Hu2014-vt,
  title   = "Smooth Representation Clustering",
  author  = "Hu, Han and Lin, Zhouchen and Feng, Jianjiang and Zhou, Jie",
  journal = "2014 IEEE Conference on Computer Vision and Pattern Recognition",
  year    =  2014
}

@MISC{Zass2005-gr,
  title   = "A unifying approach to hard and probabilistic clustering",
  author  = "Zass, R and Shashua, A",
  journal = "Tenth IEEE International Conference on Computer Vision (ICCV'05)
             Volume 1",
  year    =  2005
}

@MISC{Lane2019-ya,
  title   = "Adaptive Online k-Subspaces with Cooperative {Re-Initialization}",
  author  = "Lane, Connor and Haeffele, Benjamin D and Vidal, Rene",
  journal = "2019 IEEE/CVF International Conference on Computer Vision Workshop
             (ICCVW)",
  year    =  2019
}

@ARTICLE{Yu2008-no,
  title   = "Robust {3-D} Motion Tracking from Stereo Images: A Model-less
             Method",
  author  = "Yu, Ying Kin and Wong, Kin Hong and Or, Siu Hang and Chang,
             Michael Ming Yuen",
  journal = "Stereo Images: A Model-Less Method``, IEEE Transaction on
             Instrumentation and Measurement",
  volume  =  57,
  number  =  3,
  year    =  2008
}

@ARTICLE{Dhillon_undated-kq,
  title  = "A Unified View of Kernel k-means, Spectral Clustering and Graph
            Cuts",
  author = "Dhillon, Inderjit and Guan, Yuqiang and Kulis, Brian"
}

@ARTICLE{Li2019-za,
  title         = "Theory of Spectral Method for Union of {Subspaces-Based}
                   Random Geometry Graph",
  author        = "Li, Gen and Gu, Yuantao",
  abstract      = "Spectral Method is a commonly used scheme to cluster data
                   points lying close to Union of Subspaces by first
                   constructing a Random Geometry Graph, called Subspace
                   Clustering. This paper establishes a theory to analyze this
                   method. Based on this theory, we demonstrate the efficiency
                   of Subspace Clustering in fairly broad conditions. The
                   insights and analysis techniques developed in this paper
                   might also have implications for other random graph
                   problems. Numerical experiments demonstrate the
                   effectiveness of our theoretical study.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1907.10906"
}

@ARTICLE{Hsu_undated-jc,
  title  = "{MOVING} {CAMERA} {VIDEO} {STABILIZATION} {USING} {HOMOGRAPHY}
            {CONSISTENCY}",
  author = "Hsu, Yu-Feng and Chou, Cheng-Chuan and Shih, Ming-Yu"
}

@INPROCEEDINGS{You2017-ik,
  title           = "Provable self-representation based outlier detection in a
                     union of subspaces",
  booktitle       = "2017 {IEEE} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "You, Chong and Robinson, Daniel P and Vidal, Rene",
  publisher       = "IEEE",
  month           =  jul,
  year            =  2017,
  conference      = "2017 IEEE Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Honolulu, HI"
}

@ARTICLE{Nie_undated-ra,
  title  = "Department of Computer Science and Engineering, University of Texas
            at Arlington, {USA}",
  author = "Nie, Feiping and Wang, Hua and Deng, Cheng and Gao, Xinbo and Li,
            Xuelong and Huang, Heng"
}

@ARTICLE{Yi_undated-oi,
  title  = "Learning to Find Good Correspondences",
  author = "Yi, Kwang Moo and Trulls, Eduard and Ono, Yuki and Lepetit, Vincent
            and Salzmann, Mathieu and Fua, Pascal"
}

@ARTICLE{Kendall_undated-ac,
  title  = "{PoseNet}: A Convolutional Network for {Real-Time} {6-DOF} Camera
            Relocalization",
  author = "Kendall, Alex and Grimes, Matthew and Cipolla, Roberto"
}

@INPROCEEDINGS{Ding2020-ao,
  title     = "Robust homography estimation via dual principal component
               pursuit",
  booktitle = "{IEEE/CVF} Conference on Computer Vision and Pattern Recognition",
  author    = "Ding, Tianjiao and Yang, Yunchen and Zhu, Zhihui and Robinson,
               Daniel P and Vidal, Ren{\'e} and Kneip, Laurent and Tsakiris,
               Manolis C",
  pages     = "6080--6089",
  year      =  2020
}

@ARTICLE{Wang2017-lr,
  title         = "Nonconvex Generalization of Alternating Direction Method of
                   Multipliers for Nonlinear Equality Constrained Problems",
  author        = "Wang, Junxiang and Zhao, Liang",
  abstract      = "The classic Alternating Direction Method of Multipliers
                   (ADMM) is a popular framework to solve linear-equality
                   constrained problems. In this paper, we extend the ADMM
                   naturally to nonlinear equality-constrained problems, called
                   neADMM. The difficulty of neADMM is to solve nonconvex
                   subproblems. We provide globally optimal solutions to them
                   in two important applications. Experiments on synthetic and
                   real-world datasets demonstrate excellent performance and
                   scalability of our proposed neADMM over existing
                   state-of-the-start methods.",
  month         =  may,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1705.03412"
}

@ARTICLE{Salaun_undated-te,
  title  = "Robust and Accurate Line- and/or {Point-Based} Pose Estimation
            without Manhattan Assumptions",
  author = "Salaun, Yohann and Marlet, Renaud and Monasse, Pascal"
}

@INPROCEEDINGS{Ding2019-vw,
  title     = "Noisy Dual Principal Component Pursuit",
  booktitle = "International Conference on Machine Learning",
  author    = "Ding, Tianyu and Zhu, Zhihui and Ding, Tianjiao and Yang,
               Yunchen and Robinson, Daniel P and Tsakiris, Manolis C and
               Vidal, Ren{\'e}",
  pages     = "1617--1625",
  year      =  2019
}

@ARTICLE{Yang_undated-zk,
  title  = "Robust Statistical Estimation and Segmentation of Multiple
            Subspaces",
  author = "Yang, Allen Y and Rao, Shankar R and Ma, Yi"
}

@INPROCEEDINGS{Barath2018-fw,
  title       = "Graph-cut {RANSAC}",
  booktitle   = "Proceedings of the {IEEE} conference on computer vision and
                 pattern recognition",
  author      = "Barath, Daniel and Matas, Ji{\v r}{\'\i}",
  pages       = "6733--6741",
  institution = "Github",
  year        =  2018,
  language    = "en"
}

@ARTICLE{Elhamifar_undated-bo,
  title  = "Finding Exemplars from Pairwise Dissimilarities via Simultaneous
            Sparse Recovery",
  author = "Elhamifar, Ehsan and Sapiro, Guillermo and Vidal, Rene"
}

@ARTICLE{Salas-Moreno_undated-vh,
  title  = "Dense Planar {SLAM}",
  author = "Salas-Moreno, Renato F and Glocker, Ben and Kelly, Paul H J and
            Davison, Andrew J"
}

@MISC{Zhou_undated-pl,
  title       = "{KFNet}: {KFNet}: Learning Temporal Camera Relocalization
                 using Kalman Filtering ({CVPR} 2020 Oral)",
  author      = "Zhou, Lei",
  abstract    = "KFNet: Learning Temporal Camera Relocalization using Kalman
                 Filtering (CVPR 2020 Oral) - zlthinker/KFNet: KFNet: Learning
                 Temporal Camera Relocalization using Kalman Filtering (CVPR
                 2020 Oral)",
  institution = "Github",
  language    = "en"
}

@ARTICLE{Caron2020-el,
  title   = "Unsupervised Learning of Visual Features by Contrasting Cluster
             Assignments",
  author  = "Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal,
             Priya and Bojanowski, Piotr and Joulin, Armand",
  journal = "Advances in neural information processing systems",
  year    =  2020
}

@ARTICLE{Feng_undated-xo,
  title  = "Robust Subspace Segmentation with Block-diagonal Prior",
  author = "Feng, Jiashi and Lin, Zhouchen and Xu, Huan and Yan, Shuicheng"
}

@ARTICLE{Agarwal_undated-hf,
  title  = "A Survey of Planar Homography Estimation Techniques",
  author = "Agarwal, Anubhav and Jawahar, C V and Narayanan, P J"
}

@ARTICLE{Brown_undated-cc,
  title  = "{Multi-Image} Matching using {Multi-Scale} Oriented Patches",
  author = "Brown, Matthew and Szeliski, Richard and Winder, Simon"
}

@ARTICLE{Duff_undated-ko,
  title  = "{PLMP} - {Point-Line} Minimal Problems in Complete {Multi-View}
            Visibility",
  author = "Duff, Timothy and Kohn, Kathlen and Leykin, Anton"
}

@ARTICLE{Peng_undated-zk,
  title  = "Robust Subspace Clustering via Thresholding Ridge Regression",
  author = "Peng, Xi and Yi, Zhang and Tang, Huajin"
}

@ARTICLE{Wang_undated-sc,
  title  = "Adaptively Transforming Graph Matching",
  author = "Wang, Fudong and Xue, Nan and Zhang, Yipeng and Bai, Xiang and Xia,
            Gui-Song"
}

@ARTICLE{Liu_undated-yr,
  title  = "{FloorNet}: A Unified Framework for Floorplan Reconstruction from
            {3D} Scans",
  author = "Liu, Chen and Wu, Jiaye and Furukawa, Yasutaka"
}

@ARTICLE{Salaun_undated-wv,
  title  = "Line-based Robust {SfM} with Little Image Overlap",
  author = "Sala{\"u}n, Yohann and Marlet, Renaud and Monasse, Pascal"
}

@INPROCEEDINGS{Zhu2018-ec,
  title     = "Dual Principal Component Pursuit: Improved Analysis and
               Efficient Algorithms",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Zhu, Zhihui and Wang, Yifan and Robinson, Daniel and Naiman,
               Daniel and Vidal, Rene and Tsakiris, Manolis",
  year      =  2018
}

@ARTICLE{Schmid2000-ty,
  title   = "The Geometry and Matching of Lines and Curves Over Multiple Views",
  author  = "Schmid, Cordelia",
  journal = "Int. J. Comput. Vis.",
  volume  =  40,
  number  =  3,
  pages   = "199--233",
  year    =  2000
}

@ARTICLE{Wang_undated-vf,
  title  = "Learning a {Bi-Stochastic} Data Similarity Matrix",
  author = "Wang, Fei and Li, Ping and Konig, Arnd Christian"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Yang_undated-zg,
  title  = "ℓ-Sparse Subspace Clustering",
  author = "Yang, Yingzhen and Feng, Jiashi and Jojic, Nebojsa and Yang,
            Jianchao and Huang, Thomas S"
}

@ARTICLE{Shen_undated-jg,
  title  = "You Never Cluster Alone",
  author = "Shen, Yuming and Shen, Ziyi and Wang, Menghan and Qin, Jie and
            Torr, Philip H S and Shao, Ling"
}

@ARTICLE{Ivashechkin_undated-wl,
  title   = "{USACv20}: robust essential, fundamental and homography matrix
             estimation",
  author  = "Ivashechkin, Maksym and Barath, Daniel and Matas, Jiri",
  journal = "arXiv"
}

@ARTICLE{Nie_undated-df,
  title  = "The Constrained Laplacian Rank Algorithm for {Graph-Based}
            Clustering",
  author = "Nie, Feiping and Wang, Xiaoqian and Jordan, Michael I and Huang,
            Heng"
}

@ARTICLE{Wu_undated-lz,
  title  = "{3D} {ShapeNets}: A Deep Representation for Volumetric Shapes",
  author = "Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and
            Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong"
}

@MISC{Asano_undated-ae,
  title       = "self-label: Self-labelling via simultaneous clustering and
                 representation learning. ({ICLR} 2020)",
  author      = "Asano, Yuki M",
  abstract    = "Self-labelling via simultaneous clustering and representation
                 learning. (ICLR 2020) - yukimasano/self-label: Self-labelling
                 via simultaneous clustering and representation learning. (ICLR
                 2020)",
  institution = "Github",
  language    = "en"
}

@ARTICLE{Liu_undated-gw,
  title  = "{PlaneRCNN}: {3D} Plane Detection and Reconstruction from a Single
            Image",
  author = "Liu, Chen and Kim, Kihwan and Gu, Jinwei and Furukawa, Yasutaka and
            Kautz, Jan"
}

@ARTICLE{You_undated-kp,
  title  = "A Scalable Exemplar-based Subspace Clustering Algorithm for
            {Class-Imbalanced} Data",
  author = "You, Chong and Li, Chi and Robinson, Daniel P and Vidal, Rene"
}

@ARTICLE{Yu_undated-ti,
  title  = "{Single-Image} Piece-wise Planar {3D} Reconstruction via
            Associative Embedding",
  author = "Yu, Zehao and Zheng, Jia and Lian, Dongze and Zhou, Zihan and Gao,
            Shenghua"
}

@ARTICLE{Balntas_undated-oc,
  title  = "{HPatches}: A benchmark and evaluation of handcrafted and learned
            local descriptors",
  author = "Balntas, Vassileios and Lenc, Karel and Vedaldi, Andrea"
}

@INPROCEEDINGS{Barath2020-qx,
  title           = "{MAGSAC++}, a fast, reliable and accurate robust estimator",
  booktitle       = "2020 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "Barath, Daniel and Noskova, Jana and Ivashechkin, Maksym
                     and Matas, Jiri",
  publisher       = "IEEE",
  institution     = "Github",
  month           =  jun,
  year            =  2020,
  language        = "en",
  conference      = "2020 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Seattle, WA, USA"
}

@ARTICLE{Bian_undated-gx,
  title  = "{GMS}: Grid-based Motion Statistics for Fast, Ultra-robust Feature
            Correspondence",
  author = "Bian, Jia Wang and Lin, Wen-Yan and Matsushita, Yasuyuki"
}

@ARTICLE{Slaughter_undated-kd,
  title  = "Sparse Online {Low-Rank} Projection and Outlier Rejection ({SOLO})
            for {3-D} {Rigid-Body} Motion Registration",
  author = "Slaughter, Chris and Yang, Allen Y and Bagwell, Justin and
            Checkles, Costa and Sentis, Luis and Vishwanath, Sriram"
}

@ARTICLE{Lai_undated-nr,
  title  = "{ROBUST} {SUBSPACE} {RECOVERY} {LAYER} {FOR} {UNSUPERVISED}
            {ANOMALY} {DETECTION}",
  author = "Lai, Chieh-Hsin and Zou, Dongmian and Lerman, Gilad"
}

@ARTICLE{Campos2020-ar,
  title         = "{ORB-SLAM3}: An Accurate {Open-Source} Library for Visual,
                   {Visual-Inertial} and {Multi-Map} {SLAM}",
  author        = "Campos, Carlos and Elvira, Richard and G{\'o}mez
                   Rodr{\'\i}guez, Juan J and Montiel, Jos{\'e} M M and
                   Tard{\'o}s, Juan D",
  abstract      = "This paper presents ORB-SLAM3, the first system able to
                   perform visual, visual-inertial and multi-map SLAM with
                   monocular, stereo and RGB-D cameras, using pin-hole and
                   fisheye lens models. The first main novelty is a
                   feature-based tightly-integrated visual-inertial SLAM system
                   that fully relies on Maximum-a-Posteriori (MAP) estimation,
                   even during the IMU initialization phase. The result is a
                   system that operates robustly in real-time, in small and
                   large, indoor and outdoor environments, and is 2 to 5 times
                   more accurate than previous approaches. The second main
                   novelty is a multiple map system that relies on a new place
                   recognition method with improved recall. Thanks to it,
                   ORB-SLAM3 is able to survive to long periods of poor visual
                   information: when it gets lost, it starts a new map that
                   will be seamlessly merged with previous maps when revisiting
                   mapped areas. Compared with visual odometry systems that
                   only use information from the last few seconds, ORB-SLAM3 is
                   the first system able to reuse in all the algorithm stages
                   all previous information. This allows to include in bundle
                   adjustment co-visible keyframes, that provide high parallax
                   observations boosting accuracy, even if they are widely
                   separated in time or if they come from a previous mapping
                   session. Our experiments show that, in all sensor
                   configurations, ORB-SLAM3 is as robust as the best systems
                   available in the literature, and significantly more
                   accurate. Notably, our stereo-inertial SLAM achieves an
                   average accuracy of 3.6 cm on the EuRoC drone and 9 mm under
                   quick hand-held motions in the room of TUM-VI dataset, a
                   setting representative of AR/VR scenarios. For the benefit
                   of the community we make public the source code.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2007.11898"
}

@ARTICLE{Bejar2019-os,
  title         = "The fastest $\ell_{1,\infty}$ prox in the west",
  author        = "B{\'e}jar, Benjam{\'\i}n and Dokmani{\'c}, Ivan and Vidal,
                   Ren{\'e}",
  abstract      = "Proximal operators are of particular interest in
                   optimization problems dealing with non-smooth objectives
                   because in many practical cases they lead to optimization
                   algorithms whose updates can be computed in closed form or
                   very efficiently. A well-known example is the proximal
                   operator of the vector $\ell_1$ norm, which is given by the
                   soft-thresholding operator. In this paper we study the
                   proximal operator of the mixed $\ell_\{1,\infty\}$ matrix
                   norm and show that it can be computed in closed form by
                   applying the well-known soft-thresholding operator to each
                   column of the matrix. However, unlike the vector $\ell_1$
                   norm case where the threshold is constant, in the mixed
                   $\ell_\{1,\infty\}$ norm case each column of the matrix
                   might require a different threshold and all thresholds
                   depend on the given matrix. We propose a general iterative
                   algorithm for computing these thresholds, as well as two
                   efficient implementations that further exploit easy to
                   compute lower bounds for the mixed norm of the optimal
                   solution. Experiments on large-scale synthetic and real data
                   indicate that the proposed methods can be orders of
                   magnitude faster than state-of-the-art methods.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.03749"
}

@INPROCEEDINGS{Frahm2006-wj,
  title     = "{RANSAC} for ({Quasi-)Degenerate} data ({QDEGSAC})",
  booktitle = "2006 {IEEE} Computer Society Conference on Computer Vision and
               Pattern Recognition ({CVPR'06})",
  author    = "Frahm, J-M and Pollefeys, M",
  abstract  = "The computation of relations from a number of potential matches
               is a major task in computer vision. Often RANSAC is employed for
               the robust computation of relations such as the fundamental
               matrix. For (quasi-)degenerate data however, it often fails to
               compute the correct relation. The computed relation is always
               consistent with the data but RANSAC does not verify that it is
               unique. The paper proposes a framework that estimates the
               correct relation with the same robustness as RANSAC even for
               (quasi-)degenerate data. The approach is based on a hierarchical
               RANSAC over the number of constraints provided by the data. In
               contrast to all previously presented algorithms for
               (quasi-)degenerate data our technique does not require problem
               specific tests or models to deal with degenerate configurations.
               Accordingly it can be applied for the estimation of any relation
               on any data and is not limited to a special type of relation as
               previous approaches. The results are equivalent to the results
               achieved by state of the art approaches that employ knowledge
               about degeneracies.",
  volume    =  1,
  pages     = "453--460",
  month     =  jun,
  year      =  2006,
  keywords  = "Computer vision;Robustness;Application software;Computer
               science;Testing;Layout;Cost function;Computer Society;Pattern
               recognition"
}

@ARTICLE{Zheng2019-tm,
  title         = "{Structured3D}: A Large Photo-realistic Dataset for
                   Structured {3D} Modeling",
  author        = "Zheng, Jia and Zhang, Junfei and Li, Jing and Tang, Rui and
                   Gao, Shenghua and Zhou, Zihan",
  abstract      = "Recently, there has been growing interest in developing
                   learning-based methods to detect and utilize salient
                   semi-global or global structures, such as junctions, lines,
                   planes, cuboids, smooth surfaces, and all types of
                   symmetries, for 3D scene modeling and understanding.
                   However, the ground truth annotations are often obtained via
                   human labor, which is particularly challenging and
                   inefficient for such tasks due to the large number of 3D
                   structure instances (e.g., line segments) and other factors
                   such as viewpoints and occlusions. In this paper, we present
                   a new synthetic dataset, Structured3D, with the aim of
                   providing large-scale photo-realistic images with rich 3D
                   structure annotations for a wide spectrum of structured 3D
                   modeling tasks. We take advantage of the availability of
                   professional interior designs and automatically extract 3D
                   structures from them. We generate high-quality images with
                   an industry-leading rendering engine. We use our synthetic
                   dataset in combination with real images to train deep
                   networks for room layout estimation and demonstrate improved
                   performance on benchmark datasets.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1908.00222"
}

@ARTICLE{Dessein2016-gp,
  title         = "Regularized Optimal Transport and the Rot Mover's Distance",
  author        = "Dessein, Arnaud and Papadakis, Nicolas and Rouas, Jean-Luc",
  abstract      = "This paper presents a unified framework for smooth convex
                   regularization of discrete optimal transport problems. In
                   this context, the regularized optimal transport turns out to
                   be equivalent to a matrix nearness problem with respect to
                   Bregman divergences. Our framework thus naturally
                   generalizes a previously proposed regularization based on
                   the Boltzmann-Shannon entropy related to the
                   Kullback-Leibler divergence, and solved with the
                   Sinkhorn-Knopp algorithm. We call the regularized optimal
                   transport distance the rot mover's distance in reference to
                   the classical earth mover's distance. We develop two generic
                   schemes that we respectively call the alternate scaling
                   algorithm and the non-negative alternate scaling algorithm,
                   to compute efficiently the regularized optimal plans
                   depending on whether the domain of the regularizer lies
                   within the non-negative orthant or not. These schemes are
                   based on Dykstra's algorithm with alternate Bregman
                   projections, and further exploit the Newton-Raphson method
                   when applied to separable divergences. We enhance the
                   separable case with a sparse extension to deal with high
                   data dimensions. We also instantiate our proposed framework
                   and discuss the inherent specificities for well-known
                   regularizers and statistical divergences in the machine
                   learning and information geometry communities. Finally, we
                   demonstrate the merits of our methods with experiments using
                   synthetic data to illustrate the effect of different
                   regularizers and penalties on the solutions, as well as
                   real-world data for a pattern recognition application to
                   audio scene classification.",
  month         =  oct,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1610.06447"
}

@ARTICLE{Carlini2016-qx,
  title         = "Towards Evaluating the Robustness of Neural Networks",
  author        = "Carlini, Nicholas and Wagner, David",
  abstract      = "Neural networks provide state-of-the-art results for most
                   machine learning tasks. Unfortunately, neural networks are
                   vulnerable to adversarial examples: given an input $x$ and
                   any target classification $t$, it is possible to find a new
                   input $x'$ that is similar to $x$ but classified as $t$.
                   This makes it difficult to apply neural networks in
                   security-critical areas. Defensive distillation is a
                   recently proposed approach that can take an arbitrary neural
                   network, and increase its robustness, reducing the success
                   rate of current attacks' ability to find adversarial
                   examples from $95\%$ to $0.5\%$. In this paper, we
                   demonstrate that defensive distillation does not
                   significantly increase the robustness of neural networks by
                   introducing three new attack algorithms that are successful
                   on both distilled and undistilled neural networks with
                   $100\%$ probability. Our attacks are tailored to three
                   distance metrics used previously in the literature, and when
                   compared to previous adversarial example generation
                   algorithms, our attacks are often much more effective (and
                   never worse). Furthermore, we propose using high-confidence
                   adversarial examples in a simple transferability test we
                   show can also be used to break defensive distillation. We
                   hope our attacks will be used as a benchmark in future
                   defense attempts to create neural networks that resist
                   adversarial examples.",
  month         =  aug,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CR",
  eprint        = "1608.04644"
}

@ARTICLE{Schwartz2018-yg,
  title         = "Intrinsic Isometric Manifold Learning with Application to
                   Localization",
  author        = "Schwartz, Ariel and Talmon, Ronen",
  abstract      = "Data living on manifolds commonly appear in many
                   applications. Often this results from an inherently latent
                   low-dimensional system being observed through higher
                   dimensional measurements. We show that under certain
                   conditions, it is possible to construct an intrinsic and
                   isometric data representation, which respects an underlying
                   latent intrinsic geometry. Namely, we view the observed data
                   only as a proxy and learn the structure of a latent
                   unobserved intrinsic manifold, whereas common practice is to
                   learn the manifold of the observed data. For this purpose,
                   we build a new metric and propose a method for its robust
                   estimation by assuming mild statistical priors and by using
                   artificial neural networks as a mechanism for metric
                   regularization and parametrization. We show successful
                   application to unsupervised indoor localization in ad-hoc
                   sensor networks. Specifically, we show that our proposed
                   method facilitates accurate localization of a moving agent
                   from imaging data it collects. Importantly, our method is
                   applied in the same way to two different imaging modalities,
                   thereby demonstrating its intrinsic and modality-invariant
                   capabilities.",
  month         =  jun,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1806.00556"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{undated-sx,
  title  = "{aUVT\_MXbYPNdcVTefN@Kbg} h {e+i*Oje+VTVTYdOke`i\%lf} {QmOkVTeMQmV}
            {nUo\#gpK*Yrqs} {Ue`Ojt\#VTYrSWOkNWuv} Ewx
            XbYdy+S@J@K\#X*qzv{nUo\#gpK*YZqzv|n@}~Cw`v9s
            {hc\%X:Oj`\#bC@P\#Z*:kb\#dEk\#bk*}",
  author = "¥¦, z \!\`\textcent{}f\pounds{}¤ and »\%¾m{\^a}{\aa}{\ae},
            j$\mu$ «t°b$\neg$ and ¦\#$\neg$u, :­{\c c}¥ and «t, J$\mu$ and
            «T°B$\neg${\'A}{\^I}°W$\neg$Zf\textregistered{}, K$\mu$ and
            «Tf{\'A}{\^I}$\pm${\"I}, :­w\#$\neg$z­ and «:d­B,
            \#­b$\neg$$\mu${\'e}«t°pw¦\#$\neg$ and «t, J$\mu$ and
            {\`O}{\'o}«t­b°, k$\mu$ «t°b$\neg$:{\c c} and C°d, J and
            {\`I}+«Tf{\'A}«ªf«T­P«T$\mu$, $\neg$zfd$\neg$ and
            °B¦\#$\neg$$\times$d, :­dd$\neg$ and ­b, :$\mu$ and
            {\'Y}$\neg$\textregistered{}, \copyright{}t{\c c} and «t, J$\mu$
            and °\textregistered{}, K{\"o}«t{\th} and {\'A}\#, {\c C}"
}

@ARTICLE{Rahmani2016-cl,
  title         = "Coherence Pursuit: Fast, Simple, and Robust Principal
                   Component Analysis",
  author        = "Rahmani, Mostafa and Atia, George",
  abstract      = "This paper presents a remarkably simple, yet powerful,
                   algorithm termed Coherence Pursuit (CoP) to robust Principal
                   Component Analysis (PCA). As inliers lie in a low
                   dimensional subspace and are mostly correlated, an inlier is
                   likely to have strong mutual coherence with a large number
                   of data points. By contrast, outliers either do not admit
                   low dimensional structures or form small clusters. In either
                   case, an outlier is unlikely to bear strong resemblance to a
                   large number of data points. Given that, CoP sets an outlier
                   apart from an inlier by comparing their coherence with the
                   rest of the data points. The mutual coherences are computed
                   by forming the Gram matrix of the normalized data points.
                   Subsequently, the sought subspace is recovered from the span
                   of the subset of the data points that exhibit strong
                   coherence with the rest of the data. As CoP only involves
                   one simple matrix multiplication, it is significantly faster
                   than the state-of-the-art robust PCA algorithms. We derive
                   analytical performance guarantees for CoP under different
                   models for the distributions of inliers and outliers in both
                   noise-free and noisy settings. CoP is the first robust PCA
                   algorithm that is simultaneously non-iterative, provably
                   robust to both unstructured and structured outliers, and can
                   tolerate a large number of unstructured outliers.",
  month         =  sep,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1609.04789"
}

@ARTICLE{Tsakiris2020-fc,
  title         = "Low-rank matrix completion theory via Plucker coordinates",
  author        = "Tsakiris, Manolis C",
  abstract      = "Despite the popularity of low-rank matrix completion, the
                   majority of its theory has been developed under the
                   assumption of random observation patterns, whereas very
                   little is known about the practically relevant case of
                   non-random patterns. Specifically, a fundamental yet largely
                   open question is to describe patterns that allow for unique
                   or finitely many completions. This paper provides two such
                   families of patterns for any rank. A key to achieving this
                   is a novel formulation of low-rank matrix completion in
                   terms of Plucker coordinates, the latter a traditional tool
                   in computer vision. This connection is of potential
                   significance to a wide family of matrix and subspace
                   learning problems with incomplete data.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2004.12430"
}

@ARTICLE{Bartoli2005-fd,
  title     = "Structure-from-motion using lines: Representation,
               triangulation, and bundle adjustment",
  author    = "Bartoli, Adrien and Sturm, Peter",
  abstract  = "We address the problem of camera motion and 3D structure
               reconstruction from line correspondences across multiple views,
               from initialization to final bundle adjustment. One of the main
               difficulties when dealing with line features is their algebraic
               representation. First, we consider the triangulation problem.
               Based on Pl{\"u}cker coordinates to represent the 3D lines, we
               propose a maximum likelihood algorithm, relying on linearizing
               the Pl{\"u}cker constraint and on a Pl{\"u}cker correction
               procedure, computing the closest Pl{\"u}cker coordinates to a
               given 6-vector. Second, we consider the bundle adjustment
               problem, which is essentially a nonlinear optimization process
               on camera motion and 3D line parameters. Previous
               overparameterizations of 3D lines induce gauge freedoms and/or
               internal consistency constraints. We propose the orthonormal
               representation, which allows handy nonlinear optimization of 3D
               lines using the minimum four parameters with an unconstrained
               optimization engine. We compare our algorithms to existing ones
               on simulated and real data. Results show that our triangulation
               algorithm outperforms standard linear and bias-corrected
               quasi-linear algorithms, and that bundle adjustment using our
               orthonormal representation yields results similar to the
               standard maximum likelihood trifocal tensor algorithm, while
               being usable for any number of views.",
  journal   = "Comput. Vis. Image Underst.",
  publisher = "Elsevier BV",
  volume    =  100,
  number    =  3,
  pages     = "416--441",
  month     =  dec,
  year      =  2005,
  language  = "en"
}

@ARTICLE{Landa2021-zl,
  title    = "Doubly Stochastic Normalization of the Gaussian Kernel Is Robust
              to Heteroskedastic Noise",
  author   = "Landa, Boris and Coifman, Ronald R and Kluger, Yuval",
  abstract = "A fundamental step in many data-analysis techniques is the
              construction of an affinity matrix describing similarities
              between data points. When the data points reside in Euclidean
              space, a widespread approach is to from an affinity matrix by the
              Gaussian kernel with pairwise distances, and to follow with a
              certain normalization (e.g. the row-stochastic normalization or
              its symmetric variant). We demonstrate that the doubly-stochastic
              normalization of the Gaussian kernel with zero main diagonal
              (i.e., no self loops) is robust to heteroskedastic noise. That
              is, the doubly-stochastic normalization is advantageous in that
              it automatically accounts for observations with different noise
              variances. Specifically, we prove that in a suitable
              high-dimensional setting where heteroskedastic noise does not
              concentrate too much in any particular direction in space, the
              resulting (doubly-stochastic) noisy affinity matrix converges to
              its clean counterpart with rate m -1/2, where m is the ambient
              dimension. We demonstrate this result numerically, and show that
              in contrast, the popular row-stochastic and symmetric
              normalizations behave unfavorably under heteroskedastic noise.
              Furthermore, we provide examples of simulated and experimental
              single-cell RNA sequence data with intrinsic heteroskedasticity,
              where the advantage of the doubly-stochastic normalization for
              exploratory analysis is evident.",
  journal  = "SIAM J Math Data Sci",
  volume   =  3,
  number   =  1,
  pages    = "388--413",
  month    =  mar,
  year     =  2021,
  language = "en"
}

@ARTICLE{Lim2020-sh,
  title    = "Doubly Stochastic Subspace Clustering",
  author   = "Lim, Derek and Vidal, Ren{\'e} and Haeffele, Benjamin D",
  abstract = "Many state-of-the-art subspace clustering methods follow a
              two-step process by first constructing an affinity matrix between
              data points and then applying spectral clustering to this
              affinity. Most of the research into these methods focuses on the
              first step of generating the affinity, which often exploits the
              self-expressive property of linear subspaces, with little
              consideration typically given to the spectral clustering step
              that produces the final clustering. Moreover, existing methods
              often obtain the final affinity that is used in the spectral
              clustering step by applying ad-hoc or arbitrarily chosen
              postprocessing steps to the affinity generated by a
              self-expressive clustering formulation, which can have a
              significant impact on the overall clustering performance. In this
              work, we unify these two steps by learning both a self-expressive
              representation of the data and an affinity matrix that is
              well-normalized for spectral clustering. In our proposed models,
              we constrain the affinity matrix to be doubly stochastic, which
              results in a principled method for affinity matrix normalization
              while also exploiting known benefits of doubly stochastic
              normalization in spectral clustering. We develop a general
              framework and derive two models: one that jointly learns the
              self-expressive representation along with the doubly stochastic
              affinity, and one that sequentially solves for one then the
              other. Furthermore, we leverage sparsity in the problem to
              develop a fast active-set method for the sequential solver that
              enables efficient computation on large datasets. Experiments show
              that our method achieves state-of-the-art subspace clustering
              performance on many common datasets in computer vision.",
  journal  = "arXiv [cs.LG]",
  month    =  nov,
  year     =  2020
}

@ARTICLE{Abdolali2021-gh,
  title         = "Beyond Linear Subspace Clustering: A Comparative Study of
                   Nonlinear Manifold Clustering Algorithms",
  author        = "Abdolali, Maryam and Gillis, Nicolas",
  abstract      = "Subspace clustering is an important unsupervised clustering
                   approach. It is based on the assumption that the
                   high-dimensional data points are approximately distributed
                   around several low-dimensional linear subspaces. The
                   majority of the prominent subspace clustering algorithms
                   rely on the representation of the data points as linear
                   combinations of other data points, which is known as a
                   self-expressive representation. To overcome the restrictive
                   linearity assumption, numerous nonlinear approaches were
                   proposed to extend successful subspace clustering approaches
                   to data on a union of nonlinear manifolds. In this
                   comparative study, we provide a comprehensive overview of
                   nonlinear subspace clustering approaches proposed in the
                   last decade. We introduce a new taxonomy to classify the
                   state-of-the-art approaches into three categories, namely
                   locality preserving, kernel based, and neural network based.
                   The major representative algorithms within each category are
                   extensively compared on carefully designed synthetic and
                   real-world data sets. The detailed analysis of these
                   approaches unfolds potential research directions and
                   unsolved challenges in this field.",
  month         =  mar,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2103.10656"
}

@INPROCEEDINGS{Park2021-jf,
  title     = "Improving Unsupervised Image Clustering With Robust Learning",
  booktitle = "{IEEE/CVF} Conference on Computer Vision and Pattern Recognition",
  author    = "Park, Sungwon and Han, Sungwon and Kim, Sundong and Kim, Danu
               and Park, Sungkyu and Hong, Seunghoon and Cha, Meeyoung",
  abstract  = "Unsupervised image clustering methods often introduce
               alternative objectives to indirectly train the model and are
               subject to faulty predictions and overconfident results. To
               overcome these challenges, the current research proposes an
               innovative model RUC that is inspired by robust learning. RUC's
               novelty is at utilizing pseudo-labels of existing image
               clustering models as a noisy dataset that may include
               misclassified samples. Its retraining process can revise
               misaligned knowledge and alleviate the overconfidence problem in
               predictions. The model's flexible structure makes it possible to
               be used as an add-on module to other clustering methods and
               helps them achieve better performance on multiple datasets.
               Extensive experiments show that the proposed model can adjust
               the model confidence with better calibration and gain additional
               robustness against adversarial noise.",
  pages     = "12278--12287",
  year      =  2021
}

@ARTICLE{Fawzi2015-kb,
  title         = "Analysis of classifiers' robustness to adversarial
                   perturbations",
  author        = "Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal",
  abstract      = "The goal of this paper is to analyze an intriguing
                   phenomenon recently discovered in deep networks, namely
                   their instability to adversarial perturbations (Szegedy et.
                   al., 2014). We provide a theoretical framework for analyzing
                   the robustness of classifiers to adversarial perturbations,
                   and show fundamental upper bounds on the robustness of
                   classifiers. Specifically, we establish a general upper
                   bound on the robustness of classifiers to adversarial
                   perturbations, and then illustrate the obtained upper bound
                   on the families of linear and quadratic classifiers. In both
                   cases, our upper bound depends on a distinguishability
                   measure that captures the notion of difficulty of the
                   classification task. Our results for both classes imply that
                   in tasks involving small distinguishability, no classifier
                   in the considered set will be robust to adversarial
                   perturbations, even if a good accuracy is achieved. Our
                   theoretical framework moreover suggests that the phenomenon
                   of adversarial instability is due to the low flexibility of
                   classifiers, compared to the difficulty of the
                   classification task (captured by the distinguishability).
                   Moreover, we show the existence of a clear distinction
                   between the robustness of a classifier to random noise and
                   its robustness to adversarial perturbations. Specifically,
                   the former is shown to be larger than the latter by a factor
                   that is proportional to \textbackslashsqrt\{d\} (with d
                   being the signal dimension) for linear classifiers. This
                   result gives a theoretical explanation for the discrepancy
                   between the two robustness properties in high dimensional
                   problems, which was empirically observed in the context of
                   neural networks. To the best of our knowledge, our results
                   provide the first theoretical work that addresses the
                   phenomenon of adversarial instability recently observed for
                   deep networks. Our analysis is complemented by experimental
                   results on controlled and real-world data.",
  month         =  feb,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1502.02590"
}

@ARTICLE{Jin2021-zi,
  title     = "Image matching across wide baselines: From paper to practice",
  author    = "Jin, Yuhe and Mishkin, Dmytro and Mishchuk, Anastasiia and
               Matas, Jiri and Fua, Pascal and Yi, Kwang Moo and Trulls, Eduard",
  journal   = "Int. J. Comput. Vis.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  129,
  number    =  2,
  pages     = "517--547",
  month     =  feb,
  year      =  2021,
  language  = "en"
}

@INPROCEEDINGS{Wright2006-cu,
  title     = "Homography from Coplanar Ellipses with Application to Forensic
               Blood Splatter Reconstruction",
  booktitle = "2006 {IEEE} Computer Society Conference on Computer Vision and
               Pattern Recognition ({CVPR'06})",
  author    = "Wright, J and Wagner, A and Rao, Shankar and Ma, Yi",
  abstract  = "Reconstruction of the point source of blood splatter in a crime
               scene is an important and difficult problem in forensic science.
               We study the problem of automatically reconstructing the 3-D
               location of the victim of a shooting from photographs of planar
               surfaces with blood splattered on them. We analyze this problem
               in terms of the multiple-view geometry of planar conic sections.
               Using projective invariants associated with pairs of conic
               sections, we match images of multiple conic sections taken from
               widely separated viewpoints. We further recover the homography
               between two views using the common tangents of pairs of conic
               sections. The location of the point source is then retrieved
               from the reconstructed scene geometry. We suggest how to extend
               these results to scenes containing multiple planar surfaces, and
               verify the proposed method with experiments on both synthetic
               and real images.",
  volume    =  1,
  pages     = "1250--1257",
  month     =  jun,
  year      =  2006,
  keywords  = "Forensics;Blood;Image reconstruction;Layout;Geometry;Surface
               reconstruction;Shape;Pollution measurement;Cameras;Position
               measurement"
}

@INCOLLECTION{Camps2017-ku,
  title     = "The Interplay Between Big Data and Sparsity in Systems
               Identification",
  booktitle = "Geometric and Numerical Foundations of Movements",
  author    = "Camps, O and Sznaier, M",
  editor    = "Laumond, Jean-Paul and Mansard, Nicolas and Lasserre,
               Jean-Bernard",
  abstract  = "Recent advances in distributed control, coupled with an
               exponential growth in data gathering capabilities, have made
               feasible a wide range of applications with potential to
               profoundly impact society, from safer self-aware environments
               and smart cities to enhanced model-based medical therapies. Yet,
               achieving this vision requires addressing the challenge of
               handling large amounts of very high dimensional data. In this
               chapter, we provide a tutorial showing how to exploit the
               inherent sparsity of the data, which is present in a large class
               of identification problems, to overcome the ``curse of
               dimensionality''. The concepts presented here extend traditional
               ideas from machine learning linking big data and sparsity, to
               challenging dynamic settings. In particular, we explore the
               connections between system identification and information
               extraction from large data sets, using as an example human
               activity analysis from video data.",
  publisher = "Springer International Publishing",
  pages     = "133--159",
  year      =  2017,
  address   = "Cham"
}

@INPROCEEDINGS{Havlena2009-ix,
  title     = "Randomized structure from motion based on atomic {3D} models
               from camera triplets",
  booktitle = "Computer Vision and Pattern Recognition, 2009. {CVPR} 2009.
               {IEEE} Conference on",
  author    = "Havlena, Michal and Torii, Akihiko and Knopp, Jan and Pajdla,
               Tomas",
  abstract  = "This paper presents a new efficient technique for large-scale
               structure from motion from unordered data sets. We avoid costly
               computation of all pairwise matches and geometries by sampling
               pairs of images using the pairwise similarity scores based on
               the detected occurrences of visual words leading to a
               significant speedup. Furthermore, atomic 3D models reconstructed
               from camera triplets are used as the seeds which form the final
               large-scale 3D model when merged together. Using three views
               instead of two allows us to reveal most of the outliers of
               pairwise geometries at an early stage of the process hindering
               them from derogating the quality of the resulting 3D structure
               at later stages. The accuracy of the proposed technique is shown
               on a set of 64 images where the result of the exhaustive
               technique is known. Scalability is demonstrated on a landmark
               reconstruction from hundreds of images.",
  publisher = "Institute of Electrical and Electronics Engineers",
  pages     = "2874--2881",
  month     =  jul,
  year      =  2009
}

@ARTICLE{Redmon2015-mj,
  title         = "You Only Look Once: Unified, {Real-Time} Object Detection",
  author        = "Redmon, Joseph and Divvala, Santosh and Girshick, Ross and
                   Farhadi, Ali",
  abstract      = "We present YOLO, a new approach to object detection. Prior
                   work on object detection repurposes classifiers to perform
                   detection. Instead, we frame object detection as a
                   regression problem to spatially separated bounding boxes and
                   associated class probabilities. A single neural network
                   predicts bounding boxes and class probabilities directly
                   from full images in one evaluation. Since the whole
                   detection pipeline is a single network, it can be optimized
                   end-to-end directly on detection performance. Our unified
                   architecture is extremely fast. Our base YOLO model
                   processes images in real-time at 45 frames per second. A
                   smaller version of the network, Fast YOLO, processes an
                   astounding 155 frames per second while still achieving
                   double the mAP of other real-time detectors. Compared to
                   state-of-the-art detection systems, YOLO makes more
                   localization errors but is far less likely to predict false
                   detections where nothing exists. Finally, YOLO learns very
                   general representations of objects. It outperforms all other
                   detection methods, including DPM and R-CNN, by a wide margin
                   when generalizing from natural images to artwork on both the
                   Picasso Dataset and the People-Art Dataset.",
  month         =  jun,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1506.02640"
}

@INPROCEEDINGS{Faugeras1998-ss,
  title     = "A nonlinear method for estimating the projective geometry of 3
               views",
  booktitle = "Sixth International Conference on Computer Vision ({IEEE} Cat.
               {No.98CH36271})",
  author    = "Faugeras, O and Papadopoulo, T",
  abstract  = "This article deals with the problem of recovering the three
               trifocal tensors between three views from a set of point
               correspondences. We give a new way of deriving the trifocal
               tensor based on Grassmann-Cayley algebra that sheds some new
               light on its structure and leads to a complete characterization
               of its geometric and algebraic properties which is fairly
               institute, i.e. geometric. We give a set of algebraic
               constraints satisfied by the 27 coefficients of the trifocal
               tensor which allow to parameterize it minimally with 18
               coefficients. We then describe a robust method for estimating
               the trifocal tensor from point and line correspondences that
               uses this minimal parameterization. Experimental results show
               that this method as superior to the linear methods which had
               been previously published.",
  pages     = "477--484",
  month     =  jan,
  year      =  1998,
  keywords  = "Geometry;Tensile
               stress;Cameras;Equations;Pixel;Robustness;Calibration;Lapping;Layout;Algebra"
}

@ARTICLE{Straub_undated-hq,
  title  = "The Replica Dataset: A Digital Replica of Indoor Spaces",
  author = "Straub, Julian and Whelan, Thomas and Ma, Lingni and Chen, Yufan
            and Wijmans, Erik and Green, Simon and Engel, Jakob J and
            Mur-Artal, Raul and Ren, Carl and Verma, Shobhit and Clarkson,
            Anton and Yan, Mingfei and Budge, Brian and Yan, Yajie and Pan,
            Xiaqing and Yon, June and Zou, Yuyang and Leon, Kimberly and
            Carter, Nigel and Briales, Jesus and Gillingham, Tyler and
            Mueggler, Elias and Pesqueira, Luis and Savva, Manolis and Batra,
            Dhruv and Strasdat, Hauke M and De Nardi, Renzo and Goesele,
            Michael and Lovegrove, Steven and Newcombe, Richard"
}

@ARTICLE{Meng2017-un,
  title         = "{MagNet}: a {Two-Pronged} Defense against Adversarial
                   Examples",
  author        = "Meng, Dongyu and Chen, Hao",
  abstract      = "Deep learning has shown promising results on hard perceptual
                   problems in recent years. However, deep learning systems are
                   found to be vulnerable to small adversarial perturbations
                   that are nearly imperceptible to human. Such specially
                   crafted perturbations cause deep learning systems to output
                   incorrect decisions, with potentially disastrous
                   consequences. These vulnerabilities hinder the deployment of
                   deep learning systems where safety or security is important.
                   Attempts to secure deep learning systems either target
                   specific attacks or have been shown to be ineffective. In
                   this paper, we propose MagNet, a framework for defending
                   neural network classifiers against adversarial examples.
                   MagNet does not modify the protected classifier or know the
                   process for generating adversarial examples. MagNet includes
                   one or more separate detector networks and a reformer
                   network. Different from previous work, MagNet learns to
                   differentiate between normal and adversarial examples by
                   approximating the manifold of normal examples. Since it does
                   not rely on any process for generating adversarial examples,
                   it has substantial generalization power. Moreover, MagNet
                   reconstructs adversarial examples by moving them towards the
                   manifold, which is effective for helping classify
                   adversarial examples with small perturbation correctly. We
                   discuss the intrinsic difficulty in defending against
                   whitebox attack and propose a mechanism to defend against
                   graybox attack. Inspired by the use of randomness in
                   cryptography, we propose to use diversity to strengthen
                   MagNet. We show empirically that MagNet is effective against
                   most advanced state-of-the-art attacks in blackbox and
                   graybox scenarios while keeping false positive rate on
                   normal examples very low.",
  month         =  may,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CR",
  eprint        = "1705.09064"
}

@ARTICLE{Li2020-he,
  title    = "Robust Geometric Model Estimation Based on Scaled Welsch
              <italic>q</italic>-Norm",
  author   = "Li, Jiayuan and Hu, Qingwu and Ai, Mingyao",
  abstract = "Robust estimation, which aims to recover the geometric
              transformation from outlier contaminated observations, is
              essential for many remote sensing and photogrammetry
              applications. This article presents a novel robust geometric
              model estimation method based on scaled Welsch q-norm (lq-norm, 0
              qLS) problem and a weighted least-squares (WLS) problem] by using
              alternating direction method of multipliers (ADMM) method. For
              the WLS problem, we introduce a coarse-to-fine strategy into the
              iterative reweighted least-squares (IRLS) method. We change the
              weight function by decreasing its scale parameter. This strategy
              can largely avoid that the solver gets stuck in local minimums.
              We adapt the proposed cost into classical remote sensing tasks
              and develop new robust feature matching (RFM), robust exterior
              orientation (REO), and robust absolute orientation (RAO)
              algorithms. Both synthetic and real experiments demonstrate that
              the proposed method significantly outperforms the other compared
              state-of-the-art methods. Our method is still robust even if the
              outlier rate is up to 90\%.",
  journal  = "IEEE Trans. Geosci. Remote Sens.",
  volume   =  58,
  number   =  8,
  pages    = "5908--5921",
  month    =  aug,
  year     =  2020,
  keywords = "Robustness;Estimation;Remote
              sensing;Optimization;Standards;Convex functions;Sensitivity;Image
              orientation;model fitting;outlier removal;point cloud
              registration;robust feature matching (RFM)"
}

@INPROCEEDINGS{Triggs2000-nd,
  title     = "Bundle Adjustment --- A Modern Synthesis",
  booktitle = "Vision Algorithms: Theory and Practice",
  author    = "Triggs, Bill and McLauchlan, Philip F and Hartley, Richard I and
               Fitzgibbon, Andrew W",
  abstract  = "This paper is a survey of the theory and methods of
               photogrammetric bundle adjustment, aimed at potential
               implementors in the computer vision community. Bundle adjustment
               is the problem of refining a visual reconstruction to produce
               jointly optimal structure and viewing parameter estimates.
               Topics covered include: the choice of cost function and
               robustness; numerical optimization including sparse Newton
               methods, linearly convergent approximations, updating and
               recursive methods; gauge (datum) invariance; and quality
               control. The theory is developed for general robust cost
               functions rather than restricting attention to traditional
               nonlinear least squares.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "298--372",
  year      =  2000
}

@ARTICLE{Athalye2017-nr,
  title         = "Synthesizing Robust Adversarial Examples",
  author        = "Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and
                   Kwok, Kevin",
  abstract      = "Standard methods for generating adversarial examples for
                   neural networks do not consistently fool neural network
                   classifiers in the physical world due to a combination of
                   viewpoint shifts, camera noise, and other natural
                   transformations, limiting their relevance to real-world
                   systems. We demonstrate the existence of robust 3D
                   adversarial objects, and we present the first algorithm for
                   synthesizing examples that are adversarial over a chosen
                   distribution of transformations. We synthesize
                   two-dimensional adversarial images that are robust to noise,
                   distortion, and affine transformation. We apply our
                   algorithm to complex three-dimensional objects, using
                   3D-printing to manufacture the first physical adversarial
                   objects. Our results demonstrate the existence of 3D
                   adversarial objects in the physical world.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1707.07397"
}

@INPROCEEDINGS{You2019-kb,
  title           = "Is an affine constraint needed for affine subspace
                     clustering?",
  booktitle       = "International Conference on Computer Vision ({ICCV})",
  author          = "You, Chong and Li, Chun-Guang and Robinson, Daniel and
                     Vidal, Rene",
  abstract        = "Subspace clustering methods based on expressing each data
                     point as a linear combination of other data points have
                     achieved great success in computer vision applications
                     such as motion segmentation, face and digit clustering. In
                     face clustering, the subspaces are linear and subspace
                     clustering methods can be applied directly. In motion
                     segmentation, the subspaces are affine and an additional
                     affine constraint on the coefficients is often enforced.
                     However, since affine subspaces can always be embedded
                     into linear subspaces of one extra dimension, it is
                     unclear if the affine constraint is really necessary. This
                     paper shows, both theoretically and empirically, that when
                     the dimension of the ambient space is high relative to the
                     sum of the dimensions of the affine subspaces, the affine
                     constraint has a negligible effect on clustering
                     performance. Specifically, our analysis provides
                     conditions that guarantee the correctness of affine
                     subspace clustering methods both with and without the
                     affine constraint, and shows that these conditions are
                     satisfied for high-dimensional data. Underlying our
                     analysis is the notion of affinely independent subspaces,
                     which not only provides geometrically interpretable
                     correctness conditions, but also clarifies the
                     relationships between existing results for affine subspace
                     clustering.",
  publisher       = "IEEE",
  month           =  oct,
  year            =  2019,
  conference      = "2019 IEEE/CVF International Conference on Computer Vision
                     (ICCV)",
  location        = "Seoul, Korea (South)"
}

@ARTICLE{Ornhag2018-ro,
  title         = "Bilinear Parameterization For Differentiable
                   {Rank-Regularization}",
  author        = "{\"O}rnhag, Marcus Valtonen and Olsson, Carl and Heyden,
                   Anders",
  abstract      = "Low rank approximation is a commonly occurring problem in
                   many computer vision and machine learning applications.
                   There are two common ways of optimizing the resulting
                   models. Either the set of matrices with a given rank can be
                   explicitly parametrized using a bilinear factorization, or
                   low rank can be implicitly enforced using regularization
                   terms penalizing non-zero singular values. While the former
                   approach results in differentiable problems that can be
                   efficiently optimized using local quadratic approximation,
                   the latter is typically not differentiable (sometimes even
                   discontinuous) and requires first order subgradient or
                   splitting methods. It is well known that gradient based
                   methods exhibit slow convergence for ill-conditioned
                   problems. In this paper we show how many non-differentiable
                   regularization methods can be reformulated into smooth
                   objectives using bilinear parameterization. This allows us
                   to use standard second order methods, such as
                   Levenberg--Marquardt (LM) and Variable Projection (VarPro),
                   to achieve accurate solutions for ill-conditioned cases. We
                   show on several real and synthetic experiments that our
                   second order formulation converges to substantially more
                   accurate solutions than competing state-of-the-art methods.",
  month         =  nov,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1811.11088"
}

@ARTICLE{Chan2020-ow,
  title         = "Deep Networks from the Principle of Rate Reduction",
  author        = "Chan, Kwan Ho Ryan and Yu, Yaodong and You, Chong and Qi,
                   Haozhi and Wright, John and Ma, Yi",
  abstract      = "This work attempts to interpret modern deep (convolutional)
                   networks from the principles of rate reduction and (shift)
                   invariant classification. We show that the basic iterative
                   gradient ascent scheme for optimizing the rate reduction of
                   learned features naturally leads to a multi-layer deep
                   network, one iteration per layer. The layered architectures,
                   linear and nonlinear operators, and even parameters of the
                   network are all explicitly constructed layer-by-layer in a
                   forward propagation fashion by emulating the gradient
                   scheme. All components of this ``white box'' network have
                   precise optimization, statistical, and geometric
                   interpretation. This principled framework also reveals and
                   justifies the role of multi-channel lifting and sparse
                   coding in early stage of deep networks. Moreover, all linear
                   operators of the so-derived network naturally become
                   multi-channel convolutions when we enforce classification to
                   be rigorously shift-invariant. The derivation also indicates
                   that such a convolutional network is significantly more
                   efficient to construct and learn in the spectral domain. Our
                   preliminary simulations and experiments indicate that so
                   constructed deep network can already learn a good
                   discriminative representation even without any back
                   propagation training.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2010.14765"
}

@ARTICLE{Kileel2017-ey,
  title     = "Minimal Problems for the Calibrated Trifocal Variety",
  author    = "Kileel, Joe",
  abstract  = "The recovery of three calibrated cameras from image data is
               investigated using tools from computational algebraic geometry.
               We determine the algebraic degree for various minimal problems.
               Our formulation is based on the calibrated trifocal variety in
               computer vision, which is the configuration space for three
               calibrated cameras. Some of our calculations are done using
               homotopy continuation software, and so they rely on
               pseudo-randomness and numerical accuracy.",
  journal   = "SIAM J. Appl. Algebra Geometry",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  1,
  number    =  1,
  pages     = "575--598",
  month     =  jan,
  year      =  2017
}

@ARTICLE{Martyushev2016-iu,
  title         = "On Some Properties of Calibrated Trifocal Tensors",
  author        = "Martyushev, Evgeniy",
  abstract      = "In two-view geometry, the essential matrix describes the
                   relative position and orientation of two calibrated images.
                   In three views, a similar role is assigned to the calibrated
                   trifocal tensor. It is a particular case of the
                   (uncalibrated) trifocal tensor and thus it inherits all its
                   properties but, due to the smaller degrees of freedom,
                   satisfies a number of additional algebraic constraints. Some
                   of them are described in this paper. More specifically, we
                   define a new notion --- the trifocal essential matrix. On
                   the one hand, it is a generalization of the ordinary
                   (bifocal) essential matrix, and, on the other hand, it is
                   closely related to the calibrated trifocal tensor. We prove
                   the two necessary and sufficient conditions that
                   characterize the set of trifocal essential matrices. Based
                   on these characterizations, we propose three necessary
                   conditions on a calibrated trifocal tensor. They have a form
                   of 15 quartic and 99 quintic polynomial equations. We show
                   that in the practically significant real case the 15 quartic
                   constraints are also sufficient.",
  month         =  jan,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1601.01467"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Pintore2020-pu,
  title     = "State‐of‐the‐art in automatic {3D} reconstruction of structured
               indoor environments",
  author    = "Pintore, Giovanni and Mura, Claudio and Ganovelli, Fabio and
               Fuentes-Perez, Lizeth and Pajarola, Renato and Gobbetti, Enrico",
  journal   = "Comput. Graph. Forum",
  publisher = "Wiley",
  volume    =  39,
  number    =  2,
  pages     = "667--699",
  month     =  may,
  year      =  2020,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@ARTICLE{Dai2016-pc,
  title         = "Rolling Shutter Camera Relative Pose: Generalized Epipolar
                   Geometry",
  author        = "Dai, Yuchao and Li, Hongdong and Kneip, Laurent",
  abstract      = "The vast majority of modern consumer-grade cameras employ a
                   rolling shutter mechanism. In dynamic geometric computer
                   vision applications such as visual SLAM, the so-called
                   rolling shutter effect therefore needs to be properly taken
                   into account. A dedicated relative pose solver appears to be
                   the first problem to solve, as it is of eminent importance
                   to bootstrap any derivation of multi-view geometry. However,
                   despite its significance, it has received inadequate
                   attention to date. This paper presents a detailed
                   investigation of the geometry of the rolling shutter
                   relative pose problem. We introduce the rolling shutter
                   essential matrix, and establish its link to existing models
                   such as the push-broom cameras, summarized in a clean
                   hierarchy of multi-perspective cameras. The generalization
                   of well-established concepts from epipolar geometry is
                   completed by a definition of the Sampson distance in the
                   rolling shutter case. The work is concluded with a careful
                   investigation of the introduced epipolar geometry for
                   rolling shutter cameras on several dedicated benchmarks.",
  month         =  may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1605.00475"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{dmadd_undated-ed,
  title  = ";{)13ghjikR4=:a8mln4=D} ?{N:58mGpoUqrB8DE258oUD@8ms98}
            ?{7R[Z\textbackslash?76} tUu ?L=86vo98,
            {s9oq9DJB8>@25sU86wIyxz25:5>E4=6} {N|v}7~w
            ?{7R[Z\textbackslash?N6R8s98\textbackslashDK?76UR8}",
  author = "\%`ª«dª<m$\neg$a`dd, <nc e\copyright{} and
            ¥e, /x \textregistered{} and @,  and
            dy\textcent{}, yd\copyright{}nx¦`d and
            ½`ª¾\?\`c»\copyright{}, /º¦»\!\` x`ª«de and
            °»\copyright{}, /7 and , C\copyright{}
            ed$\pm$\?\`d»\!\`ne and º¦v`¤/, x{\aa}n
            vc Y\textcent{} and \?\`,  and mH,
            c¥¦°dce and e{\~a}\textcent{}¾, 
            º\textcent{}{\'a}{\^a}c and d{\AE}\textcent{}jN,
            \textcent{}{\'a}{\^a} x \textcent{} \?\`\!\`
            and er$\neg$M\pounds{}J­, \textcent{}«nc and ¤d¦k,
            dce and ek, N\textcent{}{\'a}dª7 and
            ed$\mu$, $\pm$¾\textcent{}vnc and e,
            N¥ª«dj$\pm${\ae}{\`e}{\^a}e¥
            /{\'e}{\ae}{\^e}d{\^a}¦r¦«º¦\!\`7 and
            d°, Nd and ¤\?\`e, K{\`a}ex{\^a}@ and d,
            {\^A}» and e{\AE}°, N»{\'\i}¥ {\^A}
            ½\textcent{}]7 and ½nd, dex  and
            \textregistered{}s»\copyright{}, \copyright{}\textcent{} 
            and ½c»\copyright{},  and yd, \textbackslashene
             ºm and »\copyright{}, d¤\?\`\textcent{}« and
            r\textcent{}={\^A}cEE$\mu$p{\^I}<{\^A}, r
            {\`A}<7 and »,  and º, »\!\`c and
            , $\pm${\`a}\textcent{}{\'a}{\^a} and , »re and N,
            c and , e¥jx{\^a}d ¤ and x¦, "
}

@ARTICLE{Lei2006-qw,
  title    = "Tri-focal tensor-based multiple video synchronization with
              subframe optimization",
  author   = "Lei, Cheng and Yang, Yee-Hong",
  abstract = "In this paper, we present a novel method for synchronizing
              multiple (more than two) uncalibrated video sequences recording
              the same event by free-moving full-perspective cameras. Unlike
              previous synchronization methods, our method takes advantage of
              tri-view geometry constraints instead of the commonly used
              two-view one for their better performance in measuring geometric
              alignment when video frames are synchronized. In particular, the
              tri-ocular geometric constraint of point/line features, which is
              evaluated by tri-focal transfer, is enforced when building the
              timeline maps for sequences to be synchronized. A hierarchical
              approach is used to reduce the computational complexity. To
              achieve subframe synchronization accuracy, the
              Levenberg-Marquardt method-based optimization is performed. The
              experimental results on several synthetic and real video datasets
              demonstrate the effectiveness and robustness of our method over
              previous methods in synchronizing full-perspective videos.",
  journal  = "IEEE Trans. Image Process.",
  volume   =  15,
  number   =  9,
  pages    = "2473--2480",
  month    =  sep,
  year     =  2006,
  language = "en"
}

@ARTICLE{Purkait2018-wr,
  title         = "Maximum Consensus Parameter Estimation by Reweighted
                   $\ell_1$ Methods",
  author        = "Purkait, Pulak and Zach, Christopher and Eriksson, Anders",
  abstract      = "Robust parameter estimation in computer vision is frequently
                   accomplished by solving the maximum consensus (MaxCon)
                   problem. Widely used randomized methods for MaxCon, however,
                   can only produce \{random\} approximate solutions, while
                   global methods are too slow to exercise on realistic problem
                   sizes. Here we analyse MaxCon as iterative reweighted
                   algorithms on the data residuals. We propose a smooth
                   surrogate function, the minimization of which leads to an
                   extremely simple iteratively reweighted algorithm for
                   MaxCon. We show that our algorithm is very efficient and in
                   many cases, yields the global solution. This makes it an
                   attractive alternative for randomized methods and global
                   optimizers. The convergence analysis of our method and its
                   fundamental differences from the other iteratively
                   reweighted methods are also presented.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1803.08602"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Nordberg2009-kq,
  title     = "A minimal parameterization of the trifocal tensor",
  booktitle = "2009 {IEEE} Conference on Computer Vision and Pattern
               Recognition",
  author    = "Nordberg, Klas",
  abstract  = "The paper describes a minimal set of 18 parameters that can
               represent any trifocal tensor consistent with the internal
               constraints. 9 parameters describe three orthogonal matrices and
               9 parameters describe 10 elements of a sparse tensor T̃ with 17
               elements in well-defined positions equal to zero. Any valid
               trifocal tensor is then given as some specific T̃ transformed by
               the orthogonal matrices in the respective image domain. The
               paper also describes a simple approach for estimating the three
               orthogonal matrices in the case of a general 3 $\times$ 3
               $\times$ 3 tensor, i.e., when the internal constraints are not
               satisfied. This can be used to accomplish a least squares
               approximation of a general tensor to a tensor that satisfies the
               internal constraints. This type of constraint enforcement, in
               turn, can be used to obtain an improved estimate of the trifocal
               tensor based on the normalized linear algorithm, with the
               constraint enforcement as a final step. This makes the algorithm
               more similar to the corresponding algorithm for estimation of
               the fundamental matrix. An experiment on synthetic data shows
               that the constraint enforcement of the trifocal tensor produces
               a significantly better result than without enforcement,
               expressed by the positions of the epipoles, given that the
               constraint enforcement is made in normalized image coordinates.",
  pages     = "1224--1230",
  month     =  jun,
  year      =  2009,
  keywords  = "Tensile stress;Matrix decomposition;Cameras;Sparse
               matrices;Computer vision;Laboratories;Least squares
               approximation;Singular value decomposition;Covariance matrix"
}

@ARTICLE{Koh2017-xi,
  title         = "Understanding Black-box Predictions via Influence Functions",
  author        = "Koh, Pang Wei and Liang, Percy",
  abstract      = "How can we explain the predictions of a black-box model? In
                   this paper, we use influence functions -- a classic
                   technique from robust statistics -- to trace a model's
                   prediction through the learning algorithm and back to its
                   training data, thereby identifying training points most
                   responsible for a given prediction. To scale up influence
                   functions to modern machine learning settings, we develop a
                   simple, efficient implementation that requires only oracle
                   access to gradients and Hessian-vector products. We show
                   that even on non-convex and non-differentiable models where
                   the theory breaks down, approximations to influence
                   functions can still provide valuable information. On linear
                   models and convolutional neural networks, we demonstrate
                   that influence functions are useful for multiple purposes:
                   understanding model behavior, debugging models, detecting
                   dataset errors, and even creating visually-indistinguishable
                   training-set attacks.",
  month         =  mar,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1703.04730"
}

@ARTICLE{Zheng2015-zu,
  title         = "Conditional Random Fields as Recurrent Neural Networks",
  author        = "Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes,
                   Bernardino and Vineet, Vibhav and Su, Zhizhong and Du,
                   Dalong and Huang, Chang and Torr, Philip H S",
  abstract      = "Pixel-level labelling tasks, such as semantic segmentation,
                   play a central role in image understanding. Recent
                   approaches have attempted to harness the capabilities of
                   deep learning techniques for image recognition to tackle
                   pixel-level labelling tasks. One central issue in this
                   methodology is the limited capacity of deep learning
                   techniques to delineate visual objects. To solve this
                   problem, we introduce a new form of convolutional neural
                   network that combines the strengths of Convolutional Neural
                   Networks (CNNs) and Conditional Random Fields (CRFs)-based
                   probabilistic graphical modelling. To this end, we formulate
                   mean-field approximate inference for the Conditional Random
                   Fields with Gaussian pairwise potentials as Recurrent Neural
                   Networks. This network, called CRF-RNN, is then plugged in
                   as a part of a CNN to obtain a deep network that has
                   desirable properties of both CNNs and CRFs. Importantly, our
                   system fully integrates CRF modelling with CNNs, making it
                   possible to train the whole deep network end-to-end with the
                   usual back-propagation algorithm, avoiding offline
                   post-processing methods for object delineation. We apply the
                   proposed method to the problem of semantic image
                   segmentation, obtaining top results on the challenging
                   Pascal VOC 2012 segmentation benchmark.",
  month         =  feb,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1502.03240"
}

@ARTICLE{Tsakiris2018-zg,
  title    = "Algebraic Clustering of Affine Subspaces",
  author   = "Tsakiris, Manolis C and Vidal, Rene",
  abstract = "Subspace clustering is an important problem in machine learning
              with many applications in computer vision and pattern
              recognition. Prior work has studied this problem using algebraic,
              iterative, statistical, low-rank and sparse representation
              techniques. While these methods have been applied to both linear
              and affine subspaces, theoretical results have only been
              established in the case of linear subspaces. For example,
              algebraic subspace clustering (ASC) is guaranteed to provide the
              correct clustering when the data points are in general position
              and the union of subspaces is transversal. In this paper we study
              in a rigorous fashion the properties of ASC in the case of affine
              subspaces. Using notions from algebraic geometry, we prove that
              the homogenization trick , which embeds points in a union of
              affine subspaces into points in a union of linear subspaces,
              preserves the general position of the points and the
              transversality of the union of subspaces in the embedded space,
              thus establishing the correctness of ASC for affine subspaces.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  40,
  number   =  2,
  pages    = "482--489",
  month    =  feb,
  year     =  2018,
  language = "en"
}

@ARTICLE{Ros2016-ke,
  title    = "Motion Estimation via Robust Decomposition With Constrained Rank",
  author   = "Ros, German and {\'A}lvarez, Jose M and Guerrero, Julio",
  abstract = "In this work, we address the problem of outlier detection for
              robust motion estimation by using modern sparse-low-rank
              decompositions, i.e., Robust PCA-like methods, to impose global
              rank constraints. Robust decompositions have shown to be good at
              splitting a corrupted matrix into an uncorrupted low-rank matrix
              and a sparse matrix, containing outliers. However, this process
              only works when matrices have relatively low rank with respect to
              their ambient space, a property not met in motion estimation
              problems. As a solution, we propose to exploit the partial
              information present in the decomposition to decide which matches
              are outliers. We provide evidences showing that even when it is
              not possible to recover an uncorrupted low-rank matrix, the
              resulting information can be exploited for outlier detection. To
              this end we propose the Robust Decomposition with Constrained
              Rank (RD-CR), a proximal gradient based method that enforces the
              rank constraints inherent to motion estimation. We also present a
              general framework to perform robust estimation for stereo Visual
              Odometry, based on our RD-CR and a simple but effective
              compressed optimization method that achieves high performance.
              Our evaluation on synthetic data and on the KITTI dataset
              demonstrates the applicability of our approach in complex
              scenarios and it yields state-of-the-art performance.",
  journal  = "IEEE Transactions on Intelligent Vehicles",
  volume   =  1,
  number   =  4,
  pages    = "346--357",
  month    =  dec,
  year     =  2016,
  keywords = "Robustness;Motion estimation;Matrix decomposition;Sparse
              matrices;Algorithm design and analysis;Robust statistics;robust
              PCA;visual odometry"
}

@ARTICLE{Kalofolias2016-tt,
  title         = "How to learn a graph from smooth signals",
  author        = "Kalofolias, Vassilis",
  abstract      = "We propose a framework that learns the graph structure
                   underlying a set of smooth signals. Given
                   $X\in\mathbb\{R\}^\{m\times n\}$ whose rows reside on the
                   vertices of an unknown graph, we learn the edge weights
                   $w\in\mathbb\{R\}_+^\{m(m-1)/2\}$ under the smoothness
                   assumption that $\text\{tr\}\{X^\top LX\}$ is small. We show
                   that the problem is a weighted $\ell$-1 minimization that
                   leads to naturally sparse solutions. We point out how known
                   graph learning or construction techniques fall within our
                   framework and propose a new model that performs better than
                   the state of the art in many settings. We present efficient,
                   scalable primal-dual based algorithms for both our model and
                   the previous state of the art, and evaluate their
                   performance on artificial and real data.",
  month         =  jan,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1601.02513"
}

@ARTICLE{Cai2008-km,
  title         = "A Singular Value Thresholding Algorithm for Matrix
                   Completion",
  author        = "Cai, Jian-Feng and Candes, Emmanuel J and Shen, Zuowei",
  abstract      = "This paper introduces a novel algorithm to approximate the
                   matrix with minimum nuclear norm among all matrices obeying
                   a set of convex constraints. This problem may be understood
                   as the convex relaxation of a rank minimization problem, and
                   arises in many important applications as in the task of
                   recovering a large matrix from a small subset of its entries
                   (the famous Netflix problem). Off-the-shelf algorithms such
                   as interior point methods are not directly amenable to large
                   problems of this kind with over a million unknown entries.
                   This paper develops a simple first-order and
                   easy-to-implement algorithm that is extremely efficient at
                   addressing problems in which the optimal solution has low
                   rank. The algorithm is iterative and produces a sequence of
                   matrices (X^k, Y^k) and at each step, mainly performs a
                   soft-thresholding operation on the singular values of the
                   matrix Y^k. There are two remarkable features making this
                   attractive for low-rank matrix completion problems. The
                   first is that the soft-thresholding operation is applied to
                   a sparse matrix; the second is that the rank of the iterates
                   X^k is empirically nondecreasing. Both these facts allow the
                   algorithm to make use of very minimal storage space and keep
                   the computational cost of each iteration low. We provide
                   numerical examples in which 1,000 by 1,000 matrices are
                   recovered in less than a minute on a modest desktop
                   computer. We also demonstrate that our approach is amenable
                   to very large scale problems by recovering matrices of rank
                   about 10 with nearly a billion unknowns from just about
                   0.4\% of their sampled entries. Our methods are connected
                   with linearized Bregman iterations for l1 minimization, and
                   we develop a framework in which one can understand these
                   algorithms in terms of well-known Lagrange multiplier
                   algorithms.",
  month         =  oct,
  year          =  2008,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "0810.3286"
}

@ARTICLE{Yang2017-ea,
  title         = "On the Suboptimality of Proximal Gradient Descent for
                   $\ell^{0}$ Sparse Approximation",
  author        = "Yang, Yingzhen and Feng, Jiashi and Jojic, Nebojsa and Yang,
                   Jianchao and Huang, Thomas S",
  abstract      = "We study the proximal gradient descent (PGD) method for
                   $\ell^\{0\}$ sparse approximation problem as well as its
                   accelerated optimization with randomized algorithms in this
                   paper. We first offer theoretical analysis of PGD showing
                   the bounded gap between the sub-optimal solution by PGD and
                   the globally optimal solution for the $\ell^\{0\}$ sparse
                   approximation problem under conditions weaker than
                   Restricted Isometry Property widely used in compressive
                   sensing literature. Moreover, we propose randomized
                   algorithms to accelerate the optimization by PGD using
                   randomized low rank matrix approximation (PGD-RMA) and
                   randomized dimension reduction (PGD-RDR). Our randomized
                   algorithms substantially reduces the computation cost of the
                   original PGD for the $\ell^\{0\}$ sparse approximation
                   problem, and the resultant sub-optimal solution still enjoys
                   provable suboptimality, namely, the sub-optimal solution to
                   the reduced problem still has bounded gap to the globally
                   optimal solution to the original problem.",
  month         =  sep,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1709.01230"
}

@ARTICLE{Mathews2016-ff,
  title         = "Multi-focal tensors as invariant differential forms",
  author        = "Mathews, James",
  abstract      = "For each relative $\operatorname\{GL\}(V)$-invariant tensor
                   $I\in \Lambda^\{p_1+1\}V^\{\vee\}\otimes .. \otimes
                   \Lambda^\{p_n+1\}V^\{\vee\}$ we construct a
                   $\operatorname\{GL\}(V)$-invariant weighted differential
                   form $\eta$ on $(\mathbb\{P\} V)^\{n\}$. Then $\eta$ is
                   expressed explicitly with respect to $n$-tuples of frames
                   for tangent spaces at points of $\mathbb\{P\} V$ to obtain
                   elements of a different tensor space. For certain invariants
                   $I$, the resulting elements are shown to be the multi-focal
                   tensors appearing in the machine vision literature (Demazure
                   1988, Longuet-Higgins 1981, Luong 1992, Faugeras 1993,
                   Faugeras and Luong 2001, Hartley and Zisserman 2003). This
                   generalizes the 3 multi-focal varieties known in dimension
                   $\dim V = 4$ to an infinite collection of special tensor
                   subvarieties. We use this framework to exhibit a new system
                   of degree 4 polynomial equations, reminiscent of the braid
                   relation, satisfied by the Euclidean trifocal variety.",
  month         =  oct,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "math.AG",
  eprint        = "1610.04294"
}

@ARTICLE{Schwab2013-iq,
  title    = "Rotation invariant features for {HARDI}",
  author   = "Schwab, Evan and Ceting{\"u}l, H Ertan and Afsari, Bijan and
              Vidal, Ren{\'e}",
  abstract = "Reducing the amount of information stored in diffusion MRI (dMRI)
              data to a set of meaningful and representative scalar values is a
              goal of much interest in medical imaging. Such features can have
              far reaching applications in segmentation, registration, and
              statistical characterization of regions of interest in the brain,
              as in comparing features between control and diseased patients.
              Currently, however, the number of biologically relevant features
              in dMRI is very limited. Moreover, existing features discard much
              of the information inherent in dMRI and embody several
              theoretical shortcomings. This paper proposes a new family of
              rotation invariant scalar features for dMRI based on the
              spherical harmonic (SH) representation of high angular resolution
              diffusion images (HARDI). These features describe the shape of
              the orientation distribution function extracted from HARDI data
              and are applicable to any reconstruction method that represents
              HARDI signals in terms of an SH basis. We further illustrate
              their significance in white matter characterization of synthetic,
              phantom and real HARDI brain datasets.",
  journal  = "Inf. Process. Med. Imaging",
  volume   =  23,
  pages    = "705--717",
  year     =  2013,
  language = "en"
}

@MISC{Cai_undated-fe,
  title        = "Efficient deep embedded subspace clustering",
  author       = "Cai, Jinyu and Fan, Jicong and Guo, Wenzhong and Wang,
                  Shiping and Zhang, Yunhe and Zhang, Zhao",
  abstract     = "Recently deep learning methods have shown significant
                  progress in data clustering tasks. Deep clustering methods
                  (including distance-based methods and subspace-based methods)
                  integrate clustering and feature learning into a unified
                  framework, where there is a mutual promotion between
                  clustering and representation. However, deep subspace
                  clustering methods are usually in the framework of
                  self-expressive model and hence have quadratic time and space
                  complexities, which prevents their applications in
                  large-scale clustering and real-time clustering. In this
                  paper, we propose a new mechanism for deep clustering. We aim
                  to learn the subspace bases from deep representation in an
                  iterative refining manner while the refined subspace bases
                  help learning the representation of the deep neural networks
                  in return. The proposed method is out of the selfexpressive
                  framework, scales to the sample size linearly, and is
                  applicable to arbitrarily large datasets and online
                  clustering scenarios. More importantly, the clustering
                  accuracy of the proposed method is much higher than its
                  competitors. Extensive comparison studies with
                  state-of-the-art clustering approaches on benchmark datasets
                  demonstrate the superiority of the proposed method.",
  howpublished = "\url{https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_Efficient_Deep_Embedded_Subspace_Clustering_CVPR_2022_paper.pdf}",
  note         = "Accessed: 2022-6-19"
}

@ARTICLE{Sohl-Dickstein2015-cs,
  title         = "Deep Unsupervised Learning using Nonequilibrium
                   Thermodynamics",
  author        = "Sohl-Dickstein, Jascha and Weiss, Eric A and
                   Maheswaranathan, Niru and Ganguli, Surya",
  abstract      = "A central problem in machine learning involves modeling
                   complex data-sets using highly flexible families of
                   probability distributions in which learning, sampling,
                   inference, and evaluation are still analytically or
                   computationally tractable. Here, we develop an approach that
                   simultaneously achieves both flexibility and tractability.
                   The essential idea, inspired by non-equilibrium statistical
                   physics, is to systematically and slowly destroy structure
                   in a data distribution through an iterative forward
                   diffusion process. We then learn a reverse diffusion process
                   that restores structure in data, yielding a highly flexible
                   and tractable generative model of the data. This approach
                   allows us to rapidly learn, sample from, and evaluate
                   probabilities in deep generative models with thousands of
                   layers or time steps, as well as to compute conditional and
                   posterior probabilities under the learned model. We
                   additionally release an open source reference implementation
                   of the algorithm.",
  month         =  mar,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1503.03585"
}

@ARTICLE{Clarke1975-jc,
  title    = "Generalized gradients and applications",
  author   = "Clarke, Frank H",
  journal  = "Trans. Amer. Math. Soc.",
  year     =  1975
}

@ARTICLE{Rockafellar1979-dy,
  title   = "Clarke's tangent cones and the boundaries of closed sets in
             $$\textbackslashbackslash$mathbb {$\{$R$\}$\^{}} n$",
  author  = "Rockafellar, Ralph",
  journal = "Nonlinear Analysis: theory, methods and applications",
  year    =  1979
}

@ARTICLE{Pai_undated-ke,
  title  = "Pursuit of a Discriminative Representation for Multiple Subspaces
            via Sequential Games",
  author = "Pai, Druv and Psenka, Michael and Chiu, Chih-Yuan and Wu, Manxi and
            Dobriban, Edgar and Ma, Yi"
}

@INPROCEEDINGS{Kneip2014-ul,
  title           = "{OpenGV}: A unified and generalized approach to real-time
                     calibrated geometric vision",
  booktitle       = "2014 {IEEE} International Conference on Robotics and
                     Automation ({ICRA})",
  author          = "Kneip, Laurent and Furgale, Paul",
  publisher       = "IEEE",
  month           =  may,
  year            =  2014,
  conference      = "2014 IEEE International Conference on Robotics and
                     Automation (ICRA)",
  location        = "Hong Kong, China"
}

@ARTICLE{Tankala2020-ep,
  title         = "{K-Deep} Simplex: Deep Manifold Learning via Local
                   Dictionaries",
  author        = "Tankala, Pranay and Tasissa, Abiy and Murphy, James M and
                   Ba, Demba",
  abstract      = "We propose K-Deep Simplex (KDS), a unified optimization
                   framework for nonlinear dimensionality reduction that
                   combines the strengths of manifold learning and sparse
                   dictionary learning. Our approach learns local dictionaries
                   that represent a data point with reconstruction coefficients
                   supported on the probability simplex. The dictionaries are
                   learned using algorithm unrolling, an increasingly popular
                   technique for structured deep learning. KDS enjoys
                   tremendous computational advantages over related approaches
                   and is both interpretable and flexible. In particular, KDS
                   is quasilinear in the number of data points with scaling
                   that depends on intrinsic geometric properties of the data.
                   We apply KDS to the unsupervised clustering problem and
                   prove theoretical performance guarantees. Experiments show
                   that the algorithm is highly efficient and performs
                   competitively on synthetic and real data sets.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2012.02134"
}

@ARTICLE{Muthukumar2022-pf,
  title         = "Adversarial robustness of sparse local Lipschitz predictors",
  author        = "Muthukumar, Ramchandran and Sulam, Jeremias",
  abstract      = "This work studies the adversarial robustness of parametric
                   functions composed of a linear predictor and a non-linear
                   representation map. Our analysis relies on sparse local
                   Lipschitzness (SLL), an extension of local Lipschitz
                   continuity that better captures the stability and reduced
                   effective dimensionality of predictors upon local
                   perturbations. SLL functions preserve a certain degree of
                   structure, given by the sparsity pattern in the
                   representation map, and include several popular hypothesis
                   classes, such as piece-wise linear models, Lasso and its
                   variants, and deep feed-forward ReLU networks. We provide a
                   tighter robustness certificate on the minimal energy of an
                   adversarial example, as well as tighter data-dependent
                   non-uniform bounds on the robust generalization error of
                   these predictors. We instantiate these results for the case
                   of deep neural networks and provide numerical evidence that
                   supports our results, shedding new insights into natural
                   regularization strategies to increase the robustness of
                   these models.",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2202.13216"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Duchi2008-yv,
  title     = "Efficient projections onto the l1-ball for learning in high
               dimensions",
  author    = "Duchi, J and Shalev-Shwartz, S and Singer, Y and {others}",
  abstract  = "… learning tasks. We show that variants of stochastic gradient
               projection methods augmented with our efficient projection …
               dient updates with ℓ1 projections outperform the exponentiated …",
  journal   = "on Machine learning",
  publisher = "dl.acm.org",
  year      =  2008
}

@ARTICLE{Hua2021-gs,
  title         = "On Feature Decorrelation in {Self-Supervised} Learning",
  author        = "Hua, Tianyu and Wang, Wenxiao and Xue, Zihui and Ren,
                   Sucheng and Wang, Yue and Zhao, Hang",
  abstract      = "In self-supervised representation learning, a common idea
                   behind most of the state-of-the-art approaches is to enforce
                   the robustness of the representations to predefined
                   augmentations. A potential issue of this idea is the
                   existence of completely collapsed solutions (i.e., constant
                   features), which are typically avoided implicitly by
                   carefully chosen implementation details. In this work, we
                   study a relatively concise framework containing the most
                   common components from recent approaches. We verify the
                   existence of complete collapse and discover another
                   reachable collapse pattern that is usually overlooked,
                   namely dimensional collapse. We connect dimensional collapse
                   with strong correlations between axes and consider such
                   connection as a strong motivation for feature decorrelation
                   (i.e., standardizing the covariance matrix). The gains from
                   feature decorrelation are verified empirically to highlight
                   the importance and the potential of this insight.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2105.00470"
}

@ARTICLE{Mei2018-jb,
  title    = "A mean field view of the landscape of two-layer neural networks",
  author   = "Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh",
  abstract = "Multilayer neural networks are among the most powerful models in
              machine learning, yet the fundamental reasons for this success
              defy mathematical understanding. Learning a neural network
              requires optimizing a nonconvex high-dimensional objective (risk
              function), a problem that is usually attacked using stochastic
              gradient descent (SGD). Does SGD converge to a global optimum of
              the risk or only to a local optimum? In the former case, does
              this happen because local minima are absent or because SGD
              somehow avoids them? In the latter, why do local minima reached
              by SGD have good generalization properties? In this paper, we
              consider a simple case, namely two-layer neural networks, and
              prove that-in a suitable scaling limit-SGD dynamics is captured
              by a certain nonlinear partial differential equation (PDE) that
              we call distributional dynamics (DD). We then consider several
              specific examples and show how DD can be used to prove
              convergence of SGD to networks with nearly ideal generalization
              error. This description allows for ``averaging out'' some of the
              complexities of the landscape of neural networks and can be used
              to prove a general convergence result for noisy SGD.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  115,
  number   =  33,
  pages    = "E7665--E7671",
  month    =  aug,
  year     =  2018,
  keywords = "Wasserstein space; gradient flow; neural networks; partial
              differential equations; stochastic gradient descent",
  language = "en"
}

@ARTICLE{Wainwright2009-wd,
  title     = "Sharp thresholds for high-dimensional and noisy sparsity
               recovery using $\ell _{1}$-constrained quadratic programming
               (lasso)",
  author    = "Wainwright, Martin J",
  journal   = "IEEE Trans. Inf. Theory",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  55,
  number    =  5,
  pages     = "2183--2202",
  month     =  may,
  year      =  2009,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}

@ARTICLE{Xu2021-do,
  title         = "{DP-SSL}: Towards Robust Semi-supervised Learning with A Few
                   Labeled Samples",
  author        = "Xu, Yi and Ding, Jiandong and Zhang, Lu and Zhou, Shuigeng",
  abstract      = "The scarcity of labeled data is a critical obstacle to deep
                   learning. Semi-supervised learning (SSL) provides a
                   promising way to leverage unlabeled data by pseudo labels.
                   However, when the size of labeled data is very small (say a
                   few labeled samples per class), SSL performs poorly and
                   unstably, possibly due to the low quality of learned pseudo
                   labels. In this paper, we propose a new SSL method called
                   DP-SSL that adopts an innovative data programming (DP)
                   scheme to generate probabilistic labels for unlabeled data.
                   Different from existing DP methods that rely on human
                   experts to provide initial labeling functions (LFs), we
                   develop a multiple-choice learning~(MCL) based approach to
                   automatically generate LFs from scratch in SSL style. With
                   the noisy labels produced by the LFs, we design a label
                   model to resolve the conflict and overlap among the noisy
                   labels, and finally infer probabilistic labels for unlabeled
                   samples. Extensive experiments on four standard SSL
                   benchmarks show that DP-SSL can provide reliable labels for
                   unlabeled data and achieve better classification performance
                   on test sets than existing SSL methods, especially when only
                   a small number of labeled samples are available. Concretely,
                   for CIFAR-10 with only 40 labeled samples, DP-SSL achieves
                   93.82\% annotation accuracy on unlabeled data and 93.46\%
                   classification accuracy on test data, which are higher than
                   the SOTA results.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2110.13740"
}

@ARTICLE{Lu2020-dt,
  title         = "Learning interaction kernels in stochastic systems of
                   interacting particles from multiple trajectories",
  author        = "Lu, Fei and Maggioni, Mauro and Tang, Sui",
  abstract      = "We consider stochastic systems of interacting particles or
                   agents, with dynamics determined by an interaction kernel
                   which only depends on pairwise distances. We study the
                   problem of inferring this interaction kernel from
                   observations of the positions of the particles, in either
                   continuous or discrete time, along multiple independent
                   trajectories. We introduce a nonparametric inference
                   approach to this inverse problem, based on a regularized
                   maximum likelihood estimator constrained to suitable
                   hypothesis spaces adaptive to data. We show that a
                   coercivity condition enables us to control the condition
                   number of this problem and prove the consistency of our
                   estimator, and that in fact it converges at a near-optimal
                   learning rate, equal to the min-max rate of $1$-dimensional
                   non-parametric regression. In particular, this rate is
                   independent of the dimension of the state space, which is
                   typically very high. We also analyze the discretization
                   errors in the case of discrete-time observations, showing
                   that it is of order $1/2$ in terms of the time gaps between
                   observations. This term, when large, dominates the sampling
                   error and the approximation error, preventing convergence of
                   the estimator. Finally, we exhibit an efficient parallel
                   algorithm to construct the estimator from data, and we
                   demonstrate the effectiveness of our algorithm with
                   numerical tests on prototype systems including stochastic
                   opinion dynamics and a Lennard-Jones model.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "2007.15174"
}

@INPROCEEDINGS{Yoon2015-gg,
  title     = "A new minimum infinity-norm solution: With application to
               capacity analysis of spacecraft reaction wheels",
  booktitle = "2015 American Control Conference ({ACC})",
  author    = "Yoon, Hyungjoo and Seo, Hyun Ho and Park, Young-Woong and Choi,
               Hong-Taek",
  abstract  = "In this study, a new algorithm is presented to find a minimum
               infinity-norm, or L$\infty$-norm, solution of an underdetermined
               linear equations system. Contrary to the previous algorithms,
               which in fact solve its `dual optimization problem' instead of
               the problem itself, the newly proposed one directly solves the
               minimum L$\infty$-norm problem using properties of the solution.
               Therefore, it is more straightforward, easy to understand, and
               deepening our understanding of the problem. In order to show the
               usability of the proposed algorithm, the present paper also
               includes an example of its applications to the capacity analysis
               of spacecraft reaction wheels. The algorithm can be used to
               define the torque/momentum envelopes, which are the maximum
               torque/momentum capacities of the reaction wheels, respectively.
               The resultant envelopes then can be used to determine the
               optimal specification requirements of the reaction wheels to
               achieve mission requirements.",
  pages     = "1241--1245",
  month     =  jul,
  year      =  2015,
  keywords  = "Wheels;Space vehicles;Arrays;Torque;Attitude control;Algorithm
               design and analysis;Optimization"
}

@ARTICLE{Zha2001-np,
  title         = "Bipartite graph partitioning and data clustering",
  author        = "Zha, H and He, X and Ding, C and Gu, M and Simon, H",
  abstract      = "Many data types arising from data mining applications can be
                   modeled as bipartite graphs, examples include terms and
                   documents in a text corpus, customers and purchasing items
                   in market basket analysis and reviewers and movies in a
                   movie recommender system. In this paper, we propose a new
                   data clustering method based on partitioning the underlying
                   bipartite graph. The partition is constructed by minimizing
                   a normalized sum of edge weights between unmatched pairs of
                   vertices of the bipartite graph. We show that an approximate
                   solution to the minimization problem can be obtained by
                   computing a partial singular value decomposition (SVD) of
                   the associated edge weight matrix of the bipartite graph. We
                   point out the connection of our clustering algorithm to
                   correspondence analysis used in multivariate analysis. We
                   also briefly discuss the issue of assigning data objects to
                   multiple clusters. In the experimental results, we apply our
                   clustering algorithm to the problem of document clustering
                   to illustrate its effectiveness and efficiency.",
  month         =  aug,
  year          =  2001,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "cs/0108018"
}

@ARTICLE{Mahmood2014-nt,
  title         = "Semi-supervised Spectral Clustering for Classification",
  author        = "Mahmood, Arif and Mian, Ajmal S",
  abstract      = "We propose a Classification Via Clustering (CVC) algorithm
                   which enables existing clustering methods to be efficiently
                   employed in classification problems. In CVC, training and
                   test data are co-clustered and class-cluster distributions
                   are used to find the label of the test data. To determine an
                   efficient number of clusters, a Semi-supervised Hierarchical
                   Clustering (SHC) algorithm is proposed. Clusters are
                   obtained by hierarchically applying two-way NCut by using
                   signs of the Fiedler vector of the normalized graph
                   Laplacian. To this end, a Direct Fiedler Vector Computation
                   algorithm is proposed. The graph cut is based on the data
                   structure and does not consider labels. Labels are used only
                   to define the stopping criterion for graph cut. We propose
                   clustering to be performed on the Grassmannian manifolds
                   facilitating the formation of spectral ensembles. The
                   proposed algorithm outperformed state-of-the-art image-set
                   classification algorithms on five standard datasets.",
  month         =  may,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1405.5737"
}

@ARTICLE{Robinson2019-da,
  title    = "Basis Pursuit and Orthogonal Matching Pursuit for
              Subspace-preserving Recovery: Theoretical Analysis",
  author   = "Robinson, Daniel P and Vidal, Rene and You, Chong",
  abstract = "Given an overcomplete dictionary $A$ and a signal $b = Ac^*$ for
              some sparse vector $c^*$ whose nonzero entries correspond to
              linearly independent columns of $A$, classical sparse signal
              recovery theory considers the problem of whether $c^*$ can be
              recovered as the unique sparsest solution to $b = A c$. It is now
              well-understood that such recovery is possible by practical
              algorithms when the dictionary $A$ is incoherent or restricted
              isometric. In this paper, we consider the more general case where
              $b$ lies in a subspace $\mathcal\{S\}_0$ spanned by a subset of
              linearly dependent columns of $A$, and the remaining columns are
              outside of the subspace. In this case, the sparsest
              representation may not be unique, and the dictionary may not be
              incoherent or restricted isometric. The goal is to have the
              representation $c$ correctly identify the subspace, i.e. the
              nonzero entries of $c$ should correspond to columns of $A$ that
              are in the subspace $\mathcal\{S\}_0$. Such a representation $c$
              is called subspace-preserving, a key concept that has found
              important applications for learning low-dimensional structures in
              high-dimensional data. We present various geometric conditions
              that guarantee subspace-preserving recovery. Among them, the
              major results are characterized by the covering radius and the
              angular distance, which capture the distribution of points in the
              subspace and the similarity between points in the subspace and
              points outside the subspace, respectively. Importantly, these
              conditions do not require the dictionary to be incoherent or
              restricted isometric. By establishing that the
              subspace-preserving recovery problem and the classical sparse
              signal recovery problem are equivalent under common assumptions
              on the latter, we show that several of our proposed conditions
              are generalizations of some well-known conditions in the sparse
              signal recovery literature.",
  journal  = "arXiv [cs.LG]",
  month    =  dec,
  year     =  2019
}

@ARTICLE{Sohn2020-da,
  title   = "Fixmatch: Simplifying semi-supervised learning with consistency
             and confidence",
  author  = "Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang,
             Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus
             and Kurakin, Alexey and Li, Chun-Liang",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  33,
  pages   = "596--608",
  year    =  2020
}

@ARTICLE{Merris1997-tb,
  title     = "{DOUBLY} {STOCHASTIC} {GRAPH} {MATRICES}",
  author    = "Merris, Russell",
  journal   = "Publikacije Elektrotehni{\v c}kog fakulteta. Serija Matematika",
  publisher = "University of Belgrade, Serbia",
  number    =  8,
  pages     = "64--71",
  year      =  1997
}

@ARTICLE{Zhu2018-mv,
  title         = "Dual Principal Component Pursuit: Probability Analysis and
                   Efficient Algorithms",
  author        = "Zhu, Zhihui and Wang, Yifan and Robinson, Daniel P and
                   Naiman, Daniel Q and Vidal, Rene and Tsakiris, Manolis C",
  abstract      = "Recent methods for learning a linear subspace from data
                   corrupted by outliers are based on convex $\ell_1$ and
                   nuclear norm optimization and require the dimension of the
                   subspace and the number of outliers to be sufficiently
                   small. In sharp contrast, the recently proposed Dual
                   Principal Component Pursuit (DPCP) method can provably
                   handle subspaces of high dimension by solving a non-convex
                   $\ell_1$ optimization problem on the sphere. However, its
                   geometric analysis is based on quantities that are difficult
                   to interpret and are not amenable to statistical analysis.
                   In this paper we provide a refined geometric analysis and a
                   new statistical analysis that show that DPCP can tolerate as
                   many outliers as the square of the number of inliers, thus
                   improving upon other provably correct robust PCA methods. We
                   also propose a scalable Projected Sub-Gradient Method method
                   (DPCP-PSGM) for solving the DPCP problem and show it admits
                   linear convergence even though the underlying optimization
                   problem is non-convex and non-smooth. Experiments on road
                   plane detection from 3D point cloud data demonstrate that
                   DPCP-PSGM can be more efficient than the traditional RANSAC
                   algorithm, which is one of the most popular methods for such
                   computer vision applications.",
  month         =  dec,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1812.09924"
}

@MISC{Yang_undated-qp,
  title        = "Sparse subspace clustering with missing entries",
  author       = "Yang, Congyuan and Robinson, Daniel and Vidal, Ren{\'e}",
  howpublished = "\url{http://proceedings.mlr.press/v37/yangf15.pdf}",
  note         = "Accessed: 2023-6-5"
}

@ARTICLE{Vogelstein2015-wr,
  title    = "Fast approximate quadratic programming for graph matching",
  author   = "Vogelstein, Joshua T and Conroy, John M and Lyzinski, Vince and
              Podrazik, Louis J and Kratzer, Steven G and Harley, Eric T and
              Fishkind, Donniell E and Vogelstein, R Jacob and Priebe, Carey E",
  abstract = "Quadratic assignment problems arise in a wide variety of domains,
              spanning operations research, graph theory, computer vision, and
              neuroscience, to name a few. The graph matching problem is a
              special case of the quadratic assignment problem, and graph
              matching is increasingly important as graph-valued data is
              becoming more prominent. With the aim of efficiently and
              accurately matching the large graphs common in big data, we
              present our graph matching algorithm, the Fast Approximate
              Quadratic assignment algorithm. We empirically demonstrate that
              our algorithm is faster and achieves a lower objective value on
              over 80\% of the QAPLIB benchmark library, compared with the
              previous state-of-the-art. Applying our algorithm to our
              motivating example, matching C. elegans connectomes
              (brain-graphs), we find that it efficiently achieves performance.",
  journal  = "PLoS One",
  volume   =  10,
  number   =  4,
  pages    = "e0121002",
  month    =  apr,
  year     =  2015,
  language = "en"
}

@ARTICLE{Laude2021-pa,
  title         = "Conjugate dualities for relative smoothness and strong
                   convexity under the light of generalized convexity",
  author        = "Laude, Emanuel and Themelis, Andreas and Patrinos,
                   Panagiotis",
  abstract      = "Relative Bregman smoothness and strong convexity have
                   recently gained considerable attention in optimization.
                   However, conjugate dualities for Bregman smoothness and
                   strong convexity remain an open problem as noted earlier by
                   Lu, Freund, and Nesterov, Relatively smooth convex
                   optimization by first-order methods, and applications, SIAM
                   Journal on Optimization, 28(1):333-354, 2018. In this paper
                   we address this question by introducing the notions of
                   relative anisotropic strong convexity and smoothness as the
                   respective dual counterparts of Bregman smoothness and
                   strong convexity. In essence, the duality holds between
                   tilt- and shift-parametrized families of upper and lower
                   bounds and can thus be examined under the light of
                   generalized convexity. In the Euclidean case this
                   specializes to the well-known conjugate duality between
                   Lipschitz smoothness and strong convexity. The two notions
                   here introduced can be thought of as anisotropic
                   generalizations of the well-known descent lemma and the
                   strong convexity subgradient inequality. Alternatively, in
                   the context of generalized convexity these characterizations
                   can be interpreted as generalized subgradient inequalities.
                   In the Euclidean case, the class of strongly convex
                   functions can be described in terms of pointwise maxima over
                   quadratics with uniform curvature. Surprisingly, in
                   contrast, the class of anisotropically strongly convex
                   functions, in general, only forms a proper subset of the
                   corresponding class of pointwise maxima, unless a certain
                   saddle-point property holds. Aside from the Euclidean case,
                   this saddle-point property is shown to hold automatically in
                   the one-dimensional or the essentially smooth case.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2112.08886"
}

@MISC{Wright_undated-ge,
  title        = "Classification via minimum incremental coding length (micl)",
  author       = "Wright, John and Ma, Y I and Tao, Yangyu and Lin, Zhouchen
                  and Shum, Heung-Yeung",
  abstract     = "We present a simple new criterion for classification, based
                  on principles from lossy data compression. The criterion
                  assigns a test sample to the class that uses the minimum
                  number of additional bits to code the test sample, subject to
                  an allowable distortion. We demonstrate the asymptotic
                  optimality of this criterion for Gaussian distributions and
                  analyze its relationships to classical classifiers. The
                  theoretical results clarify the connections between our
                  approach and popular classifiers such as MAP, RDA, k-NN, and
                  SVM, as well as unsupervised methods based on lossy coding.
                  Our formulation induces several good effects on the resulting
                  classifier. First, minimizing the lossy coding length induces
                  a regularization effect which stabilizes the (implicit)
                  density estimate in a small sample setting. Second,
                  compression provides a uniform means of handling classes of
                  varying dimension. The new criterion and its kernel and local
                  versions perform competitively on synthetic examples, as well
                  as on real imagery data such as handwritten digits and face
                  images. On these problems, the performance of our simple
                  classifier approaches the best reported results, without
                  using domain-specific information. All MATLAB code and
                  classification results are publicly available for peer
                  evaluation at http://perception.csl.uiuc.edu/coding/home.htm.",
  howpublished = "\url{https://people.eecs.berkeley.edu/~yima/psfile/MICL_SJIS.pdf}",
  note         = "Accessed: 2022-5-20"
}

@ARTICLE{Shi2000-fb,
  title    = "Normalized cuts and image segmentation",
  author   = "Shi, Jianbo and Malik, J",
  abstract = "We propose a novel approach for solving the perceptual grouping
              problem in vision. Rather than focusing on local features and
              their consistencies in the image data, our approach aims at
              extracting the global impression of an image. We treat image
              segmentation as a graph partitioning problem and propose a novel
              global criterion, the normalized cut, for segmenting the graph.
              The normalized cut criterion measures both the total
              dissimilarity between the different groups as well as the total
              similarity within the groups. We show that an efficient
              computational technique based on a generalized eigenvalue problem
              can be used to optimize this criterion. We applied this approach
              to segmenting static images, as well as motion sequences, and
              found the results to be very encouraging.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  22,
  number   =  8,
  pages    = "888--905",
  month    =  aug,
  year     =  2000,
  keywords = "Image segmentation;Brightness;Clustering algorithms;Data
              mining;Eigenvalues and eigenfunctions;Bayesian
              methods;Coherence;Tree data structures;Filling;Partitioning
              algorithms"
}

@ARTICLE{Alonso-Gutierrez_undated-ff,
  title   = "{ON} {THE} {ISOTROPY} {CONSTANT} {OF} {RANDOM} {CONVEX} {SETS}",
  author  = "{Alonso-Gutierrez}",
  journal = "Number"
}

@ARTICLE{Balestriero2022-ve,
  title         = "Contrastive and {Non-Contrastive} {Self-Supervised} Learning
                   Recover Global and Local Spectral Embedding Methods",
  author        = "Balestriero, Randall and LeCun, Yann",
  abstract      = "Self-Supervised Learning (SSL) surmises that inputs and
                   pairwise positive relationships are enough to learn
                   meaningful representations. Although SSL has recently
                   reached a milestone: outperforming supervised methods in
                   many modalities\textbackslashdots the theoretical
                   foundations are limited, method-specific, and fail to
                   provide principled design guidelines to practitioners. In
                   this paper, we propose a unifying framework under the helm
                   of spectral manifold learning to address those limitations.
                   Through the course of this study, we will rigorously
                   demonstrate that VICReg, SimCLR, BarlowTwins et al.
                   correspond to eponymous spectral methods such as Laplacian
                   Eigenmaps, Multidimensional Scaling et al. This unification
                   will then allow us to obtain (i) the closed-form optimal
                   representation for each method, (ii) the closed-form optimal
                   network parameters in the linear regime for each method,
                   (iii) the impact of the pairwise relations used during
                   training on each of those quantities and on downstream task
                   performances, and most importantly, (iv) the first
                   theoretical bridge between contrastive and non-contrastive
                   methods towards global and local spectral embedding methods
                   respectively, hinting at the benefits and limitations of
                   each. For example, (i) if the pairwise relation is aligned
                   with the downstream task, any SSL method can be employed
                   successfully and will recover the supervised method, but in
                   the low data regime, VICReg's invariance hyper-parameter
                   should be high; (ii) if the pairwise relation is misaligned
                   with the downstream task, VICReg with small invariance
                   hyper-parameter should be preferred over SimCLR or
                   BarlowTwins.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2205.11508"
}

@ARTICLE{Ng2001-gx,
  title   = "On spectral clustering: Analysis and an algorithm",
  author  = "Ng, Andrew and Jordan, Michael and Weiss, Yair",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  14,
  year    =  2001
}

@ARTICLE{Johnson_undated-pa,
  title   = "On the Effectiveness of Laplacian Normalization for Graph
             Semi-supervised Learning",
  author  = "{Johnson}",
  journal = "J. Mach. Learn. Res."
}

@ARTICLE{Pearson1901-iz,
  title     = "{LIII}. On lines and planes of closest fit to systems of points
               in space",
  author    = "Pearson, Karl",
  journal   = "The London, Edinburgh, and Dublin Philosophical Magazine and
               Journal of Science",
  publisher = "Taylor \& Francis",
  volume    =  2,
  number    =  11,
  pages     = "559--572",
  month     =  nov,
  year      =  1901
}

@ARTICLE{Sun2019-gz,
  title         = "Lifelong Spectral Clustering",
  author        = "Sun, Gan and Cong, Yang and Wang, Qianqian and Li, Jun and
                   Fu, Yun",
  abstract      = "In the past decades, spectral clustering (SC) has become one
                   of the most effective clustering algorithms. However, most
                   previous studies focus on spectral clustering tasks with a
                   fixed task set, which cannot incorporate with a new spectral
                   clustering task without accessing to previously learned
                   tasks. In this paper, we aim to explore the problem of
                   spectral clustering in a lifelong machine learning
                   framework, i.e., Lifelong Spectral Clustering (L2SC). Its
                   goal is to efficiently learn a model for a new spectral
                   clustering task by selectively transferring previously
                   accumulated experience from knowledge library. Specifically,
                   the knowledge library of L2SC contains two components: 1)
                   orthogonal basis library: capturing latent cluster centers
                   among the clusters in each pair of tasks; 2) feature
                   embedding library: embedding the feature manifold
                   information shared among multiple related tasks. As a new
                   spectral clustering task arrives, L2SC firstly transfers
                   knowledge from both basis library and feature library to
                   obtain encoding matrix, and further redefines the library
                   base over time to maximize performance across all the
                   clustering tasks. Meanwhile, a general online update
                   formulation is derived to alternatively update the basis
                   library and feature library. Finally, the empirical
                   experiments on several real-world benchmark datasets
                   demonstrate that our L2SC model can effectively improve the
                   clustering performance when comparing with other
                   state-of-the-art spectral clustering algorithms.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1911.11908"
}

@MISC{Kaba_undated-vg,
  title        = "A nullspace property for subspace-preserving recovery",
  author       = "Kaba, Mustafa D and You, Chong and Robinson, Daniel P and
                  Mallada, Enrique and Vidal, Ren{\'e}",
  abstract     = "Much of the theory for classical sparse recovery is based on
                  conditions on the dictionary that are both necessary and
                  sufficient (e.g., nullspace property) or only sufficient
                  (e.g., incoherence and restricted isometry). In contrast,
                  much of the theory for subspace-preserving recovery, the
                  theoretical underpinnings for sparse subspace classification
                  and clustering methods, is based on conditions on the
                  subspaces and the data that are only sufficient (e.g.,
                  subspace incoherence and data innerradius). This paper
                  derives a necessary and sufficient condition for
                  subspace-preserving recovery that is inspired by the
                  classical nullspace property. Based on this novel condition,
                  called here the subspace nullspace property, we derive
                  equivalent characterizations that either admit a clear
                  geometric interpretation that relates data distribution and
                  subspace separation to the recovery success, or can be
                  verified using a finite set of extreme points of a properly
                  defined set. We further exploit these characterizations to
                  derive new sufficient conditions, based on inner-radius and
                  outer-radius measures and dual bounds, that generalize
                  existing conditions and preserve the geometric
                  interpretations. These results fill an important gap in the
                  subspace-preserving recovery literature.",
  howpublished = "\url{http://proceedings.mlr.press/v139/kaba21a/kaba21a.pdf}",
  note         = "Accessed: 2022-6-7"
}

@UNPUBLISHED{Vidal2021-rh,
  title    = "Attention: {Self-Expression} Is All You Need",
  author   = "Vidal, Rene",
  abstract = "Transformer models have achieved significant improvements in
              performance for various learning tasks in natural language
              processing and computer vision. Much of their success is
              attributed to the use of attention layers that capture long-range
              interactions among data tokens (such as words and image patches)
              via attention coefficients that are global and adapted to the
              input data at test time. In this paper we study the principles
              behind attention and its connections with prior art.
              Specifically, we show that attention builds upon a long history
              of prior work on manifold learning and image processing,
              including methods such as kernel-based regression, non-local
              means, locally linear embedding, subspace clustering and sparse
              coding. Notably, we show that self-attention is closely related
              to the notion of self-expressiveness in subspace clustering,
              wherein data points to be clustered are expressed as linear
              combinations of all other points with coefficients designed to
              attend to other points in the same group, thus capturing
              long-range interactions. We also show that heuristics in sparse
              self-attention can be studied in a more principled manner using
              prior literature on sparse coding and sparse subspace clustering.
              We thus conclude that the key innovations of attention mechanisms
              relative to prior art are the use of many learnable parameters,
              and multiple heads and layers.",
  month    =  oct,
  year     =  2021
}

@INPROCEEDINGS{Ji2014-qb,
  title     = "Efficient dense subspace clustering",
  booktitle = "{IEEE} Winter Conference on Applications of Computer Vision",
  author    = "Ji, Pan and Salzmann, Mathieu and Li, Hongdong",
  abstract  = "In this paper, we tackle the problem of clustering data points
               drawn from a union of linear (or affine) subspaces. To this end,
               we introduce an efficient subspace clustering algorithm that
               estimates dense connections between the points lying in the same
               subspace. In particular, instead of following the standard
               compressive sensing approach, we formulate subspace clustering
               as a Frobenius norm minimization problem, which inherently
               yields denser con- nections between the data points. While in
               the noise-free case we rely on the self-expressiveness of the
               observations, in the presence of noise we simultaneously learn a
               clean dictionary to represent the data. Our formulation lets us
               address the subspace clustering problem efficiently. More
               specifically, the solution can be obtained in closed-form for
               outlier-free observations, and by performing a series of linear
               operations in the presence of outliers. Interestingly, we show
               that our Frobenius norm formulation shares the same solution as
               the popular nuclear norm minimization approach when the data is
               free of any noise, or, in the case of corrupted data, when a
               clean dictionary is learned. Our experimental evaluation on
               motion segmentation and face clustering demonstrates the
               benefits of our algorithm in terms of clustering accuracy and
               efficiency.",
  pages     = "461--468",
  month     =  mar,
  year      =  2014,
  keywords  = "Noise;Dictionaries;Clustering algorithms;Minimization;Noise
               measurement;Closed-form solutions;Vectors"
}

@ARTICLE{Jancey1966-tj,
  title   = "Multidimensional group analysis",
  author  = "Jancey, R C",
  journal = "Australian Journal of Botany",
  volume  =  14,
  pages   = "127--130",
  year    =  1966
}

@INPROCEEDINGS{McQueen1967-ru,
  title     = "Some methods for classification and analysis of multivariate
               observations",
  booktitle = "Fifth Berkeley Symposium on Mathematical Statistics and
               Probability",
  author    = "McQueen, James B",
  pages     = "281--297",
  year      =  1967
}

@INPROCEEDINGS{Haeffele2020-kj,
  title     = "A Critique of {Self-Expressive} Deep Subspace Clustering",
  booktitle = "International Conference on Learning Representations",
  author    = "Haeffele, Benjamin D and You, Chong and Vidal, Ren{\'e}",
  abstract  = "Subspace clustering is an unsupervised clustering technique
               designed to cluster data that is supported on a union of linear
               subspaces, with each subspace defining a cluster with dimension
               lower than the ambient space. Many existing formulations for
               this problem are based on exploiting the self-expressive
               property of linear subspaces, where any point within a subspace
               can be represented as linear combination of other points within
               the subspace. To extend this approach to data supported on a
               union of non-linear manifolds, numerous studies have proposed
               learning an embedding of the original data using a neural
               network which is regularized by a self-expressive loss function
               on the data in the embedded space to encourage a union of linear
               subspaces prior on the data in the embedded space. Here we show
               that there are a number of potential flaws with this approach
               which have not been adequately addressed in prior work. In
               particular, we show the model formulation is often ill-posed in
               that it can lead to a degenerate embedding of the data, which
               need not correspond to a union of subspaces at all and is poorly
               suited for clustering. We validate our theoretical results
               experimentally and also repeat prior experiments reported in the
               literature, where we conclude that a significant portion of the
               previously claimed performance benefits can be attributed to an
               ad-hoc post processing step rather than the deep subspace
               clustering model.",
  year      =  2020
}

@ARTICLE{Ji2015-bm,
  title         = "Shape Interaction Matrix Revisited and Robustified:
                   Efficient Subspace Clustering with Corrupted and Incomplete
                   Data",
  author        = "Ji, Pan and Salzmann, Mathieu and Li, Hongdong",
  abstract      = "The Shape Interaction Matrix (SIM) is one of the earliest
                   approaches to performing subspace clustering (i.e.,
                   separating points drawn from a union of subspaces). In this
                   paper, we revisit the SIM and reveal its connections to
                   several recent subspace clustering methods. Our analysis
                   lets us derive a simple, yet effective algorithm to
                   robustify the SIM and make it applicable to realistic
                   scenarios where the data is corrupted by noise. We justify
                   our method by intuitive examples and the matrix perturbation
                   theory. We then show how this approach can be extended to
                   handle missing data, thus yielding an efficient and general
                   subspace clustering algorithm. We demonstrate the benefits
                   of our approach over state-of-the-art subspace clustering
                   methods on several challenging motion segmentation and face
                   clustering problems, where the data includes corrupted and
                   missing measurements.",
  month         =  sep,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1509.02649"
}

@ARTICLE{Zass2006-zl,
  title   = "Doubly stochastic normalization for spectral clustering",
  author  = "{Zass} and {Shashua}",
  journal = "Advances in neural information processing systems",
  volume  =  19,
  year    =  2006
}

@ARTICLE{Sussman2011-gk,
  title         = "A consistent adjacency spectral embedding for stochastic
                   blockmodel graphs",
  author        = "Sussman, Daniel L and Tang, Minh and Fishkind, Donniell E
                   and Priebe, Carey E",
  abstract      = "We present a method to estimate block membership of nodes in
                   a random graph generated by a stochastic blockmodel. We use
                   an embedding procedure motivated by the random dot product
                   graph model, a particular example of the latent position
                   model. The embedding associates each node with a vector;
                   these vectors are clustered via minimization of a square
                   error criterion. We prove that this method is consistent for
                   assigning nodes to blocks, as only a negligible number of
                   nodes will be mis-assigned. We prove consistency of the
                   method for directed and undirected graphs. The consistent
                   block assignment makes possible consistent parameter
                   estimation for a stochastic blockmodel. We extend the result
                   in the setting where the number of blocks grows slowly with
                   the number of nodes. Our method is also computationally
                   feasible even for very large graphs. We compare our method
                   to Laplacian spectral clustering through analysis of
                   simulated data and a graph derived from Wikipedia documents.",
  month         =  aug,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1108.2228"
}

@ARTICLE{Rohe2010-lq,
  title         = "Spectral clustering and the high-dimensional stochastic
                   blockmodel",
  author        = "Rohe, Karl and Chatterjee, Sourav and Yu, Bin",
  abstract      = "Networks or graphs can easily represent a diverse set of
                   data sources that are characterized by interacting units or
                   actors. Social networks, representing people who communicate
                   with each other, are one example. Communities or clusters of
                   highly connected actors form an essential feature in the
                   structure of several empirical networks. Spectral clustering
                   is a popular and computationally feasible method to discover
                   these communities. The stochastic blockmodel [Social
                   Networks 5 (1983) 109--137] is a social network model with
                   well-defined communities; each node is a member of one
                   community. For a network generated from the Stochastic
                   Blockmodel, we bound the number of nodes ``misclustered'' by
                   spectral clustering. The asymptotic results in this paper
                   are the first clustering results that allow the number of
                   clusters in the model to grow with the number of nodes,
                   hence the name high-dimensional. In order to study spectral
                   clustering under the stochastic blockmodel, we first show
                   that under the more general latent space model, the
                   eigenvectors of the normalized graph Laplacian
                   asymptotically converge to the eigenvectors of a
                   ``population'' normalized graph Laplacian. Aside from the
                   implication for spectral clustering, this provides insight
                   into a graph visualization technique. Our method of studying
                   the eigenvectors of random matrices is original.",
  month         =  jul,
  year          =  2010,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1007.1684"
}

@INPROCEEDINGS{Ding2021-px,
  title     = "Dual Principal Component Pursuit for learning a union of
               hyperplanes: Theory and algorithms",
  booktitle = "Artificial Intelligence and Statistics",
  author    = "Ding, Tianyu and Zhu, Zhihui and Tsakiris, Manolis and Vidal,
               Ren{\'e} and Robinson, Daniel",
  abstract  = "State-of-the-art subspace clustering methods are based on convex
               formulations whose theoretical guarantees require the subspaces
               to be low-dimensional. Dual Principal Component Pursuit (DPCP)
               is a non-convex method that is specifically designed for
               learning highdimensional subspaces, such as hyperplanes.
               However, existing analyses of DPCP in the multi-hyperplane case
               lack a precise characterization of the distribution of the data
               and involve quantities that are difficult to interpret.
               Moreover, the provable algorithm based on recursive linear
               programming is not efficient. In this paper, we introduce a new
               notion of geometric dominance, which explicitly captures the
               distribution of the data, and derive both geometric and
               probabilistic conditions under which a global solution to DPCP
               is a normal vector to a geometrically dominant hyperplane. We
               then prove that the DPCP problem for a union of hyperplanes
               satisfies a Riemannian regularity condition, and use this result
               to show that a scalable Riemannian subgradient method exhibits
               (local) linear convergence to the normal vector of the
               geometrically dominant hyperplane. Finally, we show that
               integrating DPCP into popular subspace clustering schemes, such
               as K-ensembles, leads to superior or competitive performance
               over the state-of-the-art in clustering hyperplanes.",
  year      =  2021
}

@INPROCEEDINGS{Tsakiris2017-vt,
  title     = "Hyperplane Clustering via Dual Principal Component Pursuit",
  booktitle = "Proceedings of the 34th International Conference on Machine
               Learning",
  author    = "Tsakiris, Manolis C and Vidal, Ren{\'e}",
  editor    = "Precup, Doina and Teh, Yee Whye",
  abstract  = "State-of-the-art methods for clustering data drawn from a union
               of subspaces are based on sparse and low-rank representation
               theory and convex optimization algorithms. Existing results
               guaranteeing the correctness of such methods require the
               dimension of the subspaces to be small relative to the dimension
               of the ambient space. When this assumption is violated, as is,
               e.g., in the case of hyperplanes, existing methods are either
               computationally too intensive (e.g., algebraic methods) or lack
               sufficient theoretical support (e.g., K-Hyperplanes or RANSAC).
               In this paper we provide theoretical and algorithmic
               contributions to the problem of clustering data from a union of
               hyperplanes, by extending a recent subspace learning method
               called Dual Principal Component Pursuit (DPCP) to the
               multi-hyperplane case. We give theoretical guarantees under
               which, the non-convex $\ell_1$ problem associated with DPCP
               admits a unique global minimizer equal to the normal vector of
               the most dominant hyperplane. Inspired by this insight, we
               propose sequential (RANSAC-style) and iterative
               (K-Hyperplanes-style) hyperplane learning DPCP algorithms,
               which, via experiments on synthetic and real data, are shown to
               outperform or be competitive to the state-of-the-art.",
  publisher = "PMLR",
  volume    =  70,
  pages     = "3472--3481",
  series    = "Proceedings of Machine Learning Research",
  year      =  2017
}

@ARTICLE{Luo2022-yl,
  title         = "Understanding Diffusion Models: A Unified Perspective",
  author        = "Luo, Calvin",
  abstract      = "Diffusion models have shown incredible capabilities as
                   generative models; indeed, they power the current
                   state-of-the-art models on text-conditioned image generation
                   such as Imagen and DALL-E 2. In this work we review,
                   demystify, and unify the understanding of diffusion models
                   across both variational and score-based perspectives. We
                   first derive Variational Diffusion Models (VDM) as a special
                   case of a Markovian Hierarchical Variational Autoencoder,
                   where three key assumptions enable tractable computation and
                   scalable optimization of the ELBO. We then prove that
                   optimizing a VDM boils down to learning a neural network to
                   predict one of three potential objectives: the original
                   source input from any arbitrary noisification of it, the
                   original source noise from any arbitrarily noisified input,
                   or the score function of a noisified input at any arbitrary
                   noise level. We then dive deeper into what it means to learn
                   the score function, and connect the variational perspective
                   of a diffusion model explicitly with the Score-based
                   Generative Modeling perspective through Tweedie's Formula.
                   Lastly, we cover how to learn a conditional distribution
                   using diffusion models via guidance.",
  month         =  aug,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2208.11970"
}

@INPROCEEDINGS{Barath_undated-cx,
  title    = "{Multi-H}: Efficient Recovery of Tangent Planes in Stereo Images",
  author   = "Barath, Daniel",
  abstract = "Multi-H -- an efficient method for the recovery of the tangent
              planes of a set of point correspondences satisfying the epipolar
              constraint is proposed. The problem is formulated as a search for
              a labeling minimizing an energy that includes a data and spatial
              regularization terms. The number of planes is controlled by a
              combination of MeanShift [6] and $\alpha$-expansion [3].
              Experiments on the fountain-P11 3D dataset show that Multi-H
              provides highly accurate tangent plane estimates. It also
              outperforms all state-of-the-art techniques for multihomography
              estimation on the publicly available AdelaideRMF dataset. Since
              Multi-H achieves nearly error-free performance, we introduce and
              make public a more challenging dataset for multi-plane fitting
              evaluation."
}

@ARTICLE{Niu2022-iq,
  title     = "{SPICE}: Semantic {Pseudo-Labeling} for Image Clustering",
  author    = "Niu, Chuang and Shan, Hongming and Wang, Ge",
  abstract  = "The similarity among samples and the discrepancy among clusters
               are two crucial aspects of image clustering. However, current
               deep clustering methods suffer from inaccurate estimation of
               either feature similarity or semantic discrepancy. In this
               paper, we present a Semantic Pseudo-labeling-based Image
               ClustEring (SPICE) framework, which divides the clustering
               network into a feature model for measuring the instance-level
               similarity and a clustering head for identifying the
               cluster-level discrepancy. We design two semantics-aware
               pseudo-labeling algorithms, prototype pseudo-labeling and
               reliable pseudo-labeling, which enable accurate and reliable
               self-supervision over clustering. Without using any ground-truth
               label, we optimize the clustering network in three stages: 1)
               train the feature model through contrastive learning to measure
               the instance similarity; 2) train the clustering head with the
               prototype pseudo-labeling algorithm to identify cluster
               semantics; and 3) jointly train the feature model and clustering
               head with the reliable pseudo-labeling algorithm to improve the
               clustering performance. Extensive experimental results
               demonstrate that SPICE achieves significant improvements (~10\%)
               over existing methods and establishes the new state-of-the-art
               clustering results on six balanced benchmark datasets in terms
               of three popular metrics. Importantly, SPICE significantly
               reduces the gap between unsupervised and fully-supervised
               classification; e.g. there is only 2\% (91.8\% vs 93.8\%)
               accuracy difference on CIFAR-10. Our code is made publicly
               available at https://github.com/niuchuangnn/SPICE.",
  journal   = "IEEE Trans. Image Process.",
  volume    =  31,
  pages     = "7264--7278",
  month     =  nov,
  year      =  2022,
  copyright = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  language  = "en"
}

@ARTICLE{Saul2022-eq,
  title     = "A Nonlinear Matrix Decomposition for Mining the Zeros of Sparse
               Data",
  author    = "Saul, Lawrence K",
  abstract  = "We describe a simple iterative solution to a widely recurring
               problem in multivariate data analysis: given a sparse
               nonnegative matrix $\{\mathbf\{X\}\}$, how to estimate a
               low-rank matrix $\{\{\Theta\}\}$ such that $\{\{X\}\} \approx
               f(\{\{\Theta\}\})$, where $f$ is an elementwise nonlinearity? We
               develop a latent variable model for this problem and consider
               those sparsifying nonlinearities, popular in neural networks,
               that map all negative values to zero. The model seeks to explain
               the variability of sparse high-dimensional data in terms of a
               smaller number of degrees of freedom. We show that exact
               inference in this model is tractable and derive an
               expectation-maximization (EM) algorithm to estimate the low-rank
               matrix $\{\{\Theta\}\}$. Notably, we do not parameterize
               $\{\{\Theta\}\}$ as a product of smaller matrices to be
               alternately optimized; instead, we estimate $\{\{\Theta\}\}$
               directly via the singular value decomposition of matrices that
               are repeatedly inferred (at each iteration of the EM algorithm)
               from the model's posterior distribution. We use the model to
               analyze large sparse matrices that arise from data sets of
               binary, grayscale, and color images. In all of these cases, we
               find that the model discovers much lower-rank decompositions
               than purely linear approaches.",
  journal   = "SIAM Journal on Mathematics of Data Science",
  publisher = "Society for Industrial and Applied Mathematics",
  pages     = "431--463",
  month     =  jun,
  year      =  2022
}

@ARTICLE{Kyrillidis2012-hf,
  title         = "Sparse projections onto the simplex",
  author        = "Kyrillidis, Anastasios and Becker, Stephen and And, Volkan
                   Cevher and Koch, Christoph",
  abstract      = "Most learning methods with rank or sparsity constraints use
                   convex relaxations, which lead to optimization with the
                   nuclear norm or the $\ell_1$-norm. However, several
                   important learning applications cannot benefit from this
                   approach as they feature these convex norms as constraints
                   in addition to the non-convex rank and sparsity constraints.
                   In this setting, we derive efficient sparse projections onto
                   the simplex and its extension, and illustrate how to use
                   them to solve high-dimensional learning problems in quantum
                   tomography, sparse density estimation and portfolio
                   selection with non-convex constraints.",
  month         =  jun,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1206.1529"
}

@ARTICLE{Eisenberger2020-xz,
  title         = "Deep shells: Unsupervised shape correspondence with optimal
                   transport",
  author        = "Eisenberger, Marvin and Toker, Aysim and Leal-Taix{\'e},
                   Laura and Cremers, Daniel",
  abstract      = "We propose a novel unsupervised learning approach to 3D
                   shape correspondence that builds a multiscale matching
                   pipeline into a deep neural network. This approach is based
                   on smooth shells, the current state-of-the-art axiomatic
                   correspondence method, which requires an a priori stochastic
                   search over the space of initial poses. Our goal is to
                   replace this costly preprocessing step by directly learning
                   good initializations from the input surfaces. To that end,
                   we systematically derive a fully differentiable,
                   hierarchical matching pipeline from entropy regularized
                   optimal transport. This allows us to combine it with a local
                   feature extractor based on smooth, truncated spectral
                   convolution filters. Finally, we show that the proposed
                   unsupervised method significantly improves over the
                   state-of-the-art on multiple datasets, even in comparison to
                   the most recent supervised methods. Moreover, we demonstrate
                   compelling generalization results by applying our learned
                   filters to examples that significantly deviate from the
                   training set.",
  month         =  oct,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2010.15261"
}

@ARTICLE{Lanckriet_undated-ck,
  title   = "Learning the Kernel Matrix with {Semide{\O}nite} Programming",
  author  = "{Lanckriet}",
  journal = "J. Mach. Learn. Res."
}

@ARTICLE{Beck2015-ed,
  title     = "On the convergence of alternating minimization for convex
               programming with applications to iteratively reweighted least
               squares and decomposition schemes",
  author    = "Beck, Amir",
  journal   = "SIAM J. Optim.",
  publisher = "Society for Industrial \& Applied Mathematics (SIAM)",
  volume    =  25,
  number    =  1,
  pages     = "185--209",
  month     =  jan,
  year      =  2015,
  language  = "en"
}

@INPROCEEDINGS{Barath2020-co,
  title     = "Making Affine Correspondences Work in Camera Geometry
               Computation",
  booktitle = "Computer Vision -- {ECCV} 2020",
  author    = "Barath, Daniel and Polic, Michal and F{\"o}rstner, Wolfgang and
               Sattler, Torsten and Pajdla, Tomas and Kukelova, Zuzana",
  abstract  = "Local features e.g. SIFT and its affine and learned variants
               provide region-to-region rather than point-to-point
               correspondences. This has recently been exploited to create new
               minimal solvers for classical problems such as homography,
               essential and fundamental matrix estimation. The main advantage
               of such solvers is that their sample size is smaller, e.g., only
               two instead of four matches are required to estimate a
               homography. Works proposing such solvers often claim a
               significant improvement in run-time thanks to fewer RANSAC
               iterations. We show that this argument is not valid in practice
               if the solvers are used naively. To overcome this, we propose
               guidelines for effective use of region-to-region matches in the
               course of a full model estimation pipeline. We propose a method
               for refining the local feature geometries by symmetric
               intensity-based matching, combine uncertainty propagation inside
               RANSAC with preemptive model verification, show a general scheme
               for computing uncertainty of minimal solvers results, and adapt
               the sample cheirality check for homography estimation. Our
               experiments show that affine solvers can achieve accuracy
               comparable to point-based solvers at faster run-times when
               following our guidelines. We make code available at
               https://github.com/danini/affine-correspondences-for-camera-geometry.",
  publisher = "Springer International Publishing",
  pages     = "723--740",
  year      =  2020
}

@ARTICLE{Bai2018-el,
  title         = "Subgradient Descent Learns Orthogonal Dictionaries",
  author        = "Bai, Yu and Jiang, Qijia and Sun, Ju",
  abstract      = "This paper concerns dictionary learning, i.e., sparse
                   coding, a fundamental representation learning problem. We
                   show that a subgradient descent algorithm, with random
                   initialization, can provably recover orthogonal dictionaries
                   on a natural nonsmooth, nonconvex $\ell_1$ minimization
                   formulation of the problem, under mild statistical
                   assumptions on the data. This is in contrast to previous
                   provable methods that require either expensive computation
                   or delicate initialization schemes. Our analysis develops
                   several tools for characterizing landscapes of nonsmooth
                   functions, which might be of independent interest for
                   provable training of deep networks with nonsmooth
                   activations (e.g., ReLU), among numerous other applications.
                   Preliminary experiments corroborate our analysis and show
                   that our algorithm works well empirically in recovering
                   orthogonal dictionaries.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1810.10702"
}

@INPROCEEDINGS{Bardes2022-gc,
  title     = "{VICReg}: {Variance-Invariance-Covariance} Regularization for
               {Self-Supervised} Learning",
  booktitle = "International Conference on Learning Representations",
  author    = "Bardes, Adrien and Ponce, Jean and LeCun, Yann",
  abstract  = "Recent self-supervised methods for image representation learning
               are based on maximizing the agreement between embedding vectors
               from different views of the same image. A trivial solution is
               obtained when the encoder outputs constant vectors. This
               collapse problem is often avoided through implicit biases in the
               learning architecture, that often lack a clear justification or
               interpretation. In this paper, we introduce VICReg
               (Variance-Invariance-Covariance Regularization), a method that
               explicitly avoids the collapse problem with a simple
               regularization term on the variance of the embeddings along each
               dimension individually. VICReg combines the variance term with a
               decorrelation mechanism based on redundancy reduction and
               covariance regularization, and achieves results on par with the
               state of the art on several downstream tasks. In addition, we
               show that incorporating our new variance term into other methods
               helps stabilize the training and leads to performance
               improvements.",
  year      =  2022
}

@ARTICLE{Ergen2020-fl,
  title         = "Revealing the structure of deep neural networks via convex
                   duality",
  author        = "Ergen, Tolga and Pilanci, Mert",
  abstract      = "We study regularized deep neural networks (DNNs) and
                   introduce a convex analytic framework to characterize the
                   structure of the hidden layers. We show that a set of
                   optimal hidden layer weights for a norm regularized DNN
                   training problem can be explicitly found as the extreme
                   points of a convex set. For the special case of deep linear
                   networks, we prove that each optimal weight matrix aligns
                   with the previous layers via duality. More importantly, we
                   apply the same characterization to deep ReLU networks with
                   whitened data and prove the same weight alignment holds. As
                   a corollary, we also prove that norm regularized deep ReLU
                   networks yield spline interpolation for one-dimensional
                   datasets which was previously known only for two-layer
                   networks. Furthermore, we provide closed-form solutions for
                   the optimal layer weights when data is rank-one or whitened.
                   The same analysis also applies to architectures with batch
                   normalization even for arbitrary data. Therefore, we obtain
                   a complete explanation for a recent empirical observation
                   termed Neural Collapse where class means collapse to the
                   vertices of a simplex equiangular tight frame.",
  month         =  feb,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2002.09773"
}

@ARTICLE{Papyan2016-kl,
  title         = "Convolutional Neural Networks Analyzed via Convolutional
                   Sparse Coding",
  author        = "Papyan, Vardan and Romano, Yaniv and Elad, Michael",
  abstract      = "Convolutional neural networks (CNN) have led to many
                   state-of-the-art results spanning through various fields.
                   However, a clear and profound theoretical understanding of
                   the forward pass, the core algorithm of CNN, is still
                   lacking. In parallel, within the wide field of sparse
                   approximation, Convolutional Sparse Coding (CSC) has gained
                   increasing attention in recent years. A theoretical study of
                   this model was recently conducted, establishing it as a
                   reliable and stable alternative to the commonly practiced
                   patch-based processing. Herein, we propose a novel
                   multi-layer model, ML-CSC, in which signals are assumed to
                   emerge from a cascade of CSC layers. This is shown to be
                   tightly connected to CNN, so much so that the forward pass
                   of the CNN is in fact the thresholding pursuit serving the
                   ML-CSC model. This connection brings a fresh view to CNN, as
                   we are able to attribute to this architecture theoretical
                   claims such as uniqueness of the representations throughout
                   the network, and their stable estimation, all guaranteed
                   under simple local sparsity conditions. Lastly, identifying
                   the weaknesses in the above pursuit scheme, we propose an
                   alternative to the forward pass, which is connected to
                   deconvolutional, recurrent and residual networks, and has
                   better theoretical guarantees.",
  month         =  jul,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1607.08194"
}

@INPROCEEDINGS{Eisenberger2022-oo,
  title     = "A Unified Framework for Implicit Sinkhorn Differentiation",
  booktitle = "{IEEE/CVF} Conference on Computer Vision and Pattern Recognition",
  author    = "Eisenberger, Marvin and Toker, Aysim and Leal-Taix{\'e}, Laura
               and Bernard, Florian and Cremers, Daniel",
  pages     = "509--518",
  year      =  2022
}

@INPROCEEDINGS{Zbontar2021-pf,
  title       = "Barlow Twins: {Self-Supervised} Learning via Redundancy
                 Reduction",
  booktitle   = "Proceedings of the 38th International Conference on Machine
                 Learning",
  author      = "Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann
                 and Deny, Stephane",
  editor      = "Meila, Marina and Zhang, Tong",
  abstract    = "Self-supervised learning (SSL) is rapidly closing the gap with
                 supervised methods on large computer vision benchmarks. A
                 successful approach to SSL is to learn embeddings which are
                 invariant to distortions of the input sample. However, a
                 recurring issue with this approach is the existence of trivial
                 constant solutions. Most current methods avoid such solutions
                 by careful implementation details. We propose an objective
                 function that naturally avoids collapse by measuring the
                 cross-correlation matrix between the outputs of two identical
                 networks fed with distorted versions of a sample, and making
                 it as close to the identity matrix as possible. This causes
                 the embedding vectors of distorted versions of a sample to be
                 similar, while minimizing the redundancy between the
                 components of these vectors. The method is called Barlow
                 Twins, owing to neuroscientist H. Barlow's
                 redundancy-reduction principle applied to a pair of identical
                 networks. Barlow Twins does not require large batches nor
                 asymmetry between the network twins such as a predictor
                 network, gradient stopping, or a moving average on the weight
                 updates. Intriguingly it benefits from very high-dimensional
                 output vectors. Barlow Twins outperforms previous methods on
                 ImageNet for semi-supervised classification in the low-data
                 regime, and is on par with current state of the art for
                 ImageNet classification with a linear classifier head, and for
                 transfer tasks of classification and object detection.",
  publisher   = "PMLR",
  volume      =  139,
  pages       = "12310--12320",
  series      = "Proceedings of Machine Learning Research",
  institution = "Github",
  year        =  2021,
  language    = "en"
}

@ARTICLE{Elhamifar2011-ca,
  title   = "Sparse manifold clustering and embedding",
  author  = "Elhamifar, Ehsan and Vidal, Ren{\'e}",
  journal = "Advances in neural information processing systems",
  volume  =  24,
  year    =  2011
}

@ARTICLE{Li2019-dy,
  title         = "Weakly Convex Optimization over Stiefel Manifold Using
                   Riemannian {Subgradient-Type} Methods",
  author        = "Li, Xiao and Chen, Shixiang and Deng, Zengde and Qu, Qing
                   and Zhu, Zhihui and So, Anthony Man Cho",
  abstract      = "We consider a class of nonsmooth optimization problems over
                   the Stiefel manifold, in which the objective function is
                   weakly convex in the ambient Euclidean space. Such problems
                   are ubiquitous in engineering applications but still largely
                   unexplored. We present a family of Riemannian
                   subgradient-type methods -- namely Riemannain subgradient,
                   incremental subgradient, and stochastic subgradient methods
                   -- to solve these problems and show that they all have an
                   iteration complexity of $\{\cal O\}(\varepsilon^\{-4\})$ for
                   driving a natural stationarity measure below $\varepsilon$.
                   In addition, we establish the local linear convergence of
                   the Riemannian subgradient and incremental subgradient
                   methods when the problem at hand further satisfies a
                   sharpness property and the algorithms are properly
                   initialized and use geometrically diminishing stepsizes. To
                   the best of our knowledge, these are the first convergence
                   guarantees for using Riemannian subgradient-type methods to
                   optimize a class of nonconvex nonsmooth functions over the
                   Stiefel manifold. The fundamental ingredient in the proof of
                   the aforementioned convergence results is a new Riemannian
                   subgradient inequality for restrictions of weakly convex
                   functions on the Stiefel manifold, which could be of
                   independent interest. We also show that our convergence
                   results can be extended to handle a class of compact
                   embedded submanifolds of the Euclidean space. Finally, we
                   discuss the sharpness properties of various formulations of
                   the robust subspace recovery and orthogonal dictionary
                   learning problems and demonstrate the convergence
                   performance of the algorithms on both problems via numerical
                   simulations.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1911.05047"
}

@MISC{Feng_undated-wa,
  title        = "Online Robust {PCA} via Stochastic Optimization",
  author       = "Feng, Jiashi and Xu, Huan and Yan, Shuicheng",
  abstract     = "Robust PCA methods are typically based on batch optimization
                  and have to load all the samples into memory during
                  optimization. This prevents them from efficiently processing
                  big data. In this paper, we develop an Online Robust PCA
                  (OR-PCA) that processes one sample per time instance and
                  hence its memory cost is independent of the number of
                  samples, significantly enhancing the computation and storage
                  efficiency. The proposed OR-PCA is based on stochastic
                  optimization of an equivalent reformulation of the batch
                  RPCA. Indeed, we show that OR-PCA provides a sequence of
                  subspace estimations converging to the optimum of its batch
                  counterpart and hence is provably robust to sparse
                  corruption. Moreover, OR-PCA can naturally be applied for
                  tracking dynamic subspace. Comprehensive simulations on
                  subspace recovering and tracking demonstrate the robustness
                  and efficiency advantages of the OR-PCA over online PCA and
                  batch RPCA methods.",
  howpublished = "\url{https://proceedings.neurips.cc/paper/2013/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf}",
  note         = "Accessed: 2022-7-4"
}

@ARTICLE{Vaswani2017-ax,
  title         = "Robust Subspace Learning: Robust {PCA}, Robust Subspace
                   Tracking, and Robust Subspace Recovery",
  author        = "Vaswani, Namrata and Bouwmans, Thierry and Javed, Sajid and
                   Narayanamurthy, Praneeth",
  abstract      = "PCA is one of the most widely used dimension reduction
                   techniques. A related easier problem is ``subspace
                   learning'' or ``subspace estimation''. Given relatively
                   clean data, both are easily solved via singular value
                   decomposition (SVD). The problem of subspace learning or PCA
                   in the presence of outliers is called robust subspace
                   learning or robust PCA (RPCA). For long data sequences, if
                   one tries to use a single lower dimensional subspace to
                   represent the data, the required subspace dimension may end
                   up being quite large. For such data, a better model is to
                   assume that it lies in a low-dimensional subspace that can
                   change over time, albeit gradually. The problem of tracking
                   such data (and the subspaces) while being robust to outliers
                   is called robust subspace tracking (RST). This article
                   provides a magazine-style overview of the entire field of
                   robust subspace learning and tracking. In particular
                   solutions for three problems are discussed in detail: RPCA
                   via sparse+low-rank matrix decomposition (S+LR), RST via
                   S+LR, and ``robust subspace recovery (RSR)''. RSR assumes
                   that an entire data vector is either an outlier or an
                   inlier. The S+LR formulation instead assumes that outliers
                   occur on only a few data vector indices and hence are well
                   modeled as sparse corruptions.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IT",
  eprint        = "1711.09492"
}

@ARTICLE{Zhu2012-xq,
  title         = "Angles between subspaces and their tangents",
  author        = "Zhu, Peizhen and Knyazev, Andrew V",
  abstract      = "Principal angles between subspaces (PABS) (also called
                   canonical angles) serve as a classical tool in mathematics,
                   statistics, and applications, e.g., data mining.
                   Traditionally, PABS are introduced via their cosines. The
                   cosines and sines of PABS are commonly defined using the
                   singular value decomposition. We utilize the same idea for
                   the tangents, i.e., explicitly construct matrices, such that
                   their singular values are equal to the tangents of PABS,
                   using several approaches: orthonormal and non-orthonormal
                   bases for subspaces, as well as projectors. Such a
                   construction has applications, e.g., in analysis of
                   convergence of subspace iterations for eigenvalue problems.",
  month         =  sep,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "math.NA",
  eprint        = "1209.0523"
}

@ARTICLE{Sahiner2022-kk,
  title         = "Unraveling attention via convex duality: Analysis and
                   interpretations of vision transformers",
  author        = "Sahiner, Arda and Ergen, Tolga and Ozturkler, Batu and
                   Pauly, John and Mardani, Morteza and Pilanci, Mert",
  abstract      = "Vision transformers using self-attention or its proposed
                   alternatives have demonstrated promising results in many
                   image related tasks. However, the underpinning inductive
                   bias of attention is not well understood. To address this
                   issue, this paper analyzes attention through the lens of
                   convex duality. For the non-linear dot-product
                   self-attention, and alternative mechanisms such as MLP-mixer
                   and Fourier Neural Operator (FNO), we derive equivalent
                   finite-dimensional convex problems that are interpretable
                   and solvable to global optimality. The convex programs lead
                   to \{\textbackslashit block nuclear-norm regularization\}
                   that promotes low rank in the latent feature and token
                   dimensions. In particular, we show how self-attention
                   networks implicitly clusters the tokens, based on their
                   latent similarity. We conduct experiments for transferring a
                   pre-trained transformer backbone for CIFAR-100
                   classification by fine-tuning a variety of convex attention
                   heads. The results indicate the merits of the bias induced
                   by attention compared with the existing MLP or linear heads.",
  month         =  may,
  year          =  2022,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2205.08078"
}

@INPROCEEDINGS{Van_Gansbeke2020-eo,
  title     = "{SCAN}: Learning to Classify Images without Labels",
  booktitle = "European conference on computer vision",
  author    = "Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis,
               Stamatios and Proesmans, Marc and Van Gool, Luc",
  abstract  = "Can we automatically group images into semantically meaningful
               clusters when ground-truth annotations are absent? The task of
               unsupervised image classification remains an important, and open
               challenge in computer vision. Several recent approaches have
               tried to tackle this problem in an end-to-end fashion. In this
               paper, we deviate from recent works, and advocate a two-step
               approach where feature learning and clustering are decoupled.
               First, a self-supervised task from representation learning is
               employed to obtain semantically meaningful features. Second, we
               use the obtained features as a prior in a learnable clustering
               approach. In doing so, we remove the ability for cluster
               learning to depend on low-level features, which is present in
               current end-to-end learning approaches. Experimental evaluation
               shows that we outperform state-of-the-art methods by large
               margins, in particular +26.6\% on CIFAR10, +25.0\% on
               CIFAR100-20 and +21.3\% on STL10 in terms of classification
               accuracy. Furthermore, our method is the first to perform well
               on a large-scale dataset for image classification. In
               particular, we obtain promising results on ImageNet, and
               outperform several semi-supervised learning methods in the
               low-data regime without the use of any ground-truth annotations.
               The code is made publicly available at
               https://github.com/wvangansbeke/Unsupervised-Classification.",
  publisher = "Springer",
  year      =  2020
}

@ARTICLE{Amir2021-vg,
  title     = "The Trimmed Lasso: Sparse Recovery Guarantees and Practical
               Optimization by the Generalized {Soft-Min} Penalty",
  author    = "Amir, Tal and Basri, Ronen and Nadler, Boaz",
  abstract  = "We present a new approach to solve the sparse approximation or
               best subset selection problem, namely find a $k$-sparse vector
               $\{x\}\in\mathbb\{R\}^d$ that minimizes the $\ell_2$ residual
               $\lVert A\{x\}-\{y\} \rVert_2$. We consider a regularized
               approach, whereby this residual is penalized by the nonconvex
               $\{\it trimmed lasso\}$, defined as the $\ell_1$-norm of $\{x\}$
               excluding its $k$ largest-magnitude entries. We prove that the
               trimmed lasso has several appealing theoretical properties, and
               in particular derive sparse recovery guarantees assuming
               successful optimization of the penalized objective. Next, we
               show empirically that directly optimizing this objective can be
               quite challenging. Instead, we propose a surrogate for the
               trimmed lasso, called the $\{\it generalized soft-min\}$. This
               penalty smoothly interpolates between the classical lasso and
               the trimmed lasso, while taking into account all possible
               $k$-sparse patterns. The generalized soft-min penalty involves
               summation over $\binom\{d\}\{k\}$ terms, yet we derive a
               polynomial-time algorithm to compute it. This, in turn, yields a
               practical method for the original sparse approximation problem.
               Via simulations, we demonstrate its competitive performance
               compared to current state of the art.",
  journal   = "SIAM Journal on Mathematics of Data Science",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  3,
  number    =  3,
  pages     = "900--929",
  month     =  jan,
  year      =  2021
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{noauthor_1982-nk,
  title     = "Strong convexity of sets and functions",
  abstract  = "A subset C of En is strongly convex if there exists a positive
               constant k such that for all x and y in C, (x+y)/2+$\delta$ϵC
               for all $\delta$ϵEn verifying |$\delta$|≦k|x−y…",
  journal   = "J. Math. Econ.",
  publisher = "Elsevier",
  volume    =  9,
  number    = "1-2",
  pages     = "187--205",
  month     =  jan,
  year      =  1982
}

@ARTICLE{Jin2005-zw,
  title    = "A probabilistic approach for optimizing spectral clustering",
  author   = "Jin, Rong and Kang, Feng and Ding, Chris",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  18,
  year     =  2005
}

@INCOLLECTION{Goncharov2017-qn,
  title     = "Strong and Weak Convexity of Closed Sets in a Hilbert Space",
  booktitle = "Operations Research, Engineering, and Cyber Security: Trends in
               Applied Mathematics and Technology",
  author    = "Goncharov, Vladimir V and Ivanov, Grigorii E",
  editor    = "Daras, Nicholas J and Rassias, Themistocles M",
  abstract  = "We give a brief survey of the geometrical and topological
               properties of two classes of closed sets in a Hilbert space,
               which strengthen and weaken the convexity concept, respectively.
               We prove equivalence of various characterizations of these sets,
               which are partially new while partially known in the literature
               but accompanied with different proofs. Along with the uniform
               notions dating back to Efimov, Stechkin, Vial, Clarke, Stern,
               Wolenski, and others we pay attention to some local and
               pointwise constructions, which can be interpreted through
               positive and negative scalar curvatures. In the final part of
               the paper we give several applications to geometry of Hilbert
               spaces, to set-valued analysis, and to time optimal control
               problem.",
  publisher = "Springer International Publishing",
  pages     = "259--297",
  year      =  2017,
  address   = "Cham"
}

@ARTICLE{Yang2012-ne,
  title         = "Clustering by {Low-Rank} Doubly Stochastic Matrix
                   Decomposition",
  author        = "Yang, Zhirong and Oja, Erkki",
  abstract      = "Clustering analysis by nonnegative low-rank approximations
                   has achieved remarkable progress in the past decade.
                   However, most approximation approaches in this direction are
                   still restricted to matrix factorization. We propose a new
                   low-rank learning method to improve the clustering
                   performance, which is beyond matrix factorization. The
                   approximation is based on a two-step bipartite random walk
                   through virtual cluster nodes, where the approximation is
                   formed by only cluster assigning probabilities. Minimizing
                   the approximation error measured by Kullback-Leibler
                   divergence is equivalent to maximizing the likelihood of a
                   discriminative model, which endows our method with a solid
                   probabilistic interpretation. The optimization is
                   implemented by a relaxed Majorization-Minimization algorithm
                   that is advantageous in finding good local minima.
                   Furthermore, we point out that the regularized algorithm
                   with Dirichlet prior only serves as initialization.
                   Experimental results show that the new method has strong
                   performance in clustering purity for various datasets,
                   especially for large-scale manifold data.",
  month         =  jun,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1206.4676"
}

@ARTICLE{Li2021-gy,
  title         = "Provable Data Clustering via Innovation Search",
  author        = "Li, Weiwei and Rahmani, Mostafa and Li, Ping",
  abstract      = "This paper studies the subspace clustering problem in which
                   data points collected from high-dimensional ambient space
                   lie in a union of linear subspaces. Subspace clustering
                   becomes challenging when the dimension of intersection
                   between subspaces is large and most of the
                   self-representation based methods are sensitive to the
                   intersection between the span of clusters. In sharp contrast
                   to the self-representation based methods, a recently
                   proposed clustering method termed Innovation Pursuit,
                   computed a set of optimal directions (directions of
                   innovation) to build the adjacency matrix. This paper
                   focuses on the Innovation Pursuit Algorithm to shed light on
                   its impressive performance when the subspaces are heavily
                   intersected. It is shown that in contrast to most of the
                   existing methods which require the subspaces to be
                   sufficiently incoherent with each other, Innovation Pursuit
                   only requires the innovative components of the subspaces to
                   be sufficiently incoherent with each other. These new
                   sufficient conditions allow the clusters to be strongly
                   close to each other. Motivated by the presented theoretical
                   analysis, a simple yet effective projection based technique
                   is proposed which we show with both numerical and
                   theoretical results that it can boost the performance of
                   Innovation Pursuit.",
  month         =  aug,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2108.06888"
}

@INPROCEEDINGS{Lu2008-mn,
  title           = "Constrained spectral clustering through affinity
                     propagation",
  booktitle       = "2008 {IEEE} Conference on Computer Vision and Pattern
                     Recognition",
  author          = "Lu, Zhengdong and Carreira-Perpinan, Miguel A",
  abstract        = "Pairwise constraints specify whether or not two samples
                     should be in one cluster. Although it has been successful
                     to incorporate them into traditional clustering methods,
                     such as K-means, little progress has been made in
                     combining them with spectral clustering. The major
                     challenge in designing an effective constrained spectral
                     clustering is a sensible combination of the scarce
                     pairwise constraints with the original affinity matrix. We
                     propose to combine the two sources of affinity by
                     propagating the pairwise constraints information over the
                     original affinity matrix. Our method has a Gaussian
                     process interpretation and results in a closed-form
                     expression for the new affinity matrix. Experiments show
                     it outperforms state-of-the-art constrained clustering
                     methods in getting good clusterings with fewer
                     constraints, and yields good image segmentation with
                     userspecified pairwise constraints.",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2008,
  conference      = "2008 IEEE Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Anchorage, AK, USA"
}

@ARTICLE{Wang2012-jc,
  title         = "On Constrained Spectral Clustering and Its Applications",
  author        = "Wang, Xiang and Qian, Buyue and Davidson, Ian",
  abstract      = "Constrained clustering has been well-studied for algorithms
                   such as $K$-means and hierarchical clustering. However, how
                   to satisfy many constraints in these algorithmic settings
                   has been shown to be intractable. One alternative to encode
                   many constraints is to use spectral clustering, which
                   remains a developing area. In this paper, we propose a
                   flexible framework for constrained spectral clustering. In
                   contrast to some previous efforts that implicitly encode
                   Must-Link and Cannot-Link constraints by modifying the graph
                   Laplacian or constraining the underlying eigenspace, we
                   present a more natural and principled formulation, which
                   explicitly encodes the constraints as part of a constrained
                   optimization problem. Our method offers several practical
                   advantages: it can encode the degree of belief in Must-Link
                   and Cannot-Link constraints; it guarantees to lower-bound
                   how well the given constraints are satisfied using a
                   user-specified threshold; it can be solved deterministically
                   in polynomial time through generalized eigendecomposition.
                   Furthermore, by inheriting the objective function from
                   spectral clustering and encoding the constraints explicitly,
                   much of the existing analysis of unconstrained spectral
                   clustering techniques remains valid for our formulation. We
                   validate the effectiveness of our approach by empirical
                   results on both artificial and real datasets. We also
                   demonstrate an innovative use of encoding large number of
                   constraints: transfer learning via constraints.",
  month         =  jan,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1201.5338"
}

@ARTICLE{Haeffele2020-js,
  title    = "Structured {Low-Rank} Matrix Factorization: Global Optimality,
              Algorithms, and Applications",
  author   = "Haeffele, Benjamin D and Vidal, Rene",
  abstract = "Convex formulations of low-rank matrix factorization problems
              have received considerable attention in machine learning.
              However, such formulations often require solving for a matrix of
              the size of the data matrix, making it challenging to apply them
              to large scale datasets. Moreover, in many applications the data
              can display structures beyond simply being low-rank, e.g., images
              and videos present complex spatio-temporal structures that are
              largely ignored by standard low-rank methods. In this paper we
              study a matrix factorization technique that is suitable for large
              datasets and captures additional structure in the factors by
              using a particular form of regularization that includes
              well-known regularizers such as total variation and the nuclear
              norm as particular cases. Although the resulting optimization
              problem is non-convex, we show that if the size of the factors is
              large enough, under certain conditions, any local minimizer for
              the factors yields a global minimizer. A few practical algorithms
              are also provided to solve the matrix factorization problem, and
              bounds on the distance from a given approximate solution of the
              optimization problem to the global optimum are derived. Examples
              in neural calcium imaging video segmentation and hyperspectral
              compressed recovery show the advantages of our approach on
              high-dimensional datasets.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  42,
  number   =  6,
  pages    = "1468--1482",
  month    =  jun,
  year     =  2020,
  language = "en"
}

@ARTICLE{Papyan2018-iw,
  title    = "Theoretical Foundations of Deep Learning via Sparse
              Representations: A Multilayer Sparse Model and Its Connection to
              Convolutional Neural Networks",
  author   = "Papyan, Vardan and Romano, Yaniv and Sulam, Jeremias and Elad,
              Michael",
  abstract = "Modeling data is the way we-scientists-believe that information
              should be explained and handled. Indeed, models play a central
              role in practically every task in signal and image processing and
              machine learning. Sparse representation theory (we shall refer to
              it as Sparseland) puts forward an emerging, highly effective, and
              universal model. Its core idea is the description of data as a
              linear combination of few atoms taken from a dictionary of such
              fundamental elements.",
  journal  = "IEEE Signal Process. Mag.",
  volume   =  35,
  number   =  4,
  pages    = "72--89",
  month    =  jul,
  year     =  2018,
  keywords = "Data models;Machine learning;Mathematical model;Task
              analysis;Image coding;Convolution;Sparse matrices;Data
              science;Big Data"
}

@MISC{Zhu_undated-br,
  title        = "Residual-based sampling for online outlier-robust {PCA}",
  author       = "Zhu, Tianhao and Shen, Jie",
  abstract     = "Outlier-robust principal component analysis (OR-PCA) has been
                  broadly applied in scientific discovery in the last decades.
                  In this paper, we study online ORPCA, an important variant
                  that addresses the practical challenge that the data points
                  arrive in a sequential manner and the goal is to recover the
                  underlying subspace of the clean data with one pass of the
                  data. Our main contribution is the first provable algorithm
                  that enjoys comparable recovery guarantee to the best known
                  batch algorithm, while significantly improving upon the
                  state-of-the-art online ORPCA algorithms. The core technique
                  is a robust version of the residual norm which, informally
                  speaking, leverages not only the importance of a data point,
                  but also how likely it behaves as an outlier.",
  howpublished = "\url{https://proceedings.mlr.press/v162/zhu22i/zhu22i.pdf}",
  note         = "Accessed: 2022-7-19"
}

@ARTICLE{Lerman2018-ob,
  title     = "Exact Camera Location Recovery by Least Unsquared Deviations",
  author    = "Lerman, Gilad and Shi, Yunpeng and Zhang, Teng",
  abstract  = "We establish exact recovery for the Least Unsquared Deviations
               (LUD) algorithm of {\"O}zye?il and Singer. More precisely, we
               show that for sufficiently many cameras with given corrupted
               pairwise directions, where both camera locations and pairwise
               directions are generated by a special probabilistic model, the
               LUD algorithm exactly recovers the camera locations with high
               probability. A similar exact recovery guarantee for camera
               locations was established for the ShapeFit algorithm by Hand,
               Lee, and Voroninski, but with typically less corruption.",
  journal   = "SIAM J. Imaging Sci.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  11,
  number    =  4,
  pages     = "2692--2721",
  month     =  jan,
  year      =  2018
}

@ARTICLE{Bertrand2020-pt,
  title         = "Implicit differentiation of Lasso-type models for
                   hyperparameter optimization",
  author        = "Bertrand, Quentin and Klopfenstein, Quentin and Blondel,
                   Mathieu and Vaiter, Samuel and Gramfort, Alexandre and
                   Salmon, Joseph",
  abstract      = "Setting regularization parameters for Lasso-type estimators
                   is notoriously difficult, though crucial in practice. The
                   most popular hyperparameter optimization approach is
                   grid-search using held-out validation data. Grid-search
                   however requires to choose a predefined grid for each
                   parameter, which scales exponentially in the number of
                   parameters. Another approach is to cast hyperparameter
                   optimization as a bi-level optimization problem, one can
                   solve by gradient descent. The key challenge for these
                   methods is the estimation of the gradient with respect to
                   the hyperparameters. Computing this gradient via forward or
                   backward automatic differentiation is possible yet usually
                   suffers from high memory consumption. Alternatively implicit
                   differentiation typically involves solving a linear system
                   which can be prohibitive and numerically unstable in high
                   dimension. In addition, implicit differentiation usually
                   assumes smooth loss functions, which is not the case for
                   Lasso-type problems. This work introduces an efficient
                   implicit differentiation algorithm, without matrix
                   inversion, tailored for Lasso-type problems. Our approach
                   scales to high-dimensional data by leveraging the sparsity
                   of the solutions. Experiments demonstrate that the proposed
                   method outperforms a large number of standard methods to
                   optimize the error on held-out data, or the Stein Unbiased
                   Risk Estimator (SURE).",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "2002.08943"
}

@ARTICLE{Mairal2012-jh,
  title    = "Task-driven dictionary learning",
  author   = "Mairal, Julien and Bach, Francis and Ponce, Jean",
  abstract = "Modeling data with linear combinations of a few elements from a
              learned dictionary has been the focus of much recent research in
              machine learning, neuroscience, and signal processing. For
              signals such as natural images that admit such sparse
              representations, it is now well established that these models are
              well suited to restoration tasks. In this context, learning the
              dictionary amounts to solving a large-scale matrix factorization
              problem, which can be done efficiently with classical
              optimization tools. The same approach has also been used for
              learning features from data for other purposes, e.g., image
              classification, but tuning the dictionary in a supervised way for
              these tasks has proven to be more difficult. In this paper, we
              present a general formulation for supervised dictionary learning
              adapted to a wide variety of tasks, and present an efficient
              algorithm for solving the corresponding optimization problem.
              Experiments on handwritten digit classification, digital art
              identification, nonlinear inverse image problems, and compressed
              sensing demonstrate that our approach is effective in large-scale
              settings, and is well suited to supervised and semi-supervised
              classification, as well as regression tasks for data that admit
              sparse representations.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  34,
  number   =  4,
  pages    = "791--804",
  month    =  apr,
  year     =  2012,
  language = "en"
}

@ARTICLE{You2020-ei,
  title         = "Robust recovery via implicit bias of discrepant learning
                   rates for double over-parameterization",
  author        = "You, Chong and Zhu, Zhihui and Qu, Qing and Ma, Yi",
  abstract      = "Recent advances have shown that implicit bias of gradient
                   descent on over-parameterized models enables the recovery of
                   low-rank matrices from linear measurements, even with no
                   prior knowledge on the intrinsic rank. In contrast, for
                   robust low-rank matrix recovery from grossly corrupted
                   measurements, over-parameterization leads to overfitting
                   without prior knowledge on both the intrinsic rank and
                   sparsity of corruption. This paper shows that with a double
                   over-parameterization for both the low-rank matrix and
                   sparse corruption, gradient descent with discrepant learning
                   rates provably recovers the underlying matrix even without
                   prior knowledge on neither rank of the matrix nor sparsity
                   of the corruption. We further extend our approach for the
                   robust recovery of natural images by over-parameterizing
                   images with deep convolutional networks. Experiments show
                   that our method handles different test images and varying
                   corruption levels with a single learning pipeline where the
                   network width and termination conditions do not need to be
                   adjusted on a case-by-case basis. Underlying the success is
                   again the implicit bias with discrepant learning rates on
                   different over-parameterized parameters, which may bear on
                   broader applications.",
  month         =  jun,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  language      = "en",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2006.08857"
}

@MISC{Yang_undated-sh,
  title        = "Clustering by nonnegative Matrix Factorization using graph
                  random walk",
  author       = "Yang, Zhirong and Hao, Tele and Dikmen, Onur and Chen, Xi and
                  Oja, Erkki",
  abstract     = "Nonnegative Matrix Factorization (NMF) is a promising
                  relaxation technique for clustering analysis. However,
                  conventional NMF methods that directly approximate the
                  pairwise similarities using the least square error often
                  yield mediocre performance for data in curved manifolds
                  because they can capture only the immediate similarities
                  between data samples. Here we propose a new NMF clustering
                  method which replaces the approximated matrix with its
                  smoothed version using random walk. Our method can thus
                  accommodate farther relationships between data samples.
                  Furthermore, we introduce a novel regularization in the
                  proposed objective function in order to improve over spectral
                  clustering. The new learning objective is optimized by a
                  multiplicative Majorization-Minimization algorithm with a
                  scalable implementation for learning the factorizing matrix.
                  Extensive experimental results on real-world datasets show
                  that our method has strong performance in terms of cluster
                  purity.",
  howpublished = "\url{https://proceedings.neurips.cc/paper/2012/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf}",
  note         = "Accessed: 2022-7-23"
}

@ARTICLE{Li2022-ll,
  title         = "Implicit Bias of Gradient Descent on Reparametrized Models:
                   On Equivalence to Mirror Descent",
  author        = "Li, Zhiyuan and Wang, Tianhao and Lee, Jasond and Arora,
                   Sanjeev",
  abstract      = "As part of the effort to understand implicit bias of
                   gradient descent in overparametrized models, several results
                   have shown how the training trajectory on the
                   overparametrized model can be understood as mirror descent
                   on a different objective. The main result here is a
                   characterization of this phenomenon under a notion termed
                   commuting parametrization, which encompasses all the
                   previous results in this setting. It is shown that gradient
                   flow with any commuting parametrization is equivalent to
                   continuous mirror descent with a related Legendre function.
                   Conversely, continuous mirror descent with any Legendre
                   function can be viewed as gradient flow with a related
                   commuting parametrization. The latter result relies upon
                   Nash's embedding theorem.",
  month         =  jul,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2207.04036"
}

@ARTICLE{Bolte2021-wa,
  title         = "Nonsmooth implicit differentiation for machine learning and
                   optimization",
  author        = "Bolte, J{\'e}r{\^o}me and Le, Tam and Pauwels, Edouard and
                   Silveti-Falls, Antonio",
  abstract      = "In view of training increasingly complex learning
                   architectures, we establish a nonsmooth implicit function
                   theorem with an operational calculus. Our result applies to
                   most practical problems (i.e., definable problems) provided
                   that a nonsmooth form of the classical invertibility
                   condition is fulfilled. This approach allows for formal
                   subdifferentiation: for instance, replacing derivatives by
                   Clarke Jacobians in the usual differentiation formulas is
                   fully justified for a wide class of nonsmooth problems.
                   Moreover this calculus is entirely compatible with
                   algorithmic differentiation (e.g., backpropagation). We
                   provide several applications such as training deep
                   equilibrium networks, training neural nets with conic
                   optimization layers, or hyperparameter-tuning for nonsmooth
                   Lasso-type models. To show the sharpness of our assumptions,
                   we present numerical experiments showcasing the extremely
                   pathological gradient dynamics one can encounter when
                   applying implicit algorithmic differentiation without any
                   hypothesis.",
  month         =  jun,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.04350"
}

@INPROCEEDINGS{Gregor2010-rm,
  title     = "Learning fast approximations of sparse coding",
  booktitle = "the 27th International Conference on International Conference on
               Machine Learning",
  author    = "Gregor, Karol and LeCun, Yann",
  abstract  = "In Sparse Coding (SC), input vectors are reconstructed using a
               sparse linear combination of basis vectors. SC has become a
               popular method for extracting features from data. For a given
               input, SC minimizes a quadratic reconstruction error with an L1
               penalty term on the code. The process is often too slow for
               applications such as real-time pattern recognition. We proposed
               two versions of a very fast algorithm that produces approximate
               estimates of the sparse code that can be used to compute good
               visual features, or to initialize exact iterative algorithms.
               The main idea is to train a non-linear, feed-forward predictor
               with a specific architecture and a fixed depth to produce the
               best possible approximation of the sparse code. A version of the
               method, which can be seen as a trainable version of Li and
               Osher's coordinate descent method, is shown to produce
               approximate solutions with 10 times less computation than Li and
               Os-her's for the same approximation error. Unlike previous
               proposals for sparse code predictors, the system allows a kind
               of approximate ``explaining away'' to take place during
               inference. The resulting predictor is differentiable and can be
               included into globally-trained recognition systems.",
  publisher = "Omnipress",
  pages     = "399--406",
  series    = "ICML'10",
  month     =  jun,
  year      =  2010,
  address   = "Madison, WI, USA",
  location  = "Haifa, Israel"
}

@ARTICLE{Amos2017-sh,
  title         = "{OptNet}: Differentiable Optimization as a Layer in Neural
                   Networks",
  author        = "Amos, Brandon and Kolter, J Zico",
  abstract      = "This paper presents OptNet, a network architecture that
                   integrates optimization problems (here, specifically in the
                   form of quadratic programs) as individual layers in larger
                   end-to-end trainable deep networks. These layers encode
                   constraints and complex dependencies between the hidden
                   states that traditional convolutional and fully-connected
                   layers often cannot capture. We explore the foundations for
                   such an architecture: we show how techniques from
                   sensitivity analysis, bilevel optimization, and implicit
                   differentiation can be used to exactly differentiate through
                   these layers and with respect to layer parameters; we
                   develop a highly efficient solver for these layers that
                   exploits fast GPU-based batch solves within a primal-dual
                   interior point method, and which provides backpropagation
                   gradients with virtually no additional cost on top of the
                   solve; and we highlight the application of these approaches
                   in several problems. In one notable example, the method is
                   learns to play mini-Sudoku (4x4) given just input and output
                   games, with no a-priori information about the rules of the
                   game; this highlights the ability of OptNet to learn hard
                   constraints better than other neural architectures.",
  month         =  mar,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1703.00443"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Thaker_undated-pg,
  title        = "Reverse Engineering ℓ p attacks: A block-sparse optimization
                  approach with recovery guarantees",
  author       = "Thaker, Darshan and Giampouras, Paris and Vidal, Ren{\'e}",
  abstract     = "Deep neural network-based classifiers have been shown to be
                  vulnerable to imperceptible perturbations to their input,
                  such as ℓ p-bounded norm adversarial attacks. This has
                  motivated the development of many defense methods, which are
                  then broken by new attacks, and so on. This paper focuses on
                  a different but related problem of reverse engineering
                  adversarial attacks. Specifically, given an attacked signal,
                  we study conditions under which one can determine the type of
                  attack (ℓ 1 , ℓ 2 or ℓ $\infty$) and recover the clean
                  signal. We pose this problem as a block-sparse recovery
                  problem, where both the signal and the attack are assumed to
                  lie in a union of subspaces that includes one subspace per
                  class and one subspace per attack type. We derive geometric
                  conditions on the subspaces under which any attacked signal
                  can be decomposed as the sum of a clean signal plus an
                  attack. In addition, by determining the subspaces that
                  contain the signal and the attack, we can also classify the
                  signal and determine the attack type. Experiments on digit
                  and face classification demonstrate the effectiveness of the
                  proposed approach.",
  howpublished = "\url{https://proceedings.mlr.press/v162/thaker22a/thaker22a.pdf}",
  note         = "Accessed: 2022-7-26"
}

@ARTICLE{Gould2019-ez,
  title         = "Deep Declarative Networks: A New Hope",
  author        = "Gould, Stephen and Hartley, Richard and Campbell, Dylan",
  abstract      = "We explore a new class of end-to-end learnable models
                   wherein data processing nodes (or network layers) are
                   defined in terms of desired behavior rather than an explicit
                   forward function. Specifically, the forward function is
                   implicitly defined as the solution to a mathematical
                   optimization problem. Consistent with nomenclature in the
                   programming languages community, we name these models deep
                   declarative networks. Importantly, we show that the class of
                   deep declarative networks subsumes current deep learning
                   models. Moreover, invoking the implicit function theorem, we
                   show how gradients can be back-propagated through many
                   declaratively defined data processing nodes thereby enabling
                   end-to-end learning. We show how these declarative
                   processing nodes can be implemented in the popular PyTorch
                   deep learning software library allowing declarative and
                   imperative nodes to co-exist within the same network. We
                   also provide numerous insights and illustrative examples of
                   declarative nodes and demonstrate their application for
                   image and point cloud classification tasks.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1909.04866"
}

@ARTICLE{Combettes2018-ap,
  title         = "Deep Neural Network Structures Solving Variational
                   Inequalities",
  author        = "Combettes, Patrick L and Pesquet, Jean-Christophe",
  abstract      = "Motivated by structures that appear in deep neural networks,
                   we investigate nonlinear composite models alternating
                   proximity and affine operators defined on different spaces.
                   We first show that a wide range of activation operators used
                   in neural networks are actually proximity operators. We then
                   establish conditions for the averagedness of the proposed
                   composite constructs and investigate their asymptotic
                   properties. It is shown that the limit of the resulting
                   process solves a variational inequality which, in general,
                   does not derive from a minimization problem.",
  month         =  aug,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1808.07526"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wahba1965-lp,
  title     = "A Least Squares Estimate of Satellite Attitude",
  author    = "Wahba, Grace",
  abstract  = "Relaxed conditions for uniform complete observability and
               controllability of LTV systems with bounded realizations 1 1This
               work was supported by the Funda{\c c}{\~a}o para a Ci{\^e}ncia
               ea …",
  journal   = "SIAM Rev.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  7,
  number    =  3,
  pages     = "409--409",
  month     =  jul,
  year      =  1965
}

@ARTICLE{Han2022-hc,
  title         = "Geometric Graph Representation Learning via Maximizing Rate
                   Reduction",
  author        = "Han, Xiaotian and Jiang, Zhimeng and Liu, Ninghao and Song,
                   Qingquan and Li, Jundong and Hu, Xia",
  abstract      = "Learning discriminative node representations benefits
                   various downstream tasks in graph analysis such as community
                   detection and node classification. Existing graph
                   representation learning methods (e.g., based on random walk
                   and contrastive learning) are limited to maximizing the
                   local similarity of connected nodes. Such pair-wise learning
                   schemes could fail to capture the global distribution of
                   representations, since it has no explicit constraints on the
                   global geometric properties of representation space. To this
                   end, we propose Geometric Graph Representation Learning
                   (G2R) to learn node representations in an unsupervised
                   manner via maximizing rate reduction. In this way, G2R maps
                   nodes in distinct groups (implicitly stored in the adjacency
                   matrix) into different subspaces, while each subspace is
                   compact and different subspaces are dispersedly distributed.
                   G2R adopts a graph neural network as the encoder and
                   maximizes the rate reduction with the adjacency matrix.
                   Furthermore, we theoretically and empirically demonstrate
                   that rate reduction maximization is equivalent to maximizing
                   the principal angles between different subspaces.
                   Experiments on real-world datasets show that G2R outperforms
                   various baselines on node classification and community
                   detection tasks.",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2202.06241"
}

@ARTICLE{Xiao2022-cw,
  title    = "A Geometric Proximal Gradient Method for Sparse Least Squares
              Regression with Probabilistic Simplex Constraint",
  author   = "Xiao, Guiyun and Bai, Zheng-Jian",
  abstract = "In this paper, we consider the sparse least squares regression
              problem with probabilistic simplex constraint. Due to the
              probabilistic simplex constraint, one could not apply directly
              the $$\textbackslashell _1$$-regularization to the considered
              regression model. To find a sparse solution, we reformulate the
              sparse least squares regression problem as a nonconvex and
              nonsmooth $$\textbackslashell _1$$-regularized minimization
              problem over the unit sphere. Then we propose a geometric
              proximal gradient method for solving the regularized problem with
              a varied regularized parameter, where the explicit expression of
              the global solution to every involved subproblem is obtained. The
              global convergence of the proposed method is established under
              some mild assumptions. Some numerical results are reported to
              illustrate the effectiveness of the proposed algorithm.",
  journal  = "J. Sci. Comput.",
  volume   =  92,
  number   =  1,
  pages    = "22",
  month    =  jun,
  year     =  2022
}

@ARTICLE{Van_den_Berg2011-vx,
  title     = "Sparse Optimization with {Least-Squares} Constraints",
  author    = "van den Berg, Ewout and Friedlander, Michael P",
  abstract  = "The use of convex optimization for the recovery of sparse
               signals from incomplete or compressed data is now common
               practice. Motivated by the success of basis pursuit in
               recovering sparse vectors, new formulations have been proposed
               that take advantage of different types of sparsity. In this
               paper we propose an efficient algorithm for solving a general
               class of sparsifying formulations. For several common types of
               sparsity we provide applications, along with details on how to
               apply the algorithm, and experimental results.",
  journal   = "SIAM J. Optim.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  21,
  number    =  4,
  pages     = "1201--1229",
  month     =  oct,
  year      =  2011
}

@ARTICLE{Jaggi_undated-cc,
  title   = "Revisiting {Frank-Wolfe}: {Projection-Free} Sparse Convex
             Optimization",
  author  = "{Jaggi}",
  journal = "W\&CP"
}

@INPROCEEDINGS{Douik2018-zt,
  title     = "A Riemannian Approach for {Graph-Based} Clustering by Doubly
               Stochastic Matrices",
  booktitle = "2018 {IEEE} Statistical Signal Processing Workshop ({SSP})",
  author    = "Douik, Ahmed and Hassibi, Babak",
  abstract  = "Convex optimization is a well-established area with applications
               in almost all fields. However, these convex methods can be
               rather slow and computationally intensive for high dimensional
               problems. For a particular class of problems, this paper
               considers a different approach, namely Riemannian optimization.
               The main idea is to view the constrained optimization problem as
               an unconstrained one over a restricted search space (the
               manifold). Riemannian optimization explicitly exploits the
               geometry of the problem and often reduces its dimension, thereby
               potentially allowing significant speedup as compared to
               conventional approaches. The paper introduces the doubly
               stochastic, the symmetric, and the definite multinomial
               manifolds which generalize the simplex. The method is applied to
               a convex and a non-convex graph-based clustering problem.
               Theoretical analysis and simulation results demonstrate the
               efficiency of the proposed method over the state of the art as
               it outperforms conventional generic and specialized solvers,
               especially in high dimensions.",
  pages     = "806--810",
  month     =  jun,
  year      =  2018,
  keywords  = "Manifolds;Optimization;Symmetric matrices;Geometry;Search
               problems;Signal processing algorithms;Measurement;Riemannian
               optimization;manifold geometry;doubly stochastic
               matrices;symmetric matrices;convex optimization"
}

@ARTICLE{Ames2012-ih,
  title         = "Guaranteed clustering and biclustering via semidefinite
                   programming",
  author        = "Ames, Brendan P W",
  abstract      = "Identifying clusters of similar objects in data plays a
                   significant role in a wide range of applications. As a model
                   problem for clustering, we consider the densest
                   k-disjoint-clique problem, whose goal is to identify the
                   collection of k disjoint cliques of a given weighted
                   complete graph maximizing the sum of the densities of the
                   complete subgraphs induced by these cliques. In this paper,
                   we establish conditions ensuring exact recovery of the
                   densest k cliques of a given graph from the optimal solution
                   of a particular semidefinite program. In particular, the
                   semidefinite relaxation is exact for input graphs
                   corresponding to data consisting of k large, distinct
                   clusters and a smaller number of outliers. This approach
                   also yields a semidefinite relaxation for the biclustering
                   problem with similar recovery guarantees. Given a set of
                   objects and a set of features exhibited by these objects,
                   biclustering seeks to simultaneously group the objects and
                   features according to their expression levels. This problem
                   may be posed as partitioning the nodes of a weighted
                   bipartite complete graph such that the sum of the densities
                   of the resulting bipartite complete subgraphs is maximized.
                   As in our analysis of the densest k-disjoint-clique problem,
                   we show that the correct partition of the objects and
                   features can be recovered from the optimal solution of a
                   semidefinite program in the case that the given data
                   consists of several disjoint sets of objects exhibiting
                   similar features. Empirical evidence from numerical
                   experiments supporting these theoretical guarantees is also
                   provided.",
  month         =  feb,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1202.3663"
}

@ARTICLE{Roy2021-ik,
  title     = "Efficient content-based sparse attention with routing
               transformers",
  author    = "Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and
               Grangier, David",
  abstract  = "Self-attention has recently been adopted for a wide range of
               sequence modeling problems. Despite its effectiveness,
               self-attention suffers from quadratic computation and memory
               requirements with respect to sequence length. Successful
               approaches to reduce this complexity focused on attending to
               local sliding windows or a small set of locations independent of
               content. Our work proposes to learn dynamic sparse attention
               patterns that avoid allocating computation and memory to attend
               to content unrelated to the query of interest. This work builds
               upon two lines of research: It combines the modeling flexibility
               of prior work on content-based sparse attention with the
               efficiency gains from approaches based on local, temporal sparse
               attention. Our model, the Routing Transformer, endows
               self-attention with a sparse routing module based on online
               k-means while reducing the overall complexity of attention to O(
               n1.5 d) from O( n2 d) for sequence length n and hidden dimension
               d. We show that our model outperforms comparable sparse
               attention models on language modeling on Wikitext-103 (15.8 vs
               18.3 perplexity), as well as on image generation on ImageNet-64
               (3.43 vs 3.44 bits/dim) while using fewer self-attention layers.
               Additionally, we set a new state-of-the-art on the newly
               released PG-19 data-set, obtaining a test perplexity of 33.2
               with a 22 layer Routing Transformer model trained on sequences
               of length 8192. We open-source the code for Routing Transformer
               in Tensorflow. 1",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press - Journals",
  volume    =  9,
  pages     = "53--68",
  month     =  feb,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Chandrasekaran2012-by,
  title    = "The Convex Geometry of Linear Inverse Problems",
  author   = "Chandrasekaran, Venkat and Recht, Benjamin and Parrilo, Pablo A
              and Willsky, Alan S",
  abstract = "In applications throughout science and engineering one is often
              faced with the challenge of solving an ill-posed inverse problem,
              where the number of available measurements is smaller than the
              dimension of the model to be estimated. However in many practical
              situations of interest, models are constrained structurally so
              that they only have a few degrees of freedom relative to their
              ambient dimension. This paper provides a general framework to
              convert notions of simplicity into convex penalty functions,
              resulting in convex optimization solutions to linear,
              underdetermined inverse problems. The class of simple models
              considered includes those formed as the sum of a few atoms from
              some (possibly infinite) elementary atomic set; examples include
              well-studied cases from many technical fields such as sparse
              vectors (signal processing, statistics) and low-rank matrices
              (control, statistics), as well as several others including sums
              of a few permutation matrices (ranked elections, multiobject
              tracking), low-rank tensors (computer vision, neuroscience),
              orthogonal matrices (machine learning), and atomic measures
              (system identification). The convex programming formulation is
              based on minimizing the norm induced by the convex hull of the
              atomic set; this norm is referred to as the atomic norm. The
              facial structure of the atomic norm ball carries a number of
              favorable properties that are useful for recovering simple
              models, and an analysis of the underlying convex geometry
              provides sharp estimates of the number of generic measurements
              required for exact and robust recovery of models from partial
              information. These estimates are based on computing the Gaussian
              widths of tangent cones to the atomic norm ball. When the atomic
              set has algebraic structure the resulting optimization problems
              can be solved or approximated via semidefinite programming. The
              quality of these approximations affects the number of
              measurements required for recovery, and this tradeoff is
              characterized via some examples. Thus this work extends the
              catalog of simple models (beyond sparse vectors and low-rank
              matrices) that can be recovered from limited linear information
              via tractable convex programming.",
  journal  = "Found. Comut. Math.",
  volume   =  12,
  number   =  6,
  pages    = "805--849",
  month    =  dec,
  year     =  2012
}

@INPROCEEDINGS{Berthet_Quentin_and_Blondel_Mathieu_and_Teboul_Olivier_and_Cuturi_Marco_and_Vert_Jean-Philippe_and_Bach_Francis_undated-eb,
  title      = "Learning with Differentiable Perturbed Optimizers",
  booktitle  = "34th Conference on Neural Information Processing Systems",
  author     = "{Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and
                Cuturi, Marco and Vert, Jean-Philippe and Bach, Francis}",
  conference = "34th Conference on Neural Information Processing Systems"
}

@ARTICLE{Friedlander2014-eg,
  title     = "Gauge Optimization and Duality",
  author    = "Friedlander, Michael P and Mac{\^e}do, Ives and Pong, Ting Kei",
  abstract  = "Gauge functions significantly generalize the notion of a norm,
               and gauge optimization, as defined by [R. M. Freund, Math.
               Programming, 38 (1987), pp. 47--67], seeks the element of a
               convex set that is minimal with respect to a gauge function.
               This conceptually simple problem can be used to model a
               remarkable array of useful problems, including a special case of
               conic optimization, and related problems that arise in machine
               learning and signal processing. The gauge structure of these
               problems allows for a special kind of duality framework. This
               paper explores the duality framework proposed by Freund, and
               proposes a particular form of the problem that exposes some
               useful properties of the gauge optimization framework (such as
               the variational properties of its value function), and yet
               maintains most of the generality of the abstract form of gauge
               optimization.",
  journal   = "SIAM J. Optim.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  24,
  number    =  4,
  pages     = "1999--2022",
  month     =  jan,
  year      =  2014
}

@ARTICLE{Glunt_undated-rk,
  title  = "The Nearest \textbackslashDoubly Stochastic`` Matrix to a Real
            Matrix with the same First Moment",
  author = "Glunt, William and Hayden, Thomas L and Reams, Robert"
}

@INPROCEEDINGS{Ding2022-zd,
  title     = "Understanding Doubly Stochastic Clustering",
  booktitle = "the 39th International Conference on Machine Learning",
  author    = "Ding, Tianjiao and Lim, Derek and Vidal, Rene and Haeffele,
               Benjamin D",
  abstract  = "The problem of projecting a matrix onto the space of doubly
               stochastic matrices finds several applications in machine
               learning. For example, in spectral clustering, it has been shown
               that forming the normalized Laplacian matrix from a data
               affinity matrix has close connections to projecting it onto the
               set of doubly stochastic matrices. However, the analysis of why
               this projection improves clustering has been limited. In this
               paper we present theoretical conditions on the given affinity
               matrix under which its doubly stochastic projection is an ideal
               affinity matrix (i.e., it has no false connections between
               clusters, and is well-connected within each cluster). In
               particular, we show that a necessary and sufficient condition
               for a projected affinity matrix to be ideal reduces to a set of
               conditions on the input affinity that decompose along each
               cluster. Further, in the subspace clustering problem, where each
               cluster is defined by a linear subspace, we provide geometric
               conditions on the underlying subspaces which guarantee correct
               clustering via a continuous version of the problem. This allows
               us to explain theoretically the remarkable performance of a
               recently proposed doubly stochastic subspace clustering method.",
  publisher = "PMLR",
  volume    =  162,
  pages     = "5153--5165",
  series    = "Proceedings of Machine Learning Research",
  year      =  2022
}

@ARTICLE{Iguchi2015-ei,
  title         = "On the tightness of an {SDP} relaxation of k-means",
  author        = "Iguchi, Takayuki and Mixon, Dustin G and Peterson, Jesse and
                   Villar, Soledad",
  abstract      = "Recently, Awasthi et al. introduced an SDP relaxation of the
                   $k$-means problem in $\mathbb R^m$. In this work, we
                   consider a random model for the data points in which $k$
                   balls of unit radius are deterministically distributed
                   throughout $\mathbb R^m$, and then in each ball, $n$ points
                   are drawn according to a common rotationally invariant
                   probability distribution. For any fixed ball configuration
                   and probability distribution, we prove that the SDP
                   relaxation of the $k$-means problem exactly recovers these
                   planted clusters with probability $1-e^\{-\Omega(n)\}$
                   provided the distance between any two of the ball centers is
                   $>2+\epsilon$, where $\epsilon$ is an explicit function of
                   the configuration of the ball centers, and can be
                   arbitrarily small when $m$ is large.",
  month         =  may,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IT",
  eprint        = "1505.04778"
}

@ARTICLE{Likhosherstov2021-wv,
  title         = "On the Expressive Power of {Self-Attention} Matrices",
  author        = "Likhosherstov, Valerii and Choromanski, Krzysztof and
                   Weller, Adrian",
  abstract      = "Transformer networks are able to capture patterns in data
                   coming from many domains (text, images, videos, proteins,
                   etc.) with little or no change to architecture components.
                   We perform a theoretical analysis of the core component
                   responsible for signal propagation between elements, i.e.
                   the self-attention matrix. In practice, this matrix
                   typically exhibits two properties: (1) it is sparse, meaning
                   that each token only attends to a small subset of other
                   tokens; and (2) it changes dynamically depending on the
                   input to the module. With these considerations in mind, we
                   ask the following question: Can a fixed self-attention
                   module approximate arbitrary sparse patterns depending on
                   the input? How small is the hidden size $d$ required for
                   such approximation? We make progress in answering this
                   question and show that the self-attention matrix can
                   provably approximate sparse matrices, where sparsity is in
                   terms of a bounded number of nonzero elements in each row
                   and column. While the parameters of self-attention are
                   fixed, various sparse matrices can be approximated by only
                   modifying the inputs. Our proof is based on the random
                   projection technique and uses the seminal
                   Johnson-Lindenstrauss lemma. Our proof is constructive,
                   enabling us to propose an algorithm for finding adaptive
                   inputs and fixed self-attention parameters in order to
                   approximate a given matrix. In particular, we show that, in
                   order to approximate any sparse matrix up to a given
                   precision defined in terms of preserving matrix element
                   ratios, $d$ grows only logarithmically with the sequence
                   length $L$ (i.e. $d = O(\log L)$).",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.03764"
}

@MISC{Peng_undated-de,
  title       = "{ARCS}: {ARCS}: Accurate Rotation and Correspondence Search",
  author      = "Peng, Liangzu",
  abstract    = "ARCS: Accurate Rotation and Correspondence Search. Contribute
                 to liangzu/ARCS development by creating an account on GitHub.",
  institution = "Github",
  language    = "en"
}

@ARTICLE{Liu1989-gx,
  title    = "On the limited memory {BFGS} method for large scale optimization",
  author   = "Liu, Dong C and Nocedal, Jorge",
  abstract = "We study the numerical performance of a limited memory
              quasi-Newton method for large scale optimization, which we call
              the L-BFGS method. We compare its performance with that of the
              method developed by Buckley and LeNir (1985), which combines
              cycles of BFGS steps and conjugate direction steps. Our numerical
              tests indicate that the L-BFGS method is faster than the method
              of Buckley and LeNir, and is better able to use additional
              storage to accelerate convergence. We show that the L-BFGS method
              can be greatly accelerated by means of a simple scaling. We then
              compare the L-BFGS method with the partitioned quasi-Newton
              method of Griewank and Toint (1982a). The results show that, for
              some problems, the partitioned quasi-Newton method is clearly
              superior to the L-BFGS method. However we find that for other
              problems the L-BFGS method is very competitive due to its low
              iteration cost. We also study the convergence properties of the
              L-BFGS method, and prove global convergence on uniformly convex
              problems.",
  journal  = "Math. Program.",
  volume   =  45,
  number   =  1,
  pages    = "503--528",
  month    =  aug,
  year     =  1989
}

@ARTICLE{Yang2022-rg,
  title    = "Certifiably Optimal {Outlier-Robust} Geometric Perception:
              Semidefinite Relaxations and Scalable Global Optimization",
  author   = "Yang, Heng and Carlone, Luca",
  abstract = "We propose the first general and scalable framework to design
              certifiable algorithms for robust geometric perception in the
              presence of outliers. Our first contribution is to show that
              estimation using common robust costs, such as truncated least
              squares (TLS),maximum consensus, Geman-McClure, Tukey's biweight,
              among others, can be reformulated as polynomial optimization
              problems(POPs). By focusing on the TLS cost, our second
              contribution is to exploit sparsity in the POP and propose a
              sparse semidefinite programming (SDP) relaxation that is much
              smaller than the standard Lasserre's hierarchy while preserving
              empirical exactness, i.e., the SDP recovers the optimizer of the
              nonconvex POP with an optimality certificate. Our third
              contribution is to solve the SDP relaxations at an unprecedented
              scale and accuracy by presenting STRIDE, a solver that blends
              global descent on the convex SDP with fast local search on the
              nonconvex POP. Our fourth contribution is an evaluation of the
              proposed framework on six geometric perception problems including
              single and multiple rotation averaging, point cloud and mesh
              registration, absolute pose estimation, and category-level object
              pose and shape estimation. Our experiments demonstrate that (i)
              our sparse SDP relaxation is empirically exact with up to
              60\%-90\% outliers across applications; (ii) while still being
              far from real-time, STRIDE is up to 100 times faster than
              existing SDP solvers on mediumscale problems, and is the only
              solver that can solve large-scale SDPs with hundreds of thousands
              of constraints to high accuracy; (iii) STRIDE safeguards existing
              fast heuristics for robust estimation (e.g., RANSAC or Graduated
              Non-Convexity), i.e., it certifies global optimality if the
              heuristic estimates are optimal, or detects and allows escaping
              local optima when the heuristic estimates are suboptimal.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   = "PP",
  month    =  may,
  year     =  2022,
  language = "en"
}

@ARTICLE{Peters2019-qe,
  title         = "Sparse {Sequence-to-Sequence} Models",
  author        = "Peters, Ben and Niculae, Vlad and Martins, Andr{\'e} F T",
  abstract      = "Sequence-to-sequence models are a powerful workhorse of NLP.
                   Most variants employ a softmax transformation in both their
                   attention mechanism and output layer, leading to dense
                   alignments and strictly positive output probabilities. This
                   density is wasteful, making models less interpretable and
                   assigning probability mass to many implausible outputs. In
                   this paper, we propose sparse sequence-to-sequence models,
                   rooted in a new family of $\alpha$-entmax transformations,
                   which includes softmax and sparsemax as particular cases,
                   and is sparse for any $\alpha > 1$. We provide fast
                   algorithms to evaluate these transformations and their
                   gradients, which scale well for large vocabulary sizes. Our
                   models are able to produce sparse alignments and to assign
                   nonzero probability to a short list of plausible outputs,
                   sometimes rendering beam search exact. Experiments on
                   morphological inflection and machine translation reveal
                   consistent gains over dense models.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.05702"
}

@ARTICLE{Martins2016-ja,
  title         = "From Softmax to Sparsemax: A Sparse Model of Attention and
                   {Multi-Label} Classification",
  author        = "Martins, Andr{\'e} F T and Astudillo, Ram{\'o}n Fernandez",
  abstract      = "We propose sparsemax, a new activation function similar to
                   the traditional softmax, but able to output sparse
                   probabilities. After deriving its properties, we show how
                   its Jacobian can be efficiently computed, enabling its use
                   in a network trained with backpropagation. Then, we propose
                   a new smooth and convex loss function which is the sparsemax
                   analogue of the logistic loss. We reveal an unexpected
                   connection between this new loss and the Huber
                   classification loss. We obtain promising empirical results
                   in multi-label classification problems and in
                   attention-based neural networks for natural language
                   inference. For the latter, we achieve a similar performance
                   as the traditional softmax, but with a selective, more
                   compact, attention focus.",
  month         =  feb,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1602.02068"
}

@ARTICLE{Correia2019-cw,
  title         = "Adaptively Sparse Transformers",
  author        = "Correia, Gon{\c c}alo M and Niculae, Vlad and Martins,
                   Andr{\'e} F T",
  abstract      = "Attention mechanisms have become ubiquitous in NLP. Recent
                   architectures, notably the Transformer, learn powerful
                   context-aware word representations through layered,
                   multi-headed attention. The multiple heads learn diverse
                   types of word relationships. However, with standard softmax
                   attention, all attention heads are dense, assigning a
                   non-zero weight to all context words. In this work, we
                   introduce the adaptively sparse Transformer, wherein
                   attention heads have flexible, context-dependent sparsity
                   patterns. This sparsity is accomplished by replacing softmax
                   with $\alpha$-entmax: a differentiable generalization of
                   softmax that allows low-scoring words to receive precisely
                   zero weight. Moreover, we derive a method to automatically
                   learn the $\alpha$ parameter -- which controls the shape and
                   sparsity of $\alpha$-entmax -- allowing attention heads to
                   choose between focused or spread-out behavior. Our
                   adaptively sparse Transformer improves interpretability and
                   head diversity when compared to softmax Transformers on
                   machine translation datasets. Findings of the quantitative
                   and qualitative analysis of our approach include that heads
                   in different layers learn different sparsity preferences and
                   tend to be more diverse in their attention distributions
                   than softmax Transformers. Furthermore, at no cost in
                   accuracy, sparsity in attention heads helps to uncover
                   different head specializations.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.00015"
}

@ARTICLE{Song2019-dp,
  title         = "Generative Modeling by Estimating Gradients of the Data
                   Distribution",
  author        = "Song, Yang and Ermon, Stefano",
  abstract      = "We introduce a new generative model where samples are
                   produced via Langevin dynamics using gradients of the data
                   distribution estimated with score matching. Because
                   gradients can be ill-defined and hard to estimate when the
                   data resides on low-dimensional manifolds, we perturb the
                   data with different levels of Gaussian noise, and jointly
                   estimate the corresponding scores, i.e., the vector fields
                   of gradients of the perturbed data distribution for all
                   noise levels. For sampling, we propose an annealed Langevin
                   dynamics where we use gradients corresponding to gradually
                   decreasing noise levels as the sampling process gets closer
                   to the data manifold. Our framework allows flexible model
                   architectures, requires no sampling during training or the
                   use of adversarial methods, and provides a learning
                   objective that can be used for principled model comparisons.
                   Our models produce samples comparable to GANs on MNIST,
                   CelebA and CIFAR-10 datasets, achieving a new
                   state-of-the-art inception score of 8.87 on CIFAR-10.
                   Additionally, we demonstrate that our models learn effective
                   representations via image inpainting experiments.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1907.05600"
}

@INPROCEEDINGS{Sander2021-tl,
  title     = "Sinkformers: Transformers with doubly stochastic attention",
  booktitle = "International Conference on Artificial Intelligence and
               Statistics",
  author    = "Sander, Michael E and Ablin, Pierre and Blondel, Mathieu and
               Peyr{\'e}, Gabriel",
  abstract  = "Attention based models such as Transformers involve pairwise
               interactions between data points, modeled with a learnable
               attention matrix. Importantly, this attention matrix is
               normalized with the SoftMax operator, which makes it row-wise
               stochastic. In this paper, we propose instead to use Sinkhorn's
               algorithm to make attention matrices doubly stochastic. We call
               the resulting model a Sinkformer. We show that the row-wise
               stochastic attention matrices in classical Transformers get
               close to doubly stochastic matrices as the number of epochs
               increases, justifying the use of Sinkhorn normalization as an
               informative prior. On the theoretical side, we show that, unlike
               the SoftMax operation, this normalization makes it possible to
               understand the iterations of self-attention modules as a
               discretized gradient-flow for the Wasserstein metric. We also
               show in the infinite number of samples limit that, when
               rescaling both attention matrices and depth, Sinkformers operate
               a heat diffusion. On the experimental side, we show that
               Sinkformers enhance model accuracy in vision and natural
               language processing tasks. In particular, on 3D shapes
               classification, Sinkformers lead to a significant improvement.",
  month     =  oct,
  year      =  2021,
  copyright = "http://creativecommons.org/licenses/by/4.0/"
}

@MISC{Jiang_undated-bs,
  title        = "Assessing generalization via disagreement",
  author       = "Jiang, Yiding and Nagarajan, Vaishnavh and Baek, Christina
                  and Zico Kolter, J",
  abstract     = "We empirically show that the test error of deep networks can
                  be estimated by training the same architecture on the same
                  training set but with two different runs of Stochastic
                  Gradient Descent (SGD), and then measuring the disagreement
                  rate between the two networks on unlabeled test data. This
                  builds on-and is a stronger version of-the observation in
                  Nakkiran \& Bansal (2020), which requires the runs to be on
                  separate training sets. We further theoretically show that
                  this peculiar phenomenon arises from the well-calibrated
                  nature of ensembles of SGD-trained models. This finding not
                  only provides a simple empirical measure to directly predict
                  the test error using unlabeled test data, but also
                  establishes a new conceptual connection between
                  generalization and calibration.",
  howpublished = "\url{https://openreview.net/pdf?id=WvOGCEAQhxl}",
  note         = "Accessed: 2022-10-19"
}

@MISC{Cohen_undated-kx,
  title  = "{GRADIENT} {DESCENT} {ON} {NEURAL} {NETWORKS} {TYPI-} {CALLY}
            {OCCURS} {AT} {THE} {EDGE} {OF} {STABILITY}",
  author = "Cohen, Jeremy and Kaur, Simran and Li, Yuanzhi and Zico Kolter, J
            and Talwalkar, Ameet"
}

@MISC{Malitsky_undated-rz,
  title  = "Adaptive Gradient Descent without Descent",
  author = "Malitsky, Yura and Mishchenko, Konstantin"
}

@ARTICLE{Tan2023-hh,
  title         = "Contrastive Learning Is Spectral Clustering On Similarity
                   Graph",
  author        = "Tan, Zhiquan and Zhang, Yifan and Yang, Jingqin and Yuan,
                   Yang",
  abstract      = "Contrastive learning is a powerful self-supervised learning
                   method, but we have a limited theoretical understanding of
                   how it works and why it works. In this paper, we prove that
                   contrastive learning with the standard InfoNCE loss is
                   equivalent to spectral clustering on the similarity graph.
                   Using this equivalence as the building block, we extend our
                   analysis to the CLIP model and rigorously characterize how
                   similar multi-modal objects are embedded together. Motivated
                   by our theoretical insights, we introduce the kernel mixture
                   loss, incorporating novel kernel functions that outperform
                   the standard Gaussian kernel on several vision datasets.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2303.15103"
}

@ARTICLE{Tay2020-rw,
  title         = "Sparse Sinkhorn Attention",
  author        = "Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald
                   and Juan, Da-Cheng",
  abstract      = "We propose Sparse Sinkhorn Attention, a new efficient and
                   sparse method for learning to attend. Our method is based on
                   differentiable sorting of internal representations.
                   Concretely, we introduce a meta sorting network that learns
                   to generate latent permutations over sequences. Given sorted
                   sequences, we are then able to compute quasi-global
                   attention with only local windows, improving the memory
                   efficiency of the attention module. To this end, we propose
                   new algorithmic innovations such as Causal Sinkhorn
                   Balancing and SortCut, a dynamic sequence truncation method
                   for tailoring Sinkhorn Attention for encoding and/or
                   decoding purposes. Via extensive experiments on algorithmic
                   seq2seq sorting, language modeling, pixel-wise image
                   generation, document classification and natural language
                   inference, we demonstrate that our memory efficient Sinkhorn
                   Attention method is competitive with vanilla attention and
                   consistently outperforms recently proposed efficient
                   Transformer models such as Sparse Transformers.",
  month         =  feb,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2002.11296"
}

@MISC{Jiang_undated-dn,
  title        = "Subspace learning for effective meta-learning",
  author       = "Jiang, Weisen and Kwok, James T and Zhang, Yu",
  abstract     = "Meta-learning aims to extract meta-knowledge from historical
                  tasks to accelerate learning on new tasks. Typical
                  meta-learning algorithms like MAML learn a globally-shared
                  meta-model for all tasks. However, when the task environments
                  are complex, task model parameters are diverse and a common
                  meta-model is insufficient to capture all the meta-knowledge.
                  To address this challenge, in this paper, task model
                  parameters are structured into multiple subspaces, and each
                  subspace represents one type of meta-knowledge. We propose an
                  algorithm to learn the meta-parameters (i.e., subspace
                  bases). We theoretically study the generalization properties
                  of the learned subspaces. Experiments on regression and
                  classification metalearning datasets verify the effectiveness
                  of the proposed algorithm.",
  howpublished = "\url{https://proceedings.mlr.press/v162/jiang22b/jiang22b.pdf}",
  note         = "Accessed: 2022-8-26"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Robbins1951-jb,
  title     = "A Stochastic Approximation Method",
  author    = "Robbins, Herbert and Monro, Sutton",
  abstract  = "[Let M(x) denote the expected value at level x of the response
               to a certain experiment. M(x) is assumed to be a monotone
               function of x but is unknown to the experimenter, and it is
               desired to find the solution x = $\vartheta$ of the equation
               M(x) = $\alpha$, where $\alpha$ is a given constant. We give a
               method for making successive experiments at levels x1,x2,⋯ in
               such a way that xn will tend to $\vartheta$ in probability.]",
  journal   = "Ann. Math. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  22,
  number    =  3,
  pages     = "400--407",
  year      =  1951
}

@ARTICLE{You2017-xl,
  title    = "Large Batch Training of Convolutional Networks",
  author   = "You, Yang and Gitman, Igor and Ginsburg, Boris",
  abstract = "A common way to speed up training of large convolutional networks
              is to add computational units. Training is then performed using
              data-parallel synchronous Stochastic Gradient Descent (SGD) with
              mini-batch divided between computational units. With an increase
              in the number of nodes, the batch size grows. But training with
              large batch size often results in the lower model accuracy. We
              argue that the current recipe for large batch training (linear
              learning rate scaling with warm-up) is not general enough and
              training may diverge. To overcome this optimization difficulties
              we propose a new training algorithm based on Layer-wise Adaptive
              Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch
              size of 8K, and Resnet-50 to a batch size of 32K without loss in
              accuracy.",
  journal  = "arXiv [cs.CV]",
  month    =  aug,
  year     =  2017
}

@INPROCEEDINGS{Ji2018-bu,
  title     = "Invariant Information Clustering for Unsupervised Image
               Classification and Segmentation",
  booktitle = "{IEEE/CVF} International Conference on Computer Vision",
  author    = "Ji, Xu and Henriques, Jo{\~a}o F and Vedaldi, Andrea",
  abstract  = "We present a novel clustering objective that learns a neural
               network classifier from scratch, given only unlabelled data
               samples. The model discovers clusters that accurately match
               semantic classes, achieving state-of-the-art results in eight
               unsupervised clustering benchmarks spanning image classification
               and segmentation. These include STL10, an unsupervised variant
               of ImageNet, and CIFAR10, where we significantly beat the
               accuracy of our closest competitors by 6.6 and 9.5 absolute
               percentage points respectively. The method is not specialised to
               computer vision and operates on any paired dataset samples; in
               our experiments we use random transforms to obtain a pair from
               each image. The trained network directly outputs semantic
               labels, rather than high dimensional representations that need
               external processing to be usable for semantic clustering. The
               objective is simply to maximise mutual information between the
               class assignments of each pair. It is easy to implement and
               rigorously grounded in information theory, meaning we
               effortlessly avoid degenerate solutions that other clustering
               methods are susceptible to. In addition to the fully
               unsupervised mode, we also test two semi-supervised settings.
               The first achieves 88.8\% accuracy on STL10 classification,
               setting a new global state-of-the-art over all existing methods
               (whether supervised, semi-supervised or unsupervised). The
               second shows robustness to 90\% reductions in label coverage, of
               relevance to applications that wish to make use of small amounts
               of labels. github.com/xu-ji/IIC",
  month     =  jul,
  year      =  2018
}

@ARTICLE{Ntelemis2021-gz,
  title    = "Information Maximization Clustering via {Multi-View}
              {Self-Labelling}",
  author   = "Ntelemis, Foivos and Jin, Yaochu and Thomas, Spencer A",
  abstract = "Image clustering is a particularly challenging computer vision
              task, which aims to generate annotations without human
              supervision. Recent advances focus on the use of self-supervised
              learning strategies in image clustering, by first learning
              valuable semantics and then clustering the image representations.
              These multiple-phase algorithms, however, increase the
              computational time and their final performance is reliant on the
              first stage. By extending the self-supervised approach, we
              propose a novel single-phase clustering method that
              simultaneously learns meaningful representations and assigns the
              corresponding annotations. This is achieved by integrating a
              discrete representation into the self-supervised paradigm through
              a classifier net. Specifically, the proposed clustering objective
              employs mutual information, and maximizes the dependency between
              the integrated discrete representation and a discrete probability
              distribution. The discrete probability distribution is derived
              though the self-supervised process by comparing the learnt latent
              representation with a set of trainable prototypes. To enhance the
              learning performance of the classifier, we jointly apply the
              mutual information across multi-crop views. Our empirical results
              show that the proposed framework outperforms state-of-the-art
              techniques with the average accuracy of 89.1\% and 49.0\%,
              respectively, on CIFAR-10 and CIFAR-100/20 datasets. Finally, the
              proposed method also demonstrates attractive robustness to
              parameter settings, making it ready to be applicable to other
              datasets.",
  journal  = "arXiv [cs.CV]",
  month    =  mar,
  year     =  2021
}

@INPROCEEDINGS{Zhou2022-yq,
  title     = "On the Optimization Landscape of Neural Collapse under {MSE}
               Loss: Global Optimality with Unconstrained Features",
  booktitle = "International Conference in Machine Learning",
  author    = "Zhou, Jinxin and Li, Xiao and Ding, Tianyu and You, Chong and
               Qu, Qing and Zhu, Zhihui",
  abstract  = "When training deep neural networks for classification tasks, an
               intriguing empirical phenomenon has been widely observed in the
               last-layer classifiers and features, where (i) the class means
               and the last-layer classifiers all collapse to the vertices of a
               Simplex Equiangular Tight Frame (ETF) up to scaling, and (ii)
               cross-example within-class variability of last-layer activations
               collapses to zero. This phenomenon is called Neural Collapse
               (NC), which seems to take place regardless of the choice of loss
               functions. In this work, we justify NC under the mean squared
               error (MSE) loss, where recent empirical evidence shows that it
               performs comparably or even better than the de-facto
               cross-entropy loss. Under a simplified unconstrained feature
               model, we provide the first global landscape analysis for
               vanilla nonconvex MSE loss and show that the (only!) global
               minimizers are neural collapse solutions, while all other
               critical points are strict saddles whose Hessian exhibit
               negative curvature directions. Furthermore, we justify the usage
               of rescaled MSE loss by probing the optimization landscape
               around the NC solutions, showing that the landscape can be
               improved by tuning the rescaling hyperparameters. Finally, our
               theoretical findings are experimentally verified on practical
               network architectures.",
  month     =  mar,
  year      =  2022
}

@INPROCEEDINGS{Lezama2018-ej,
  title     = "{OL{\'E}}: Orthogonal Low-rank Embedding, A Plug and Play
               Geometric Loss for Deep Learning",
  booktitle = "{IEEE} Conference on Computer Vision and Pattern Recognition",
  author    = "Lezama, Jos{\'e} and Qiu, Qiang and Mus{\'e}, Pablo and Sapiro,
               Guillermo",
  abstract  = "Deep neural networks trained using a softmax layer at the top
               and the cross-entropy loss are ubiquitous tools for image
               classification. Yet, this does not naturally enforce intra-class
               similarity nor inter-class margin of the learned deep
               representations. To simultaneously achieve these two goals,
               different solutions have been proposed in the literature, such
               as the pairwise or triplet losses. However, such solutions carry
               the extra task of selecting pairs or triplets, and the extra
               computational burden of computing and learning for many
               combinations of them. In this paper, we propose a plug-and-play
               loss term for deep networks that explicitly reduces intra-class
               variance and enforces inter-class margin simultaneously, in a
               simple and elegant geometric manner. For each class, the deep
               features are collapsed into a learned linear subspace, or union
               of them, and inter-class subspaces are pushed to be as
               orthogonal as possible. Our proposed Orthogonal Low-rank
               Embedding (OL\textbackslash'E) does not require carefully
               crafting pairs or triplets of samples for training, and works
               standalone as a classification loss, being the first reported
               deep metric learning framework of its kind. Because of the
               improved margin between features of different classes, the
               resulting deep networks generalize better, are more
               discriminative, and more robust. We demonstrate improved
               classification performance in general object recognition,
               plugging the proposed loss term into existing off-the-shelf
               architectures. In particular, we show the advantage of the
               proposed loss in the small data/model scenario, and we
               significantly advance the state-of-the-art on the Stanford
               STL-10 benchmark.",
  pages     = "8109--8118",
  year      =  2018
}

@ARTICLE{Qiu2013-bi,
  title         = "Learning Transformations for Clustering and Classification",
  author        = "Qiu, Qiang and Sapiro, Guillermo",
  abstract      = "A low-rank transformation learning framework for subspace
                   clustering and classification is here proposed. Many
                   high-dimensional data, such as face images and motion
                   sequences, approximately lie in a union of low-dimensional
                   subspaces. The corresponding subspace clustering problem has
                   been extensively studied in the literature to partition such
                   high-dimensional data into clusters corresponding to their
                   underlying low-dimensional subspaces. However,
                   low-dimensional intrinsic structures are often violated for
                   real-world observations, as they can be corrupted by errors
                   or deviate from ideal models. We propose to address this by
                   learning a linear transformation on subspaces using matrix
                   rank, via its convex surrogate nuclear norm, as the
                   optimization criteria. The learned linear transformation
                   restores a low-rank structure for data from the same
                   subspace, and, at the same time, forces a a maximally
                   separated structure for data from different subspaces. In
                   this way, we reduce variations within subspaces, and
                   increase separation between subspaces for a more robust
                   subspace clustering. This proposed learned robust subspace
                   clustering framework significantly enhances the performance
                   of existing subspace clustering methods. Basic theoretical
                   results here presented help to further support the
                   underlying framework. To exploit the low-rank structures of
                   the transformed subspaces, we further introduce a fast
                   subspace clustering technique, which efficiently combines
                   robust PCA with sparse modeling. When class labels are
                   present at the training stage, we show this low-rank
                   transformation framework also significantly enhances
                   classification performance. Extensive experiments using
                   public datasets are presented, showing that the proposed
                   approach significantly outperforms state-of-the-art methods
                   for subspace clustering and classification.",
  month         =  sep,
  year          =  2013,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1309.2074"
}

@ARTICLE{Braverman2021-vu,
  title         = "Sublinear Time Spectral Density Estimation",
  author        = "Braverman, Vladimir and Krishnan, Aditya and Musco,
                   Christopher",
  abstract      = "We present a new sublinear time algorithm for approximating
                   the spectral density (eigenvalue distribution) of an
                   $n\times n$ normalized graph adjacency or Laplacian matrix.
                   The algorithm recovers the spectrum up to $\epsilon$
                   accuracy in the Wasserstein-1 distance in $O(n\cdot
                   \text\{poly\}(1/\epsilon))$ time given sample access to the
                   graph. This result compliments recent work by David
                   Cohen-Steiner, Weihao Kong, Christian Sohler, and Gregory
                   Valiant (2018), which obtains a solution with runtime
                   independent of $n$, but exponential in $1/\epsilon$. We
                   conjecture that the trade-off between dimension dependence
                   and accuracy is inherent. Our method is simple and works
                   well experimentally. It is based on a Chebyshev polynomial
                   moment matching method that employees randomized estimators
                   for the matrix trace. We prove that, for any Hermitian $A$,
                   this moment matching method returns an $\epsilon$
                   approximation to the spectral density using just
                   $O(\{1\}/\{\epsilon\})$ matrix-vector products with $A$. By
                   leveraging stability properties of the Chebyshev polynomial
                   three-term recurrence, we then prove that the method is
                   amenable to the use of coarse approximate matrix-vector
                   products. Our sublinear time algorithm follows from
                   combining this result with a novel sampling algorithm for
                   approximating matrix-vector products with a normalized graph
                   adjacency matrix. Of independent interest, we show a similar
                   result for the widely used \textbackslashemph\{kernel
                   polynomial method\} (KPM), proving that this practical
                   algorithm nearly matches the theoretical guarantees of our
                   moment matching method. Our analysis uses tools from
                   Jackson's seminal work on approximation with positive
                   polynomial kernels.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DS",
  eprint        = "2104.03461"
}

@ARTICLE{Li2019-tn,
  title         = "Learning {Energy-Based} Models in {High-Dimensional} Spaces
                   with Multi-scale Denoising Score Matching",
  author        = "Li, Zengyi and Chen, Yubei and Sommer, Friedrich T",
  abstract      = "Energy-Based Models (EBMs) assign unnormalized
                   log-probability to data samples. This functionality has a
                   variety of applications, such as sample synthesis, data
                   denoising, sample restoration, outlier detection, Bayesian
                   reasoning, and many more. But training of EBMs using
                   standard maximum likelihood is extremely slow because it
                   requires sampling from the model distribution. Score
                   matching potentially alleviates this problem. In particular,
                   denoising score matching
                   \textbackslashcitep\{vincent2011connection\} has been
                   successfully used to train EBMs. Using noisy data samples
                   with one fixed noise level, these models learn fast and
                   yield good results in data denoising
                   \textbackslashcitep\{saremi2019neural\}. However,
                   demonstrations of such models in high quality sample
                   synthesis of high dimensional data were lacking. Recently,
                   \textbackslashcitet\{song2019generative\} have shown that a
                   generative model trained by denoising score matching
                   accomplishes excellent sample synthesis, when trained with
                   data samples corrupted with multiple levels of noise. Here
                   we provide analysis and empirical evidence showing that
                   training with multiple noise levels is necessary when the
                   data dimension is high. Leveraging this insight, we propose
                   a novel EBM trained with multi-scale denoising score
                   matching. Our model exhibits data generation performance
                   comparable to state-of-the-art techniques such as GANs, and
                   sets a new baseline for EBMs. The proposed model also
                   provides density information and performs well in an image
                   inpainting task.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1910.07762"
}

@ARTICLE{Chen2022-mz,
  title         = "{Intra-Instance} {VICReg}: Bag of {Self-Supervised} Image
                   Patch Embedding",
  author        = "Chen, Yubei and Bardes, Adrien and Li, Zengyi and LeCun,
                   Yann",
  abstract      = "Recently, self-supervised learning (SSL) has achieved
                   tremendous empirical advancements in learning image
                   representation. However, our understanding and knowledge of
                   the representation are still limited. This work shows that
                   the success of the SOTA siamese-network-based SSL approaches
                   is primarily based on learning a representation of image
                   patches. Particularly, we show that when we learn a
                   representation only for fixed-scale image patches and
                   aggregate different patch representations linearly for an
                   image (instance), it can achieve on par or even better
                   results than the baseline methods on several benchmarks.
                   Further, we show that the patch representation aggregation
                   can also improve various SOTA baseline methods by a large
                   margin. We also establish a formal connection between the
                   SSL objective and the image patches co-occurrence statistics
                   modeling, which supplements the prevailing invariance
                   perspective. By visualizing the nearest neighbors of
                   different image patches in the embedding space and
                   projection space, we show that while the projection has more
                   invariance, the embedding space tends to preserve more
                   equivariance and locality. Finally, we propose a hypothesis
                   for the future direction based on the discovery of this
                   work.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2206.08954"
}

@ARTICLE{Wald2021-hy,
  title         = "On calibration and out-of-domain generalization",
  author        = "Wald, Yoav and Feder, Amir and Greenfeld, Daniel and Shalit,
                   Uri",
  abstract      = "Out-of-domain (OOD) generalization is a significant
                   challenge for machine learning models. Many techniques have
                   been proposed to overcome this challenge, often focused on
                   learning models with certain invariance properties. In this
                   work, we draw a link between OOD performance and model
                   calibration, arguing that calibration across multiple
                   domains can be viewed as a special case of an invariant
                   representation leading to better OOD generalization.
                   Specifically, we show that under certain conditions, models
                   which achieve \textbackslashemph\{multi-domain calibration\}
                   are provably free of spurious correlations. This leads us to
                   propose multi-domain calibration as a measurable and
                   trainable surrogate for the OOD performance of a classifier.
                   We therefore introduce methods that are easy to apply and
                   allow practitioners to improve multi-domain calibration by
                   training or modifying an existing model, leading to better
                   performance on unseen domains. Using four datasets from the
                   recently proposed WILDS OOD benchmark, as well as the
                   Colored MNIST dataset, we demonstrate that training or
                   tuning models so they are calibrated across multiple domains
                   leads to significantly improved performance on unseen test
                   domains. We believe this intriguing connection between
                   calibration and OOD generalization is promising from both a
                   practical and theoretical point of view.",
  month         =  feb,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2102.10395"
}

@ARTICLE{Wang2016-gs,
  title   = "Noisy Sparse Subspace Clustering",
  author  = "Wang, Yu-Xiang and Xu, Huan",
  journal = "Journal of Machine Learning Research",
  year    =  2016
}

@ARTICLE{Papyan2020-go,
  title     = "Prevalence of neural collapse during the terminal phase of deep
               learning training",
  author    = "Papyan, Vardan and Han, X Y and Donoho, David L",
  abstract  = "Modern practice for training classification deepnets involves a
               terminal phase of training (TPT), which begins at the epoch
               where training error first vanishes. During TPT, the training
               error stays effectively zero, while training loss is pushed
               toward zero. Direct measurements of TPT, for three prototypical
               deepnet architectures and across seven canonical classification
               datasets, expose a pervasive inductive bias we call neural
               collapse (NC), involving four deeply interconnected phenomena.
               (NC1) Cross-example within-class variability of last-layer
               training activations collapses to zero, as the individual
               activations themselves collapse to their class means. (NC2) The
               class means collapse to the vertices of a simplex equiangular
               tight frame (ETF). (NC3) Up to rescaling, the last-layer
               classifiers collapse to the class means or in other words, to
               the simplex ETF (i.e., to a self-dual configuration). (NC4) For
               a given activation, the classifier's decision collapses to
               simply choosing whichever class has the closest train class mean
               (i.e., the nearest class center [NCC] decision rule). The
               symmetric and very simple geometry induced by the TPT confers
               important benefits, including better generalization performance,
               better robustness, and better interpretability.",
  journal   = "Proceedings of the National Academy of Sciences of the United
               States of America",
  publisher = "National Acad Sciences",
  volume    =  117,
  number    =  40,
  pages     = "24652--24663",
  month     =  oct,
  year      =  2020,
  keywords  = "adversarial robustness; deep learning; inductive bias; nearest
               class center; simplex equiangular tight frame",
  language  = "en"
}

@INPROCEEDINGS{Elhamifar2009-bw,
  title     = "Sparse subspace clustering",
  booktitle = "{IEEE} Conference on Computer Vision and Pattern Recognition",
  author    = "Elhamifar, Ehsan and Vidal, Rene",
  abstract  = "We propose a method based on sparse representation (SR) to
               cluster data drawn from multiple low-dimensional linear or
               affine subspaces embedded in a high-dimensional space. Our
               method is based on the fact that each point in a union of
               subspaces has a SR with respect to a dictionary formed by all
               other data points. In general, finding such a SR is NP hard. Our
               key contribution is to show that, under mild assumptions, the SR
               can be obtained `exactly' by using l1 optimization. The
               segmentation of the data is obtained by applying spectral
               clustering to a similarity matrix built from this SR. Our method
               can handle noise, outliers as well as missing data. We apply our
               subspace clustering algorithm to the problem of segmenting
               multiple motions in video. Experiments on 167 video sequences
               show that our approach significantly outperforms
               state-of-the-art methods.",
  pages     = "2790--2797",
  month     =  jun,
  year      =  2009,
  keywords  = "Strontium;Principal component analysis;Image coding;Image
               segmentation;Polynomials;Iterative
               methods;Dictionaries;Clustering algorithms;Video
               sequences;Information theory"
}

@ARTICLE{Li2018-qr,
  title    = "On Geometric Analysis of Affine Sparse Subspace Clustering",
  author   = "Li, Chun-Guang and You, Chong and Vidal, Ren{\'e}",
  abstract = "Sparse subspace clustering (SSC) is a state-of-the-art method for
              segmenting a set of data points drawn from a union of subspaces
              into their respective subspaces. It is now well understood that
              SSC produces subspace-preserving data affinity under broad
              geometric conditions but suffers from a connectivity issue. In
              this paper, we develop a novel geometric analysis for a variant
              of SSC, named affine SSC (ASSC), for the problem of clustering
              data from a union of affine subspaces. Our contributions include
              a new concept called affine independence for capturing the
              arrangement of a collection of affine subspaces. Under the affine
              independence assumption, we show that ASSC is guaranteed to
              produce subspace-preserving affinity. Moreover, inspired by the
              phenomenon that the $\ell _1$ regularization no longer induces
              sparsity when the solution is nonnegative, we further show that
              subspace-preserving recovery can be achieved under much weaker
              conditions for all data points other than the extreme points of
              samples from each subspace. In addition, we confirm a curious
              observation that the affinity produced by ASSC may be
              subspace-dense---which could guarantee the subspace-preserving
              affinity of ASSC to produce correct clustering under rather weak
              conditions. We validate the theoretical findings on carefully
              designed synthetic data and evaluate the performance of ASSC on
              several real datasets.",
  journal  = "IEEE J. Sel. Top. Signal Process.",
  volume   =  12,
  number   =  6,
  pages    = "1520--1533",
  month    =  dec,
  year     =  2018,
  keywords = "Clustering methods;Motion segmentation;System
              identification;Affine subspace clustering;affine sparse subspace
              clustering;subspace-preserving property;nonnegative
              solution;subspace-dense solution"
}

@INPROCEEDINGS{Tsakiris2018-ej,
  title     = "Theoretical Analysis of Sparse Subspace Clustering with Missing
               Entries",
  booktitle = "International Conference on Machine Learning",
  author    = "Tsakiris, Manolis and Vidal, Rene",
  editor    = "Dy, Jennifer and Krause, Andreas",
  abstract  = "Sparse Subspace Clustering (SSC) is a popular unsupervised
               machine learning method for clustering data lying close to an
               unknown union of low-dimensional linear subspaces; a problem
               with numerous applications in pattern recognition and computer
               vision. Even though the behavior of SSC for complete data is by
               now well-understood, little is known about its theoretical
               properties when applied to data with missing entries. In this
               paper we give theoretical guarantees for SSC with incomplete
               data, and provide theoretical evidence that projecting the
               zero-filled data onto the observation pattern of the point being
               expressed can lead to substantial improvement in performance; a
               phenomenon already known experimentally. The main insight of our
               analysis is that even though this projection induces additional
               missing entries, this is counterbalanced by the fact that the
               projected and zero-filled data are in effect incomplete points
               associated with the union of the corresponding projected
               subspaces, with respect to which the point being expressed is
               complete. The significance of this phenomenon potentially
               extends to the entire class of self-expressive methods.",
  pages     = "4975--4984",
  year      =  2018
}

@INPROCEEDINGS{Wang2015-yc,
  title     = "A deterministic analysis of noisy Sparse Subspace Clustering for
               dimensionality-reduced data",
  booktitle = "International Conference on Machine Learning",
  author    = "Wang, Yining and Wang, Yu-Xiang and Singh, Aarti",
  abstract  = "Subspace clustering groups data into several lowrank subspaces.
               In this paper, we propose a theoretical framework to analyze a
               popular optimization-based algorithm, Sparse Subspace Clustering
               (SSC), when the data dimension is compressed via some random
               projection algorithms. We show SSC provably succeeds if the
               random projection is a subspace embedding, which includes random
               Gaussian projection, uniform row sampling, FJLT, sketching, etc.
               Our analysis applies to the most general deterministic setting
               and is able to handle both adversarial and stochastic noise. It
               also results in the first algorithm for privacy-preserved
               subspace clustering.",
  year      =  2015
}

@ARTICLE{Bach2011-fy,
  title         = "Structured sparsity through convex optimization",
  author        = "Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and
                   Obozinski, Guillaume",
  abstract      = "Sparse estimation methods are aimed at using or obtaining
                   parsimonious representations of data or models. While
                   naturally cast as a combinatorial optimization problem,
                   variable or feature selection admits a convex relaxation
                   through the regularization by the $\ell_1$-norm. In this
                   paper, we consider situations where we are not only
                   interested in sparsity, but where some structural prior
                   knowledge is available as well. We show that the
                   $\ell_1$-norm can then be extended to structured norms built
                   on either disjoint or overlapping groups of variables,
                   leading to a flexible framework that can deal with various
                   structures. We present applications to unsupervised
                   learning, for structured sparse principal component analysis
                   and hierarchical dictionary learning, and to supervised
                   learning in the context of non-linear variable selection.",
  month         =  sep,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1109.2397"
}

@MISC{Arora_undated-jh,
  title        = "A {COMPRESSED} {SENSING} {VIEW} {OF} {UNSUPERVISED} {TEXT}
                  {EMBEDDINGS}, {BAG-OF-n-GRAMS}, {AND} {LSTMS}",
  author       = "Arora, Sanjeev and Khodak, Mikhail and Saunshi, Nikunj and
                  Vodrahalli, Kiran",
  abstract     = "Low-dimensional vector embeddings, computed using LSTMs or
                  simpler techniques, are a popular approach for capturing the
                  ``meaning'' of text and a form of unsupervised learning
                  useful for downstream tasks. However, their power is not
                  theoretically understood. The current paper derives formal
                  understanding by looking at the subcase of linear embedding
                  schemes. Using the theory of compressed sensing we show that
                  representations combining the constituent word vectors are
                  essentially information-preserving linear measurements of
                  Bag-of-n-Grams (BonG) representations of text. This leads to
                  a new theoretical result about LSTMs: low-dimensional
                  embeddings derived from a low-memory LSTM are provably at
                  least as powerful on classification tasks, up to small error,
                  as a linear classifier over BonG vectors, a result that
                  extensive empirical work has thus far been unable to show.
                  Our experiments support these theoretical findings and
                  establish strong, simple, and unsupervised baselines on
                  standard benchmarks that in some cases are state of the art
                  among word-level methods. We also show a surprising new
                  property of embeddings such as GloVe and word2vec: they form
                  a good sensing matrix for text that is more efficient than
                  random matrices, the standard sparse recovery tool, which may
                  explain why they lead to better representations in practice.",
  howpublished = "\url{https://oar.princeton.edu/bitstream/88435/pr13v8j/1/CompressedSensingView.pdf}",
  note         = "Accessed: 2022-9-7"
}

@ARTICLE{Donoho2005-bj,
  title    = "Sparse nonnegative solution of underdetermined linear equations
              by linear programming",
  author   = "Donoho, David L and Tanner, Jared",
  abstract = "Consider an underdetermined system of linear equations y = Ax
              with known y and d x n matrix A. We seek the nonnegative x with
              the fewest nonzeros satisfying y = Ax. In general, this problem
              is NP-hard. However, for many matrices A there is a threshold
              phenomenon: if the sparsest solution is sufficiently sparse, it
              can be found by linear programming. We explain this by the theory
              of convex polytopes. Let a(j) denote the jth column of A, 1 < or
              = j < or = n, let a0 = 0 and P denote the convex hull of the
              a(j). We say the polytope P is outwardly k-neighborly if every
              subset of k vertices not including 0 spans a face of P. We show
              that outward k-neighborliness is equivalent to the statement
              that, whenever y = Ax has a nonnegative solution with at most k
              nonzeros, it is the nonnegative solution to y = Ax having minimal
              sum. We also consider weak neighborliness, where the overwhelming
              majority of k-sets of a(j)s not containing 0 span a face of P.
              This implies that most nonnegative vectors x with k nonzeros are
              uniquely recoverable from y = Ax by linear programming. Numerous
              corollaries follow by invoking neighborliness results. For
              example, for most large n by 2n underdetermined systems having a
              solution with fewer nonzeros than roughly half the number of
              equations, the sparsest solution can be found by linear
              programming.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  102,
  number   =  27,
  pages    = "9446--9451",
  month    =  jul,
  year     =  2005,
  language = "en"
}

@ARTICLE{Ma2022-uc,
  title         = "On the Principles of Parsimony and {Self-Consistency} for
                   the Emergence of Intelligence",
  author        = "Ma, Yi and Tsao, Doris and Shum, Heung-Yeung",
  abstract      = "Ten years into the revival of deep networks and artificial
                   intelligence, we propose a theoretical framework that sheds
                   light on understanding deep networks within a bigger picture
                   of Intelligence in general. We introduce two fundamental
                   principles, Parsimony and Self-consistency, that address two
                   fundamental questions regarding Intelligence: what to learn
                   and how to learn, respectively. We believe the two
                   principles are the cornerstones for the emergence of
                   Intelligence, artificial or natural. While these two
                   principles have rich classical roots, we argue that they can
                   be stated anew in entirely measurable and computable ways.
                   More specifically, the two principles lead to an effective
                   and efficient computational framework, compressive
                   closed-loop transcription, that unifies and explains the
                   evolution of modern deep networks and many artificial
                   intelligence practices. While we mainly use modeling of
                   visual data as an example, we believe the two principles
                   will unify understanding of broad families of autonomous
                   intelligent systems and provide a framework for
                   understanding the brain.",
  month         =  jul,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2207.04630"
}

@INPROCEEDINGS{Tirer2022-sy,
  title     = "Extended unconstrained features model for exploring deep neural
               collapse",
  booktitle = "International Conference on Machine Learning",
  author    = "Tirer, Tom and Bruna, Joan",
  abstract  = "The modern strategy for training deep neural networks for
               classification tasks includes optimizing the network's weights
               even after the training error vanishes to further push the
               training loss toward zero. Recently, a phenomenon termed
               ``neural collapse'' (NC) has been empirically observed in this
               training procedure. Specifically, it has been shown that the
               learned features (the output of the penultimate layer) of
               within-class samples converge to their mean, and the means of
               different classes exhibit a certain tight frame structure, which
               is also aligned with the last layer's weights. Recent papers
               have shown that minimizers with this structure emerge when
               optimizing a simplified ``unconstrained features model'' (UFM)
               with a regularized cross-entropy loss. In this paper, we further
               analyze and extend the UFM. First, we study the UFM for the
               regularized MSE loss, and show that the minimizers' features can
               have a more delicate structure than in the cross-entropy case.
               This affects also the structure of the weights. Then, we extend
               the UFM by adding another layer of weights as well as ReLU
               nonlinearity to the model and generalize our previous results.
               Finally, we empirically demonstrate the usefulness of our
               nonlinear extended UFM in modeling the NC phenomenon that occurs
               with practical networks.",
  month     =  feb,
  year      =  2022,
  copyright = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/"
}

@INPROCEEDINGS{Patel2014-gx,
  title           = "Kernel sparse subspace clustering",
  booktitle       = "2014 {IEEE} International Conference on Image Processing
                     ({ICIP})",
  author          = "Patel, Vishal M and Vidal, Rene",
  abstract        = "Subspace clustering refers to the problem of grouping data
                     points that lie in a union of low-dimensional subspaces.
                     One successful approach for solving this problem is sparse
                     subspace clustering, which is based on a sparse
                     representation of the data. In this paper, we extend SSC
                     to non-linear manifolds by using the kernel trick. We show
                     that the alternating direction method of multipliers can
                     be used to efficiently find kernel sparse representations.
                     Various experiments on synthetic as well real datasets
                     show that non-linear mappings lead to sparse
                     representation that give better clustering results than
                     state-ofthe-art methods.",
  publisher       = "IEEE",
  month           =  oct,
  year            =  2014,
  conference      = "2014 IEEE International Conference on Image Processing
                     (ICIP)",
  location        = "Paris, France"
}

@ARTICLE{Chen2020-ij,
  title         = "Manifold Proximal Point Algorithms for Dual Principal
                   Component Pursuit and Orthogonal Dictionary Learning",
  author        = "Chen, Shixiang and Deng, Zengde and Ma, Shiqian and So,
                   Anthony Man-Cho",
  abstract      = "We consider the problem of maximizing the $\ell_1$ norm of a
                   linear map over the sphere, which arises in various machine
                   learning applications such as orthogonal dictionary learning
                   (ODL) and robust subspace recovery (RSR). The problem is
                   numerically challenging due to its nonsmooth objective and
                   nonconvex constraint, and its algorithmic aspects have not
                   been well explored. In this paper, we show how the manifold
                   structure of the sphere can be exploited to design fast
                   algorithms for tackling this problem. Specifically, our
                   contribution is threefold. First, we present a manifold
                   proximal point algorithm (ManPPA) for the problem and show
                   that it converges at a sublinear rate. Furthermore, we show
                   that ManPPA can achieve a quadratic convergence rate when
                   applied to the ODL and RSR problems. Second, we propose a
                   stochastic variant of ManPPA called StManPPA, which is well
                   suited for large-scale computation, and establish its
                   sublinear convergence rate. Both ManPPA and StManPPA have
                   provably faster convergence rates than existing
                   subgradient-type methods. Third, using ManPPA as a building
                   block, we propose a new approach to solving a matrix analog
                   of the problem, in which the sphere is replaced by the
                   Stiefel manifold. The results from our extensive numerical
                   experiments on the ODL and RSR problems demonstrate the
                   efficiency and efficacy of our proposed methods.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2005.02356"
}

@ARTICLE{Vaswani2017-kt,
  title         = "Attention Is All You Need",
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  abstract      = "The dominant sequence transduction models are based on
                   complex recurrent or convolutional neural networks in an
                   encoder-decoder configuration. The best performing models
                   also connect the encoder and decoder through an attention
                   mechanism. We propose a new simple network architecture, the
                   Transformer, based solely on attention mechanisms,
                   dispensing with recurrence and convolutions entirely.
                   Experiments on two machine translation tasks show these
                   models to be superior in quality while being more
                   parallelizable and requiring significantly less time to
                   train. Our model achieves 28.4 BLEU on the WMT 2014
                   English-to-German translation task, improving over the
                   existing best results, including ensembles by over 2 BLEU.
                   On the WMT 2014 English-to-French translation task, our
                   model establishes a new single-model state-of-the-art BLEU
                   score of 41.8 after training for 3.5 days on eight GPUs, a
                   small fraction of the training costs of the best models from
                   the literature. We show that the Transformer generalizes
                   well to other tasks by applying it successfully to English
                   constituency parsing both with large and limited training
                   data.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1706.03762"
}

@MISC{Mohan_undated-lp,
  title        = "Iterative reweighted algorithms for matrix rank minimization",
  author       = "Mohan, Karthik and Fazel, Maryam",
  abstract     = "The problem of minimizing the rank of a matrix subject to
                  affine constraints has many applications in machine learning,
                  and is known to be NP-hard. One of the tractable relaxations
                  proposed for this problem is nuclear norm (or trace norm)
                  minimization of the matrix, which is guaranteed to find the
                  minimum rank matrix under suitable assumptions. In this
                  paper, we propose a family of Iterative Reweighted Least
                  Squares algorithms IRLS-p (with 0 $\leq$ p $\leq$ 1), as a
                  computationally efficient way to improve over the performance
                  of nuclear norm minimization. The algorithms can be viewed as
                  (locally) minimizing certain smooth approximations to the
                  rank function. When p = 1, we give theoretical guarantees
                  similar to those for nuclear norm minimization, i.e.,
                  recovery of low-rank matrices under certain assumptions on
                  the operator defining the constraints. For p < 1, IRLS-p
                  shows better empirical performance in terms of recovering
                  low-rank matrices than nuclear norm minimization. We provide
                  an efficient implementation for IRLS-p, and also present a
                  related family of algorithms, sIRLS-p. These algorithms
                  exhibit competitive run times and improved recovery when
                  compared to existing algorithms for random instances of the
                  matrix completion problem, as well as on the MovieLens movie
                  recommendation data set.",
  howpublished = "\url{https://faculty.washington.edu/mfazel/MohanFazel-11.pdf}",
  note         = "Accessed: 2022-9-15"
}

@BOOK{Boumal2023-kc,
  title     = "An Introduction to Optimization on Smooth Manifolds",
  author    = "Boumal, Nicolas",
  abstract  = "Optimization on Riemannian manifolds-the result of smooth
               geometry and optimization merging into one elegant modern
               framework-spans many areas of science and engineering, including
               machine learning, computer vision, signal processing, dynamical
               systems and scientific computing. This text introduces the
               differential geometry and Riemannian geometry concepts that will
               help students and researchers in applied mathematics, computer
               science and engineering gain a firm mathematical grounding to
               use these tools confidently in their research. Its charts-last
               approach will prove more intuitive from an optimizer's
               viewpoint, and all definitions and theorems are motivated to
               build time-tested optimization algorithms. Starting from first
               principles, the text goes on to cover current research on topics
               including worst-case complexity and geodesic convexity. Readers
               will appreciate the tricks of the trade for conducting research
               and for numerical implementations sprinkled throughout the book.",
  publisher = "Cambridge University Press",
  month     =  mar,
  year      =  2023,
  language  = "en"
}

@TECHREPORT{Lloyd1957-wk,
  title       = "Least squares quantization in {PCM}",
  author      = "Lloyd, Stuart",
  abstract    = "It has long been realized that in pulse-code modulation (PCM),
                 with a given ensemble of signals to handle, the quantum values
                 should be spaced more closely in the voltage regions where the
                 signal amplitude is more likely to fall. It has been shown by
                 Panter and Dite that, in the limit as the number of quanta
                 becomes infinite, the asymptotic fractional density of quanta
                 per unit voltage should vary as the one-third power of the
                 probability density per unit voltage of signal amplitudes. In
                 this paper the corresponding result for any finite number of
                 quanta is derived; that is, necessary conditions are found
                 that the quanta and associated quantization intervals of an
                 optimum finite quantization scheme must satisfy. The
                 optimization criterion used is that the average quantization
                 noise power be a minimum. It is shown that the result obtained
                 here goes over into the Panter and Dite result as the number
                 of quanta become large. The optimum quautization schemes
                 for2^bquanta,b=1,2, \textbackslashcdots, 7, are given
                 numerically for Gaussian and for Laplacian distribution of
                 signal amplitudes.",
  institution = "Bell Laboratories",
  year        =  1957
}

@ARTICLE{Forgey1965-xd,
  title     = "Cluster analysis of multivariate data: Efficiency vs.
               interpretability of classification",
  author    = "Forgey, Edward",
  journal   = "Biometrics",
  publisher = "Oxford",
  year      =  1965
}

@INPROCEEDINGS{Arthur2006-is,
  title     = "k-means++: The Advantages of Careful Seeding",
  booktitle = "the Eighteenth Annual {ACM-SIAM} Symposium on Discrete
               Algorithms",
  author    = "Arthur, David and Vassilvitskii, Sergei",
  abstract  = "The k-means method is a widely used clustering technique that
               seeks to minimize the average squared distance between points in
               the same cluster. Although it offers no accuracy guarantees, its
               simplicity and speed are very appealing in practice. By
               augmenting k-means with a simple, randomized seeding technique,
               we obtain an algorithm that is $O(\log k)$-competitive with the
               optimal clustering. Experiments show our augmentation improves
               both the speed and the accuracy of k-means, often quite
               dramatically.",
  publisher = "Society for Industrial and Applied Mathematics",
  month     =  jun,
  year      =  2006,
  keywords  = "k-means, clustering, seeding"
}

@ARTICLE{Bahmani2012-ip,
  title    = "Scalable {K-Means++}",
  author   = "Bahmani, Bahman and Moseley, Benjamin and Vattani, Andrea and
              Kumar, Ravi and Vassilvitskii, Sergei",
  abstract = "Over half a century old and showing no signs of aging, k-means
              remains one of the most popular data processing algorithms. As is
              well-known, a proper initialization of k-means is crucial for
              obtaining a good final solution. The recently proposed k-means++
              initialization algorithm achieves this, obtaining an initial set
              of centers that is provably close to the optimum solution. A
              major downside of the k-means++ is its inherent sequential
              nature, which limits its applicability to massive data: one must
              make k passes over the data to find a good initial set of
              centers. In this work we show how to drastically reduce the
              number of passes needed to obtain, in parallel, a good
              initialization. This is unlike prevailing efforts on
              parallelizing k-means that have mostly focused on the
              post-initialization phases of k-means. We prove that our proposed
              initialization algorithm k-means|| obtains a nearly optimal
              solution after a logarithmic number of passes, and then show that
              in practice a constant number of passes suffices. Experimental
              evaluation on real-world large-scale data demonstrates that
              k-means|| outperforms k-means++ in both sequential and parallel
              settings.",
  journal  = "Proceedings VLDB Endowment",
  volume   =  5,
  number   =  7,
  month    =  mar,
  year     =  2012
}

@INPROCEEDINGS{Bradley1996-cz,
  title     = "Clustering via Concave Minimization",
  booktitle = "Advances in neural information processing systems",
  author    = "Bradley, Paul and Mangasarian, Olvi and Street, W",
  abstract  = "The problem of assigning m points in the n-dimensional real
               space Rn to k clusters is formulated as that of determining k
               centers in Rn such that the sum of distances of each point to
               the nearest center is minimized. If a polyhedral distance is
               used, the problem can be formulated as that of minimizing a
               piecewise-linear concave function on a polyhedral set which is
               shown to be equivalent to a bilinear program: minimizing a
               bilinear function on a polyhedral set. A fast finite k-Median
               Algorithm consisting of solving few linear programs in closed
               form leads to a stationary point of the bilinear program.
               Computational testing on a number of realworld databases was
               carried out. On the Wisconsin Diagnostic Breast Cancer (WDBC)
               database, k-Median training set correctness was comparable to
               that of the k-Mean Algorithm, however its testing set
               correctness was better. Additionally, on the Wisconsin
               Prognostic Breast Cancer (WPBC) database, distinct and
               clinically important survival curves were extracted by the
               k-Median Algorithm, whereas the k-Mean Algorithm failed to
               obtain such distinct survival curves for the same database.",
  year      =  1996
}

@INPROCEEDINGS{Caron2018-hb,
  title     = "Deep Clustering for Unsupervised Learning of Visual Features",
  booktitle = "European conference on computer vision",
  author    = "Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and
               Douze, Matthijs",
  abstract  = "Clustering is a class of unsupervised learning methods that has
               been extensively applied and studied in computer vision. Little
               work has been done to adapt it to the end-to-end training of
               visual features on large scale datasets. In this work, we
               present DeepCluster, a clustering method that jointly learns the
               parameters of a neural network and the cluster assignments of
               the resulting features. DeepCluster iteratively groups the
               features with a standard clustering algorithm, k-means, and uses
               the subsequent assignments as supervision to update the weights
               of the network. We apply DeepCluster to the unsupervised
               training of convolutional neural networks on large datasets like
               ImageNet and YFCC100M. The resulting model outperforms the
               current state of the art by a significant margin on all the
               standard benchmarks.",
  pages     = "132--149",
  month     =  jul,
  year      =  2018
}

@ARTICLE{Peng2017-gf,
  title    = "Deep Sparse Subspace Clustering",
  author   = "Peng, Xi and Feng, Jiashi and Xiao, Shijie and Lu, Jiwen and Yi,
              Zhang and Yan, Shuicheng",
  abstract = "In this paper, we present a deep extension of Sparse Subspace
              Clustering, termed Deep Sparse Subspace Clustering (DSSC).
              Regularized by the unit sphere distribution assumption for the
              learned deep features, DSSC can infer a new data affinity matrix
              by simultaneously satisfying the sparsity principle of SSC and
              the nonlinearity given by neural networks. One of the appealing
              advantages brought by DSSC is: when original real-world data do
              not meet the class-specific linear subspace distribution
              assumption, DSSC can employ neural networks to make the
              assumption valid with its hierarchical nonlinear transformations.
              To the best of our knowledge, this is among the first deep
              learning based subspace clustering methods. Extensive experiments
              are conducted on four real-world datasets to show the proposed
              DSSC is significantly superior to 12 existing methods for
              subspace clustering.",
  journal  = "arXiv [cs.CV]",
  month    =  sep,
  year     =  2017
}

@INPROCEEDINGS{Kheirandishfard2020-zc,
  title           = "Deep low-rank subspace clustering",
  booktitle       = "2020 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition Workshops",
  author          = "Kheirandishfard, Mohsen and Zohrizadeh, Fariba and
                     Kamangar, Farhad",
  abstract        = "This paper is concerned with developing a novel approach
                     to tackle the problem of subspace clustering. The approach
                     introduces a convolutional autoencoder-based architecture
                     to generate low-rank representations (LRR) of input data
                     which are proven to be very suitable for subspace
                     clustering. We propose to insert a fully-connected linear
                     layer and its transpose between the encoder and decoder to
                     implicitly impose a rank constraint on the learned
                     representations. We train this architecture by minimizing
                     a standard deep subspace clustering loss function and then
                     recover underlying subspaces by applying a variant of
                     spectral clustering technique. Extensive experiments on
                     benchmark datasets demonstrate that the proposed model can
                     not only achieve very competitive clustering results using
                     a relatively small network architecture but also can
                     maintain its high level of performance across a wide range
                     of LRRs. This implies that the model can be appropriately
                     combined with the state-of-the-art subspace clustering
                     architectures to produce more accurate results.",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2020,
  conference      = "2020 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition Workshops (CVPRW)",
  location        = "Seattle, WA, USA"
}

@ARTICLE{Abavisani2018-lt,
  title    = "Deep Multimodal Subspace Clustering Networks",
  author   = "Abavisani, Mahdi and Patel, Vishal M",
  abstract = "We present convolutional neural network (CNN) based approaches
              for unsupervised multimodal subspace clustering. The proposed
              framework consists of three main stages - multimodal encoder,
              self-expressive layer, and multimodal decoder. The encoder takes
              multimodal data as input and fuses them to a latent space
              representation. The self-expressive layer is responsible for
              enforcing the self-expressiveness property and acquiring an
              affinity matrix corresponding to the data points. The decoder
              reconstructs the original input data. The network uses the
              distance between the decoder's reconstruction and the original
              input in its training. We investigate early, late and
              intermediate fusion techniques and propose three different
              encoders corresponding to them for spatial fusion. The
              self-expressive layers and multimodal decoders are essentially
              the same for different spatial fusion-based approaches. In
              addition to various spatial fusion-based methods, an affinity
              fusion-based network is also proposed in which the
              self-expressive layer corresponding to different modalities is
              enforced to be the same. Extensive experiments on three datasets
              show that the proposed methods significantly outperform the
              state-of-the-art multimodal subspace clustering methods.",
  journal  = "IEEE Journal of Selected Topics in Signal Processing",
  volume   =  12,
  number   =  6,
  pages    = "1601--1614",
  month    =  apr,
  year     =  2018
}

@ARTICLE{Dai2022-ui,
  title    = "Ctrl: Closed-loop transcription to an ldr via minimaxing rate
              reduction",
  author   = "Dai, Xili and Tong, Shengbang and Li, Mingyang and Wu, Ziyang and
              Psenka, Michael and Chan, Kwan Ho Ryan and Zhai, Pengyuan and Yu,
              Yaodong and Yuan, Xiaojun and Shum, Heung Yeung and Ma, Yi",
  abstract = "This work proposes a new computational framework for learning a
              structured generative model for real-world datasets. In
              particular, we propose to learn a closed-loop transcription
              between a multi-class multi-dimensional data distribution and a
              linear discriminative representation (LDR) in the feature space
              that consists of multiple independent multi-dimensional linear
              subspaces. In particular, we argue that the optimal encoding and
              decoding mappings sought can be formulated as the equilibrium
              point of a two-player minimax game between the encoder and
              decoder. A natural utility function for this game is the
              so-called rate reduction, a simple information-theoretic measure
              for distances between mixtures of subspace-like Gaussians in the
              feature space. Our formulation draws inspiration from closed-loop
              error feedback from control systems and avoids expensive
              evaluating and minimizing approximated distances between
              arbitrary distributions in either the data space or the feature
              space. To a large extent, this new formulation unifies the
              concepts and benefits of Auto-Encoding and GAN and naturally
              extends them to the settings of learning a both discriminative
              and generative representation for multi-class and
              multi-dimensional real-world data. Our extensive experiments on
              many benchmark imagery datasets demonstrate tremendous potential
              of this new closed-loop formulation: under fair comparison,
              visual quality of the learned decoder and classification
              performance of the encoder is competitive and often better than
              existing methods based on GAN, VAE, or a combination of both.
              Unlike existing generative models, the so learned features of the
              multiple classes are structured: different classes are explicitly
              mapped onto corresponding independent principal subspaces in the
              feature space. Source code can be found at
              https://github.com/Delay-Xili/LDR.",
  journal  = "Entropy",
  year     =  2022,
  language = "en"
}

@ARTICLE{Laclau2017-uu,
  title         = "Co-clustering through Optimal Transport",
  author        = "Laclau, Charlotte and Redko, Ievgen and Matei, Basarab and
                   Bennani, Youn{\`e}s and Brault, Vincent",
  abstract      = "In this paper, we present a novel method for co-clustering,
                   an unsupervised learning approach that aims at discovering
                   homogeneous groups of data instances and features by
                   grouping them simultaneously. The proposed method uses the
                   entropy regularized optimal transport between empirical
                   measures defined on data instances and features in order to
                   obtain an estimated joint probability density function
                   represented by the optimal coupling matrix. This matrix is
                   further factorized to obtain the induced row and columns
                   partitions using multiscale representations approach. To
                   justify our method theoretically, we show how the solution
                   of the regularized optimal transport can be seen from the
                   variational inference perspective thus motivating its use
                   for co-clustering. The algorithm derived for the proposed
                   method and its kernelized version based on the notion of
                   Gromov-Wasserstein distance are fast, accurate and can
                   determine automatically the number of both row and column
                   clusters. These features are vividly demonstrated through
                   extensive experimental evaluations.",
  month         =  may,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1705.06189"
}

@INCOLLECTION{Poursaeed2019-en,
  title     = "Deep fundamental matrix estimation without correspondences",
  booktitle = "Lecture Notes in Computer Science",
  author    = "Poursaeed, Omid and Yang, Guandao and Prakash, Aditya and Fang,
               Qiuren and Jiang, Hanqing and Hariharan, Bharath and Belongie,
               Serge",
  abstract  = "Estimating fundamental matrices is a classic problem in computer
               vision. Traditional methods rely heavily on the correctness of
               estimated key-point correspondences, which can be noisy and
               unreliable. As a result, it is difficult for these methods to
               handle image pairs with large occlusion or significantly
               different camera poses. In this paper, we propose novel neural
               network architectures to estimate fundamental matrices in an
               end-to-end manner without relying on point correspondences. New
               modules and layers are introduced in order to preserve
               mathematical properties of the fundamental matrix as a
               homogeneous rank-2 matrix with seven degrees of freedom. We
               analyze performance of the proposed models using various metrics
               on the KITTI dataset, and show that they achieve competitive
               performance with traditional methods without the need for
               extracting correspondences.",
  publisher = "Springer International Publishing",
  pages     = "485--497",
  series    = "Lecture notes in computer science",
  year      =  2019,
  address   = "Cham"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Zhao2022-pt,
  title    = "An Efficient Solution to {Non-Minimal} Case Essential Matrix
              Estimation",
  author   = "Zhao, Ji",
  abstract = "Finding relative pose between two calibrated images is a
              fundamental task in computer vision. Given five point
              correspondences, the classical five-point methods can be used to
              calculate the essential matrix efficiently. For the case of N ( )
              inlier point correspondences, which is called N-point problem,
              existing methods are either inefficient or prone to local minima.
              In this paper, we propose a certifiably globally optimal and
              efficient solver for the N-point problem. First we formulate the
              problem as a quadratically constrained quadratic program (QCQP).
              Then a certifiably globally optimal solution to this problem is
              obtained by semidefinite relaxation. This allows us to obtain
              certifiably globally optimal solutions to the original non-convex
              QCQPs in polynomial time. The theoretical guarantees of the
              semidefinite relaxation are also provided, including tightness
              and local stability. To deal with outliers, we propose a robust
              N-point method using M-estimators. Though global optimality
              cannot be guaranteed for the overall robust framework, the
              proposed robust N-point method can achieve good performance when
              the outlier ratio is not high. Extensive experiments on synthetic
              and real-world datasets demonstrated that our N-point method is 2
              ∼ 3 orders of magnitude faster than state-of-the-art methods.
              Moreover, our robust N-point method outperforms state-of-the-art
              methods in terms of robustness and accuracy.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  44,
  number   =  4,
  pages    = "1777--1792",
  month    =  apr,
  year     =  2022,
  language = "en"
}

@MISC{Ene_undated-qn,
  title        = "Improved convergence for`1 and`1 regression via iteratively
                  reweighted least squares",
  author       = "Ene, Alina and Vladu, Adrian",
  abstract     = "The iteratively reweighted least squares method (IRLS) is a
                  popular technique used in practice for solving regression
                  problems. Various versions of this method have been proposed,
                  but their theoretical analyses failed to capture the good
                  practical performance. In this paper we propose a simple and
                  natural version of IRLS for solving`1 and`1 regression, which
                  provably converges to a (1 + ``)approximate solution in O(m
                  1/3 log(1/'')/`` 2/3 + log m/'' 2) iterations, where m is the
                  number of rows of the input matrix. Interestingly, this
                  running time is independent of the conditioning of the input,
                  and the dominant term of the running time depends sublinearly
                  in `` 1 , which is atypical for the optimization of
                  non-smooth functions. This improves upon the more complex
                  algorithms of Chin et al. (ITCS '12), and Christiano et al.
                  (STOC '11) by a factor of at least 1/'' 2 , and yields a
                  truly efficient natural algorithm for the slime mold dynamics
                  (Straszak-Vishnoi, SODA '16, ITCS '16, ITCS '17).",
  howpublished = "\url{http://proceedings.mlr.press/v97/ene19a/ene19a.pdf}",
  note         = "Accessed: 2022-10-7"
}

@MISC{Alexandre_Daspremont_undated-kd,
  title        = "Relaxations and Randomized Methods for Nonconvex {QCQPs}",
  author       = "Alexandre D'aspremont, Stephen Boyd",
  howpublished = "\url{https://web.stanford.edu/class/ee392o/relaxations.pdf}",
  note         = "Accessed: 2022-10-5"
}

@ARTICLE{Lofberg2009-jh,
  title     = "Dualize it: software for automatic primal and dual conversions
               of conic programs",
  author    = "L{\"o}fberg, Johan",
  abstract  = "Many optimization problems gain from being interpreted and
               solved in either primal or dual forms. For a user with a
               particular application, one of these forms is usually much more
               natural to use, but this is not always the most efficient. This
               paper presents an implementation in the optimization modelling
               tool YALMIP that allows the user to define conic optimization
               problems in a preferred format, and then automatically derive a
               symbolic YALMIP model of the dual of this problem, solve the
               dual, and recover original variables. Applications in flexible
               generation of sum-of-squares programs, and efficient
               formulations of large-scale experiment design problems are used
               as illustrative examples.",
  journal   = "Optim. Methods Softw.",
  publisher = "Taylor \& Francis",
  volume    =  24,
  number    =  3,
  pages     = "313--325",
  month     =  jun,
  year      =  2009
}

@ARTICLE{Pope2021-yh,
  title         = "The Intrinsic Dimension of Images and Its Impact on Learning",
  author        = "Pope, Phillip and Zhu, Chen and Abdelkader, Ahmed and
                   Goldblum, Micah and Goldstein, Tom",
  abstract      = "It is widely believed that natural image data exhibits
                   low-dimensional structure despite the high dimensionality of
                   conventional pixel representations. This idea underlies a
                   common intuition for the remarkable success of deep learning
                   in computer vision. In this work, we apply dimension
                   estimation tools to popular datasets and investigate the
                   role of low-dimensional structure in deep learning. We find
                   that common natural image datasets indeed have very low
                   intrinsic dimension relative to the high number of pixels in
                   the images. Additionally, we find that low dimensional
                   datasets are easier for neural networks to learn, and models
                   solving these tasks generalize better from training to test
                   data. Along the way, we develop a technique for validating
                   our dimension estimation tools on synthetic data generated
                   by GANs allowing us to actively manipulate the intrinsic
                   dimension by controlling the image generation process. Code
                   for our experiments may be found here
                   https://github.com/ppope/dimensions.",
  month         =  apr,
  year          =  2021,
  keywords      = "Pointers",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2104.08894"
}

@ARTICLE{Birdal2021-wg,
  title         = "Intrinsic Dimension, Persistent Homology and Generalization
                   in Neural Networks",
  author        = "Birdal, Tolga and Lou, Aaron and Guibas, Leonidas and {\c
                   S}im{\c s}ekli, Umut",
  abstract      = "Disobeying the classical wisdom of statistical learning
                   theory, modern deep neural networks generalize well even
                   though they typically contain millions of parameters.
                   Recently, it has been shown that the trajectories of
                   iterative optimization algorithms can possess fractal
                   structures, and their generalization error can be formally
                   linked to the complexity of such fractals. This complexity
                   is measured by the fractal's intrinsic dimension, a quantity
                   usually much smaller than the number of parameters in the
                   network. Even though this perspective provides an
                   explanation for why overparametrized networks would not
                   overfit, computing the intrinsic dimension (e.g., for
                   monitoring generalization during training) is a notoriously
                   difficult task, where existing methods typically fail even
                   in moderate ambient dimensions. In this study, we consider
                   this problem from the lens of topological data analysis
                   (TDA) and develop a generic computational tool that is built
                   on rigorous mathematical foundations. By making a novel
                   connection between learning theory and TDA, we first
                   illustrate that the generalization error can be equivalently
                   bounded in terms of a notion called the 'persistent homology
                   dimension' (PHD), where, compared with prior work, our
                   approach does not require any additional geometrical or
                   statistical assumptions on the training dynamics. Then, by
                   utilizing recently established theoretical results and TDA
                   tools, we develop an efficient algorithm to estimate PHD in
                   the scale of modern deep neural networks and further provide
                   visualization tools to help understand generalization in
                   deep learning. Our experiments show that the proposed
                   approach can efficiently compute a network's intrinsic
                   dimension in a variety of settings, which is predictive of
                   the generalization error.",
  month         =  nov,
  year          =  2021,
  keywords      = "Pointers",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2111.13171"
}

@ARTICLE{Fort2022-ka,
  title         = "What does a deep neural network confidently perceive? The
                   effective dimension of high certainty class manifolds and
                   their low confidence boundaries",
  author        = "Fort, Stanislav and Cubuk, Ekin Dogus and Ganguli, Surya and
                   Schoenholz, Samuel S",
  abstract      = "Deep neural network classifiers partition input space into
                   high confidence regions for each class. The geometry of
                   these class manifolds (CMs) is widely studied and intimately
                   related to model performance; for example, the margin
                   depends on CM boundaries. We exploit the notions of Gaussian
                   width and Gordon's escape theorem to tractably estimate the
                   effective dimension of CMs and their boundaries through
                   tomographic intersections with random affine subspaces of
                   varying dimension. We show several connections between the
                   dimension of CMs, generalization, and robustness. In
                   particular we investigate how CM dimension depends on 1) the
                   dataset, 2) architecture (including ResNet, WideResNet \&
                   Vision Transformer), 3) initialization, 4) stage of
                   training, 5) class, 6) network width, 7) ensemble size, 8)
                   label randomization, 9) training set size, and 10)
                   robustness to data corruption. Together a picture emerges
                   that higher performing and more robust models have higher
                   dimensional CMs. Moreover, we offer a new perspective on
                   ensembling via intersections of CMs. Our code is at
                   https://github.com/stanislavfort/slice-dice-optimize/",
  month         =  oct,
  year          =  2022,
  keywords      = "Pointers",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2210.05546"
}

@ARTICLE{Qu2020-sy,
  title         = "Finding the Sparsest Vectors in a Subspace: Theory,
                   Algorithms, and Applications",
  author        = "Qu, Qing and Zhu, Zhihui and Li, Xiao and Tsakiris, Manolis
                   C and Wright, John and Vidal, Ren{\'e}",
  abstract      = "The problem of finding the sparsest vector (direction) in a
                   low dimensional subspace can be considered as a homogeneous
                   variant of the sparse recovery problem, which finds
                   applications in robust subspace recovery, dictionary
                   learning, sparse blind deconvolution, and many other
                   problems in signal processing and machine learning. However,
                   in contrast to the classical sparse recovery problem, the
                   most natural formulation for finding the sparsest vector in
                   a subspace is usually nonconvex. In this paper, we overview
                   recent advances on global nonconvex optimization theory for
                   solving this problem, ranging from geometric analysis of its
                   optimization landscapes, to efficient optimization
                   algorithms for solving the associated nonconvex optimization
                   problem, to applications in machine intelligence,
                   representation learning, and imaging sciences. Finally, we
                   conclude this review by pointing out several interesting
                   open problems for future research.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2001.06970"
}

@ARTICLE{Shi2021-jl,
  title         = "Optimal Pose and Shape Estimation for Category-level {3D}
                   Object Perception",
  author        = "Shi, Jingnan and Yang, Heng and Carlone, Luca",
  abstract      = "We consider a category-level perception problem, where one
                   is given 3D sensor data picturing an object of a given
                   category (e.g. a car), and has to reconstruct the pose and
                   shape of the object despite intra-class variability (i.e.
                   different car models have different shapes). We consider an
                   active shape model, where -- for an object category -- we
                   are given a library of potential CAD models describing
                   objects in that category, and we adopt a standard
                   formulation where pose and shape estimation are formulated
                   as a non-convex optimization. Our first contribution is to
                   provide the first certifiably optimal solver for pose and
                   shape estimation. In particular, we show that rotation
                   estimation can be decoupled from the estimation of the
                   object translation and shape, and we demonstrate that (i)
                   the optimal object rotation can be computed via a tight
                   (small-size) semidefinite relaxation, and (ii) the
                   translation and shape parameters can be computed in
                   closed-form given the rotation. Our second contribution is
                   to add an outlier rejection layer to our solver, hence
                   making it robust to a large number of misdetections. Towards
                   this goal, we wrap our optimal solver in a robust estimation
                   scheme based on graduated non-convexity. To further enhance
                   robustness to outliers, we also develop the first
                   graph-theoretic formulation to prune outliers in
                   category-level perception, which removes outliers via convex
                   hull and maximum clique computations; the resulting approach
                   is robust to 70\%-90\% outliers. Our third contribution is
                   an extensive experimental evaluation. Besides providing an
                   ablation study on a simulated dataset and on the PASCAL3D+
                   dataset, we combine our solver with a deep-learned keypoint
                   detector, and show that the resulting approach improves over
                   the state of the art in vehicle pose estimation in the
                   ApolloScape datasets.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2104.08383"
}

@ARTICLE{Rosen2016-rw,
  title         = "{SE-Sync}: A Certifiably Correct Algorithm for
                   Synchronization over the Special Euclidean Group",
  author        = "Rosen, David M and Carlone, Luca and Bandeira, Afonso S and
                   Leonard, John J",
  abstract      = "Many important geometric estimation problems take the form
                   of synchronization over the special Euclidean group:
                   estimate the values of a set of poses given a set of
                   relative measurements between them. This problem is
                   typically formulated as a nonconvex maximum-likelihood
                   estimation that is computationally hard to solve in general.
                   Nevertheless, in this paper we present an algorithm that is
                   able to efficiently recover certifiably globally optimal
                   solutions of the special Euclidean synchronization problem
                   in a non-adversarial noise regime. The crux of our approach
                   is the development of a semidefinite relaxation of the
                   maximum-likelihood estimation whose minimizer provides an
                   exact MLE so long as the magnitude of the noise corrupting
                   the available measurements falls below a certain critical
                   threshold; furthermore, whenever exactness obtains, it is
                   possible to verify this fact a posteriori, thereby
                   certifying the optimality of the recovered estimate. We
                   develop a specialized optimization scheme for solving
                   large-scale instances of this relaxation by exploiting its
                   low-rank, geometric, and graph-theoretic structure to reduce
                   it to an equivalent optimization problem on a
                   low-dimensional Riemannian manifold, and design a
                   truncated-Newton trust-region method to solve this reduction
                   efficiently. Finally, we combine this fast optimization
                   approach with a simple rounding procedure to produce our
                   algorithm, SE-Sync. Experimental evaluation on a variety of
                   simulated and real-world pose-graph SLAM datasets shows that
                   SE-Sync is able to recover certifiably globally optimal
                   solutions when the available measurements are corrupted by
                   noise up to an order of magnitude greater than that
                   typically encountered in robotics and computer vision
                   applications, and does so more than an order of magnitude
                   faster than the Gauss-Newton-based approach that forms the
                   basis of current state-of-the-art techniques.",
  month         =  dec,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1612.07386"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gorski2007-hk,
  title     = "Biconvex sets and optimization with biconvex functions: a survey
               and extensions",
  author    = "Gorski, J and Pfeuffer, F and Klamroth, K",
  abstract  = "… We review theoretical results for biconvex sets and biconvex
               functions and survey existing methods and results for general
               biconvex optimization problems. We recall that a set S ⊆ Rk …",
  journal   = "Math. Methods Oper. Res.",
  publisher = "Springer",
  year      =  2007
}

@ARTICLE{Dupuis2019-ub,
  title         = "The Geometry of Sparse Analysis Regularization",
  author        = "Dupuis, Xavier and Vaiter, Samuel",
  abstract      = "Analysis sparsity is a common prior in inverse problem or
                   machine learning including special cases such as Total
                   Variation regularization, Edge Lasso and Fused Lasso. We
                   study the geometry of the solution set (a polyhedron) of the
                   analysis l1-regularization (with l2 data fidelity term) when
                   it is not reduced to a singleton without any assumption of
                   the analysis dictionary nor the degradation operator. In
                   contrast with most theoretical work, we do not focus on
                   giving uniqueness and/or stability results, but rather
                   describe a worst-case scenario where the solution set can be
                   big in terms of dimension. Leveraging a fine analysis of the
                   sub-level set of the regularizer itself, we draw a
                   connection between support of a solution and the minimal
                   face containing it, and in particular prove that extremal
                   points can be recovered thanks to an algebraic test.
                   Moreover, we draw a connection between the sign pattern of a
                   solution and the ambient dimension of the smallest face
                   containing it. Finally, we show that any arbitrary
                   sub-polyhedra of the level set can be seen as a solution set
                   of sparse analysis regularization with explicit parameters.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1907.01769"
}

@ARTICLE{Hong2014-wu,
  title    = "Optimality conditions for the nonlinear programming problems on
              Riemannian manifolds",
  author   = "Hong, Wei and Zhang, Lei-Hong and Song, Ruyi",
  abstract = "In recent years, many traditional optimization methods have been
              successfully generalized to minimize objective functions on
              manifolds. In this paper, we first extend the general traditional
              constrained optimization problem to a nonlinear programming
              problem built upon a general Riemannian manifold M , and discuss
              the first-order and the second-order optimality conditions. By
              exploiting the differential geometry structure of the underlying
              manifold M , we show that, in the language of differential
              geometry, the first-order and the second-order optimality
              conditions of the nonlinear programming problem on M coincide
              with the traditional optimality conditions. When the objective
              function is nonsmooth Lipschitz continuous, we extend the Clarke
              generalized gradient, tangent and normal cone, and establish the
              first-order optimality conditions. For the case when M is an
              embedded submanifold of R m , formed by a set of equality
              constraints, we show that the optimality conditions can be
              derived directly from the traditional results on R m .",
  journal  = "Pac. J. Optim.",
  year     =  2014
}

@ARTICLE{Tron2017-jw,
  title     = "The Space of Essential Matrices as a Riemannian Quotient
               Manifold",
  author    = "Tron, Roberto and Daniilidis, Kostas",
  abstract  = "The essential matrix, which encodes the epipolar constraint
               between points in two projective views, is a cornerstone of
               modern computer vision. Previous works have proposed different
               characterizations of the space of essential matrices as a
               Riemannian manifold. However, they either do not consider the
               symmetric role played by the two views or do not fully take into
               account the geometric peculiarities of the epipolar constraint.
               We address these limitations with a characterization as a
               quotient manifold that can be easily interpreted in terms of
               camera poses. While our main focus is on theoretical aspects, we
               include applications to optimization problems in computer
               vision.",
  journal   = "SIAM J. Imaging Sci.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  10,
  number    =  3,
  pages     = "1416--1445",
  month     =  jan,
  year      =  2017
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Serych2016-cb,
  title     = "Fast L1-based {RANSAC} for homography estimation",
  booktitle = "21st Computer Vision Winter Workshop",
  author    = "Serych, Jonas and Matas, Jiˇri and Drbohlav, Ondˇrej",
  year      =  2016
}

@ARTICLE{Beinert2021-dz,
  title    = "Robust {PCA} via Regularized Reaper with a {Matrix-Free} Proximal
              Algorithm",
  author   = "Beinert, Robert and Steidl, Gabriele",
  abstract = "Principal component analysis (PCA) is known to be sensitive to
              outliers, so that various robust PCA variants were proposed in
              the literature. A recent model, called reaper, aims to find the
              principal components by solving a convex optimization problem.
              Usually the number of principal components must be determined in
              advance and the minimization is performed over symmetric positive
              semi-definite matrices having the size of the data, although the
              number of principal components is substantially smaller. This
              prohibits its use if the dimension of the data is large which is
              often the case in image processing. In this paper, we propose a
              regularized version of reaper which enforces the sparsity of the
              number of principal components by penalizing the nuclear norm of
              the corresponding orthogonal projector. If only an upper bound on
              the number of principal components is available, our approach can
              be combined with the L-curve method to reconstruct the
              appropriate subspace. Our second contribution is a matrix-free
              algorithm to find a minimizer of the regularized reaper which is
              also suited for high-dimensional data. The algorithm couples a
              primal-dual minimization approach with a thick-restarted Lanczos
              process. This appears to be the first efficient convex
              variational method for robust PCA that can handle
              high-dimensional data. As a side result, we discuss the topic of
              the bias in robust PCA. Numerical examples demonstrate the
              performance of our algorithm.",
  journal  = "J. Math. Imaging Vis.",
  volume   =  63,
  number   =  5,
  pages    = "626--649",
  month    =  feb,
  year     =  2021,
  keywords = "Matrix-free PCA; PCA offset; Regularized reaper; Robust PCA;
              Thick-restarted Lanczos algorithm",
  language = "en"
}

@MISC{Sharma2016-sl,
  title        = "Optimization with Birkhoff Polytopes",
  author       = "Sharma, Utkarsh and Pillai, Harish K and Ee, Iit",
  abstract     = "We study the characterization of families of parallel planes
                  based on their intersections with the Birkhoff polytope. We
                  use two methods to this end. The first is by reembedding the
                  polytope in a Euclidean space of the same number of
                  dimensions as the dimensionality of the polytope. The second
                  is by an analysis of projections of the polytope into an
                  appropriate Euclidean space, such that the projection
                  completely characterizes the polytope.",
  year         =  2016,
  howpublished = "\url{https://d1wqtxts1xzle7.cloudfront.net/50271212/OptimizationWithBirkhoffPolytopes-with-cover-page-v2.pdf?Expires=1665444313&Signature=EWtht9EZVCqMyQdtNKMRhZ0q8BGG7fbTMOSWMfjOK9b6HI0bMlTGCmx2OVKbwg-dATM3k-0Cz08O5iIWKLtQaPjiO8lmOpx4ABuCY9x6mR43kUPQf8twOGmbvrnogPUL-BkBqmHjXn327RE7ob6QducLfm4PMF8oamURenTRJllhaXCyULKSWuXzoKgRyxE2ON0KSYjYoPOluMkKQqFS4WKQ3NfkMx9i2pa~KjXUoIu6jQmisTOm6PrZF0PDJ~buqYlFScRwfV6go2pdV7C4tvpeQZ9yPIxHr9rGSMIQaUsXs7HYZNbrZa52LWY8uTFCIc-QKWRGUm~U8G3Rq-P2kQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA}",
  note         = "Accessed: 2022-10-10"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Davis2018-jx,
  title    = "Pattern-avoiding polytopes",
  author   = "Davis, Robert and Sagan, Bruce",
  abstract = "Two well-known polytopes whose vertices are indexed by
              permutations in the symmetric group Sn are the permutohedron Pn
              and the Birkhoff polytope Bn. We consider polytopes Pn($\Pi$) and
              Bn($\Pi$), whose vertices correspond to the permutations in Sn
              avoiding a set of patterns $\Pi$. For various choices of $\Pi$,
              we explore the Ehrhart polynomials and h∗-vectors of these
              polytopes as well as other aspects of their combinatorial
              structure. For Pn($\Pi$), we consider all subsets $\Pi$⊆S3 and
              are able to provide results in most cases. To illustrate,
              Pn(123,132) is a Pitman--Stanley polytope, the number of interior
              lattice points in Pn(132,312) is a derangement number, and the
              normalized volume of Pn(123,231,312) is the number of trees on n
              vertices. The polytopes Bn($\Pi$) seem much more difficult to
              analyze, so we focus on four particular choices of $\Pi$. First
              we show that the Bn(231,321) is exactly the Chan--Robbins--Yuen
              polytope. Next we prove that for any $\Pi$ containing \{123,312\}
              we have h∗(Bn($\Pi$))=1. Finally, we study Bn(132,312) and
              B˜n(123), where the tilde indicates that we choose vertices
              corresponding to alternating permutations avoiding the pattern
              123. In both cases we use order complexes of posets and
              techniques from toric algebra to construct regular, unimodular
              triangulations of the polytopes. The posets involved turn out to
              be isomorphic to the lattices of Young diagrams contained in a
              certain shape, and this permits us to give an exact expression
              for the normalized volumes of the corresponding polytopes via the
              hook formula. Finally, Stanley's theory of
              (P,$\omega$)-partitions allows us to show that their h∗-vectors
              are symmetric and unimodal. Various questions and conjectures are
              presented throughout.",
  journal  = "European J. Combin.",
  volume   =  74,
  pages    = "48--84",
  month    =  dec,
  year     =  2018
}

@ARTICLE{Brualdi1977-yg,
  title   = "Convex polyhedra of doubly stochastic matrices. I. Applications of
             the permanent function",
  author  = "Brualdi, Richard A and Gibson, Peter M",
  journal = "Journal of Combinatorial Theory, Series A",
  year    =  1977
}

@ARTICLE{De_Loera2009-pq,
  title    = "A generating function for all semi-magic squares and the volume
              of the Birkhoff polytope",
  author   = "De Loera, J A and Liu, F and Yoshida, R",
  abstract = "We present a multivariate generating function for all n$\times$n
              nonnegative integral matrices with all row and column sums equal
              to a positive integer t, the so called semi-magic squares. As a
              consequence we obtain formulas for all coefficients of the
              Ehrhart polynomial of the polytope Bnof n$\times$n
              doubly-stochastic matrices, also known as the Birkhoff polytope.
              In particular we derive formulas for the volumes of Bnand any of
              its faces.",
  journal  = "J. Algebraic Combin.",
  volume   =  30,
  number   =  1,
  pages    = "113--139",
  month    =  aug,
  year     =  2009
}

@ARTICLE{Aytekin2022-mp,
  title         = "Neural Networks are Decision Trees",
  author        = "Aytekin, Caglar",
  abstract      = "In this manuscript, we show that any feedforward neural
                   network having piece-wise linear activation functions can be
                   represented as a decision tree. The representation is
                   equivalence and not an approximation, thus keeping the
                   accuracy of the neural network exactly as is. We believe
                   that this work paves the way to tackle the black-box nature
                   of neural networks. We share equivalent trees of some neural
                   networks and show that besides providing interpretability,
                   tree representation can also achieve some computational
                   advantages. The analysis holds both for fully connected and
                   convolutional networks, which may or may not also include
                   skip connections and/or normalizations.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2210.05189"
}

@ARTICLE{Landa2022-mq,
  title         = "Robust Inference of Manifold Density and Geometry by Doubly
                   Stochastic Scaling",
  author        = "Landa, Boris and Cheng, Xiuyuan",
  abstract      = "The Gaussian kernel and its traditional normalizations
                   (e.g., row-stochastic) are popular approaches for assessing
                   similarities between data points, commonly used for manifold
                   learning and clustering, as well as supervised and
                   semi-supervised learning on graphs. In many practical
                   situations, the data can be corrupted by noise that
                   prohibits traditional affinity matrices from correctly
                   assessing similarities, especially if the noise magnitudes
                   vary considerably across the data, e.g., under
                   heteroskedasticity or outliers. An alternative approach that
                   provides a more stable behavior under noise is the doubly
                   stochastic normalization of the Gaussian kernel. In this
                   work, we investigate this normalization in a setting where
                   points are sampled from an unknown density on a
                   low-dimensional manifold embedded in high-dimensional space
                   and corrupted by possibly strong, non-identically
                   distributed, sub-Gaussian noise. We establish the pointwise
                   concentration of the doubly stochastic affinity matrix and
                   its scaling factors around certain population forms. We then
                   utilize these results to develop several tools for robust
                   inference. First, we derive a robust density estimator that
                   can substantially outperform the standard kernel density
                   estimator under high-dimensional noise. Second, we provide
                   estimators for the pointwise noise magnitudes, the pointwise
                   signal magnitudes, and the pairwise Euclidean distances
                   between clean data points. Lastly, we derive robust graph
                   Laplacian normalizations that approximate popular manifold
                   Laplacians, including the Laplace Beltrami operator, showing
                   that the local geometry of the manifold can be recovered
                   under high-dimensional noise. We exemplify our results in
                   simulations and on real single-cell RNA-sequencing data. In
                   the latter, we show that our proposed normalizations are
                   robust to technical variability associated with different
                   cell types.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "2209.08004"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Knuth_undated-js,
  title        = "Mathematical Writing",
  author       = "Knuth, Donald E and Larrabee, Tracy and Roberts, Paul M",
  abstract     = "This report is based on a course of the same name given at
                  Stanford University during autumn quarter, 1987. Here's the
                  catalog description: CS 209. Mathematical Writing-Issues of
                  technical writing and the effective presentation of
                  mathematics and computer science. Preparation of theses,
                  papers, books, and ``literate'' computer programs. A term
                  paper on a topic of your choice; this paper may be used for
                  credit in another course. The first three lectures were a
                  ``minicourse'' that summarized the basics. About two hundred
                  people attended those three sessions, which were devoted
                  primarily to a discussion of the points in §1 of this report.
                  An exercise (§2) and a suggested solution (§3) were also part
                  of the minicourse. The remaining 28 lectures covered these
                  and other issues in depth. We saw many examples of ``before''
                  and ``after'' from manuscripts in progress. We learned how to
                  avoid excessive subscripts and superscripts. We discussed the
                  documentation of algorithms, computer programs, and user
                  manuals. We considered the process of refereeing and editing.
                  We studied how to make effective diagrams and tables, and how
                  to find appropriate quotations to spice up a text. Some of
                  the material duplicated some of what would be discussed in
                  writing classes offered by the English department, but the
                  vast majority of the lectures were devoted to issues that are
                  specific to mathematics and/or computer science.",
  howpublished = "\url{https://jmlr.csail.mit.edu/reviewing-papers/knuth_mathematical_writing.pdf}",
  note         = "Accessed: 2022-10-14"
}

@MISC{Zhou_undated-hl,
  title       = "{IBD}: {IBD}: Interpretable Basis Decomposition for Visual
                 Explanation",
  author      = "Zhou, Bolei and Sun, Yiyou and Bau, David and Torralba,
                 Antonio",
  abstract    = "IBD: Interpretable Basis Decomposition for Visual Explanation
                 - GitHub - CSAILVision/IBD: IBD: Interpretable Basis
                 Decomposition for Visual Explanation",
  institution = "Github",
  language    = "en"
}

@MISC{Lrjconan_undated-am,
  title       = "{GRBM}: {Gaussian-Bernoulli} Restricted Boltzmann Machines",
  author      = "{lrjconan}",
  abstract    = "Gaussian-Bernoulli Restricted Boltzmann Machines. Contribute
                 to DSL-Lab/GRBM development by creating an account on GitHub.",
  institution = "Github",
  language    = "en"
}

@ARTICLE{Zhou2018-oe,
  title         = "On the Fenchel Duality between Strong Convexity and
                   Lipschitz Continuous Gradient",
  author        = "Zhou, Xingyu",
  abstract      = "We provide a simple proof for the Fenchel duality between
                   strong convexity and Lipschitz continuous gradient. To this
                   end, we first establish equivalent conditions of convexity
                   for a general function that may not be differentiable. By
                   utilizing these equivalent conditions, we can directly
                   obtain equivalent conditions for strong convexity and
                   Lipschitz continuous gradient. Based on these results, we
                   can easily prove Fenchel duality. Beside this main result,
                   we also identify several conditions that are implied by
                   strong convexity or Lipschitz continuous gradient, but are
                   not necessarily equivalent to them. This means that these
                   conditions are more general than strong convexity or
                   Lipschitz continuous gradient themselves.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1803.06573"
}

@ARTICLE{Authors_undated-uz,
  title  = "{VARIATIONAL} {INFORMATION} {PURSUIT} {FOR} {INTER-} {PRETABLE}
            {PREDICTIONS}",
  author = "Authors, Anonymous"
}

@ARTICLE{Wang2022-ek,
  title    = "Disentangled Representation Learning for Recommendation",
  author   = "Wang, Xin and Chen, Hong and Zhou, Yuwei and Ma, Jianxin and Zhu,
              Wenwu",
  abstract = "There exist complex interactions among a large number of latent
              factors behind the decision making processes of different
              individuals, which drive the various user behavior patterns in
              recommender systems. These factors hidden in those diverse
              behaviors demonstrate highly entangled patterns, covering from
              high-level user intentions to low-level individual preferences.
              Uncovering the disentanglement of these latent factors can
              benefit in enhanced robustness, interpretability, and
              controllability during representation learning for
              recommendation. However, the large degree of entanglement within
              latent factors poses great challenges for learning
              representations that disentangle them, and remains largely
              unexplored. In this paper, we present the SEMantic MACRo-mIcro
              Disentangled Variational Auto-Encoder (SEM-MacridVAE) model for
              learning disentangled representations from user behaviors, taking
              item semantic information into account. Our SEM-MacridVAE model
              achieves macro disentanglement by inferring the high-level
              concepts associated with user intentions through a prototype
              routing mechanism, and guarantees the micro disentanglement
              through a micro-disentanglement regularizer stemming from an
              information-theoretic interpretation of VAEs, which forces each
              dimension of the representations to independently reflect an
              isolated low-level factor. The semantic information extracted
              from candidate items is utilized to further boost the
              recommendation performances. Empirical experiments demonstrate
              that our proposed approach is able to achieve significant
              improvement over the state-of-the-art baselines.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   = "PP",
  month    =  feb,
  year     =  2022,
  language = "en"
}

@ARTICLE{Ochs2015-ab,
  title     = "On Iteratively Reweighted Algorithms for Nonsmooth Nonconvex
               Optimization in Computer Vision",
  author    = "Ochs, Peter and Dosovitskiy, Alexey and Brox, Thomas and Pock,
               Thomas",
  abstract  = "Natural image statistics indicate that we should use nonconvex
               norms for most regularization tasks in image processing and
               computer vision. Still, they are rarely used in practice due to
               the challenge of optimization. Recently, iteratively reweighed
               $\ell_1$ minimization (IRL1) has been proposed as a way to
               tackle a class of nonconvex functions by solving a sequence of
               convex $\ell_2$-$\ell_1$ problems. We extend the problem class
               to the sum of a convex function and a (nonconvex) nondecreasing
               function applied to another convex function. The proposed
               algorithm sequentially optimizes suitably constructed convex
               majorizers. Convergence to a critical point is proved when the
               Kurdyka--?ojasiewicz property and additional mild restrictions
               hold for the objective function. The efficiency and practical
               importance of the algorithm are demonstrated in computer vision
               tasks such as image denoising and optical flow. Most
               applications seek smooth results with sharp discontinuities.
               These are achieved by combining nonconvexity with higher order
               regularization.",
  journal   = "SIAM J. Imaging Sci.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  8,
  number    =  1,
  pages     = "331--372",
  month     =  jan,
  year      =  2015
}

@ARTICLE{Lugmayr2022-zc,
  title         = "{RePaint}: Inpainting using Denoising Diffusion
                   Probabilistic Models",
  author        = "Lugmayr, Andreas and Danelljan, Martin and Romero, Andres
                   and Yu, Fisher and Timofte, Radu and Van Gool, Luc",
  abstract      = "Free-form inpainting is the task of adding new content to an
                   image in the regions specified by an arbitrary binary mask.
                   Most existing approaches train for a certain distribution of
                   masks, which limits their generalization capabilities to
                   unseen mask types. Furthermore, training with pixel-wise and
                   perceptual losses often leads to simple textural extensions
                   towards the missing areas instead of semantically meaningful
                   generation. In this work, we propose RePaint: A Denoising
                   Diffusion Probabilistic Model (DDPM) based inpainting
                   approach that is applicable to even extreme masks. We employ
                   a pretrained unconditional DDPM as the generative prior. To
                   condition the generation process, we only alter the reverse
                   diffusion iterations by sampling the unmasked regions using
                   the given image information. Since this technique does not
                   modify or condition the original DDPM network itself, the
                   model produces high-quality and diverse output images for
                   any inpainting form. We validate our method for both faces
                   and general-purpose image inpainting using standard and
                   extreme masks. RePaint outperforms state-of-the-art
                   Autoregressive, and GAN approaches for at least five out of
                   six mask distributions. Github Repository: git.io/RePaint",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2201.09865"
}

@ARTICLE{Marshall2017-cw,
  title         = "Manifold learning with bi-stochastic kernels",
  author        = "Marshall, Nicholas F and Coifman, Ronald R",
  abstract      = "In this paper we answer the following question: what is the
                   infinitesimal generator of the diffusion process defined by
                   a kernel that is normalized such that it is bi-stochastic
                   with respect to a specified measure? More precisely, under
                   the assumption that data is sampled from a Riemannian
                   manifold we determine how the resulting infinitesimal
                   generator depends on the potentially nonuniform distribution
                   of the sample points, and the specified measure for the
                   bi-stochastic normalization. In a special case, we
                   demonstrate a connection to the heat kernel. We consider
                   both the case where only a single data set is given, and the
                   case where a data set and a reference set are given. The
                   spectral theory of the constructed operators is studied, and
                   Nystr\textbackslash``om extension formulas for the gradients
                   of the eigenfunctions are computed. Applications to discrete
                   point sets and manifold learning are discussed.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1711.06711"
}

@ARTICLE{Bonnabel2011-ks,
  title         = "Stochastic gradient descent on Riemannian manifolds",
  author        = "Bonnabel, Silvere",
  abstract      = "Stochastic gradient descent is a simple approach to find the
                   local minima of a cost function whose evaluations are
                   corrupted by noise. In this paper, we develop a procedure
                   extending stochastic gradient descent algorithms to the case
                   where the function is defined on a Riemannian manifold. We
                   prove that, as in the Euclidian case, the gradient descent
                   algorithm converges to a critical point of the cost
                   function. The algorithm has numerous potential applications,
                   and is illustrated here by four examples. In particular a
                   novel gossip algorithm on the set of covariance matrices is
                   derived and tested numerically.",
  month         =  nov,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1111.5280"
}

@ARTICLE{Arias-Castro2017-tt,
  title         = "{RANSAC} Algorithms for Subspace Recovery and Subspace
                   Clustering",
  author        = "Arias-Castro, Ery and Wang, Jue",
  abstract      = "We consider the RANSAC algorithm in the context of subspace
                   recovery and subspace clustering. We derive some theory and
                   perform some numerical experiments. We also draw some
                   correspondences with the methods of Hardt and Moitra (2013)
                   and Chen and Lerman (2009b).",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "1711.11220"
}

@ARTICLE{Fischler1981-pr,
  title     = "Random sample consensus: a paradigm for model fitting with
               applications to image analysis and automated cartography",
  author    = "Fischler, Martin A and Bolles, Robert C",
  journal   = "Communications of the ACM",
  publisher = "ACM New York, NY, USA",
  volume    =  24,
  pages     = "381--395",
  year      =  1981
}

@INPROCEEDINGS{Chan_undated-ri,
  title       = "{VARIATIONAL} {INFORMATION} {PURSUIT} {FOR} {INTER-PRETABLE}
                 {P} {REDICTIONS}",
  author      = "Chan, Ryan",
  abstract    = "Official Implementation for Variational Information Pursuit
                 for Interpretable Predictions (ICLR 2023) -
                 ryanchankh/VariationalInformationPursuit: Official
                 Implementation for Variational Information Pursuit for
                 Interpretable Predictions (ICLR 2023)",
  institution = "Github",
  language    = "en"
}

@INPROCEEDINGS{Li_undated-rq,
  title       = "Scaling {Language-Image} Pre-training via Masking",
  author      = "Li, Yanghao and Fan, Haoqi and Hu, Ronghang and Feichtenhofer,
                 Christoph and He, Kaiming",
  abstract    = "Official Open Source code for ``Scaling Language-Image
                 Pre-training via Masking'' - GitHub - facebookresearch/flip:
                 Official Open Source code for ``Scaling Language-Image
                 Pre-training v...",
  institution = "Github",
  language    = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Michelot1986-gp,
  title    = "A finite algorithm for finding the projection of a point onto the
              canonical simplex of ∝n",
  author   = "Michelot, C",
  abstract = "An algorithm of successive location of the solution is developed
              for the problem of finding the projection of a point onto the
              canonical simplex in the Euclidean space ℝn. This algorithm
              converges in a finite number of steps. Each iteration consists in
              finding the projection of a point onto an affine subspace and
              requires only explicit and very simple computations.",
  journal  = "J. Optim. Theory Appl.",
  volume   =  50,
  number   =  1,
  pages    = "195--200",
  month    =  jul,
  year     =  1986
}

@ARTICLE{Lu2017-wi,
  title         = "The expressive power of neural networks: A view from the
                   width",
  author        = "Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu,
                   Zhiqiang and Wang, Liwei",
  abstract      = "The expressive power of neural networks is important for
                   understanding deep learning. Most existing works consider
                   this problem from the view of the depth of a network. In
                   this paper, we study how width affects the expressiveness of
                   neural networks. Classical results state that depth-bounded
                   (e.g. depth-$2$) networks with suitable activation functions
                   are universal approximators. We show a universal
                   approximation theorem for width-bounded ReLU networks:
                   width-$(n+4)$ ReLU networks, where $n$ is the input
                   dimension, are universal approximators. Moreover, except for
                   a measure zero set, all functions cannot be approximated by
                   width-$n$ ReLU networks, which exhibits a phase transition.
                   Several recent works demonstrate the benefits of depth by
                   proving the depth-efficiency of neural networks. That is,
                   there are classes of deep networks which cannot be realized
                   by any shallow network whose size is no more than an
                   exponential bound. Here we pose the dual question on the
                   width-efficiency of ReLU networks: Are there wide networks
                   that cannot be realized by narrow networks whose size is not
                   substantially larger? We show that there exist classes of
                   wide networks which cannot be realized by any narrow network
                   whose depth is no more than a polynomial bound. On the other
                   hand, we demonstrate by extensive experiments that narrow
                   networks whose size exceed the polynomial bound by a
                   constant factor can approximate wide and shallow network
                   with high accuracy. Our results provide more comprehensive
                   evidence that depth is more effective than width for the
                   expressiveness of ReLU networks.",
  month         =  sep,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1709.02540"
}

@ARTICLE{Theis2015-fn,
  title         = "A note on the evaluation of generative models",
  author        = "Theis, Lucas and van den Oord, A{\"a}ron and Bethge,
                   Matthias",
  abstract      = "Probabilistic generative models can be used for compression,
                   denoising, inpainting, texture synthesis, semi-supervised
                   learning, unsupervised feature learning, and other tasks.
                   Given this wide range of applications, it is not surprising
                   that a lot of heterogeneity exists in the way these models
                   are formulated, trained, and evaluated. As a consequence,
                   direct comparison between models is often difficult. This
                   article reviews mostly known but often underappreciated
                   properties relating to the evaluation and interpretation of
                   generative models with a focus on image models. In
                   particular, we show that three of the currently most
                   commonly used criteria---average log-likelihood, Parzen
                   window estimates, and visual fidelity of samples---are
                   largely independent of each other when the data is
                   high-dimensional. Good performance with respect to one
                   criterion therefore need not imply good performance with
                   respect to the other criteria. Our results show that
                   extrapolation from one criterion to another is not warranted
                   and generative models need to be evaluated directly with
                   respect to the application(s) they were intended for. In
                   addition, we provide examples demonstrating that Parzen
                   window estimates should generally be avoided.",
  month         =  nov,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1511.01844"
}

@ARTICLE{Theis2015-hq,
  title         = "A note on the evaluation of generative models",
  author        = "Theis, Lucas and van den Oord, A{\"a}ron and Bethge,
                   Matthias",
  abstract      = "Probabilistic generative models can be used for compression,
                   denoising, inpainting, texture synthesis, semi-supervised
                   learning, unsupervised feature learning, and other tasks.
                   Given this wide range of applications, it is not surprising
                   that a lot of heterogeneity exists in the way these models
                   are formulated, trained, and evaluated. As a consequence,
                   direct comparison between models is often difficult. This
                   article reviews mostly known but often underappreciated
                   properties relating to the evaluation and interpretation of
                   generative models with a focus on image models. In
                   particular, we show that three of the currently most
                   commonly used criteria---average log-likelihood, Parzen
                   window estimates, and visual fidelity of samples---are
                   largely independent of each other when the data is
                   high-dimensional. Good performance with respect to one
                   criterion therefore need not imply good performance with
                   respect to the other criteria. Our results show that
                   extrapolation from one criterion to another is not warranted
                   and generative models need to be evaluated directly with
                   respect to the application(s) they were intended for. In
                   addition, we provide examples demonstrating that Parzen
                   window estimates should generally be avoided.",
  month         =  nov,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1511.01844"
}

@ARTICLE{Cai2018-qx,
  title     = "Data recovery on a manifold from linear samples: theory and
               computation",
  author    = "Cai, Jian-Feng and Rong, Yi and Wang, Yang and Xu, Zhiqiang",
  journal   = "Ann. Acad. Rom. Sci. Ser. Math. Appl.",
  publisher = "International Press of Boston",
  volume    =  3,
  number    =  1,
  pages     = "337--365",
  year      =  2018
}

@ARTICLE{Shinde2020-ia,
  title         = "Memory-efficient structured convex optimization via extreme
                   point sampling",
  author        = "Shinde, Nimita and Narayanan, Vishnu and Saunderson, James",
  abstract      = "Memory is a key computational bottleneck when solving
                   large-scale convex optimization problems such as
                   semidefinite programs (SDPs). In this paper, we focus on the
                   regime in which storing an $n\times n$ matrix decision
                   variable is prohibitive. To solve SDPs in this regime, we
                   develop a randomized algorithm that returns a random vector
                   whose covariance matrix is near-feasible and near-optimal
                   for the SDP. We show how to develop such an algorithm by
                   modifying the Frank-Wolfe algorithm to systematically
                   replace the matrix iterates with random vectors. As an
                   application of this approach, we show how to implement the
                   Goemans-Williamson approximation algorithm for
                   \textbackslashtextsc\{MaxCut\} using $\mathcal\{O\}(n)$
                   memory in addition to the memory required to store the
                   problem instance. We then extend our approach to deal with a
                   broader range of structured convex optimization problems,
                   replacing decision variables with random extreme points of
                   the feasible region.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2006.10945"
}

@ARTICLE{Cohen2018-ni,
  title         = "A General Theory of Equivariant {CNNs} on Homogeneous Spaces",
  author        = "Cohen, Taco and Geiger, Mario and Weiler, Maurice",
  abstract      = "We present a general theory of Group equivariant
                   Convolutional Neural Networks (G-CNNs) on homogeneous spaces
                   such as Euclidean space and the sphere. Feature maps in
                   these networks represent fields on a homogeneous base space,
                   and layers are equivariant maps between spaces of fields.
                   The theory enables a systematic classification of all
                   existing G-CNNs in terms of their symmetry group, base
                   space, and field type. We also consider a fundamental
                   question: what is the most general kind of equivariant
                   linear map between feature spaces (fields) of given types?
                   Following Mackey, we show that such maps correspond
                   one-to-one with convolutions using equivariant kernels, and
                   characterize the space of such kernels.",
  month         =  nov,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1811.02017"
}

@ARTICLE{Hutchinson2020-ae,
  title         = "{LieTransformer}: Equivariant self-attention for Lie Groups",
  author        = "Hutchinson, Michael and Le Lan, Charline and Zaidi,
                   Sheheryar and Dupont, Emilien and Teh, Yee Whye and Kim,
                   Hyunjik",
  abstract      = "Group equivariant neural networks are used as building
                   blocks of group invariant neural networks, which have been
                   shown to improve generalisation performance and data
                   efficiency through principled parameter sharing. Such works
                   have mostly focused on group equivariant convolutions,
                   building on the result that group equivariant linear maps
                   are necessarily convolutions. In this work, we extend the
                   scope of the literature to self-attention, that is emerging
                   as a prominent building block of deep learning models. We
                   propose the LieTransformer, an architecture composed of
                   LieSelfAttention layers that are equivariant to arbitrary
                   Lie groups and their discrete subgroups. We demonstrate the
                   generality of our approach by showing experimental results
                   that are competitive to baseline methods on a wide range of
                   tasks: shape counting on point clouds, molecular property
                   regression and modelling particle trajectories under
                   Hamiltonian dynamics.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2012.10885"
}

@INPROCEEDINGS{Rifai2011-tz,
  title     = "Contractive auto-encoders: explicit invariance during feature
               extraction",
  booktitle = "Proceedings of the 28th International Conference on
               International Conference on Machine Learning",
  author    = "Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot,
               Xavier and Bengio, Yoshua",
  abstract  = "We present in this paper a novel approach for training
               deterministic auto-encoders. We show that by adding a well
               chosen penalty term to the classical reconstruction cost
               function, we can achieve results that equal or surpass those
               attained by other regularized auto-encoders as well as denoising
               auto-encoders on a range of datasets. This penalty term
               corresponds to the Frobenius norm of the Jacobian matrix of the
               encoder activations with respect to the input. We show that this
               penalty term results in a localized space contraction which in
               turn yields robust features on the activation layer.
               Furthermore, we show how this penalty term is related to both
               regularized auto-encoders and denoising auto-encoders and how it
               can be seen as a link between deterministic and
               non-deterministic auto-encoders. We find empirically that this
               penalty helps to carve a representation that better captures the
               local directions of variation dictated by the data,
               corresponding to a lower-dimensional non-linear manifold, while
               being more invariant to the vast majority of directions
               orthogonal to the manifold. Finally, we show that by using the
               learned features to initialize a MLP, we achieve state of the
               art classification error on a range of datasets, surpassing
               other methods of pretraining.",
  publisher = "Omnipress",
  pages     = "833--840",
  series    = "ICML'11",
  month     =  jun,
  year      =  2011,
  address   = "Madison, WI, USA",
  location  = "Bellevue, Washington, USA"
}

@ARTICLE{Cohen2019-dd,
  title         = "Certified Adversarial Robustness via Randomized Smoothing",
  author        = "Cohen, Jeremy M and Rosenfeld, Elan and Zico Kolter, J",
  abstract      = "We show how to turn any classifier that classifies well
                   under Gaussian noise into a new classifier that is
                   certifiably robust to adversarial perturbations under the
                   $\ell_2$ norm. This ``randomized smoothing'' technique has
                   been proposed recently in the literature, but existing
                   guarantees are loose. We prove a tight robustness guarantee
                   in $\ell_2$ norm for smoothing with Gaussian noise. We use
                   randomized smoothing to obtain an ImageNet classifier with
                   e.g. a certified top-1 accuracy of 49\% under adversarial
                   perturbations with $\ell_2$ norm less than 0.5 (=127/255).
                   No certified defense has been shown feasible on ImageNet
                   except for smoothing. On smaller-scale datasets where
                   competing approaches to certified $\ell_2$ robustness are
                   viable, smoothing delivers higher certified accuracies. Our
                   strong empirical results suggest that randomized smoothing
                   is a promising direction for future research into
                   adversarially robust classification. Code and models are
                   available at http://github.com/locuslab/smoothing.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1902.02918"
}

@ARTICLE{Wei2016-pf,
  title     = "Guarantees of Riemannian Optimization for Low Rank Matrix
               Recovery",
  author    = "Wei, Ke and Cai, Jian-Feng and Chan, Tony F and Leung, Shingyu",
  abstract  = "We establish theoretical recovery guarantees of a family of
               Riemannian optimization algorithms for low rank matrix recovery,
               which is about recovering an $m\times n$ rank $r$ matrix from $p
               < mn$ number of linear measurements. The algorithms are first
               interpreted as iterative hard thresholding algorithms with
               subspace projections. Based on this connection, we show that
               provided the restricted isometry constant $R_\{3r\}$ of the
               sensing operator is less than $C_\kappa /\sqrt\{r\}$, the
               Riemannian gradient descent algorithm and a restarted variant of
               the Riemannian conjugate gradient algorithm are guaranteed to
               converge linearly to the underlying rank $r$ matrix if they are
               initialized by one step hard thresholding. Empirical evaluation
               shows that the algorithms are able to recover a low rank matrix
               from nearly the minimum number of measurements necessary.",
  journal   = "SIAM J. Matrix Anal. Appl.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  37,
  number    =  3,
  pages     = "1198--1222",
  month     =  jan,
  year      =  2016
}

@ARTICLE{Breiding2022-gg,
  title         = "Average degree of the essential variety",
  author        = "Breiding, Paul and Fairchild, Samantha and Santarsiero,
                   Pierpaola and Shehu, Elima",
  abstract      = "The essential variety is an algebraic subvariety of
                   dimension $5$ in real projective space
                   $\mathbb\{R\}\mathrm\{P\}^\{8\}$ which encodes the relative
                   pose of two calibrated pinhole cameras. The $5$-point
                   algorithm in computer vision computes the real points in the
                   intersection of the essential variety with a linear space of
                   codimension $5$. The degree of the essential variety is
                   $10$, so this intersection consists of 10 complex points in
                   general. We compute the expected number of real intersection
                   points when the linear space is random. We focus on two
                   probability distributions for linear spaces. The first
                   distribution is invariant under the action of the orthogonal
                   group $\mathrm\{O\}(9)$ acting on linear spaces in
                   $\mathbb\{R\}\mathrm\{P\}^\{8\}$. In this case, the expected
                   number of real intersection points is equal to $4$. The
                   second distribution is motivated from computer vision and is
                   defined by choosing 5 point correspondences in the image
                   planes $\mathbb\{R\}\mathrm\{P\}^2\times
                   \mathbb\{R\}\mathrm\{P\}^2$ uniformly at random. A Monte
                   Carlo computation suggests that with high probability the
                   expected value lies in the interval $(3.95 - 0.05,\ 3.95 +
                   0.05)$.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "math.AG",
  eprint        = "2212.01596"
}

@INPROCEEDINGS{Briales2018-bh,
  title     = "A certifiably globally optimal solution to the non-minimal
               relative pose problem",
  booktitle = "Proceedings of the {IEEE} Conference on Computer Vision and
               Pattern Recognition",
  author    = "Briales, Jesus and Kneip, Laurent and Gonzalez-Jimenez, Javier",
  pages     = "145--154",
  year      =  2018
}

@INPROCEEDINGS{Deng2009-mq,
  title     = "{ImageNet}: A large-scale hierarchical image database",
  booktitle = "2009 {IEEE} Conference on Computer Vision and Pattern
               Recognition",
  author    = "Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and
               Li, Kai and Fei-Fei, Li",
  abstract  = "The explosion of image data on the Internet has the potential to
               foster more sophisticated and robust models and algorithms to
               index, retrieve, organize and interact with images and
               multimedia data. But exactly how such data can be harnessed and
               organized remains a critical problem. We introduce here a new
               database called ``ImageNet'', a large-scale ontology of images
               built upon the backbone of the WordNet structure. ImageNet aims
               to populate the majority of the 80,000 synsets of WordNet with
               an average of 500--1000 clean and full resolution images. This
               will result in tens of millions of annotated images organized by
               the semantic hierarchy of WordNet. This paper offers a detailed
               analysis of ImageNet in its current state: 12 subtrees with 5247
               synsets and 3.2 million images in total. We show that ImageNet
               is much larger in scale and diversity and much more accurate
               than the current image datasets. Constructing such a large-scale
               database is a challenging task. We describe the data collection
               scheme with Amazon Mechanical Turk. Lastly, we illustrate the
               usefulness of ImageNet through three simple applications in
               object recognition, image classification and automatic object
               clustering. We hope that the scale, accuracy, diversity and
               hierarchical structure of ImageNet can offer unparalleled
               opportunities to researchers in the computer vision community
               and beyond.",
  pages     = "248--255",
  month     =  jun,
  year      =  2009,
  keywords  = "Large-scale systems;Image
               databases;Explosions;Internet;Robustness;Information
               retrieval;Image retrieval;Multimedia databases;Ontologies;Spine"
}

@ARTICLE{Zhou2020-ip,
  title         = "Deep Semantic Dictionary Learning for Multi-label Image
                   Classification",
  author        = "Zhou, Fengtao and Huang, Sheng and Xing, Yun",
  abstract      = "Compared with single-label image classification, multi-label
                   image classification is more practical and challenging. Some
                   recent studies attempted to leverage the semantic
                   information of categories for improving multi-label image
                   classification performance. However, these semantic-based
                   methods only take semantic information as type of
                   complements for visual representation without further
                   exploitation. In this paper, we present an innovative path
                   towards the solution of the multi-label image classification
                   which considers it as a dictionary learning task. A novel
                   end-to-end model named Deep Semantic Dictionary Learning
                   (DSDL) is designed. In DSDL, an auto-encoder is applied to
                   generate the semantic dictionary from class-level semantics
                   and then such dictionary is utilized for representing the
                   visual features extracted by Convolutional Neural Network
                   (CNN) with label embeddings. The DSDL provides a simple but
                   elegant way to exploit and reconcile the label, semantic and
                   visual spaces simultaneously via conducting the dictionary
                   learning among them. Moreover, inspired by iterative
                   optimization of traditional dictionary learning, we further
                   devise a novel training strategy named Alternately
                   Parameters Update Strategy (APUS) for optimizing DSDL, which
                   alternately optimizes the representation coefficients and
                   the semantic dictionary in forward and backward propagation.
                   Extensive experimental results on three popular benchmarks
                   demonstrate that our method achieves promising performances
                   in comparison with the state-of-the-arts. Our codes and
                   models have been released at
                   \{https://github.com/ZFT-CQU/DSDL\}.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2012.12509"
}

@INPROCEEDINGS{Peng2022-tr,
  title     = "Semidefinite Relaxations of Truncated {Least-Squares} in Robust
               Rotation Search: Tight or Not",
  booktitle = "Computer Vision -- {ECCV} 2022",
  author    = "Peng, Liangzu and Fazlyab, Mahyar and Vidal, Ren{\'e}",
  abstract  = "The rotation search problem aims to find a 3D rotation that best
               aligns a given number of point pairs. To induce robustness
               against outliers for rotation search, prior work considers
               truncated least-squares (TLS), which is a non-convex
               optimization problem, and its semidefinite relaxation (SDR) as a
               tractable alternative. Whether or not this SDR is theoretically
               tight in the presence of noise, outliers, or both has remained
               largely unexplored. We derive conditions that characterize the
               tightness of this SDR, showing that the tightness depends on the
               noise level, the truncation parameters of TLS, and the outlier
               distribution (random or clustered). In particular, we give a
               short proof for the tightness in the noiseless and outlier-free
               case, as opposed to the lengthy analysis of prior work.",
  publisher = "Springer Nature Switzerland",
  pages     = "673--691",
  year      =  2022
}

@ARTICLE{He2018-cg,
  title         = "Boosted sparse and low-rank tensor regression",
  author        = "He, Lifang and Chen, Kun and Xu, Wanwan and Zhou, Jiayu and
                   Wang, Fei",
  abstract      = "We propose a sparse and low-rank tensor regression model to
                   relate a univariate outcome to a feature tensor, in which
                   each unit-rank tensor from the CP decomposition of the
                   coefficient tensor is assumed to be sparse. This structure
                   is both parsimonious and highly interpretable, as it implies
                   that the outcome is related to the features through a few
                   distinct pathways, each of which may only involve subsets of
                   feature dimensions. We take a divide-and-conquer strategy to
                   simplify the task into a set of sparse unit-rank tensor
                   regression problems. To make the computation efficient and
                   scalable, for the unit-rank tensor regression, we propose a
                   stagewise estimation procedure to efficiently trace out its
                   entire solution path. We show that as the step size goes to
                   zero, the stagewise solution paths converge exactly to those
                   of the corresponding regularized regression. The superior
                   performance of our approach is demonstrated on various
                   real-world and synthetic examples.",
  month         =  nov,
  year          =  2018,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1811.01158"
}

@ARTICLE{Huang2019-ig,
  title         = "On Universal Features for {High-Dimensional} Learning and
                   Inference",
  author        = "Huang, Shao-Lun and Makur, Anuran and Wornell, Gregory W and
                   Zheng, Lizhong",
  abstract      = "We consider the problem of identifying universal
                   low-dimensional features from high-dimensional data for
                   inference tasks in settings involving learning. For such
                   problems, we introduce natural notions of universality and
                   we show a local equivalence among them. Our analysis is
                   naturally expressed via information geometry, and represents
                   a conceptually and computationally useful analysis. The
                   development reveals the complementary roles of the singular
                   value decomposition,
                   Hirschfeld-Gebelein-R\textbackslash'enyi maximal
                   correlation, the canonical correlation and principle
                   component analyses of Hotelling and Pearson, Tishby's
                   information bottleneck, Wyner's common information, Ky Fan
                   $k$-norms, and Brieman and Friedman's alternating
                   conditional expectations algorithm. We further illustrate
                   how this framework facilitates understanding and optimizing
                   aspects of learning systems, including multinomial logistic
                   (softmax) regression and the associated neural network
                   architecture, matrix factorization methods for collaborative
                   filtering and other applications, rank-constrained
                   multivariate linear regression, and forms of semi-supervised
                   learning.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1911.09105"
}

@INPROCEEDINGS{Elhamifar2012-ei,
  title           = "See all by looking at a few: Sparse modeling for finding
                     representative objects",
  booktitle       = "2012 {IEEE} Conference on Computer Vision and Pattern
                     Recognition",
  author          = "Elhamifar, E and Sapiro, G and Vidal, R",
  abstract        = "We consider the problem of finding a few representatives
                     for a dataset, i.e., a subset of data points that
                     efficiently describes the entire dataset. We assume that
                     each data point can be expressed as a linear combination
                     of the representatives and formulate the problem of
                     finding the representatives as a sparse multiple
                     measurement vector problem. In our formulation, both the
                     dictionary and the measurements are given by the data
                     matrix, and the unknown sparse codes select the
                     representatives via convex optimization. In general, we do
                     not assume that the data are lowrank or distributed around
                     cluster centers. When the data do come from a collection
                     of low-rank models, we show that our method automatically
                     selects a few representatives from each low-rank model. We
                     also analyze the geometry of the representatives and
                     discuss their relationship to the vertices of the convex
                     hull of the data. We show that our framework can be
                     extended to detect and reject outliers in datasets, and to
                     efficiently deal with new observations and large datasets.
                     The proposed framework and theoretical foundations are
                     illustrated with examples in video summarization and image
                     classification using representatives.",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2012,
  conference      = "2012 IEEE Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Providence, RI"
}

@MISC{Radford_undated-vc,
  title       = "{CLIP}: {CLIP} (Contrastive {Language-Image} Pretraining),
                 Predict the most relevant text snippet given an image",
  author      = "Radford, Alec and Kim, 1 Jong Wook and Hallacy, 1 Chris and
                 Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and
                 Sastry, Girish and Askell, Amanda and Mishkin, Pamela and
                 Clark, Jack and Krueger, Gretchen and Sutskever, Ilya",
  abstract    = "CLIP (Contrastive Language-Image Pretraining), Predict the
                 most relevant text snippet given an image - GitHub -
                 openai/CLIP: CLIP (Contrastive Language-Image Pretraining),
                 Predict the most relevant text snippet given an image",
  institution = "Github",
  language    = "en"
}

@ARTICLE{Schreuder2020-lj,
  title         = "Bounding the expectation of the supremum of empirical
                   processes indexed by H{\"o}lder classes",
  author        = "Schreuder, Nicolas",
  abstract      = "In this note, we provide upper bounds on the expectation of
                   the supremum of empirical processes indexed by
                   H\textbackslash``older classes of any smoothness and for any
                   distribution supported on a bounded set in $\mathbb R^d$.
                   These results can be alternatively seen as non-asymptotic
                   risk bounds, when the unknown distribution is estimated by
                   its empirical counterpart, based on $n$ independent
                   observations, and the error of estimation is quantified by
                   the integral probability metrics (IPM). In particular, the
                   IPM indexed by a H\textbackslash''older class is considered
                   and the corresponding rates are derived. These results
                   interpolate between the two well-known extreme cases: the
                   rate $n^\{-1/d\}$ corresponding to the Wassertein-1 distance
                   (the least smooth case) and the fast rate $n^\{-1/2\}$
                   corresponding to very smooth functions (for instance,
                   functions from an RKHS defined by a bounded kernel).",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "2003.13530"
}

@ARTICLE{Ergen2022-om,
  title         = "Convexifying Transformers: Improving optimization and
                   understanding of transformer networks",
  author        = "Ergen, Tolga and Neyshabur, Behnam and Mehta, Harsh",
  abstract      = "Understanding the fundamental mechanism behind the success
                   of transformer networks is still an open problem in the deep
                   learning literature. Although their remarkable performance
                   has been mostly attributed to the self-attention mechanism,
                   the literature still lacks a solid analysis of these
                   networks and interpretation of the functions learned by
                   them. To this end, we study the training problem of
                   attention/transformer networks and introduce a novel convex
                   analytic approach to improve the understanding and
                   optimization of these networks. Particularly, we first
                   introduce a convex alternative to the self-attention
                   mechanism and reformulate the regularized training problem
                   of transformer networks with our alternative convex
                   attention. Then, we cast the reformulation as a convex
                   optimization problem that is interpretable and easier to
                   optimize. Moreover, as a byproduct of our convex analysis,
                   we reveal an implicit regularization mechanism, which
                   promotes sparsity across tokens. Therefore, we not only
                   improve the optimization of attention/transformer networks
                   but also provide a solid theoretical understanding of the
                   functions learned by them. We also demonstrate the
                   effectiveness of our theory through several numerical
                   experiments.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2211.11052"
}

@ARTICLE{Vidal2008-vn,
  title     = "Multiframe motion segmentation with missing data using
               {PowerFactorization} and {GPCA}",
  author    = "Vidal, Ren{\'e} and Tron, Roberto and Hartley, Richard",
  journal   = "Int. J. Comput. Vis.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  79,
  number    =  1,
  pages     = "85--105",
  month     =  aug,
  year      =  2008,
  language  = "en"
}

@ARTICLE{Yang2019-pc,
  title         = "A Quaternion-based Certifiably Optimal Solution to the Wahba
                   Problem with Outliers",
  author        = "Yang, Heng and Carlone, Luca",
  abstract      = "The Wahba problem, also known as rotation search, seeks to
                   find the best rotation to align two sets of vector
                   observations given putative correspondences, and is a
                   fundamental routine in many computer vision and robotics
                   applications. This work proposes the first polynomial-time
                   certifiably optimal approach for solving the Wahba problem
                   when a large number of vector observations are outliers. Our
                   first contribution is to formulate the Wahba problem using a
                   Truncated Least Squares (TLS) cost that is insensitive to a
                   large fraction of spurious correspondences. The second
                   contribution is to rewrite the problem using unit
                   quaternions and show that the TLS cost can be framed as a
                   Quadratically-Constrained Quadratic Program (QCQP). Since
                   the resulting optimization is still highly non-convex and
                   hard to solve globally, our third contribution is to develop
                   a convex Semidefinite Programming (SDP) relaxation. We show
                   that while a naive relaxation performs poorly in general,
                   our relaxation is tight even in the presence of large noise
                   and outliers. We validate the proposed algorithm, named
                   QUASAR (QUAternion-based Semidefinite relAxation for Robust
                   alignment), in both synthetic and real datasets showing that
                   the algorithm outperforms RANSAC, robust local optimization
                   techniques, global outlier-removal procedures, and
                   Branch-and-Bound methods. QUASAR is able to compute
                   certifiably optimal solutions (i.e. the relaxation is exact)
                   even in the case when 95\% of the correspondences are
                   outliers.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1905.12536"
}

@ARTICLE{Karimi2016-aa,
  title         = "Linear Convergence of Gradient and {Proximal-Gradient}
                   Methods Under the {Polyak-{\L}ojasiewicz} Condition",
  author        = "Karimi, Hamed and Nutini, Julie and Schmidt, Mark",
  abstract      = "In 1963, Polyak proposed a simple condition that is
                   sufficient to show a global linear convergence rate for
                   gradient descent. This condition is a special case of the
                   \textbackslashL\{\}ojasiewicz inequality proposed in the
                   same year, and it does not require strong convexity (or even
                   convexity). In this work, we show that this much-older
                   Polyak-\textbackslashL\{\}ojasiewicz (PL) inequality is
                   actually weaker than the main conditions that have been
                   explored to show linear convergence rates without strong
                   convexity over the last 25 years. We also use the PL
                   inequality to give new analyses of randomized and greedy
                   coordinate descent methods, sign-based gradient descent
                   methods, and stochastic gradient methods in the classic
                   setting (with decreasing or constant step-sizes) as well as
                   the variance-reduced setting. We further propose a
                   generalization that applies to proximal-gradient methods for
                   non-smooth optimization, leading to simple proofs of linear
                   convergence of these methods. Along the way, we give simple
                   convergence results for a wide variety of problems in
                   machine learning: least squares, logistic regression,
                   boosting, resilient backpropagation, L1-regularization,
                   support vector machines, stochastic dual coordinate ascent,
                   and stochastic variance-reduced gradient methods.",
  month         =  aug,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1608.04636"
}

@ARTICLE{Amir2020-nz,
  title         = "The Trimmed Lasso: Sparse Recovery Guarantees and Practical
                   Optimization by the Generalized {Soft-Min} Penalty",
  author        = "Amir, Tal and Basri, Ronen and Nadler, Boaz",
  abstract      = "We present a new approach to solve the sparse approximation
                   or best subset selection problem, namely find a $k$-sparse
                   vector $\{\bf x\}\in\mathbb\{R\}^d$ that minimizes the
                   $\ell_2$ residual $\lVert A\{\bf x\}-\{\bf y\} \rVert_2$. We
                   consider a regularized approach, whereby this residual is
                   penalized by the non-convex $\textit\{trimmed lasso\}$,
                   defined as the $\ell_1$-norm of $\{\bf x\}$ excluding its
                   $k$ largest-magnitude entries. We prove that the trimmed
                   lasso has several appealing theoretical properties, and in
                   particular derive sparse recovery guarantees assuming
                   successful optimization of the penalized objective. Next, we
                   show empirically that directly optimizing this objective can
                   be quite challenging. Instead, we propose a surrogate for
                   the trimmed lasso, called the $\textit\{generalized
                   soft-min\}$. This penalty smoothly interpolates between the
                   classical lasso and the trimmed lasso, while taking into
                   account all possible $k$-sparse patterns. The generalized
                   soft-min penalty involves summation over $\binom\{d\}\{k\}$
                   terms, yet we derive a polynomial-time algorithm to compute
                   it. This, in turn, yields a practical method for the
                   original sparse approximation problem. Via simulations, we
                   demonstrate its competitive performance compared to current
                   state of the art.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2005.09021"
}

@ARTICLE{Barath2023-ju,
  title         = "A Large Scale Homography Benchmark",
  author        = "Barath, Daniel and Mishkin, Dmytro and Polic, Michal and
                   F{\"o}rstner, Wolfgang and Matas, Jiri",
  abstract      = "We present a large-scale dataset of Planes in 3D, Pi3D, of
                   roughly 1000 planes observed in 10 000 images from the 1DSfM
                   dataset, and HEB, a large-scale homography estimation
                   benchmark leveraging Pi3D. The applications of the Pi3D
                   dataset are diverse, e.g. training or evaluating monocular
                   depth, surface normal estimation and image matching
                   algorithms. The HEB dataset consists of 226 260 homographies
                   and includes roughly 4M correspondences. The homographies
                   link images that often undergo significant viewpoint and
                   illumination changes. As applications of HEB, we perform a
                   rigorous evaluation of a wide range of robust estimators and
                   deep learning-based correspondence filtering methods,
                   establishing the current state-of-the-art in robust
                   homography estimation. We also evaluate the uncertainty of
                   the SIFT orientations and scales w.r.t. the ground truth
                   coming from the underlying homographies and provide codes
                   for comparing uncertainty of custom detectors. The dataset
                   is available at
                   \textbackslashurl\{https://github.com/danini/homography-benchmark\}.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2302.09997"
}

@ARTICLE{Liu2020-ru,
  title         = "Loss landscapes and optimization in over-parameterized
                   non-linear systems and neural networks",
  author        = "Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail",
  abstract      = "The success of deep learning is due, to a large extent, to
                   the remarkable effectiveness of gradient-based optimization
                   methods applied to large neural networks. The purpose of
                   this work is to propose a modern view and a general
                   mathematical framework for loss landscapes and efficient
                   optimization in over-parameterized machine learning models
                   and systems of non-linear equations, a setting that includes
                   over-parameterized deep neural networks. Our starting
                   observation is that optimization problems corresponding to
                   such systems are generally not convex, even locally. We
                   argue that instead they satisfy PL$^*$, a variant of the
                   Polyak-Lojasiewicz condition on most (but not all) of the
                   parameter space, which guarantees both the existence of
                   solutions and efficient optimization by (stochastic)
                   gradient descent (SGD/GD). The PL$^*$ condition of these
                   systems is closely related to the condition number of the
                   tangent kernel associated to a non-linear system showing how
                   a PL$^*$-based non-linear theory parallels classical
                   analyses of over-parameterized linear equations. We show
                   that wide neural networks satisfy the PL$^*$ condition,
                   which explains the (S)GD convergence to a global minimum.
                   Finally we propose a relaxation of the PL$^*$ condition
                   applicable to ``almost'' over-parameterized systems.",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2003.00307"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{noauthor_1963-xc,
  title     = "Gradient methods for the minimisation of functionals",
  abstract  = "Let tf(t) be a functional defined in the (real) Hubert space H.
               The problem consists in finding its minimum value tff∗ = inf
               tf(x) and some minimum po…",
  journal   = "USSR Computational Mathematics and Mathematical Physics",
  publisher = "No longer published by Elsevier",
  volume    =  3,
  number    =  4,
  pages     = "864--878",
  month     =  jan,
  year      =  1963
}

@ARTICLE{Peng2012-ph,
  title    = "{RASL}: robust alignment by sparse and low-rank decomposition for
              linearly correlated images",
  author   = "Peng, Yigang and Ganesh, Arvind and Wright, John and Xu, Wenli
              and Ma, Yi",
  abstract = "This paper studies the problem of simultaneously aligning a batch
              of linearly correlated images despite gross corruption (such as
              occlusion). Our method seeks an optimal set of image domain
              transformations such that the matrix of transformed images can be
              decomposed as the sum of a sparse matrix of errors and a low-rank
              matrix of recovered aligned images. We reduce this extremely
              challenging optimization problem to a sequence of convex programs
              that minimize the sum of l1-norm and nuclear norm of the two
              component matrices, which can be efficiently solved by scalable
              convex optimization techniques. We verify the efficacy of the
              proposed robust alignment algorithm with extensive experiments on
              both controlled and uncontrolled real data, demonstrating higher
              accuracy and efficiency than existing methods over a wide range
              of realistic misalignments and corruptions.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  34,
  number   =  11,
  pages    = "2233--2246",
  month    =  nov,
  year     =  2012,
  language = "en"
}

@INPROCEEDINGS{Chen2017-ls,
  title           = "Manifold constrained low-rank decomposition",
  booktitle       = "2017 {IEEE} International Conference on Computer Vision
                     Workshops ({ICCVW})",
  author          = "Chen, Chen and Zhang, Baochang and Del Bue, Alessio and
                     Murino, Vittorio",
  abstract        = "Low-rank decomposition (LRD) is a state-of-the-art method
                     for visual data reconstruction and modelling. However, it
                     is a very challenging problem when the image data contains
                     significant occlusion, noise, illumination variation, and
                     misalignment from rotation or viewpoint changes. We
                     leverage the specific structure of data in order to
                     improve the performance of LRD when the data are not
                     ideal. To this end, we propose a new framework that embeds
                     manifold priors into LRD. To implement the framework, we
                     design an alternating direction method of multipliers
                     (ADMM) method which efficiently integrates the manifold
                     constraints during the optimization process. The proposed
                     approach is successfully used to calculate low-rank models
                     from face images, handwritten digits and planar surface
                     images. The results show a consistent increase of
                     performance when compared to the state-of-the-art over a
                     wide range of realistic image misalignments and
                     corruptions.",
  publisher       = "IEEE",
  month           =  oct,
  year            =  2017,
  conference      = "2017 IEEE International Conference on Computer Vision
                     Workshop (ICCVW)",
  location        = "Venice"
}

@ARTICLE{Sedghi2019-ez,
  title     = "Robust manifold learning via conformity pursuit",
  author    = "Sedghi, Mahlagha and Atia, George and Georgiopoulos, Michael",
  journal   = "IEEE Signal Process. Lett.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  26,
  number    =  3,
  pages     = "425--429",
  month     =  mar,
  year      =  2019,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}

@ARTICLE{Rahmani2017-cu,
  title     = "Coherence pursuit: Fast, simple, and robust principal component
               analysis",
  author    = "Rahmani, Mostafa and Atia, George K",
  journal   = "IEEE Trans. Signal Process.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  65,
  number    =  23,
  pages     = "6260--6275",
  month     =  dec,
  year      =  2017,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}

@INPROCEEDINGS{Chen2017-ta,
  title     = "{L1-2DPCA} Revisit via Optimization on Product Manifolds",
  booktitle = "2017 International Conference on Digital Image Computing:
               Techniques and Applications ({DICTA})",
  author    = "Chen, Haoran and Sun, Yanfeng and Gao, Junbin and Hu, Yongli and
               Ju, Fujiao",
  abstract  = "The PCA dimensionality reduction algorithm for 2D data with the
               Laplacian noise model, i.e., L1-2DPCA, not only preserves the
               structural relation among 2D data, but also is robust for data
               outliers. The algorithm relies on the EM algorithm with great
               computational cost. In order to learn intrinsic information more
               consistently, this paper takes a view of manifold optimization
               for the model based on the L1-norm average reconstruction error
               and develops an efficient optimization algorithm (called as
               L1-2DPCAM) on product manifolds. This way has avoided from using
               a greedy strategy employed in the existing state-of-the-art
               2DPCA algorithms. The proposed algorithm has been assessed on
               the tasks of image reconstruction and classification. The
               experimental results demonstrate that the proposed algorithm
               outperforms two state-of-the-art algorithms.",
  pages     = "1--7",
  month     =  nov,
  year      =  2017,
  keywords  = "Manifolds;Optimization;Principal component analysis;Image
               reconstruction;Two dimensional displays;Data models;Algorithm
               design and analysis"
}

@ARTICLE{Bronstein2016-eg,
  title         = "Consistent Discretization and Minimization of the {L1} Norm
                   on Manifolds",
  author        = "Bronstein, Alex and Choukroun, Yoni and Kimmel, Ron and
                   Sela, Matan",
  abstract      = "The L1 norm has been tremendously popular in signal and
                   image processing in the past two decades due to its
                   sparsity-promoting properties. More recently, its
                   generalization to non-Euclidean domains has been found
                   useful in shape analysis applications. For example, in
                   conjunction with the minimization of the Dirichlet energy,
                   it was shown to produce a compactly supported quasi-harmonic
                   orthonormal basis, dubbed as compressed manifold modes. The
                   continuous L1 norm on the manifold is often replaced by the
                   vector l1 norm applied to sampled functions. We show that
                   such an approach is incorrect in the sense that it does not
                   consistently discretize the continuous norm and warn against
                   its sensitivity to the specific sampling. We propose two
                   alternative discretizations resulting in an
                   iteratively-reweighed l2 norm. We demonstrate the proposed
                   strategy on the compressed modes problem, which reduces to a
                   sequence of simple eigendecomposition problems not requiring
                   non-convex optimization on Stiefel manifolds and producing
                   more stable and accurate results.",
  month         =  sep,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NA",
  eprint        = "1609.05434"
}

@ARTICLE{Muthukumar2022-ot,
  title         = "Adversarial robustness of sparse local Lipschitz predictors",
  author        = "Muthukumar, Ramchandran and Sulam, Jeremias",
  abstract      = "This work studies the adversarial robustness of parametric
                   functions composed of a linear predictor and a non-linear
                   representation map. Our analysis relies on sparse local
                   Lipschitzness (SLL), an extension of local Lipschitz
                   continuity that better captures the stability and reduced
                   effective dimensionality of predictors upon local
                   perturbations. SLL functions preserve a certain degree of
                   structure, given by the sparsity pattern in the
                   representation map, and include several popular hypothesis
                   classes, such as piece-wise linear models, Lasso and its
                   variants, and deep feed-forward ReLU networks. We provide a
                   tighter robustness certificate on the minimal energy of an
                   adversarial example, as well as tighter data-dependent
                   non-uniform bounds on the robust generalization error of
                   these predictors. We instantiate these results for the case
                   of deep neural networks and provide numerical evidence that
                   supports our results, shedding new insights into natural
                   regularization strategies to increase the robustness of
                   these models.",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2202.13216"
}

@ARTICLE{Golikov2022-en,
  title         = "Neural Tangent Kernel: A Survey",
  author        = "Golikov, Eugene and Pokonechnyy, Eduard and Korviakov,
                   Vladimir",
  abstract      = "A seminal work [Jacot et al., 2018] demonstrated that
                   training a neural network under specific parameterization is
                   equivalent to performing a particular kernel method as width
                   goes to infinity. This equivalence opened a promising
                   direction for applying the results of the rich literature on
                   kernel methods to neural nets which were much harder to
                   tackle. The present survey covers key results on kernel
                   convergence as width goes to infinity, finite-width
                   corrections, applications, and a discussion of the
                   limitations of the corresponding method.",
  month         =  aug,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2208.13614"
}

@ARTICLE{Costa2004-ts,
  title     = "Geodesic entropic graphs for dimension and entropy estimation in
               manifold learning",
  author    = "Costa, J A and Hero, A O",
  journal   = "IEEE Trans. Signal Process.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  52,
  number    =  8,
  pages     = "2210--2221",
  month     =  aug,
  year      =  2004
}

@INCOLLECTION{Achtert2006-re,
  title     = "Finding hierarchies of subspace clusters",
  booktitle = "Lecture Notes in Computer Science",
  author    = "Achtert, Elke and B{\"o}hm, Christian and Kriegel, Hans-Peter
               and Kr{\"o}ger, Peer and M{\"u}ller-Gorman, Ina and Zimek,
               Arthur",
  publisher = "Springer Berlin Heidelberg",
  pages     = "446--453",
  series    = "Lecture notes in computer science",
  year      =  2006,
  address   = "Berlin, Heidelberg"
}

@ARTICLE{Jalali2017-ft,
  title   = "Subspace clustering via tangent cones",
  author  = "Jalali, Amin and Willett, Rebecca",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  30,
  year    =  2017
}

@ARTICLE{Zhai2020-ve,
  title    = "Complete Dictionary Learning via {L4-Norm} Maximization over the
              Orthogonal Group",
  author   = "Zhai, Yuexiang and Yang, Zitong and Liao, Zhenyu and Wright, John
              and Ma, Yi",
  journal  = "J. Mach. Learn. Res.",
  volume   =  21,
  number   =  165,
  pages    = "1--68",
  year     =  2020
}

@MISC{Douik_undated-zt,
  title        = "Low-rank Riemannian optimization on positive semidefinite
                  stochastic matrices with applications to graph clustering",
  author       = "Douik, Ahmed and Hassibi, Babak",
  howpublished = "\url{http://proceedings.mlr.press/v80/douik18a/douik18a.pdf}",
  note         = "Accessed: 2023-1-17"
}

@INPROCEEDINGS{Paschalidou2020-pj,
  title           = "Learning unsupervised hierarchical part decomposition of
                     {3D} objects from a single {RGB} image",
  booktitle       = "2020 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "Paschalidou, Despoina and Van Gool, Luc and Geiger,
                     Andreas",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2020,
  conference      = "2020 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Seattle, WA, USA"
}

@ARTICLE{Heppert2022-oq,
  title         = "{Category-Independent} Articulated Object Tracking with
                   Factor Graphs",
  author        = "Heppert, Nick and Migimatsu, Toki and Yi, Brent and Chen,
                   Claire and Bohg, Jeannette",
  abstract      = "Robots deployed in human-centric environments may need to
                   manipulate a diverse range of articulated objects, such as
                   doors, dishwashers, and cabinets. Articulated objects often
                   come with unexpected articulation mechanisms that are
                   inconsistent with categorical priors: for example, a drawer
                   might rotate about a hinge joint instead of sliding open. We
                   propose a category-independent framework for predicting the
                   articulation models of unknown objects from sequences of
                   RGB-D images. The prediction is performed by a two-step
                   process: first, a visual perception module tracks object
                   part poses from raw images, and second, a factor graph takes
                   these poses and infers the articulation model including the
                   current configuration between the parts as a 6D twist. We
                   also propose a manipulation-oriented metric to evaluate
                   predicted joint twists in terms of how well a compliant
                   robot controller would be able to manipulate the articulated
                   object given the predicted twist. We demonstrate that our
                   visual perception and factor graph modules outperform
                   baselines on simulated data and show the applicability of
                   our factor graph on real world data.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2205.03721"
}

@ARTICLE{Franca2018-sv,
  title         = "A Nonsmooth Dynamical Systems Perspective on Accelerated
                   Extensions of {ADMM}",
  author        = "Fran{\c c}a, Guilherme and Robinson, Daniel P and Vidal,
                   Ren{\'e}",
  abstract      = "Recently, there has been great interest in connections
                   between continuous-time dynamical systems and optimization
                   methods, notably in the context of accelerated methods for
                   smooth and unconstrained problems. In this paper we extend
                   this perspective to nonsmooth and constrained problems by
                   obtaining differential inclusions associated to novel
                   accelerated variants of the alternating direction method of
                   multipliers (ADMM). Through a Lyapunov analysis, we derive
                   rates of convergence for these dynamical systems in
                   different settings that illustrate an interesting tradeoff
                   between decaying versus constant damping strategies. We also
                   obtain modified equations capturing fine-grained details of
                   these methods, which have improved stability and preserve
                   the leading order convergence rates. An extension to general
                   nonlinear equality and inequality constraints in connection
                   with singular perturbation theory is provided.",
  month         =  aug,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1808.04048"
}

@ARTICLE{Franca2021-cp,
  title    = "Gradient flows and proximal splitting methods: A unified view on
              accelerated and stochastic optimization",
  author   = "Fran{\c c}a, Guilherme and Robinson, Daniel P and Vidal, Ren{\'e}",
  abstract = "Optimization is at the heart of machine learning, statistics, and
              many applied scientific disciplines. It also has a long history
              in physics, ranging from the minimal action principle to finding
              ground states of disordered systems such as spin glasses.
              Proximal algorithms form a class of methods that are broadly
              applicable and are particularly well-suited to nonsmooth,
              constrained, large-scale, and distributed optimization problems.
              There are essentially five proximal algorithms currently known,
              each proposed in seminal work: Forward-backward splitting, Tseng
              splitting, Douglas-Rachford, alternating direction method of
              multipliers, and the more recent Davis-Yin. These methods sit on
              a higher level of abstraction compared to gradient-based ones,
              with deep roots in nonlinear functional analysis. In this paper
              we show that all of these methods are actually different
              discretizations of a single differential equation, namely, the
              simple gradient flow which dates back to Cauchy (1847). An
              important aspect behind many of the success stories in machine
              learning relies on ``accelerating'' the convergence of
              first-order methods. However, accelerated methods are notoriously
              difficult to analyze, counterintuitive, and without an underlying
              guiding principle. We show that similar discretization schemes
              applied to Newton's equation with an additional dissipative
              force, which we refer to as accelerated gradient flow, allow us
              to obtain accelerated variants of all these proximal
              algorithms-the majority of which are new although some recover
              known cases in the literature. Furthermore, we extend these
              methods to stochastic settings, allowing us to make connections
              with Langevin and Fokker-Planck equations. Similar ideas apply to
              gradient descent, heavy ball, and Nesterov's method which are
              simpler. Our results therefore provide a unified framework from
              which several important optimization methods are nothing but
              simulations of classical dissipative systems.",
  journal  = "Phys Rev E",
  volume   =  103,
  number   = "5-1",
  pages    = "053304",
  month    =  may,
  year     =  2021,
  language = "en"
}

@MISC{Anonymous_authors_undated-tp,
  title        = "Encoding recurrence into transformers",
  author       = "{Anonymous authors}",
  howpublished = "\url{https://openreview.net/pdf?id=7YfHla7IxBJ}",
  note         = "Accessed: 2023-2-17"
}

@ARTICLE{Trillos2019-do,
  title         = "Geometric structure of graph Laplacian embeddings",
  author        = "Trillos, Nicolas Garcia and Hoffmann, Franca and Hosseini,
                   Bamdad",
  abstract      = "We analyze the spectral clustering procedure for identifying
                   coarse structure in a data set $x_1, \dots, x_n$, and in
                   particular study the geometry of graph Laplacian embeddings
                   which form the basis for spectral clustering algorithms.
                   More precisely, we assume that the data is sampled from a
                   mixture model supported on a manifold $\mathcal\{M\}$
                   embedded in $\mathbb\{R\}^d$, and pick a connectivity
                   length-scale $\varepsilon>0$ to construct a kernelized graph
                   Laplacian. We introduce a notion of a well-separated mixture
                   model which only depends on the model itself, and prove that
                   when the model is well separated, with high probability the
                   embedded data set concentrates on cones that are centered
                   around orthogonal vectors. Our results are meaningful in the
                   regime where $\varepsilon = \varepsilon(n)$ is allowed to
                   decay to zero at a slow enough rate as the number of data
                   points grows. This rate depends on the intrinsic dimension
                   of the manifold on which the data is supported.",
  month         =  jan,
  year          =  2019,
  keywords      = "clustering",
  archivePrefix = "arXiv",
  primaryClass  = "math.SP",
  eprint        = "1901.10651"
}

@ARTICLE{Allen-Zhu2017-ff,
  title         = "Much Faster Algorithms for Matrix Scaling",
  author        = "Allen-Zhu, Zeyuan and Li, Yuanzhi and Oliveira, Rafael and
                   Wigderson, Avi",
  abstract      = "We develop several efficient algorithms for the classical
                   \textbackslashemph\{Matrix Scaling\} problem, which is used
                   in many diverse areas, from preconditioning linear systems
                   to approximation of the permanent. On an input $n\times n$
                   matrix $A$, this problem asks to find diagonal (scaling)
                   matrices $X$ and $Y$ (if they exist), so that $X A Y$
                   $\varepsilon$-approximates a doubly stochastic, or more
                   generally a matrix with prescribed row and column sums. We
                   address the general scaling problem as well as some
                   important special cases. In particular, if $A$ has $m$
                   nonzero entries, and if there exist $X$ and $Y$ with
                   polynomially large entries such that $X A Y$ is doubly
                   stochastic, then we can solve the problem in total
                   complexity $\tilde\{O\}(m + n^\{4/3\})$. This greatly
                   improves on the best known previous results, which were
                   either $\tilde\{O\}(n^4)$ or $O(m n^\{1/2\}/\varepsilon)$.
                   Our algorithms are based on tailor-made first and second
                   order techniques, combined with other recent advances in
                   continuous optimization, which may be of independent
                   interest for solving similar problems.",
  month         =  apr,
  year          =  2017,
  keywords      = "clustering",
  archivePrefix = "arXiv",
  primaryClass  = "cs.DS",
  eprint        = "1704.02315"
}

@ARTICLE{Idel2016-bp,
  title         = "A review of matrix scaling and Sinkhorn's normal form for
                   matrices and positive maps",
  author        = "Idel, Martin",
  abstract      = "Given a nonnegative matrix $A$, can you find diagonal
                   matrices $D_1,~D_2$ such that $D_1AD_2$ is doubly
                   stochastic? The answer to this question is known as
                   Sinkhorn's theorem. It has been proved with a wide variety
                   of methods, each presenting a variety of possible
                   generalisations. Recently, generalisations such as to
                   positive maps between matrix algebras have become more and
                   more interesting for applications. This text gives a review
                   of over 70 years of matrix scaling. The focus lies on the
                   mathematical landscape surrounding the problem and its
                   solution as well as the generalisation to positive maps and
                   contains hardly any nontrivial unpublished results.",
  month         =  sep,
  year          =  2016,
  keywords      = "clustering",
  archivePrefix = "arXiv",
  primaryClass  = "math.RA",
  eprint        = "1609.06349"
}

@ARTICLE{Ah-Pine2022-fy,
  title    = "Learning doubly stochastic and nearly idempotent affinity matrix
              for graph-based clustering",
  author   = "Ah-Pine, Julien",
  abstract = "In graph-based clustering, a relevant affinity matrix is crucial
              for good results. Double stochasticity of the affinity matrix has
              been shown to be an important condition, both in theory and in
              practice. In this paper, we emphasize idempotency as another key
              condition. In fact, a theorem from Sinkhorn, R. (1968) allows us
              to exhibit the bijective relationship between the set of doubly
              stochastic and idempotent matrices of order n (modulo permutation
              of rows and columns) on the one hand, and the set of possible
              partitions of a set of n objects on the other hand. Thereby, both
              properties are necessary and sufficient conditions for properly
              modeling the clustering or graph partitioning tasks using
              matrices. Yet, this leads to a NP-hard discrete optimization
              problem. In this context, our main contribution is the
              introduction of a new relaxed model that efficiently learns a
              double stochastic and nearly idempotent affinity matrix for
              graph-based clustering. Our approach leverages existing
              properties between doubly stochastic and idempotent matrices on
              the one hand, and their associated Laplacian matrices on the
              other hand. The resulting optimization problem is bi-convex and
              can be addressed by an Alternating Direction Method of
              Multipliers scheme. Furthermore, our model requires less
              parameters to set in contrast to most of recent works. The
              experimental results we obtained using several real-world
              benchmarks, exhibit the interest of our method and the importance
              of taking into account idempotency in graph-based clustering.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  299,
  number   =  3,
  pages    = "1069--1078",
  month    =  jun,
  year     =  2022,
  keywords = "Machine learning; Graph-based clustering; Doubly stochastic
              affinity matrix; Idempotent matrix; ADMM;clustering"
}

@MISC{Boucheron_undated-tz,
  title        = "Concentration Inequalities",
  author       = "Boucheron, Ephane and Lugosi, Abor and Bousquet, Olivier",
  howpublished = "\url{http://www.econ.upf.edu/~lugosi/mlss_conc.pdf}",
  note         = "Accessed: 2023-1-26"
}

@ARTICLE{Wang2019-ms,
  title     = "Provable subspace clustering: When {LRR} meets {SSC}",
  author    = "Wang, Yu-Xiang and Xu, Huan and Leng, Chenlei",
  journal   = "IEEE Trans. Inf. Theory",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  65,
  number    =  9,
  pages     = "5406--5432",
  month     =  sep,
  year      =  2019,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}

@ARTICLE{Liao1997-vc,
  title    = "Random Motion of a Rigid Body",
  author   = "Liao, Ming",
  abstract = "We study the random motion of a rigid body through a stochastic
              differential equation on the special orthogonal group SO(d).",
  journal  = "J. Theoret. Probab.",
  volume   =  10,
  number   =  1,
  pages    = "201--211",
  month    =  jan,
  year     =  1997
}

@MISC{Tian_undated-fy,
  title        = "Mixed-membership community detection via line graph curvature",
  author       = "Tian, Yu and Lubberts, Zachary and Weber, Melanie and
                  Sanborn, Sophia and Shewmake, Christian and Azeglio, Simone
                  and Bernardo, Arianna Di and Miolane, Nina",
  howpublished = "\url{https://openreview.net/pdf?id=84JZ20nB9KP}",
  note         = "Accessed: 2023-1-31"
}

@ARTICLE{Gould2019-lv,
  title         = "Deep Declarative Networks: A New Hope",
  author        = "Gould, Stephen and Hartley, Richard and Campbell, Dylan",
  abstract      = "We explore a new class of end-to-end learnable models
                   wherein data processing nodes (or network layers) are
                   defined in terms of desired behavior rather than an explicit
                   forward function. Specifically, the forward function is
                   implicitly defined as the solution to a mathematical
                   optimization problem. Consistent with nomenclature in the
                   programming languages community, we name these models deep
                   declarative networks. Importantly, we show that the class of
                   deep declarative networks subsumes current deep learning
                   models. Moreover, invoking the implicit function theorem, we
                   show how gradients can be back-propagated through many
                   declaratively defined data processing nodes thereby enabling
                   end-to-end learning. We show how these declarative
                   processing nodes can be implemented in the popular PyTorch
                   deep learning software library allowing declarative and
                   imperative nodes to co-exist within the same network. We
                   also provide numerous insights and illustrative examples of
                   declarative nodes and demonstrate their application for
                   image and point cloud classification tasks.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1909.04866"
}

@ARTICLE{Agrawal2019-ew,
  title         = "Differentiable Convex Optimization Layers",
  author        = "Agrawal, Akshay and Amos, Brandon and Barratt, Shane and
                   Boyd, Stephen and Diamond, Steven and Kolter, Zico",
  abstract      = "Recent work has shown how to embed differentiable
                   optimization problems (that is, problems whose solutions can
                   be backpropagated through) as layers within deep learning
                   architectures. This method provides a useful inductive bias
                   for certain problems, but existing software for
                   differentiable optimization layers is rigid and difficult to
                   apply to new settings. In this paper, we propose an approach
                   to differentiating through disciplined convex programs, a
                   subclass of convex optimization problems used by
                   domain-specific languages (DSLs) for convex optimization. We
                   introduce disciplined parametrized programming, a subset of
                   disciplined convex programming, and we show that every
                   disciplined parametrized program can be represented as the
                   composition of an affine map from parameters to problem
                   data, a solver, and an affine map from the solver's solution
                   to a solution of the original problem (a new form we refer
                   to as affine-solver-affine form). We then demonstrate how to
                   efficiently differentiate through each of these components,
                   allowing for end-to-end analytical differentiation through
                   the entire convex program. We implement our methodology in
                   version 1.1 of CVXPY, a popular Python-embedded DSL for
                   convex optimization, and additionally implement
                   differentiable layers for disciplined convex programs in
                   PyTorch and TensorFlow 2.0. Our implementation significantly
                   lowers the barrier to using convex optimization problems in
                   differentiable programs. We present applications in linear
                   machine learning models and in stochastic control, and we
                   show that our layer is competitive (in execution time)
                   compared to specialized differentiable solvers from past
                   work.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.12430"
}

@ARTICLE{Zhang2009-gr,
  title         = "Median K-flats for hybrid linear modeling with many outliers",
  author        = "Zhang, Teng and Szlam, Arthur and Lerman, Gilad",
  abstract      = "We describe the Median K-Flats (MKF) algorithm, a simple
                   online method for hybrid linear modeling, i.e., for
                   approximating data by a mixture of flats. This algorithm
                   simultaneously partitions the data into clusters while
                   finding their corresponding best approximating l1 d-flats,
                   so that the cumulative l1 error is minimized. The current
                   implementation restricts d-flats to be d-dimensional linear
                   subspaces. It requires a negligible amount of storage, and
                   its complexity, when modeling data consisting of N points in
                   D-dimensional Euclidean space with K d-dimensional linear
                   subspaces, is of order O(n K d D+n d^2 D), where n is the
                   number of iterations required for convergence (empirically
                   on the order of 10^4). Since it is an online algorithm, data
                   can be supplied to it incrementally and it can incrementally
                   produce the corresponding output. The performance of the
                   algorithm is carefully evaluated using synthetic and real
                   data.",
  month         =  sep,
  year          =  2009,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "0909.3123"
}

@ARTICLE{Selim1984-gc,
  title    = "K-means-type algorithms: a generalized convergence theorem and
              characterization of local optimality",
  author   = "Selim, S Z and Ismail, M A",
  abstract = "The K-means algorithm is a commonly used technique in cluster
              analysis. In this paper, several questions about the algorithm
              are addressed. The clustering problem is first cast as a
              nonconvex mathematical program. Then, a rigorous proof of the
              finite convergence of the K-means-type algorithm is given for any
              metric. It is shown that under certain conditions the algorithm
              may fail to converge to a local minimum, and that it converges
              under differentiability conditions to a Kuhn-Tucker point.
              Finally, a method for obtaining a local-minimum solution is
              given.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  6,
  number   =  1,
  pages    = "81--87",
  month    =  jan,
  year     =  1984,
  language = "en"
}

@INPROCEEDINGS{Vidal2003-sw,
  title     = "An algebraic geometric approach to the identification of a class
               of linear hybrid systems",
  booktitle = "42nd {IEEE} International Conference on Decision and Control
               ({IEEE} Cat. {No.03CH37475})",
  author    = "Vidal, R and Soatto, S and Ma, Yi and Sastry, S",
  abstract  = "We propose an algebraic geometric solution to the identification
               of a class of linear hybrid systems. We show that the
               identification of the model parameters can be decoupled from the
               inference of the hybrid state and the switching mechanism
               generating the transitions, hence we do not constraint the
               switches to be separated by a minimum dwell time. The decoupling
               is obtained from the so-called hybrid decoupling constraint,
               which establishes a connection between linear hybrid system
               identification, polynomial factorization and hyperplane
               clustering. In essence, we represent the number of discrete
               states n as the degree of a homogeneous polynomial p and the
               model parameters as factors of p. We then show that one can
               estimate n from a rank constraint on the data, the coefficients
               of p from a linear system, and the model parameters from the
               derivatives of p. The solution is closed form if and only if
               n/spl les/4. Once the model parameters have been identified, the
               estimation of the hybrid state becomes a simpler problem.
               Although our algorithm is designed for noiseless data, we also
               present simulation results with noisy data.",
  volume    =  1,
  pages     = "167--172 Vol.1",
  month     =  dec,
  year      =  2003,
  keywords  = "Observers;State estimation;Hybrid power
               systems;Switches;Polynomials;Electronic mail;Quadratic
               programming;System identification;Linear systems;Algorithm
               design and analysis"
}

@ARTICLE{Torr1998-ef,
  title     = "Robust detection of degenerate configurations while estimating
               the fundamental matrix",
  author    = "Torr, P H S and Zisserman, A and Maybank, S J",
  abstract  = "We present a new method for the detection of multiple solutions
               or degeneracy when estimating thefundamental matrix, with
               specific emphasis on robustness to data contamination
               (mismatches). The fundamental matrix encapsulates all the
               information on camera motion and internal parameters available
               from image feature correspondences between two views. It is
               often used as a first step in structure from motion algorithms.
               If the set of correspondences is degenerate, then this structure
               cannot be accurately recovered and many solutions explain the
               data equally well. It is essential that we are alerted to such
               eventualities. As current feature matchers are very prone to
               mismatching the degeneracy detection method must also be robust
               to outliers.In this paper a definition of degeneracy is given
               and all two-view nondegenerate and degenerate cases are
               catalogued in a logical way by introducing the language of
               varieties from algebraic geometry. It is then shown how each of
               the cases can be robustly determined from image correspondences
               via a scoring function we develop. These ideas define a
               methodology which allows the simultaneous detection of
               degeneracy and outliers. The method is called PLUNDER-DL and is
               a generalization of the robust estimator RANSAC.The method is
               evaluated on many differing pairs of real images. In particular
               it is demonstrated that proper modeling of degeneracy in the
               presence of outliers enables the detection of mismatches which
               would otherwise be missed. All processing including point
               matching, degeneracy detection, and outlier detection is
               automatic.",
  journal   = "Comput. Vis. Image Underst.",
  publisher = "Elsevier BV",
  volume    =  71,
  number    =  3,
  pages     = "312--333",
  month     =  sep,
  year      =  1998,
  language  = "en"
}

@INPROCEEDINGS{Aftab2015-qk,
  title     = "Convergence of Iteratively Re-weighted Least Squares to Robust
               {M-Estimators}",
  booktitle = "2015 {IEEE} Winter Conference on Applications of Computer Vision",
  author    = "Aftab, Khurrum and Hartley, Richard",
  abstract  = "This paper presents a way of using the Iteratively Reweighted
               Least Squares (IRLS) method to minimize several robust cost
               functions such as the Huber function, the Cauchy function and
               others. It is known that IRLS (otherwise known as Weiszfeld)
               techniques are generally more robust to outliers than the
               corresponding least squares methods, but the full range of
               robust M-estimators that are amenable to IRLS has not been
               investigated. In this paper we address this question and show
               that IRLS methods can be used to minimize most common robust
               M-estimators. An exact condition is given and proved for
               decrease of the cost, from which convergence follows. In
               addition to the advantage of increased robustness, the proposed
               algorithm is far simpler than the standard L1 Weiszfeld
               algorithm. We show the applicability of the proposed algorithm
               to the rotation averaging, triangulation and point cloud
               alignment problems.",
  pages     = "480--487",
  month     =  jan,
  year      =  2015,
  keywords  = "Robustness;Cost
               function;Convergence;Minimization;Three-dimensional
               displays;Measurement;Computer vision"
}

@ARTICLE{Aftab2015-nu,
  title    = "Generalized Weiszfeld Algorithms for Lq Optimization",
  author   = "Aftab, Khurrum and Hartley, Richard and Trumpf, Jochen",
  abstract = "In many computer vision applications, a desired model of some
              type is computed by minimizing a cost function based on several
              measurements. Typically, one may compute the model that minimizes
              the L2 cost, that is the sum of squares of measurement errors
              with respect to the model. However, the Lq solution which
              minimizes the sum of the qth power of errors usually gives more
              robust results in the presence of outliers for some values of q,
              for example, q = 1. The Weiszfeld algorithm is a classic
              algorithm for finding the geometric L1 mean of a set of points in
              Euclidean space. It is provably optimal and requires neither
              differentiation, nor line search. The Weiszfeld algorithm has
              also been generalized to find the L1 mean of a set of points on a
              Riemannian manifold of non-negative curvature. This paper shows
              that the Weiszfeld approach may be extended to a wide variety of
              problems to find an Lq mean for 1 $\leq$ q <; 2, while
              maintaining simplicity and provable convergence. We apply this
              problem to both single-rotation averaging (under which the
              algorithm provably finds the global Lq optimum) and multiple
              rotation averaging (for which no such proof exists). Experimental
              results of Lq optimization for rotations show the improved
              reliability and robustness compared to L2 optimization.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  37,
  number   =  4,
  pages    = "728--745",
  month    =  apr,
  year     =  2015,
  language = "en"
}

@ARTICLE{Zhang2020-wn,
  title         = "From Symmetry to Geometry: Tractable Nonconvex Problems",
  author        = "Zhang, Yuqian and Qu, Qing and Wright, John",
  abstract      = "As science and engineering have become increasingly
                   data-driven, the role of optimization has expanded to touch
                   almost every stage of the data analysis pipeline, from
                   signal and data acquisition to modeling and prediction. The
                   optimization problems encountered in practice are often
                   nonconvex. While challenges vary from problem to problem,
                   one common source of nonconvexity is nonlinearity in the
                   data or measurement model. Nonlinear models often exhibit
                   symmetries, creating complicated, nonconvex objective
                   landscapes, with multiple equivalent solutions.
                   Nevertheless, simple methods (e.g., gradient descent) often
                   perform surprisingly well in practice. The goal of this
                   survey is to highlight a class of tractable nonconvex
                   problems, which can be understood through the lens of
                   symmetries. These problems exhibit a characteristic
                   geometric structure: local minimizers are symmetric copies
                   of a single ``ground truth'' solution, while other critical
                   points occur at balanced superpositions of symmetric copies
                   of the ground truth, and exhibit negative curvature in
                   directions that break the symmetry. This structure enables
                   efficient methods to obtain global minimizers. We discuss
                   examples of this phenomenon arising from a wide range of
                   problems in imaging, signal processing, and data analysis.
                   We highlight the key role of symmetry in shaping the
                   objective landscape and discuss the different roles of
                   rotational and discrete symmetries. This area is rich with
                   observed phenomena and open problems; we close by
                   highlighting directions for future research.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2007.06753"
}

@ARTICLE{Rao2010-do,
  title    = "Motion segmentation in the presence of outlying, incomplete, or
              corrupted trajectories",
  author   = "Rao, Shankar and Tron, Roberto and Vidal, Ren{\'e} and Ma, Yi",
  abstract = "In this paper, we study the problem of segmenting tracked feature
              point trajectories of multiple moving objects in an image
              sequence. Using the affine camera model, this problem can be cast
              as the problem of segmenting samples drawn from multiple linear
              subspaces. In practice, due to limitations of the tracker,
              occlusions, and the presence of nonrigid objects in the scene,
              the obtained motion trajectories may contain grossly mistracked
              features, missing entries, or corrupted entries. In this paper,
              we develop a robust subspace separation scheme that deals with
              these practical issues in a unified mathematical framework. Our
              methods draw strong connections between lossy compression, rank
              minimization, and sparse representation. We test our methods
              extensively on the Hopkins155 motion segmentation database and
              other motion sequences with outliers and missing data. We compare
              the performance of our methods to state-of-the-art motion
              segmentation methods based on expectation-maximization and
              spectral clustering. For data without outliers or missing
              information, the results of our methods are on par with the
              state-of-the-art results and, in many cases, exceed them. In
              addition, our methods give surprisingly good performance in the
              presence of the three types of pathological trajectories
              mentioned above. All code and results are publicly available at
              http://perception.csl.uiuc.edu/coding/motion/.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  32,
  number   =  10,
  pages    = "1832--1845",
  month    =  oct,
  year     =  2010,
  language = "en"
}

@PHDTHESIS{Ding2021-og,
  title    = "Subspace learning for data arising from a union of subspaces of
              high relative dimension",
  author   = "Ding, Tianyu",
  year     =  2021,
  school   = "Johns Hopkins University"
}

@ARTICLE{Boumal2016-wt,
  title         = "Global rates of convergence for nonconvex optimization on
                   manifolds",
  author        = "Boumal, Nicolas and Absil, P-A and Cartis, Coralia",
  abstract      = "We consider the minimization of a cost function $f$ on a
                   manifold $M$ using Riemannian gradient descent and
                   Riemannian trust regions (RTR). We focus on satisfying
                   necessary optimality conditions within a tolerance
                   $\varepsilon$. Specifically, we show that, under
                   Lipschitz-type assumptions on the pullbacks of $f$ to the
                   tangent spaces of $M$, both of these algorithms produce
                   points with Riemannian gradient smaller than $\varepsilon$
                   in $O(1/\varepsilon^2)$ iterations. Furthermore, RTR returns
                   a point where also the Riemannian Hessian's least eigenvalue
                   is larger than $-\varepsilon$ in $O(1/\varepsilon^3)$
                   iterations. There are no assumptions on initialization. The
                   rates match their (sharp) unconstrained counterparts as a
                   function of the accuracy $\varepsilon$ (up to constants) and
                   hence are sharp in that sense. These are the first
                   deterministic results for global rates of convergence to
                   approximate first- and second-order Karush-Kuhn-Tucker
                   points on manifolds. They apply in particular for
                   optimization constrained to compact submanifolds of
                   $\mathbb\{R\}^n$, under simpler assumptions.",
  month         =  may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1605.08101"
}

@INCOLLECTION{Barath2018-nv,
  title     = "Multi-class model fitting by energy minimization and
               mode-seeking",
  booktitle = "Computer Vision -- {ECCV} 2018",
  author    = "Barath, Daniel and Matas, Jiri",
  abstract  = "We propose a general formulation, called Multi-X, for
               multi-class multi-instance model fitting-the problem of
               interpreting the input data as a mixture of noisy observations
               originating from multiple instances of multiple classes. We
               extend the commonly used $\alpha$-expansion-based technique with
               a new move in the label space. The move replaces a set of labels
               with the corresponding density mode in the model parameter
               domain, thus achieving fast and robust optimization. Key
               optimization parameters like the bandwidth of the mode seeking
               are set automatically within the algorithm. Considering that a
               group of outliers may form spatially coherent structures in the
               data, we propose a cross-validation-based technique removing
               statistically insignificant instances. Multi-X outperforms
               significantly the state-of-the-art on publicly available
               datasets for diverse problems: multiple plane and rigid motion
               detection; motion segmentation; simultaneous plane and cylinder
               fitting; circle and line fitting.",
  publisher = "Springer International Publishing",
  pages     = "229--245",
  series    = "Lecture notes in computer science",
  year      =  2018,
  address   = "Cham"
}

@ARTICLE{Van_Hoorebeeck2022-wr,
  title         = "Projection onto quadratic hypersurfaces",
  author        = "Van Hoorebeeck, Lo{\"\i}c and Absil, P-A and Papavasiliou,
                   Anthony",
  abstract      = "We address the problem of projecting a point onto a
                   quadratic hypersurface, more specifically a central quadric.
                   We show how this problem reduces to finding a given root of
                   a scalar-valued nonlinear function. We completely
                   characterize one of the optimal solutions of the projection
                   as either the unique root of this nonlinear function on a
                   given interval, or as a point that belongs to a finite set
                   of computable solutions. We then leverage this projection
                   and the recent advancements in splitting methods to compute
                   the projection onto the intersection of a box and a
                   quadratic hypersurface with alternating projections and
                   Douglas-Rachford splitting methods. We test these methods on
                   a practical problem from the power systems literature, and
                   show that they outperform IPOPT and Gurobi in terms of
                   objective, execution time and feasibility of the solution.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2204.02087"
}

@INPROCEEDINGS{Wong2011-ul,
  title     = "Dynamic and hierarchical multi-structure geometric model fitting",
  booktitle = "2011 International Conference on Computer Vision",
  author    = "Wong, Hoi Sim and Chin, Tat-Jun and Yu, Jin and Suter, David",
  abstract  = "The ability to generate good model hypotheses is instrumental to
               accurate and robust geometric model fitting. We present a novel
               dynamic hypothesis generation algorithm for robust fitting of
               multiple structures. Underpinning our method is a fast guided
               sampling scheme enabled by analysing correlation of preferences
               induced by data and hypothesis residuals. Our method
               progressively accumulates evidence in the search space, and uses
               the information to dynamically (1) identify outliers, (2) filter
               unpromising hypotheses, and (3) bias the sampling for active
               discovery of multiple structures in the data-All achieved
               without sacrificing the speed associated with sampling-based
               methods. Our algorithm yields a disproportionately higher number
               of good hypotheses among the sampling outcomes, i.e., most
               hypotheses correspond to the genuine structures in the data.
               This directly supports a novel hierarchical model fitting
               algorithm that elicits the underlying stratified manner in which
               the structures are organized, allowing more meaningful results
               than traditional ``flat'' multi-structure fitting.",
  pages     = "1044--1051",
  month     =  nov,
  year      =  2011,
  keywords  = "Heuristic algorithms;Data models;Computational
               modeling;Algorithm design and analysis;Sampling
               methods;Filtering;Image color analysis"
}

@ARTICLE{noauthor_undated-qx,

}

@ARTICLE{Gillard2022-my,
  title    = "Polynomial whitening for high-dimensional data",
  author   = "Gillard, Jonathan and O'Riordan, Emily and Zhigljavsky, Anatoly",
  abstract = "The inverse square root of a covariance matrix is often desirable
              for performing data whitening in the process of applying many
              common multivariate data analysis methods. Direct calculation of
              the inverse square root is not available when the covariance
              matrix is either singular or nearly singular, as often occurs in
              high dimensions. We develop new methods, which we broadly call
              polynomial whitening, to construct a low-degree polynomial in the
              empirical covariance matrix which has similar properties to the
              true inverse square root of the covariance matrix (should it
              exist). Our method does not suffer in singular or near-singular
              settings, and is computationally tractable in high dimensions. We
              demonstrate that our construction of low-degree polynomials
              provides a good substitute for high-dimensional inverse square
              root covariance matrices, in both $$d < N$$and $$d
              \textbackslashge N$$cases. We offer examples on data whitening,
              outlier detection and principal component analysis to demonstrate
              the performance of the proposed method.",
  journal  = "Comput. Stat.",
  month    =  sep,
  year     =  2022
}

@ARTICLE{Peng2020-xd,
  title    = "Linear Regression Without Correspondences via Concave
              Minimization",
  author   = "Peng, Liangzu and Tsakiris, Manolis C",
  abstract = "Linear regression without correspondences concerns the recovery
              of a signal in the linear regression setting, where the
              correspondences between the observations and the linear
              functionals are unknown. The associated maximum likelihood
              function is NP-hard to compute when the signal has dimension
              larger than one. To optimize this objective function we
              reformulate it as a concave minimization problem, which we solve
              via branch-and-bound. This is supported by a computable search
              space to branch, an effective lower bounding scheme via convex
              envelope minimization and a refined upper bound, all naturally
              arising from the concave minimization reformulation. The
              resulting algorithm outperforms state-of-the-art methods for
              fully shuffled data and remains tractable for up to 8-dimensional
              signals, an untouched regime in prior work.",
  journal  = "IEEE Signal Process. Lett.",
  volume   =  27,
  pages    = "1580--1584",
  year     =  2020,
  keywords = "Minimization;Upper bound;Signal processing algorithms;Linear
              regression;Linear programming;Sensors;Estimation;Linear
              regression without correspondences;unlabeled sensing;homomorphic
              sensing;concave minimization;branch-and-bound;linear assignment
              problem"
}

@ARTICLE{Ma2008-br,
  title     = "Estimation of Subspace Arrangements with Applications in
               Modeling and Segmenting Mixed Data",
  author    = "Ma, Yi and Yang, Allen Y and Derksen, Harm and Fossum, Robert",
  abstract  = "Recently many scientific and engineering applications have
               involved the challenging task of analyzing large amounts of
               unsorted high-dimensional data that have very complicated
               structures. From both geometric and statistical points of view,
               such unsorted data are considered mixed as different parts of
               the data have significantly different structures which cannot be
               described by a single model. In this paper we propose to use
               subspace arrangements?a union of multiple subspaces?for modeling
               mixed data: each subspace in the arrangement is used to model
               just a homogeneous subset of the data. Thus, multiple subspaces
               together can capture the heterogeneous structures within the
               data set. In this paper, we give a comprehensive introduction to
               a new approach for the estimation of subspace arrangements. This
               is known as generalized principal component analysis (GPCA). In
               particular, we provide a comprehensive summary of important
               algebraic properties and statistical facts that are crucial for
               making the inference of subspace arrangements both efficient and
               robust, even when the given data are corrupted by noise or
               contaminated with outliers. This new method in many ways
               improves and generalizes extant methods for modeling or
               clustering mixed data. There have been successful applications
               of this new method to many real-world problems in computer
               vision, image processing, and system identification. In this
               paper, we will examine several of those representative
               applications. This paper is intended to be expository in nature.
               However, in order that this may serve as a more complete
               reference for both theoreticians and practitioners, we take the
               liberty of filling in several gaps between the theory and the
               practice in the existing literature.",
  journal   = "SIAM Rev.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  50,
  number    =  3,
  pages     = "413--458",
  month     =  jan,
  year      =  2008
}

@INPROCEEDINGS{Lu2013-cz,
  title     = "Correlation Adaptive Subspace Segmentation by Trace Lasso",
  booktitle = "2013 {IEEE} International Conference on Computer Vision",
  author    = "Lu, Canyi and Feng, Jiashi and Lin, Zhouchen and Yan, Shuicheng",
  abstract  = "This paper studies the subspace segmentation problem. Given a
               set of data points drawn from a union of subspaces, the goal is
               to partition them into their underlying subspaces they were
               drawn from. The spectral clustering method is used as the
               framework. It requires to find an affinity matrix which is close
               to block diagonal, with nonzero entries corresponding to the
               data point pairs from the same subspace. In this work, we argue
               that both sparsity and the grouping effect are important for
               subspace segmentation. A sparse affinity matrix tends to be
               block diagonal, with less connections between data points from
               different subspaces. The grouping effect ensures that the highly
               corrected data which are usually from the same subspace can be
               grouped together. Sparse Subspace Clustering (SSC), by using
               ell^1-minimization, encourages sparsity for data selection, but
               it lacks of the grouping effect. On the contrary, Low-Rank
               Representation (LRR), by rank minimization, and Least Squares
               Regression (LSR), by ell^2-regularization, exhibit strong
               grouping effect, but they are short in subset selection. Thus
               the obtained affinity matrix is usually very sparse by SSC, yet
               very dense by LRR and LSR. In this work, we propose the
               Correlation Adaptive Subspace Segmentation (CASS) method by
               using trace Lasso. CASS is a data correlation dependent method
               which simultaneously performs automatic data selection and
               groups correlated data together. It can be regarded as a method
               which adaptively balances SSC and LSR. Both theoretical and
               experimental results show the effectiveness of CASS.",
  pages     = "1345--1352",
  month     =  dec,
  year      =  2013,
  keywords  = "Sparse matrices;Correlation;Vectors;Noise;Silicon;Least squares
               approximations;Image segmentation"
}

@INPROCEEDINGS{Favaro2011-zj,
  title     = "A closed form solution to robust subspace estimation and
               clustering",
  booktitle = "{CVPR} 2011",
  author    = "Favaro, Paolo and Vidal, Ren{\'e} and Ravichandran, Avinash",
  abstract  = "We consider the problem of fitting one or more subspaces to a
               collection of data points drawn from the subspaces and corrupted
               by noise/outliers. We pose this problem as a rank minimization
               problem, where the goal is to decompose the corrupted data
               matrix as the sum of a clean, self-expressive, low-rank
               dictionary plus a matrix of noise/outliers. Our key contribution
               is to show that, for noisy data, this non-convex problem can be
               solved very efficiently and in closed form from the SVD of the
               noisy data matrix. Remarkably, this is true for both one or more
               subspaces. An important difference with respect to existing
               methods is that our framework results in a polynomial
               thresholding of the singular values with minimal shrinkage.
               Indeed, a particular case of our framework in the case of a
               single subspace leads to classical PCA, which requires no
               shrinkage. In the case of multiple subspaces, our framework
               provides an affinity matrix that can be used to cluster the data
               according to the sub-spaces. In the case of data corrupted by
               outliers, a closed-form solution appears elusive. We thus use an
               augmented Lagrangian optimization framework, which requires a
               combination of our proposed polynomial thresholding operator
               with the more traditional shrinkage-thresholding operator.",
  pages     = "1801--1807",
  month     =  jun,
  year      =  2011,
  keywords  = "Sparse matrices;Noise;Minimization;Matrix
               decomposition;Principal component
               analysis;Estimation;Dictionaries"
}

@TECHREPORT{Krizhevsky2009-ee,
  title    = "Learning multiple layers of features from tiny images",
  author   = "Krizhevsky, Alex",
  year     =  2009
}

@ARTICLE{Sadeghi2022-ox,
  title         = "C3: Cross-instance guided Contrastive Clustering",
  author        = "Sadeghi, Mohammadreza and Hojjati, Hadi and Armanfard,
                   Narges",
  abstract      = "Clustering is the task of gathering similar data samples
                   into clusters without using any predefined labels. It has
                   been widely studied in machine learning literature, and
                   recent advancements in deep learning have revived interest
                   in this field. Contrastive clustering (CC) models are a
                   staple of deep clustering in which positive and negative
                   pairs of each data instance are generated through data
                   augmentation. CC models aim to learn a feature space where
                   instance-level and cluster-level representations of positive
                   pairs are grouped together. Despite improving the SOTA,
                   these algorithms ignore the cross-instance patterns, which
                   carry essential information for improving clustering
                   performance. In this paper, we propose a novel contrastive
                   clustering method, Cross-instance guided Contrastive
                   Clustering (C3), that considers the cross-sample
                   relationships to increase the number of positive pairs. In
                   particular, we define a new loss function that identifies
                   similar instances using the instance-level representation
                   and encourages them to aggregate together. Extensive
                   experimental evaluations show that our proposed method can
                   outperform state-of-the-art algorithms on benchmark computer
                   vision datasets: we improve the clustering accuracy by
                   6.8\%, 2.8\%, 4.9\%, 1.3\% and 0.4\% on CIFAR-10, CIFAR-100,
                   ImageNet-10, ImageNet-Dogs, and Tiny-ImageNet, respectively.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2211.07136"
}

@INPROCEEDINGS{Ge2015-rd,
  title     = "Escaping from saddle points --- online stochastic gradient for
               tensor decomposition",
  booktitle = "The 28th Conference on Learning Theory",
  author    = "Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang",
  abstract  = "We analyze stochastic gradient descent for optimizing non-convex
               functions. In many cases for non-convex functions the goal is to
               find a reasonable local minimum, and the main concern is that
               gradient updates are trapped in saddle points. In this paper we
               identify strict saddle property for non-convex problem that
               allows for efficient optimization. Using this property we show
               that stochastic gradient descent converges to a local minimum in
               a polynomial number of iterations. To the best of our knowledge
               this is the first work that gives global convergence guarantees
               for stochastic gradient descent on non-convex functions with
               exponentially many local minima and saddle points. Our analysis
               can be applied to orthogonal tensor decomposition, which is
               widely used in learning a rich class of latent variable models.
               We propose a new optimization formulation for the tensor
               decomposition problem that has strict saddle property. As a
               result we get the first online algorithm for orthogonal tensor
               decomposition with global convergence guarantee.",
  month     =  mar,
  year      =  2015,
  copyright = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/"
}

@INPROCEEDINGS{Ge2017-kf,
  title     = "On the Optimization Landscape of Tensor Decompositions",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Ge, Rong and Ma, Tengyu",
  abstract  = "Non-convex optimization with local search heuristics has been
               widely used in machine learning, achieving many state-of-art
               results. It becomes increasingly important to understand why
               they can work for these NP-hard problems on typical data. The
               landscape of many objective functions in learning has been
               conjectured to have the geometric property that ``all local
               optima are (approximately) global optima'', and thus they can be
               solved efficiently by local search algorithms. However,
               establishing such property can be very difficult. In this paper,
               we analyze the optimization landscape of the random
               over-complete tensor decomposition problem, which has many
               applications in unsupervised learning, especially in learning
               latent variable models. In practice, it can be efficiently
               solved by gradient ascent on a non-convex objective. We show
               that for any small constant $\epsilon > 0$, among the set of
               points with function values $(1+\epsilon)$-factor larger than
               the expectation of the function, all the local maxima are
               approximate global maxima. Previously, the best-known result
               only characterizes the geometry in small neighborhoods around
               the true components. Our result implies that even with an
               initialization that is barely better than the random guess, the
               gradient ascent algorithm is guaranteed to solve this problem.
               Our main technique uses Kac-Rice formula and random matrix
               theory. To our best knowledge, this is the first time when
               Kac-Rice formula is successfully applied to counting the number
               of local minima of a highly-structured random polynomial with
               dependent coefficients.",
  month     =  jun,
  year      =  2017
}

@MISC{Giampouras_undated-rr,
  title        = "A novel variational form of the Schatten-p quasi-norm",
  author       = "Giampouras, Paris and Vidal, Ren{\'e} and Rontogiannis,
                  Athanasios and Haeffele, Benjamin D",
  howpublished = "\url{https://proceedings.neurips.cc/paper/2020/file/f53eb4122d5e2ce81a12093f8f9ce922-Paper.pdf}",
  note         = "Accessed: 2023-3-9"
}

@INPROCEEDINGS{Rennie2005-fd,
  title           = "Fast maximum margin matrix factorization for collaborative
                     prediction",
  booktitle       = "Proceedings of the 22nd international conference on
                     Machine learning - {ICML} '05",
  author          = "Rennie, Jasson D M and Srebro, Nathan",
  abstract        = "Maximum Margin Matrix Factorization (MMMF) was recently
                     suggested (Srebro et al., 2005) as a convex, infinite
                     dimensional alternative to low-rank approximations and
                     standard factor models. MMMF can be formulated as a
                     semi-definite programming (SDP) and learned using standard
                     SDP solvers. However, current SDP solvers can only handle
                     MMMF problems on matrices of dimensionality up to a few
                     hundred. Here, we investigate a direct gradient-based
                     optimization method for MMMF and demonstrate it on large
                     collaborative prediction problems. We compare against
                     results obtained by Marlin (2004) and find that MMMF
                     substantially outperforms all nine methods he tested.",
  publisher       = "ACM Press",
  year            =  2005,
  address         = "New York, New York, USA",
  conference      = "the 22nd international conference",
  location        = "Bonn, Germany"
}

@INPROCEEDINGS{Baek2022-bj,
  title     = "Efficient maximal coding rate reduction by variational forms",
  booktitle = "{IEEE/CVF} Conference on Computer Vision and Pattern Recognition",
  author    = "Baek, Christina and Wu, Ziyang and Chan, Kwan Ho Ryan and Ding,
               Tianjiao and Ma, Yi and Haeffele, Benjamin D",
  abstract  = "The principle of Maximal Coding Rate Reduction (MCR$^2$) has
               recently been proposed as a training objective for learning
               discriminative low-dimensional structures intrinsic to
               high-dimensional data to allow for more robust training than
               standard approaches, such as cross-entropy minimization.
               However, despite the advantages that have been shown for MCR$^2$
               training, MCR$^2$ suffers from a significant computational cost
               due to the need to evaluate and differentiate a significant
               number of log-determinant terms that grows linearly with the
               number of classes. By taking advantage of variational forms of
               spectral functions of a matrix, we reformulate the MCR$^2$
               objective to a form that can scale significantly without
               compromising training accuracy. Experiments in image
               classification demonstrate that our proposed formulation results
               in a significant speed up over optimizing the original MCR$^2$
               objective directly and often results in higher quality learned
               representations. Further, our approach may be of independent
               interest in other models that require computation of
               log-determinant forms, such as in system identification or
               normalizing flow models.",
  pages     = "500--508",
  year      =  2022
}

@INPROCEEDINGS{Ivashechkin2021-yg,
  title           = "{VSAC}: Efficient and Accurate Estimator for {H} and {F}",
  booktitle       = "2021 {IEEE/CVF} International Conference on Computer
                     Vision ({ICCV})",
  author          = "Ivashechkin, Maksym and Barath, Daniel and Matas, Jiri",
  publisher       = "IEEE",
  month           =  oct,
  year            =  2021,
  conference      = "2021 IEEE/CVF International Conference on Computer Vision
                     (ICCV)",
  location        = "Montreal, QC, Canada"
}

@ARTICLE{Ramirez2011-vh,
  title         = "An {MDL} framework for sparse coding and dictionary learning",
  author        = "Ram{\'\i}rez, Ignacio and Sapiro, Guillermo",
  abstract      = "The power of sparse signal modeling with learned
                   over-complete dictionaries has been demonstrated in a
                   variety of applications and fields, from signal processing
                   to statistical inference and machine learning. However, the
                   statistical properties of these models, such as
                   under-fitting or over-fitting given sets of data, are still
                   not well characterized in the literature. As a result, the
                   success of sparse modeling depends on hand-tuning critical
                   parameters for each data and application. This work aims at
                   addressing this by providing a practical and objective
                   characterization of sparse models by means of the Minimum
                   Description Length (MDL) principle -- a well established
                   information-theoretic approach to model selection in
                   statistical inference. The resulting framework derives a
                   family of efficient sparse coding and dictionary learning
                   algorithms which, by virtue of the MDL principle, are
                   completely parameter free. Furthermore, such framework
                   allows to incorporate additional prior information to
                   existing models, such as Markovian dependencies, or to
                   define completely new problem formulations, including in the
                   matrix analysis area, in a natural way. These virtues will
                   be demonstrated with parameter-free algorithms for the
                   classic image denoising and classification problems, and for
                   low-rank matrix recovery in video applications.",
  month         =  oct,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IT",
  eprint        = "1110.2436"
}

@ARTICLE{Xu2022-gs,
  title     = "Identification of different dairy products using Raman
               spectroscopy combined with fused lasso distributionally robust
               logistic regression",
  author    = "Xu, Xiang and Xiao, Wentao and Cao, Yiyun and Zhang, Zhengyong",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2022,
  language  = "en"
}

@ARTICLE{Zha2021-mv,
  title         = "Invertible Attention",
  author        = "Zha, Jiajun and Zhong, Yiran and Zhang, Jing and Hartley,
                   Richard and Zheng, Liang",
  abstract      = "Attention has been proved to be an efficient mechanism to
                   capture long-range dependencies. However, so far it has not
                   been deployed in invertible networks. This is due to the
                   fact that in order to make a network invertible, every
                   component within the network needs to be a bijective
                   transformation, but a normal attention block is not. In this
                   paper, we propose invertible attention that can be plugged
                   into existing invertible models. We mathematically and
                   experimentally prove that the invertibility of an attention
                   model can be achieved by carefully constraining its
                   Lipschitz constant. We validate the invertibility of our
                   invertible attention on image reconstruction task with 3
                   popular datasets: CIFAR-10, SVHN, and CelebA. We also show
                   that our invertible attention achieves similar performance
                   in comparison with normal non-invertible attention on dense
                   prediction tasks. The code is available at
                   https://github.com/Schwartz-Zha/InvertibleAttention",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2106.09003"
}

@ARTICLE{Garrido2022-vw,
  title         = "On the duality between contrastive and non-contrastive
                   self-supervised learning",
  author        = "Garrido, Quentin and Chen, Yubei and Bardes, Adrien and
                   Najman, Laurent and Lecun, Yann",
  abstract      = "Recent approaches in self-supervised learning of image
                   representations can be categorized into different families
                   of methods and, in particular, can be divided into
                   contrastive and non-contrastive approaches. While
                   differences between the two families have been thoroughly
                   discussed to motivate new approaches, we focus more on the
                   theoretical similarities between them. By designing
                   contrastive and covariance based non-contrastive criteria
                   that can be related algebraically and shown to be equivalent
                   under limited assumptions, we show how close those families
                   can be. We further study popular methods and introduce
                   variations of them, allowing us to relate this theoretical
                   result to current practices and show the influence (or lack
                   thereof) of design choices on downstream performance.
                   Motivated by our equivalence result, we investigate the low
                   performance of SimCLR and show how it can match VICReg's
                   with careful hyperparameter tuning, improving significantly
                   over known baselines. We also challenge the popular
                   assumptions that contrastive and non-contrastive methods,
                   respectively, need large batch sizes and output dimensions.
                   Our theoretical and quantitative results suggest that the
                   numerical gaps between contrastive and non-contrastive
                   methods in certain regimes can be closed given better
                   network design choices and hyperparameter tuning. The
                   evidence shows that unifying different SOTA methods is an
                   important direction to build a better understanding of
                   self-supervised learning.",
  month         =  jun,
  year          =  2022,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2206.02574"
}

@ARTICLE{Geng2021-ai,
  title         = "Is Attention Better Than Matrix Decomposition?",
  author        = "Geng, Zhengyang and Guo, Meng-Hao and Chen, Hongxu and Li,
                   Xia and Wei, Ke and Lin, Zhouchen",
  abstract      = "As an essential ingredient of modern deep learning,
                   attention mechanism, especially self-attention, plays a
                   vital role in the global correlation discovery. However, is
                   hand-crafted attention irreplaceable when modeling the
                   global context? Our intriguing finding is that
                   self-attention is not better than the matrix decomposition
                   (MD) model developed 20 years ago regarding the performance
                   and computational cost for encoding the long-distance
                   dependencies. We model the global context issue as a
                   low-rank recovery problem and show that its optimization
                   algorithms can help design global information blocks. This
                   paper then proposes a series of Hamburgers, in which we
                   employ the optimization algorithms for solving MDs to
                   factorize the input representations into sub-matrices and
                   reconstruct a low-rank embedding. Hamburgers with different
                   MDs can perform favorably against the popular global context
                   module self-attention when carefully coping with gradients
                   back-propagated through MDs. Comprehensive experiments are
                   conducted in the vision tasks where it is crucial to learn
                   the global context, including semantic segmentation and
                   image generation, demonstrating significant improvements
                   over self-attention and its variants.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2109.04553"
}

@ARTICLE{Venkataramanan2014-ut,
  title         = "The {Rate-Distortion} Function and {Excess-Distortion}
                   Exponent of Sparse Regression Codes with Optimal Encoding",
  author        = "Venkataramanan, Ramji and Tatikonda, Sekhar",
  abstract      = "This paper studies the performance of sparse regression
                   codes for lossy compression with the squared-error
                   distortion criterion. In a sparse regression code, codewords
                   are linear combinations of subsets of columns of a design
                   matrix. It is shown that with minimum-distance encoding,
                   sparse regression codes achieve the Shannon rate-distortion
                   function for i.i.d. Gaussian sources $R^*(D)$ as well as the
                   optimal excess-distortion exponent. This completes a
                   previous result which showed that $R^*(D)$ and the optimal
                   exponent were achievable for distortions below a certain
                   threshold. The proof of the rate-distortion result is based
                   on the second moment method, a popular technique to show
                   that a non-negative random variable $X$ is strictly positive
                   with high probability. In our context, $X$ is the number of
                   codewords within target distortion $D$ of the source
                   sequence. We first identify the reason behind the failure of
                   the standard second moment method for certain distortions,
                   and illustrate the different failure modes via a stylized
                   example. We then use a refinement of the second moment
                   method to show that $R^*(D)$ is achievable for all
                   distortion values. Finally, the refinement technique is
                   applied to Suen's correlation inequality to prove the
                   achievability of the optimal Gaussian excess-distortion
                   exponent.",
  month         =  jan,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IT",
  eprint        = "1401.5272"
}

@ARTICLE{Gao2018-ic,
  title         = "Rate distortion for model compression: From theory to
                   practice",
  author        = "Gao, Weihao and Liu, Yu-Han and Wang, Chong and Oh, Sewoong",
  abstract      = "The enormous size of modern deep neural networks makes it
                   challenging to deploy those models in memory and
                   communication limited scenarios. Thus, compressing a trained
                   model without a significant loss in performance has become
                   an increasingly important task. Tremendous advances has been
                   made recently, where the main technical building blocks are
                   parameter pruning, parameter sharing (quantization), and
                   low-rank factorization. In this paper, we propose principled
                   approaches to improve upon the common heuristics used in
                   those building blocks, namely pruning and quantization. We
                   first study the fundamental limit for model compression via
                   the rate distortion theory. We bring the rate distortion
                   function from data compression to model compression to
                   quantify this fundamental limit. We prove a lower bound for
                   the rate distortion function and prove its achievability for
                   linear models. Although this achievable compression scheme
                   is intractable in practice, this analysis motivates a novel
                   model compression framework. This framework provides a new
                   objective function in model compression, which can be
                   applied together with other classes of model compressor such
                   as pruning or quantization. Theoretically, we prove that the
                   proposed scheme is optimal for compressing one-hidden-layer
                   ReLU neural networks. Empirically, we show that the proposed
                   scheme improves upon the baseline in the
                   compression-accuracy tradeoff.",
  month         =  oct,
  year          =  2018,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.IT",
  eprint        = "1810.06401"
}

@ARTICLE{Chen2021-zt,
  title         = "Scatterbrain: Unifying Sparse and Low-rank Attention
                   Approximation",
  author        = "Chen, Beidi and Dao, Tri and Winsor, Eric and Song, Zhao and
                   Rudra, Atri and R{\'e}, Christopher",
  abstract      = "Recent advances in efficient Transformers have exploited
                   either the sparsity or low-rank properties of attention
                   matrices to reduce the computational and memory bottlenecks
                   of modeling long sequences. However, it is still challenging
                   to balance the trade-off between model quality and
                   efficiency to perform a one-size-fits-all approximation for
                   different tasks. To better understand this trade-off, we
                   observe that sparse and low-rank approximations excel in
                   different regimes, determined by the softmax temperature in
                   attention, and sparse + low-rank can outperform each
                   individually. Inspired by the classical robust-PCA algorithm
                   for sparse and low-rank decomposition, we propose
                   Scatterbrain, a novel way to unify sparse (via locality
                   sensitive hashing) and low-rank (via kernel feature map)
                   attention for accurate and efficient approximation. The
                   estimation is unbiased with provably low error. We
                   empirically show that Scatterbrain can achieve 2.1x lower
                   error than baselines when serving as a drop-in replacement
                   in BigGAN image generation and pre-trained T2T-ViT. On a
                   pre-trained T2T Vision transformer, even without
                   fine-tuning, Scatterbrain can reduce 98\% of attention
                   memory at the cost of only 1\% drop in accuracy. We
                   demonstrate Scatterbrain for end-to-end training with up to
                   4 points better perplexity and 5 points better average
                   accuracy than sparse or low-rank efficient transformers on
                   language modeling and long-range-arena tasks.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2110.15343"
}

@ARTICLE{Tay2022-xl,
  title     = "Efficient Transformers: A Survey",
  author    = "Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler,
               Donald",
  abstract  = "Transformer model architectures have garnered immense interest
               lately due to their effectiveness across a range of domains like
               language, vision, and reinforcement learning. In the field of
               natural language processing for example, Transformers have
               become an indispensable staple in the modern deep learning
               stack. Recently, a dizzying number of ``X-former'' models have
               been proposed---Reformer, Linformer, Performer, Longformer, to
               name a few---which improve upon the original Transformer
               architecture, many of which make improvements around
               computational and memory efficiency. With the aim of helping the
               avid researcher navigate this flurry, this article characterizes
               a large and thoughtful selection of recent efficiency-flavored
               ``X-former'' models, providing an organized and comprehensive
               overview of existing work and models across multiple domains.",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  55,
  number    =  6,
  pages     = "1--28",
  month     =  dec,
  year      =  2022,
  address   = "New York, NY, USA",
  keywords  = "deep learning, attention, Transformers, neural networks"
}

@INCOLLECTION{Bomze1999-ap,
  title     = "The Maximum Clique Problem",
  booktitle = "Handbook of Combinatorial Optimization: Supplement Volume A",
  author    = "Bomze, Immanuel M and Budinich, Marco and Pardalos, Panos M and
               Pelillo, Marcello",
  editor    = "Du, Ding-Zhu and Pardalos, Panos M",
  abstract  = "The maximum clique problem is a classical problem in
               combinatorial optimization which finds important applications in
               different domains. In this paper we try to give a survey of
               results concerning algorithms, complexity, and applications of
               this problem, and also provide an updated bibliography. Of
               course, we build upon precursory works with similar goals [39,
               232, 266].",
  publisher = "Springer US",
  pages     = "1--74",
  year      =  1999,
  address   = "Boston, MA",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Zhang2016-zi,
  title    = "One condition for solution uniqueness and robustness of both
              l1-synthesis and l1-analysis minimizations",
  author   = "Zhang, Hui and Yan, Ming and Yin, Wotao",
  abstract = "The ℓ1-synthesis model and the ℓ1-analysis model recover
              structured signals from their undersampled measurements. The
              solution of the former is a sparse sum of dictionary atoms, and
              that of the latter makes sparse correlations with dictionary
              atoms. This paper addresses the question: when can we trust these
              models to recover specific signals? We answer the question with a
              condition that is both necessary and sufficient to guarantee the
              recovery to be unique and exact and, in the presence of
              measurement noise, to be robust. The condition is one--for--all
              in the sense that it applies to both the ℓ1-synthesis and
              ℓ1-analysis models, to both constrained and unconstrained
              formulations, and to both the exact recovery and robust recovery
              cases. Furthermore, a convex infinity--norm optimization problem
              is introduced for numerically verifying the condition. A
              comprehensive comparison with related existing conditions is
              included.",
  journal  = "Adv. Comput. Math.",
  volume   =  42,
  number   =  6,
  pages    = "1381--1399",
  month    =  dec,
  year     =  2016
}

@ARTICLE{Bhaskar2012-bc,
  title         = "Atomic norm denoising with applications to line spectral
                   estimation",
  author        = "Bhaskar, Badri Narayan and Tang, Gongguo and Recht, Benjamin",
  abstract      = "Motivated by recent work on atomic norms in inverse
                   problems, we propose a new approach to line spectral
                   estimation that provides theoretical guarantees for the
                   mean-squared-error (MSE) performance in the presence of
                   noise and without knowledge of the model order. We propose
                   an abstract theory of denoising with atomic norms and
                   specialize this theory to provide a convex optimization
                   problem for estimating the frequencies and phases of a
                   mixture of complex exponentials. We show that the associated
                   convex optimization problem can be solved in polynomial time
                   via semidefinite programming (SDP). We also show that the
                   SDP can be approximated by an l1-regularized least-squares
                   problem that achieves nearly the same error rate as the SDP
                   but can scale to much larger problems. We compare both SDP
                   and l1-based approaches with classical line spectral
                   analysis methods and demonstrate that the SDP outperforms
                   the l1 optimization which outperforms MUSIC, Cadzow's, and
                   Matrix Pencil approaches in terms of MSE over a wide range
                   of signal-to-noise ratios.",
  month         =  apr,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IT",
  eprint        = "1204.0562"
}

@ARTICLE{Tsakiris2018-lg,
  title         = "Theoretical Analysis of Sparse Subspace Clustering with
                   Missing Entries",
  author        = "Tsakiris, Manolis C and Vidal, Rene",
  abstract      = "Sparse Subspace Clustering (SSC) is a popular unsupervised
                   machine learning method for clustering data lying close to
                   an unknown union of low-dimensional linear subspaces; a
                   problem with numerous applications in pattern recognition
                   and computer vision. Even though the behavior of SSC for
                   complete data is by now well-understood, little is known
                   about its theoretical properties when applied to data with
                   missing entries. In this paper we give theoretical
                   guarantees for SSC with incomplete data, and analytically
                   establish that projecting the zero-filled data onto the
                   observation pattern of the point being expressed leads to a
                   substantial improvement in performance. The main insight
                   that stems from our analysis is that even though the
                   projection induces additional missing entries, this is
                   counterbalanced by the fact that the projected and
                   zero-filled data are in effect incomplete points associated
                   with the union of the corresponding projected subspaces,
                   with respect to which the point being expressed is complete.
                   The significance of this phenomenon potentially extends to
                   the entire class of self-expressive methods.",
  month         =  jan,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1801.00393"
}

@ARTICLE{Mahdizadehaghdam2019-gc,
  title    = "Deep Dictionary Learning: A {PARametric} {NETwork} Approach",
  author   = "Mahdizadehaghdam, Shahin and Panahi, Ashkan and Krim, Hamid and
              Dai, Liyi",
  abstract = "Deep dictionary learning seeks multiple dictionaries at different
              image scales to capture complementary coherent characteristics.
              We propose a method for learning a hierarchy of synthesis
              dictionaries with an image classification goal. The dictionaries
              and classification parameters are trained by a classification
              objective, and the sparse features are extracted by reducing a
              reconstruction loss in each layer. The reconstruction objectives
              in some sense regularize the classification problem and inject
              source signal information in the extracted features. The
              performance of the proposed hierarchical method increases by
              adding more layers, which consequently makes this model easier to
              tune and adapt. The proposed algorithm furthermore, shows
              remarkably lower fooling rate in presence of adversarial
              perturbation. The validation of the proposed approach is based on
              its classification performance using four benchmark datasets and
              is compared to a CNN of similar size.",
  journal  = "IEEE Trans. Image Process.",
  month    =  may,
  year     =  2019,
  language = "en"
}

@ARTICLE{Agterberg2022-wd,
  title         = "Entrywise Recovery Guarantees for Sparse {PCA} via
                   Sparsistent Algorithms",
  author        = "Agterberg, Joshua and Sulam, Jeremias",
  abstract      = "Sparse Principal Component Analysis (PCA) is a prevalent
                   tool across a plethora of subfields of applied statistics.
                   While several results have characterized the recovery error
                   of the principal eigenvectors, these are typically in
                   spectral or Frobenius norms. In this paper, we provide
                   entrywise $\ell_\{2,\infty\}$ bounds for Sparse PCA under a
                   general high-dimensional subgaussian design. In particular,
                   our results hold for any algorithm that selects the correct
                   support with high probability, those that are sparsistent.
                   Our bound improves upon known results by providing a finer
                   characterization of the estimation error, and our proof uses
                   techniques recently developed for entrywise subspace
                   perturbation theory.",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "2202.04061"
}

@ARTICLE{Pesce2022-jy,
  title         = "Subspace clustering in high-dimensions: Phase transitions \&
                   {Statistical-to-Computational} gap",
  author        = "Pesce, Luca and Loureiro, Bruno and Krzakala, Florent and
                   Zdeborov{\'a}, Lenka",
  abstract      = "A simple model to study subspace clustering is the
                   high-dimensional $k$-Gaussian mixture model where the
                   cluster means are sparse vectors. Here we provide an exact
                   asymptotic characterization of the statistically optimal
                   reconstruction error in this model in the high-dimensional
                   regime with extensive sparsity, i.e. when the fraction of
                   non-zero components of the cluster means $\rho$, as well as
                   the ratio $\alpha$ between the number of samples and the
                   dimension are fixed, while the dimension diverges. We
                   identify the information-theoretic threshold below which
                   obtaining a positive correlation with the true cluster means
                   is statistically impossible. Additionally, we investigate
                   the performance of the approximate message passing (AMP)
                   algorithm analyzed via its state evolution, which is
                   conjectured to be optimal among polynomial algorithm for
                   this task. We identify in particular the existence of a
                   statistical-to-computational gap between the algorithm that
                   require a signal-to-noise ratio $\lambda_\{\text\{alg\}\}
                   \ge k / \sqrt\{\alpha\} $ to perform better than random, and
                   the information theoretic threshold at
                   $\lambda_\{\text\{it\}\} \approx \sqrt\{-k \rho
                   \log\{\rho\}\} / \sqrt\{\alpha\}$. Finally, we discuss the
                   case of sub-extensive sparsity $\rho$ by comparing the
                   performance of the AMP with other sparsity-enhancing
                   algorithms, such as sparse-PCA and diagonal thresholding.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "2205.13527"
}

@ARTICLE{Canyi_Lu2016-nk,
  title    = "Convex Sparse Spectral Clustering: {Single-View} to {Multi-View}",
  author   = "{Canyi Lu} and {Shuicheng Yan} and {Zhouchen Lin}",
  abstract = "Spectral clustering (SC) is one of the most widely used methods
              for data clustering. It first finds a low-dimensional embedding U
              of data by computing the eigenvectors of the normalized Laplacian
              matrix, and then performs k-means on UT to get the final
              clustering result. In this paper, we observe that, in the ideal
              case, UUT should be block diagonal and thus sparse. Therefore, we
              propose the sparse SC (SSC) method that extends the SC with
              sparse regularization on UUT. To address the computational issue
              of the nonconvex SSC model, we propose a novel convex relaxation
              of SSC based on the convex hull of the fixed rank projection
              matrices. Then, the convex SSC model can be efficiently solved by
              the alternating direction method of multipliers Furthermore, we
              propose the pairwise SSC that extends SSC to boost the clustering
              performance by using the multi-view information of data.
              Experimental comparisons with several baselines on real-world
              datasets testify to the efficacy of our proposed methods.",
  journal  = "IEEE Trans. Image Process.",
  volume   =  25,
  number   =  6,
  pages    = "2833--2843",
  month    =  jun,
  year     =  2016,
  language = "en"
}

@ARTICLE{Agterberg2022-ip,
  title         = "Joint Spectral Clustering in Multilayer {Degree-Corrected}
                   Stochastic Blockmodels",
  author        = "Agterberg, Joshua and Lubberts, Zachary and Arroyo,
                   Jes{\'u}s",
  abstract      = "Modern network datasets are often composed of multiple
                   layers, either as different views, time-varying
                   observations, or independent sample units, resulting in
                   collections of networks over the same set of vertices but
                   with potentially different connectivity patterns on each
                   network. These data require models and methods that are
                   flexible enough to capture local and global differences
                   across the networks, while at the same time being
                   parsimonious and tractable to yield computationally
                   efficient and theoretically sound solutions that are capable
                   of aggregating information across the networks. This paper
                   considers the multilayer degree-corrected stochastic
                   blockmodel, where a collection of networks share the same
                   community structure, but degree-corrections and block
                   connection probability matrices are permitted to be
                   different. We establish the identifiability of this model
                   and propose a spectral clustering algorithm for community
                   detection in this setting. Our theoretical results
                   demonstrate that the misclustering error rate of the
                   algorithm improves exponentially with multiple network
                   realizations, even in the presence of significant layer
                   heterogeneity with respect to degree corrections, signal
                   strength, and spectral properties of the block connection
                   probability matrices. Simulation studies show that this
                   approach improves on existing multilayer community detection
                   methods in this challenging regime. Furthermore, in a case
                   study of US airport data through January 2016 -- September
                   2021, we find that this methodology identifies meaningful
                   community structure and trends in airport popularity
                   influenced by pandemic impacts on travel.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "2212.05053"
}

@ARTICLE{Chapel2021-ex,
  title         = "Unbalanced Optimal Transport through non-negative penalized
                   linear regression",
  author        = "Chapel, Laetitia and Flamary, R{\'e}mi and Wu, Haoran and
                   F{\'e}votte, C{\'e}dric and Gasso, Gilles",
  abstract      = "This paper addresses the problem of Unbalanced Optimal
                   Transport (UOT) in which the marginal conditions are relaxed
                   (using weighted penalties in lieu of equality) and no
                   additional regularization is enforced on the OT plan. In
                   this context, we show that the corresponding optimization
                   problem can be reformulated as a non-negative penalized
                   linear regression problem. This reformulation allows us to
                   propose novel algorithms inspired from inverse problems and
                   nonnegative matrix factorization. In particular, we consider
                   majorization-minimization which leads in our setting to
                   efficient multiplicative updates for a variety of penalties.
                   Furthermore, we derive for the first time an efficient
                   algorithm to compute the regularization path of UOT with
                   quadratic penalties. The proposed algorithm provides a
                   continuity of piece-wise linear OT plans converging to the
                   solution of balanced OT (corresponding to infinite penalty
                   weights). We perform several numerical experiments on
                   simulated and real data illustrating the new algorithms, and
                   provide a detailed discussion about more sophisticated
                   optimization tools that can further be used to solve OT
                   problems thanks to our reformulation.",
  month         =  jun,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2106.04145"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{noauthor_2016-jt,
  title     = "A geometrical stability condition for compressed sensing",
  abstract  = "During the last decade, the paradigm of compressed sensing has
               gained significant importance in the signal processing
               community. While the original id…",
  journal   = "Linear Algebra Appl.",
  publisher = "North-Holland",
  volume    =  504,
  pages     = "406--432",
  month     =  sep,
  year      =  2016
}

@MISC{Rolet_undated-rw,
  title        = "Fast dictionary learning with a smoothed Wasserstein loss",
  author       = "Rolet, Antoine and Cuturi, Marco and Peyr{\'e}, Gabriel",
  howpublished = "\url{http://proceedings.mlr.press/v51/rolet16.pdf}",
  note         = "Accessed: 2023-4-6"
}

@MISC{Meyer_undated-se,
  title        = "Linear regression under fixed-rank constraints: A Riemannian
                  approach",
  author       = "Meyer, Gilles and Sepulchre, Rodolphe",
  howpublished = "\url{https://orbi.uliege.be/bitstream/2268/90930/1/meyer-icml-11.pdf}",
  note         = "Accessed: 2023-4-28"
}

@ARTICLE{Chattopadhyay2022-jm,
  title    = "Interpretable by Design: Learning Predictors by Composing
              Interpretable Queries",
  author   = "Chattopadhyay, Aditya and Slocum, Stewart and Haeffele, Benjamin
              D and Vidal, Rene and Geman, Donald",
  abstract = "There is a growing concern about typically opaque decision-making
              with high-performance machine learning algorithms. Providing an
              explanation of the reasoning process in domain-specific terms can
              be crucial for adoption in risk-sensitive domains such as
              healthcare. We argue that machine learning algorithms should be
              interpretable by design and that the language in which these
              interpretations are expressed should be domain- and
              task-dependent. Consequently, we base our model's prediction on a
              family of user-defined and task-specific binary functions of the
              data, each having a clear interpretation to the end-user. We then
              minimize the expected number of queries needed for accurate
              prediction on any given input. As the solution is generally
              intractable, following prior work, we choose the queries
              sequentially based on information gain. However, in contrast to
              previous work, we need not assume the queries are conditionally
              independent. Instead, we leverage a stochastic generative model
              (VAE) and an MCMC algorithm (Unadjusted Langevin) to select the
              most informative query about the input based on previous
              query-answers. This enables the online determination of a query
              chain of whatever depth is required to resolve prediction
              ambiguities. Finally, experiments on vision and NLP tasks
              demonstrate the efficacy of our approach and its superiority over
              post-hoc explanations.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   = "PP",
  month    =  nov,
  year     =  2022,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Poe_undated-gz,
  title        = "Necessary and sufficient conditions for simultaneous state
                  and input recovery of linear systems with sparse inputs by
                  ℓ-minimization",
  author       = "Poe, Kyle and Mallada, Enrique and Vidal, Rene",
  howpublished = "\url{https://mallada.ece.jhu.edu/pubs/2023-Preprint-PMV.pdf}",
  note         = "Accessed: 2023-4-13"
}

@INPROCEEDINGS{Zhao2020-lq,
  title           = "A certifiably globally optimal solution to generalized
                     essential matrix estimation",
  booktitle       = "2020 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "Zhao, Ji and Xu, Wanting and Kneip, Laurent",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2020,
  conference      = "2020 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Seattle, WA, USA"
}

@ARTICLE{Song2023-wv,
  title         = "Consistency Models",
  author        = "Song, Yang and Dhariwal, Prafulla and Chen, Mark and
                   Sutskever, Ilya",
  abstract      = "Diffusion models have made significant breakthroughs in
                   image, audio, and video generation, but they depend on an
                   iterative generation process that causes slow sampling speed
                   and caps their potential for real-time applications. To
                   overcome this limitation, we propose consistency models, a
                   new family of generative models that achieve high sample
                   quality without adversarial training. They support fast
                   one-step generation by design, while still allowing for
                   few-step sampling to trade compute for sample quality. They
                   also support zero-shot data editing, like image inpainting,
                   colorization, and super-resolution, without requiring
                   explicit training on these tasks. Consistency models can be
                   trained either as a way to distill pre-trained diffusion
                   models, or as standalone generative models. Through
                   extensive experiments, we demonstrate that they outperform
                   existing distillation techniques for diffusion models in
                   one- and few-step generation. For example, we achieve the
                   new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on
                   ImageNet 64x64 for one-step generation. When trained as
                   standalone generative models, consistency models also
                   outperform single-step, non-adversarial generative models on
                   standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN
                   256x256.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2303.01469"
}

@ARTICLE{Spath1987-ty,
  title     = "On orthogonal linear l1 approximation",
  author    = "Spath, H and Watson, G A",
  journal   = "Numerische Mathematik",
  publisher = "Springer",
  volume    =  51,
  pages     = "531--543",
  year      =  1987
}

@INPROCEEDINGS{Lebeda2012-df,
  title      = "Fixing the locally optimized {RANSAC}",
  booktitle  = "Procedings of the British Machine Vision Conference 2012",
  author     = "Lebeda, Karel and Matas, Jir{\'\i} and Chum, Ondrej",
  publisher  = "British Machine Vision Association",
  number     =  95,
  year       =  2012,
  conference = "British Machine Vision Conference 2012",
  location   = "Surrey"
}

@INPROCEEDINGS{Tsakiris2015-fs,
  title           = "Dual Principal Component Pursuit",
  booktitle       = "2015 {IEEE} International Conference on Computer Vision
                     Workshop ({ICCVW})",
  author          = "Tsakiris, Manolis C and Vidal, Rene",
  publisher       = "IEEE",
  month           =  dec,
  year            =  2015,
  conference      = "2015 IEEE International Conference on Computer Vision:
                     Workshop (ICCVW)",
  location        = "Santiago"
}

@ARTICLE{Sun2015-ml,
  title         = "When Are Nonconvex Problems Not Scary?",
  author        = "Sun, Ju and Qu, Qing and Wright, John",
  abstract      = "In this note, we focus on smooth nonconvex optimization
                   problems that obey: (1) all local minimizers are also
                   global; and (2) around any saddle point or local maximizer,
                   the objective has a negative directional curvature. Concrete
                   applications such as dictionary learning, generalized phase
                   retrieval, and orthogonal tensor decomposition are known to
                   induce such structures. We describe a second-order
                   trust-region algorithm that provably converges to a global
                   minimizer efficiently, without special initializations.
                   Finally we highlight alternatives, and open problems in this
                   direction.",
  month         =  oct,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1510.06096"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{noauthor_2018-oc,
  title     = "Further notes on Birkhoff--von Neumann decomposition of doubly
               stochastic matrices",
  abstract  = "The well-known Birkhoff--von Neumann (BvN) decomposition
               expresses a doubly stochastic matrix as a convex combination of
               a number of permutation matric…",
  journal   = "Linear Algebra Appl.",
  publisher = "North-Holland",
  volume    =  554,
  pages     = "68--78",
  month     =  oct,
  year      =  2018
}

@ARTICLE{Laurent2017-nh,
  title         = "Deep linear neural networks with arbitrary loss: All local
                   minima are global",
  author        = "Laurent, Thomas and von Brecht, James",
  abstract      = "We consider deep linear networks with arbitrary convex
                   differentiable loss. We provide a short and elementary proof
                   of the fact that all local minima are global minima if the
                   hidden layers are either 1) at least as wide as the input
                   layer, or 2) at least as wide as the output layer. This
                   result is the strongest possible in the following sense: If
                   the loss is convex and Lipschitz but not differentiable then
                   deep linear networks can have sub-optimal local minima.",
  month         =  dec,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1712.01473"
}

@INPROCEEDINGS{Ding2021-mq,
  title           = "Globally optimal relative pose estimation with gravity
                     prior",
  booktitle       = "2021 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "Ding, Yaqing and Barath, Daniel and Yang, Jian and Kong,
                     Hui and Kukelova, Zuzana",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2021,
  conference      = "2021 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Nashville, TN, USA"
}

@INPROCEEDINGS{Kuntz2023-lw,
  title     = "Particle algorithms for maximum likelihood training of latent
               variable models",
  booktitle = "Proceedings of The 26th International Conference on Artificial
               Intelligence and Statistics",
  author    = "Kuntz, Juan and Lim, Jen Ning and Johansen, Adam M",
  editor    = "Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem",
  abstract  = "Neal and Hinton (1998) recast maximum likelihood estimation of
               any given latent variable model as the minimization of a free
               energy functional F, and the EM algorithm as coordinate descent
               applied to F. Here, we explore alternative ways to optimize the
               functional. In particular, we identify various gradient flows
               associated with F and show that their limits coincide with F's
               stationary points. By discretizing the flows, we obtain
               practical particle-based algorithms for maximum likelihood
               estimation in broad classes of latent variable models. The novel
               algorithms scale to high-dimensional settings and perform well
               in numerical experiments.",
  publisher = "PMLR",
  volume    =  206,
  pages     = "5134--5180",
  series    = "Proceedings of Machine Learning Research",
  year      =  2023
}

@INPROCEEDINGS{Chum2005-cw,
  title      = "Matching with {PROSAC} --- progressive sample consensus",
  booktitle  = "2005 {IEEE} Computer Society Conference on Computer Vision and
                Pattern Recognition ({CVPR'05})",
  author     = "Chum, O and Matas, J",
  publisher  = "IEEE",
  year       =  2005,
  conference = "2005 IEEE Computer Society Conference on Computer Vision and
                Pattern Recognition (CVPR'05)",
  location   = "San Diego, CA, USA"
}

@INPROCEEDINGS{Yang2014-dt,
  title     = "Optimal Essential Matrix Estimation via {Inlier-Set}
               Maximization",
  booktitle = "Computer Vision -- {ECCV} 2014",
  author    = "Yang, Jiaolong and Li, Hongdong and Jia, Yunde",
  abstract  = "In this paper, we extend the globally optimal ``rotation space
               search'' method [11] to essential matrix estimation in the
               presence of feature mismatches or outliers. The problem is
               formulated as inlier-set cardinality maximization, and solved
               via branch-and-bound global optimization which searches the
               entire essential manifold formed by all essential matrices. Our
               main contributions include an explicit, geometrically meaningful
               essential manifold parametrization using a 5D direct product
               space of a solid 2D disk and a solid 3D ball, as well as
               efficient closed-form bounding functions. Experiments on both
               synthetic data and real images have confirmed the efficacy of
               our method. The method is mostly suitable for applications where
               robustness and accuracy are paramount. It can also be used as a
               benchmark for method evaluation.",
  publisher = "Springer International Publishing",
  pages     = "111--126",
  year      =  2014
}

@ARTICLE{Yang2019-bg,
  title         = "Graduated {Non-Convexity} for Robust Spatial Perception:
                   From {Non-Minimal} Solvers to Global Outlier Rejection",
  author        = "Yang, Heng and Antonante, Pasquale and Tzoumas, Vasileios
                   and Carlone, Luca",
  abstract      = "Semidefinite Programming (SDP) and Sums-of-Squares (SOS)
                   relaxations have led to certifiably optimal non-minimal
                   solvers for several robotics and computer vision problems.
                   However, most non-minimal solvers rely on least-squares
                   formulations, and, as a result, are brittle against
                   outliers. While a standard approach to regain robustness
                   against outliers is to use robust cost functions, the latter
                   typically introduce other non-convexities, preventing the
                   use of existing non-minimal solvers. In this paper, we
                   enable the simultaneous use of non-minimal solvers and
                   robust estimation by providing a general-purpose approach
                   for robust global estimation, which can be applied to any
                   problem where a non-minimal solver is available for the
                   outlier-free case. To this end, we leverage the
                   Black-Rangarajan duality between robust estimation and
                   outlier processes (which has been traditionally applied to
                   early vision problems), and show that graduated
                   non-convexity (GNC) can be used in conjunction with
                   non-minimal solvers to compute robust solutions, without
                   requiring an initial guess. Although GNC's global optimality
                   cannot be guaranteed, we demonstrate the empirical
                   robustness of the resulting robust non-minimal solvers in
                   applications, including point cloud and mesh registration,
                   pose graph optimization, and image-based object pose
                   estimation (also called shape alignment). Our solvers are
                   robust to 70-80\% of outliers, outperform RANSAC, are more
                   accurate than specialized local solvers, and faster than
                   specialized global solvers. We also propose the first
                   certifiably optimal non-minimal solver for shape alignment
                   using SOS relaxation.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1909.08605"
}

@ARTICLE{Antonante2022-tu,
  title     = "Outlier-robust estimation: Hardness, minimally tuned algorithms,
               and applications",
  author    = "Antonante, Pasquale and Tzoumas, Vasileios and Yang, Heng and
               Carlone, Luca",
  journal   = "IEEE Trans. Robot.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  38,
  number    =  1,
  pages     = "281--301",
  month     =  feb,
  year      =  2022,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}

@ARTICLE{Cai2022-ik,
  title         = "{Semantic-Enhanced} Image Clustering",
  author        = "Cai, Shaotian and Qiu, Liping and Chen, Xiaojun and Zhang,
                   Qin and Chen, Longteng",
  abstract      = "Image clustering is an important and open-challenging task
                   in computer vision. Although many methods have been proposed
                   to solve the image clustering task, they only explore images
                   and uncover clusters according to the image features, thus
                   being unable to distinguish visually similar but
                   semantically different images. In this paper, we propose to
                   investigate the task of image clustering with the help of a
                   visual-language pre-training model. Different from the
                   zero-shot setting, in which the class names are known, we
                   only know the number of clusters in this setting. Therefore,
                   how to map images to a proper semantic space and how to
                   cluster images from both image and semantic spaces are two
                   key problems. To solve the above problems, we propose a
                   novel image clustering method guided by the visual-language
                   pre-training model CLIP, named
                   \textbackslashtextbf\{Semantic-Enhanced Image Clustering
                   (SIC)\}. In this new method, we propose a method to map the
                   given images to a proper semantic space first and efficient
                   methods to generate pseudo-labels according to the
                   relationships between images and semantics. Finally, we
                   propose performing clustering with consistency learning in
                   both image space and semantic space, in a self-supervised
                   learning fashion. The theoretical result of convergence
                   analysis shows that our proposed method can converge at a
                   sublinear speed. Theoretical analysis of expectation risk
                   also shows that we can reduce the expected risk by improving
                   neighborhood consistency, increasing prediction confidence,
                   or reducing neighborhood imbalance. Experimental results on
                   five benchmark datasets clearly show the superiority of our
                   new method.",
  month         =  aug,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2208.09849"
}

@ARTICLE{Adaloglou2023-ye,
  title         = "Exploring the Limits of Deep Image Clustering using
                   Pretrained Models",
  author        = "Adaloglou, Nikolas and Michels, Felix and Kalisch, Hamza and
                   Kollmann, Markus",
  abstract      = "We present a general methodology that learns to classify
                   images without labels by leveraging pretrained feature
                   extractors. Our approach involves self-distillation training
                   of clustering heads, based on the fact that nearest
                   neighbors in the pretrained feature space are likely to
                   share the same label. We propose a novel objective to learn
                   associations between images by introducing a variant of
                   pointwise mutual information together with instance
                   weighting. We demonstrate that the proposed objective is
                   able to attenuate the effect of false positive pairs while
                   efficiently exploiting the structure in the pretrained
                   feature space. As a result, we improve the clustering
                   accuracy over $k$-means on $17$ different pretrained models
                   by $6.1$\% and $12.2$\% on ImageNet and CIFAR100,
                   respectively. Finally, using self-supervised pretrained
                   vision transformers we push the clustering accuracy on
                   ImageNet to $61.6$\%. The code will be open-sourced.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2303.17896"
}

@ARTICLE{Russakovsky2015-ax,
  title    = "{ImageNet} Large Scale Visual Recognition Challenge",
  author   = "Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan
              and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and
              Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and
              Berg, Alexander C and Fei-Fei, Li",
  abstract = "The ImageNet Large Scale Visual Recognition Challenge is a
              benchmark in object category classification and detection on
              hundreds of object categories and millions of images. The
              challenge has been run annually from 2010 to present, attracting
              participation from more than fifty institutions. This paper
              describes the creation of this benchmark dataset and the advances
              in object recognition that have been possible as a result. We
              discuss the challenges of collecting large-scale ground truth
              annotation, highlight key breakthroughs in categorical object
              recognition, provide a detailed analysis of the current state of
              the field of large-scale image classification and object
              detection, and compare the state-of-the-art computer vision
              accuracy with human accuracy. We conclude with lessons learned in
              the 5 years of the challenge, and propose future directions and
              improvements.",
  journal  = "Int. J. Comput. Vis.",
  volume   =  115,
  number   =  3,
  pages    = "211--252",
  month    =  dec,
  year     =  2015
}

@INPROCEEDINGS{Souvenir2005-sy,
  title     = "Manifold clustering",
  booktitle = "Tenth {IEEE} International Conference on Computer Vision
               ({ICCV'05}) Volume 1",
  author    = "Souvenir, R and Pless, R",
  abstract  = "Manifold learning has become a vital tool in data driven methods
               for interpretation of video, motion capture, and handwritten
               character data when they lie on a low dimensional, nonlinear
               manifold. This work extends manifold learning to classify and
               parameterize unlabeled data which lie on multiple, intersecting
               manifolds. This approach significantly increases the domain to
               which manifold learning methods can be applied, allowing
               parameterization of example manifolds such as figure eights and
               intersecting paths which are quite common in natural data sets.
               This approach introduces several technical contributions which
               may be of broader interest, including node-weighted
               multidimensional scaling and a fast algorithm for weighted
               low-rank approximation for rank-one weight matrices. We show
               examples for intersecting manifolds of mixed topology and
               dimension and demonstrations on human motion capture data.",
  volume    =  1,
  pages     = "648--653 Vol. 1",
  month     =  oct,
  year      =  2005,
  keywords  = "Manifolds;Clustering algorithms;Multidimensional
               systems;Approximation algorithms;Independent component
               analysis;Humans;Computer science;Data
               engineering;Drives;Learning systems"
}

@ARTICLE{Chen2018-eh,
  title         = "The Sparse Manifold Transform",
  author        = "Chen, Yubei and Paiton, Dylan M and Olshausen, Bruno A",
  abstract      = "We present a signal representation framework called the
                   sparse manifold transform that combines key ideas from
                   sparse coding, manifold learning, and slow feature analysis.
                   It turns non-linear transformations in the primary sensory
                   signal space into linear interpolations in a
                   representational embedding space while maintaining
                   approximate invertibility. The sparse manifold transform is
                   an unsupervised and generative framework that explicitly and
                   simultaneously models the sparse discreteness and
                   low-dimensional manifold structure found in natural scenes.
                   When stacked, it also models hierarchical composition. We
                   provide a theoretical description of the transform and
                   demonstrate properties of the learned representation on both
                   synthetic data and natural videos.",
  month         =  jun,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1806.08887"
}

@INPROCEEDINGS{Barath2019-xc,
  title           = "{MAGSAC}: Marginalizing Sample Consensus",
  booktitle       = "2019 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "Barath, Daniel and Matas, Jiri and Noskova, Jana",
  abstract        = "The MAGSAC algorithm for robust model fitting without
                     using an inlier-outlier threshold - danini/magsac: The
                     MAGSAC algorithm for robust model fitting without using an
                     inlier-outlier threshold",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2019,
  copyright       = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  language        = "en",
  conference      = "2019 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Long Beach, CA, USA"
}

@ARTICLE{Lajoie2018-uj,
  title         = "Modeling Perceptual Aliasing in {SLAM} via
                   {Discrete-Continuous} Graphical Models",
  author        = "Lajoie, Pierre-Yves and Hu, Siyi and Beltrame, Giovanni and
                   Carlone, Luca",
  abstract      = "Perceptual aliasing is one of the main causes of failure for
                   Simultaneous Localization and Mapping (SLAM) systems
                   operating in the wild. Perceptual aliasing is the phenomenon
                   where different places generate a similar visual (or, in
                   general, perceptual) footprint. This causes spurious
                   measurements to be fed to the SLAM estimator, which
                   typically results in incorrect localization and mapping
                   results. The problem is exacerbated by the fact that those
                   outliers are highly correlated, in the sense that perceptual
                   aliasing creates a large number of mutually-consistent
                   outliers. Another issue stems from the fact that most
                   state-of-the-art techniques rely on a given trajectory guess
                   (e.g., from odometry) to discern between inliers and
                   outliers and this makes the resulting pipeline brittle,
                   since the accumulation of error may result in incorrect
                   choices and recovery from failures is far from trivial. This
                   work provides a unified framework to model perceptual
                   aliasing in SLAM and provides practical algorithms that can
                   cope with outliers without relying on any initial guess. We
                   present two main contributions. The first is a
                   Discrete-Continuous Graphical Model (DC-GM) for SLAM: the
                   continuous portion of the DC-GM captures the standard SLAM
                   problem, while the discrete portion describes the selection
                   of the outliers and models their correlation. The second
                   contribution is a semidefinite relaxation to perform
                   inference in the DC-GM that returns estimates with provable
                   sub-optimality guarantees. Experimental results on standard
                   benchmarking datasets show that the proposed technique
                   compares favorably with state-of-the-art methods while not
                   relying on an initial guess for optimization.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "1810.11692"
}

@ARTICLE{Maunu2019-yd,
  title         = "Robust Subspace Recovery with Adversarial Outliers",
  author        = "Maunu, Tyler and Lerman, Gilad",
  abstract      = "We study the problem of robust subspace recovery (RSR) in
                   the presence of adversarial outliers. That is, we seek a
                   subspace that contains a large portion of a dataset when
                   some fraction of the data points are arbitrarily corrupted.
                   We first examine a theoretical estimator that is intractable
                   to calculate and use it to derive information-theoretic
                   bounds of exact recovery. We then propose two tractable
                   estimators: a variant of RANSAC and a simple relaxation of
                   the theoretical estimator. The two estimators are fast to
                   compute and achieve state-of-the-art theoretical performance
                   in a noiseless RSR setting with adversarial outliers. The
                   former estimator achieves better theoretical guarantees in
                   the noiseless case, while the latter estimator is robust to
                   small noise, and its guarantees significantly improve with
                   non-adversarial models of outliers. We give a complete
                   comparison of guarantees for the adversarial RSR problem, as
                   well as a short discussion on the estimation of affine
                   subspaces.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1904.03275"
}

@ARTICLE{Barath2019-fw,
  title         = "Progressive {NAPSAC}: sampling from gradually growing
                   neighborhoods",
  author        = "Barath, Daniel and Ivashechkin, Maksym and Matas, Jiri",
  abstract      = "We propose Progressive NAPSAC, P-NAPSAC in short, which
                   merges the advantages of local and global sampling by
                   drawing samples from gradually growing neighborhoods.
                   Exploiting the fact that nearby points are more likely to
                   originate from the same geometric model, P-NAPSAC finds
                   local structures earlier than global samplers. We show that
                   the progressive spatial sampling in P-NAPSAC can be
                   integrated with PROSAC sampling, which is applied to the
                   first, location-defining, point. P-NAPSAC is embedded in
                   USAC, a state-of-the-art robust estimation pipeline, which
                   we further improve by implementing its local optimization as
                   in Graph-Cut RANSAC. We call the resulting estimator USAC*.
                   The method is tested on homography and fundamental matrix
                   fitting on a total of 10,691 models from seven publicly
                   available datasets. USAC* with P-NAPSAC outperforms
                   reference methods in terms of speed on all problems.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1906.02295"
}

@INPROCEEDINGS{Torr2002-nh,
  title     = "Napsac: High noise, high dimensional robust estimation-it's in
               the bag",
  booktitle = "British Machine Vision Conference ({BMVC})",
  author    = "Torr, Philip Hilaire and Nasuto, Slawomir J and Bishop, John
               Mark",
  abstract  = "An umber of the most powerful robust estimation algorithms, such
               as RANSAC, MINPRAN and LMS ,h ave their basis in selecting
               random minimal sets of data to instantiate hypotheses. However,
               their performance degrades in higher dimensional spaces due to
               the exponentially decreasing probability of sampling a set that
               is composed entirely of inliers. In order to overcome this,
               rather than picking sets at random, a new strategy is proposed
               that alters the way samples are taken, under the assumption that
               inliers will tend to be closer to one another than outliers.
               Based on this premise, the NAPSAC (N Adjacent Points SAmple
               Consensus) algorithm is derived and its performance is shown to
               be superior to RANSAC in both high noise and high dimensional
               spaces.",
  year      =  2002
}

@ARTICLE{Torr2000-nr,
  title     = "{MLESAC}: A New Robust Estimator with Application to Estimating
               Image Geometry",
  author    = "Torr, P H S and Zisserman, A",
  abstract  = "A new method is presented for robustly estimating multiple view
               relations from point correspondences. The method comprises two
               parts. The first is a new robust estimator MLESAC which is a
               generalization of the RANSAC estimator. It adopts the same
               sampling strategy as RANSAC to generate putative solutions, but
               chooses the solution that maximizes the likelihood rather than
               just the number of inliers. The second part of the algorithm is
               a general purpose method for automatically parameterizing these
               relations, using the output of MLESAC. A difficulty with
               multiview image relations is that there are often nonlinear
               constraints between the parameters, making optimization a
               difficult task. The parameterization method overcomes the
               difficulty of nonlinear constraints and conducts a constrained
               optimization. The method is general and its use is illustrated
               for the estimation of fundamental matrices, image--image
               homographies, and quadratic transformations. Results are given
               for both synthetic and real images. It is demonstrated that the
               method gives results equal or superior to those of previous
               approaches.",
  journal   = "Comput. Vis. Image Underst.",
  publisher = "Academic Press",
  volume    =  78,
  number    =  1,
  pages     = "138--156",
  month     =  apr,
  year      =  2000
}

@ARTICLE{Stewart1995-wx,
  title    = "{MINPRAN}: a new robust estimator for computer vision",
  author   = "Stewart, C V",
  abstract = "MINPRAN is a new robust estimator capable of finding good fits in
              data sets containing more than 50\% outliers. Unlike other
              techniques that handle large outlier percentages, MINPRAN does
              not rely on a known error bound for the good data. Instead, it
              assumes the bad data are randomly distributed within the dynamic
              range of the sensor. Based on this, MINPRAN uses random sampling
              to search for the fit and the inliers to the fit that are least
              likely to have occurred randomly. It runs in time O(N/sup 2/+SN
              log N), where S is the number of random samples and N is the
              number of data points. We demonstrate analytically that MINPRAN
              distinguished good fits to random data and MINPRAN finds accurate
              fits and nearly the correct number of inliers, regardless of the
              percentage of true inliers. We confirm MINPRAN's properties
              experimentally on synthetic data and show it compares favorably
              to least median of squares. Finally, we apply MINPRAN to fitting
              planar surface patches and eliminating outliers in range data
              taken from complicated scenes.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  17,
  number   =  10,
  pages    = "925--938",
  month    =  oct,
  year     =  1995,
  keywords = "Robustness;Computer vision;Surface fitting;Electric
              breakdown;Computer errors;Parameter estimation;Dynamic
              range;Sampling methods;Tin;Surface reconstruction"
}

@INPROCEEDINGS{Chum2003-ln,
  title     = "Locally Optimized {RANSAC}",
  booktitle = "Pattern Recognition",
  author    = "Chum, Ond{\v r}ej and Matas, Ji{\v r}{\'\i} and Kittler, Josef",
  abstract  = "A new enhancement of ransac, the locally optimized ransac
               (lo-ransac), is introduced. It has been observed that, to find
               an optimal solution (with a given probability), the number of
               samples drawn in ransac is significantly higher than predicted
               from the mathematical model. This is due to the incorrect
               assumption, that a model with parameters computed from an
               outlier-free sample is consistent with all inliers. The
               assumption rarely holds in practice. The locally optimized
               ransac makes no new assumptions about the data, on the contrary
               -- it makes the above-mentioned assumption valid by applying
               local optimization to the solution estimated from the random
               sample.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "236--243",
  year      =  2003
}

@ARTICLE{Yang2021-tq,
  title         = "Boosting {RANSAC} via Dual Principal Component Pursuit",
  author        = "Yang, Yunchen and Zhang, Xinyue and Ding, Tianjiao and
                   Robinson, Daniel P and Vidal, Rene and Tsakiris, Manolis C",
  abstract      = "In this paper, we revisit the problem of local optimization
                   in RANSAC. Once a so-far-the-best model has been found, we
                   refine it via Dual Principal Component Pursuit (DPCP), a
                   robust subspace learning method with strong theoretical
                   support and efficient algorithms. The proposed DPCP-RANSAC
                   has far fewer parameters than existing methods and is
                   scalable. Experiments on estimating two-view homographies,
                   fundamental and essential matrices, and three-view
                   homographic tensors using large-scale datasets show that our
                   approach consistently has higher accuracy than
                   state-of-the-art alternatives.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2110.02918"
}

@ARTICLE{Bhojanapalli2016-cp,
  title         = "Global optimality of local search for low rank matrix
                   recovery",
  author        = "Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro,
                   Nathan",
  abstract      = "We show that there are no spurious local minima in the
                   non-convex factorized parametrization of low-rank matrix
                   recovery from incoherent linear measurements. With noisy
                   measurements we show all local minima are very close to a
                   global optimum. Together with a curvature bound at saddle
                   points, this yields a polynomial time global convergence
                   guarantee for stochastic gradient descent \{\textbackslashem
                   from random initialization\}.",
  month         =  may,
  year          =  2016,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1605.07221"
}

@BOOK{Hartley2003-ki,
  title     = "Multiple View Geometry in Computer Vision",
  author    = "Hartley, Richard and Zisserman, Andrew",
  abstract  = "A basic problem in computer vision is to understand the
               structure of a real world scene given several images of it.
               Techniques for solving this problem are taken from projective
               geometry and photogrammetry. Here, the authors cover the
               geometric principles and their algebraic representation in terms
               of camera projection matrices, the fundamental matrix and the
               trifocal tensor. The theory and methods of computation of these
               entities are discussed with real examples, as is their use in
               the reconstruction of scenes from multiple images. The new
               edition features an extended introduction covering the key ideas
               in the book (which itself has been updated with additional
               examples and appendices) and significant new results which have
               appeared since the first edition. Comprehensive background
               material is provided, so readers familiar with linear algebra
               and basic numerical methods can understand the projective
               geometry and estimation algorithms presented, and implement the
               algorithms directly from the book.",
  publisher = "Cambridge University Press",
  year      =  2003,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Zhu2019-za,
  title     = "A linearly convergent method for non-smooth non-convex
               optimization on the grassmannian with applications to robust
               subspace and dictionary learning",
  booktitle = "Adv. Neural Inf. Process. Syst.",
  author    = "Zhu, Zhihui and Ding, Tianyu and Robinson, Daniel and Tsakiris,
               Manolis and Vidal, Ren{\'e}",
  abstract  = "… of the projected Riemannian subgradient method with different
               choices of step size. We observe linear convergence for the
               geometrically diminishing step size, which converges much …",
  publisher = "proceedings.neurips.cc",
  volume    =  32,
  year      =  2019
}

@ARTICLE{Qu2019-ko,
  title         = "A nonconvex approach for exact and efficient multichannel
                   sparse blind deconvolution",
  author        = "Qu, Qing and Li, Xiao and Zhu, Zhihui",
  abstract      = "We study the multi-channel sparse blind deconvolution
                   (MCS-BD) problem, whose task is to simultaneously recover a
                   kernel $\mathbf a$ and multiple sparse inputs $\{\mathbf
                   x_i\}_\{i=1\}^p$ from their circulant convolution $\mathbf
                   y_i = \mathbf a \circledast \mathbf x_i $ ($i=1,\cdots,p$).
                   We formulate the task as a nonconvex optimization problem
                   over the sphere. Under mild statistical assumptions of the
                   data, we prove that the vanilla Riemannian gradient descent
                   (RGD) method, with random initializations, provably recovers
                   both the kernel $\mathbf a$ and the signals $\{\mathbf
                   x_i\}_\{i=1\}^p$ up to a signed shift ambiguity. In
                   comparison with state-of-the-art results, our work shows
                   significant improvements in terms of sample complexity and
                   computational efficiency. Our theoretical results are
                   corroborated by numerical experiments, which demonstrate
                   superior performance of the proposed approach over the
                   previous methods on both synthetic and real datasets.",
  month         =  aug,
  year          =  2019,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "eess.SP",
  eprint        = "1908.10776"
}

@ARTICLE{Goldfarb2017-wo,
  title    = "Using negative curvature in solving nonlinear programs",
  author   = "Goldfarb, Donald and Mu, Cun and Wright, John and Zhou, Chaoxu",
  abstract = "Minimization methods that search along a curvilinear path
              composed of a non-ascent negative curvature direction in addition
              to the direction of steepest descent, dating back to the late
              1970s, have been an effective approach to finding a stationary
              point of a function at which its Hessian is positive
              semidefinite. For constrained nonlinear programs arising from
              recent applications, the primary goal is to find a stationary
              point that satisfies the second-order necessary optimality
              conditions. Motivated by this, we generalize the approach of
              using negative curvature directions from unconstrained
              optimization to equality constrained problems and prove that our
              proposed negative curvature method is guaranteed to converge to a
              stationary point satisfying second-order necessary conditions.",
  journal  = "Comput. Optim. Appl.",
  volume   =  68,
  number   =  3,
  pages    = "479--502",
  month    =  dec,
  year     =  2017
}

@ARTICLE{Gilboa2018-or,
  title         = "Efficient Dictionary Learning with Gradient Descent",
  author        = "Gilboa, Dar and Buchanan, Sam and Wright, John",
  abstract      = "Randomly initialized first-order optimization algorithms are
                   the method of choice for solving many high-dimensional
                   nonconvex problems in machine learning, yet general
                   theoretical guarantees cannot rule out convergence to
                   critical points of poor objective value. For some highly
                   structured nonconvex problems however, the success of
                   gradient descent can be understood by studying the geometry
                   of the objective. We study one such problem -- complete
                   orthogonal dictionary learning, and provide converge
                   guarantees for randomly initialized gradient descent to the
                   neighborhood of a global optimum. The resulting rates scale
                   as low order polynomials in the dimension even though the
                   objective possesses an exponential number of saddle points.
                   This efficient convergence can be viewed as a consequence of
                   negative curvature normal to the stable manifolds associated
                   with saddle points, and we provide evidence that this
                   feature is shared by other nonconvex problems of importance
                   as well.",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1809.10313"
}

@BOOK{Harris2013-vu,
  title     = "Algebraic Geometry: A First Course",
  author    = "Harris, Joe",
  abstract  = "This book is based on one-semester courses given at Harvard in
               1984, at Brown in 1985, and at Harvard in 1988. It is intended
               to be, as the title suggests, a first introduction to the
               subject. Even so, a few words are in order about the purposes of
               the book. Algebraic geometry has developed tremendously over the
               last century. During the 19th century, the subject was practiced
               on a relatively concrete, down-to-earth level; the main objects
               of study were projective varieties, and the techniques for the
               most part were grounded in geometric constructions. This
               approach flourished during the middle of the century and reached
               its culmination in the work of the Italian school around the end
               of the 19th and the beginning of the 20th centuries. Ultimately,
               the subject was pushed beyond the limits of its foundations: by
               the end of its period the Italian school had progressed to the
               point where the language and techniques of the subject could no
               longer serve to express or carry out the ideas of its best
               practitioners.",
  publisher = "Springer Science \& Business Media",
  month     =  nov,
  year      =  2013,
  language  = "en"
}

@BOOK{Ma2003-nu,
  title     = "An Invitation to {3-D} Vision",
  author    = "Ma, Yi and Soatto, Stefano and Ko{\v s}eck{\'a}, Jana and
               Shankar Sastry, S",
  publisher = "Springer New York",
  year      =  2003
}

@ARTICLE{Bandeira2016-fi,
  title         = "On the low-rank approach for semidefinite programs arising
                   in synchronization and community detection",
  author        = "Bandeira, Afonso S and Boumal, Nicolas and Voroninski,
                   Vladislav",
  abstract      = "To address difficult optimization problems, convex
                   relaxations based on semidefinite programming are now common
                   place in many fields. Although solvable in polynomial time,
                   large semidefinite programs tend to be computationally
                   challenging. Over a decade ago, exploiting the fact that in
                   many applications of interest the desired solutions are low
                   rank, Burer and Monteiro proposed a heuristic to solve such
                   semidefinite programs by restricting the search space to
                   low-rank matrices. The accompanying theory does not explain
                   the extent of the empirical success. We focus on
                   Synchronization and Community Detection problems and provide
                   theoretical guarantees shedding light on the remarkable
                   efficiency of this heuristic.",
  month         =  feb,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1602.04426"
}

@ARTICLE{Boumal2013-mo,
  title         = "Manopt, a Matlab toolbox for optimization on manifolds",
  author        = "Boumal, Nicolas and Mishra, Bamdev and Absil, P-A and
                   Sepulchre, Rodolphe",
  abstract      = "Optimization on manifolds is a rapidly developing branch of
                   nonlinear optimization. Its focus is on problems where the
                   smooth geometry of the search space can be leveraged to
                   design efficient numerical algorithms. In particular,
                   optimization on manifolds is well-suited to deal with rank
                   and orthogonality constraints. Such structured constraints
                   appear pervasively in machine learning applications,
                   including low-rank matrix completion, sensor network
                   localization, camera network registration, independent
                   component analysis, metric learning, dimensionality
                   reduction and so on. The Manopt toolbox, available at
                   www.manopt.org, is a user-friendly, documented piece of
                   software dedicated to simplify experimenting with state of
                   the art Riemannian optimization algorithms. We aim
                   particularly at reaching practitioners outside our field.",
  month         =  aug,
  year          =  2013,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.MS",
  eprint        = "1308.5200"
}

@ARTICLE{Razaviyayn2012-oi,
  title         = "A Unified Convergence Analysis of Block Successive
                   Minimization Methods for Nonsmooth Optimization",
  author        = "Razaviyayn, Meisam and Hong, Mingyi and Luo, Zhi-Quan",
  abstract      = "The block coordinate descent (BCD) method is widely used for
                   minimizing a continuous function f of several block
                   variables. At each iteration of this method, a single block
                   of variables is optimized, while the remaining variables are
                   held fixed. To ensure the convergence of the BCD method, the
                   subproblem to be optimized in each iteration needs to be
                   solved exactly to its unique optimal solution.
                   Unfortunately, these requirements are often too restrictive
                   for many practical scenarios. In this paper, we study an
                   alternative inexact BCD approach which updates the variable
                   blocks by successively minimizing a sequence of
                   approximations of f which are either locally tight upper
                   bounds of f or strictly convex local approximations of f. We
                   focus on characterizing the convergence properties for a
                   fairly wide class of such methods, especially for the cases
                   where the objective functions are either non-differentiable
                   or nonconvex. Our results unify and extend the existing
                   convergence results for many classical algorithms such as
                   the BCD method, the difference of convex functions (DC)
                   method, the expectation maximization (EM) algorithm, as well
                   as the alternating proximal minimization algorithm.",
  month         =  sep,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1209.2385"
}

@ARTICLE{Zilber2023-tp,
  title         = "Imbalanced Mixed Linear Regression",
  author        = "Zilber, Pini and Nadler, Boaz",
  abstract      = "We consider the problem of mixed linear regression (MLR),
                   where each observed sample belongs to one of $K$ unknown
                   linear models. In practical applications, the proportions of
                   the $K$ components are often imbalanced. Unfortunately, most
                   MLR methods do not perform well in such settings. Motivated
                   by this practical challenge, in this work we propose
                   Mix-IRLS, a novel, simple and fast algorithm for MLR with
                   excellent performance on both balanced and imbalanced
                   mixtures. In contrast to popular approaches that recover the
                   $K$ models simultaneously, Mix-IRLS does it sequentially
                   using tools from robust regression. Empirically, Mix-IRLS
                   succeeds in a broad range of settings where other methods
                   fail. These include imbalanced mixtures, small sample sizes,
                   presence of outliers, and an unknown number of models $K$.
                   In addition, Mix-IRLS outperforms competing methods on
                   several real-world datasets, in some cases by a large
                   margin. We complement our empirical results by deriving a
                   recovery guarantee for Mix-IRLS, which highlights its
                   advantage on imbalanced mixtures.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "2301.12559"
}

@ARTICLE{Yaras_undated-zg,
  title    = "Neural Collapse with Normalized Features: A Geometric Analysis
              over the Riemannian Manifold",
  author   = "Yaras, Can",
  abstract = "Experimental code for ``Neural Collapse with Normalized Features:
              A Geometric Analysis over the Riemannian Manifold'' -
              cjyaras/normalized-neural-collapse: Experimental code for
              ``Neural Collapse with Normalized Features: A Geometric Analysis
              over the Riemannian Manifold''",
  language = "en"
}

@ARTICLE{Ravishankar2013-jj,
  title    = "Learning Sparsifying Transforms",
  author   = "Ravishankar, Saiprasad and Bresler, Yoram",
  abstract = "The sparsity of signals and images in a certain transform domain
              or dictionary has been exploited in many applications in signal
              and image processing. Analytical sparsifying transforms such as
              Wavelets and DCT have been widely used in compression standards.
              Recently, synthesis sparsifying dictionaries that are directly
              adapted to the data have become popular especially in
              applications such as image denoising, inpainting, and medical
              image reconstruction. While there has been extensive research on
              learning synthesis dictionaries and some recent work on learning
              analysis dictionaries, the idea of learning sparsifying
              transforms has received no attention. In this work, we propose
              novel problem formulations for learning sparsifying transforms
              from data. The proposed alternating minimization algorithms give
              rise to well-conditioned square transforms. We show the
              superiority of our approach over analytical sparsifying
              transforms such as the DCT for signal and image representation.
              We also show promising performance in signal denoising using the
              learnt sparsifying transforms. The proposed approach is much
              faster than previous approaches involving learnt synthesis, or
              analysis dictionaries.",
  journal  = "IEEE Trans. Signal Process.",
  volume   =  61,
  number   =  5,
  pages    = "1072--1086",
  month    =  mar,
  year     =  2013,
  keywords = "Transforms;Dictionaries;Analytical models;Encoding;Noise
              measurement;Algorithm design and analysis;Signal
              analysis;Compressed sensing;dictionary learning;signal
              denoising;sparse representation;sparsifying transforms"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Jaillet2010-zh,
  title     = "An {L1} criterion for dictionary learning by subspace
               identification",
  booktitle = "2010 {IEEE} International Conference on Acoustics, Speech and
               Signal Processing",
  author    = "Jaillet, Florent and Gribonval, R{\'e}mi and Plumbley, Mark D
               and Zayyani, Hadi",
  abstract  = "We propose an ℓ1 criterion for dictionary learning for sparse
               signal representation. Instead of directly searching for the
               dictionary vectors, our dictionary learning approach identifies
               vectors that are orthogonal to the subspaces in which the
               training data concentrate. We study conditions on the
               coefficients of training data that guarantee that ideal normal
               vectors deduced from the dictionary are local optima of the
               criterion. We illustrate the behavior of the criterion on a 2D
               example, showing that the local minima correspond to ideal
               normal vectors when the number of training data is sufficient.
               We conclude by describing an algorithm that can be used to
               optimize the criterion in higher dimension.",
  pages     = "5482--5485",
  month     =  mar,
  year      =  2010,
  keywords  = "Dictionaries;Training data;Sparse matrices;Vectors;Matrix
               decomposition;Computer science;Signal representations;Data
               analysis;FETs;Blind source separation;Sparse
               representation;dictionary learning;non-convex optimization"
}

@ARTICLE{Elhamifar2011-ts,
  title         = "{Block-Sparse} Recovery via Convex Optimization",
  author        = "Elhamifar, Ehsan and Vidal, Rene",
  abstract      = "Given a dictionary that consists of multiple blocks and a
                   signal that lives in the range space of only a few blocks,
                   we study the problem of finding a block-sparse
                   representation of the signal, i.e., a representation that
                   uses the minimum number of blocks. Motivated by signal/image
                   processing and computer vision applications, such as face
                   recognition, we consider the block-sparse recovery problem
                   in the case where the number of atoms in each block is
                   arbitrary, possibly much larger than the dimension of the
                   underlying subspace. To find a block-sparse representation
                   of a signal, we propose two classes of non-convex
                   optimization programs, which aim to minimize the number of
                   nonzero coefficient blocks and the number of nonzero
                   reconstructed vectors from the blocks, respectively. Since
                   both classes of problems are NP-hard, we propose convex
                   relaxations and derive conditions under which each class of
                   the convex programs is equivalent to the original non-convex
                   formulation. Our conditions depend on the notions of mutual
                   and cumulative subspace coherence of a dictionary, which are
                   natural generalizations of existing notions of mutual and
                   cumulative coherence. We evaluate the performance of the
                   proposed convex programs through simulations as well as real
                   experiments on face recognition. We show that treating the
                   face recognition problem as a block-sparse recovery problem
                   improves the state-of-the-art results by 10\% with only 25\%
                   of the training data.",
  month         =  apr,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "1104.0654"
}

@ARTICLE{Cai2023-qj,
  title    = "Matrix Completion with {Cross-Concentrated} Sampling: Bridging
              Uniform Sampling and {CUR} Sampling",
  author   = "Cai, Hanqin and Huang, Longxiu and Li, Pengyu and Needell, Deanna",
  abstract = "While uniform sampling has been widely studied in the matrix
              completion literature, CUR sampling approximates a low-rank
              matrix via row and column samples. Unfortunately, both sampling
              models lack flexibility for various circumstances in real-world
              applications. In this work, we propose a novel and
              easy-to-implement sampling strategy, coined Cross-Concentrated
              Sampling (CCS). By bridging uniform sampling and CUR sampling,
              CCS provides extra flexibility that can potentially save sampling
              costs in applications. In addition, we also provide a sufficient
              condition for CCS-based matrix completion. Moreover, we propose a
              highly efficient non-convex algorithm, termed Iterative CUR
              Completion (ICURC), for the proposed CCS model. Numerical
              experiments verify the empirical advantages of CCS and ICURC
              against uniform sampling and its baseline algorithms, on both
              synthetic and real-world datasets.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   = "PP",
  month    =  mar,
  year     =  2023,
  language = "en"
}

@INPROCEEDINGS{Lane2019-to,
  title     = "Classifying and comparing approaches to subspace clustering with
               missing data",
  booktitle = "Proceedings of the {IEEE/CVF} International Conference on
               Computer Vision Workshops",
  author    = "Lane, Connor and Boger, Ron and You, Chong and Tsakiris, Manolis
               and Haeffele, Benjamin and Vidal, Rene",
  pages     = "0--0",
  year      =  2019
}

@ARTICLE{Dikkala2021-fv,
  title         = "For Manifold Learning, Deep Neural Networks can be Locality
                   Sensitive Hash Functions",
  author        = "Dikkala, Nishanth and Kaplun, Gal and Panigrahy, Rina",
  abstract      = "It is well established that training deep neural networks
                   gives useful representations that capture essential features
                   of the inputs. However, these representations are poorly
                   understood in theory and practice. In the context of
                   supervised learning an important question is whether these
                   representations capture features informative for
                   classification, while filtering out non-informative noisy
                   ones. We explore a formalization of this question by
                   considering a generative process where each class is
                   associated with a high-dimensional manifold and different
                   classes define different manifolds. Under this model, each
                   input is produced using two latent vectors: (i) a ``manifold
                   identifier'' $\gamma$ and; (ii)~a ``transformation
                   parameter'' $\theta$ that shifts examples along the surface
                   of a manifold. E.g., $\gamma$ might represent a canonical
                   image of a dog, and $\theta$ might stand for variations in
                   pose, background or lighting. We provide theoretical and
                   empirical evidence that neural representations can be viewed
                   as LSH-like functions that map each input to an embedding
                   that is a function of solely the informative $\gamma$ and
                   invariant to $\theta$, effectively recovering the manifold
                   identifier $\gamma$. An important consequence of this
                   behavior is one-shot learning to unseen classes.",
  month         =  mar,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2103.06875"
}

@ARTICLE{Ge2016-oq,
  title         = "Matrix Completion has No Spurious Local Minimum",
  author        = "Ge, Rong and Lee, Jason D and Ma, Tengyu",
  abstract      = "Matrix completion is a basic machine learning problem that
                   has wide applications, especially in collaborative filtering
                   and recommender systems. Simple non-convex optimization
                   algorithms are popular and effective in practice. Despite
                   recent progress in proving various non-convex algorithms
                   converge from a good initial point, it remains unclear why
                   random or arbitrary initialization suffices in practice. We
                   prove that the commonly used non-convex objective function
                   for \textbackslashtextit\{positive semidefinite\} matrix
                   completion has no spurious local minima --- all local minima
                   must also be global. Therefore, many popular optimization
                   algorithms such as (stochastic) gradient descent can
                   provably solve positive semidefinite matrix completion with
                   \textbackslashtextit\{arbitrary\} initialization in
                   polynomial time. The result can be generalized to the
                   setting when the observed entries contain noise. We believe
                   that our main proof strategy can be useful for understanding
                   geometric properties of other statistical problems involving
                   partial or noisy observations.",
  month         =  may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1605.07272"
}

@INPROCEEDINGS{Chen2020-xc,
  title           = "Stochastic sparse subspace clustering",
  booktitle       = "2020 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition ({CVPR})",
  author          = "Chen, Ying and Li, Chun-Guang and You, Chong",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2020,
  conference      = "2020 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition (CVPR)",
  location        = "Seattle, WA, USA"
}

@ARTICLE{Cao2023-cy,
  title         = "Unsupervised Learning of Robust Spectral Shape Matching",
  author        = "Cao, Dongliang and Roetzer, Paul and Bernard, Florian",
  abstract      = "We propose a novel learning-based approach for robust 3D
                   shape matching. Our method builds upon deep functional maps
                   and can be trained in a fully unsupervised manner. Previous
                   deep functional map methods mainly focus on predicting
                   optimised functional maps alone, and then rely on
                   off-the-shelf post-processing to obtain accurate point-wise
                   maps during inference. However, this two-stage procedure for
                   obtaining point-wise maps often yields sub-optimal
                   performance. In contrast, building upon recent insights
                   about the relation between functional maps and point-wise
                   maps, we propose a novel unsupervised loss to couple the
                   functional maps and point-wise maps, and thereby directly
                   obtain point-wise maps without any post-processing. Our
                   approach obtains accurate correspondences not only for
                   near-isometric shapes, but also for more challenging
                   non-isometric shapes and partial shapes, as well as shapes
                   with different discretisation or topological noise. Using a
                   total of nine diverse datasets, we extensively evaluate the
                   performance and demonstrate that our method substantially
                   outperforms previous state-of-the-art methods, even compared
                   to recent supervised methods. Our code is available at
                   https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2304.14419"
}

@MISC{Oshri_undated-ui,
  title        = "Unsupervised learning of dense shape correspondence",
  author       = "Oshri, Halimi and Technion, Israel and Rodol{\`a}, Emanuele
                  and Technion, Alex Bronstein and Technion, Ron Kimmel",
  howpublished = "\url{https://openaccess.thecvf.com/content_CVPR_2019/papers/Halimi_Unsupervised_Learning_of_Dense_Shape_Correspondence_CVPR_2019_paper.pdf}",
  note         = "Accessed: 2023-6-6"
}

@INCOLLECTION{Halimi2019-cx,
  title     = "Chapter 7 - Computable invariants for curves and surfaces",
  booktitle = "Handbook of Numerical Analysis",
  author    = "Halimi, Oshri and Raviv, Dan and Aflalo, Yonathan and Kimmel,
               Ron",
  editor    = "Kimmel, Ron and Tai, Xue-Cheng",
  abstract  = "During the last decade the trend in image analysis has been to
               shift from axiomatically derived measures into ones that are
               extracted empirically from data samples. The problem is that
               accurate geometric data are often unavailable and thus, for
               proper data augmentation, the research community has resorted
               yet again to axiomatic construction of invariant measures for
               curves and surfaces. For some geometric problems, such as shape
               classification and matching, it is appealing to adopt learning
               approaches due to their potential accuracy and computational
               efficiency. Nevertheless, even within the deep learning arena,
               geometric invariants offer a natural criterion for the learning
               process. Using geometric invariants one can overcome the need
               for annotated data and replace it by a purely geometric measure,
               leading to an unsupervised or a semisupervised learning schemes.
               Here, we review such constructions that are useful for the
               geometric analysis of visual information. The measures we
               explore include the construction of a scale or similarity
               invariant arc-length for curves and surfaces, an affine
               invariant one, resulting spectral geometries, and potential
               signatures that reflect the result of the discrepancies between
               the corresponding metric spaces. As an example we study novel
               signatures for surfaces known as the self functional map, which
               allow us to translate the problem of shape matching into that of
               computing distances between matrices.",
  publisher = "Elsevier",
  volume    =  20,
  pages     = "273--314",
  month     =  jan,
  year      =  2019,
  keywords  = "Scale invariant; Affine invariant; Differential geometry;
               Laplace--Beltrami operator; Shape analysis; Gaussian curvature;
               53B21; 58D17"
}

@ARTICLE{Aberdam2019-jy,
  title     = "Multi-layer sparse coding: The holistic way",
  author    = "Aberdam, Aviad and Sulam, Jeremias and Elad, Michael",
  journal   = "SIAM J. Math. Data Sci.",
  publisher = "Society for Industrial \& Applied Mathematics (SIAM)",
  volume    =  1,
  number    =  1,
  pages     = "46--77",
  month     =  jan,
  year      =  2019,
  language  = "en"
}

@INPROCEEDINGS{Ornhag2020-bf,
  title     = "Bilinear Parameterization For Differentiable
               {Rank-Regularization}",
  booktitle = "2020 {IEEE/CVF} Conference on Computer Vision and Pattern
               Recognition Workshops ({CVPRW})",
  author    = "{\"O}rnhag, Marcus Valtonen and Olsson, Carl and Heyden, Anders",
  abstract  = "Low rank approximation is a commonly occurring problem in many
               computer vision and machine learning applications. There are two
               common ways of optimizing the resulting models. Either the set
               of matrices with a given rank can be explicitly parametrized
               using a bilinear factorization, or low rank can be implicitly
               enforced using regularization terms penalizing non-zero singular
               values. While the former approach results in differentiable
               problems that can be efficiently optimized using local quadratic
               approximation, the latter is typically not differentiable
               (sometimes even discontinuous) and requires first order
               subgradient or splitting methods. It is well known that gradient
               based methods exhibit slow convergence for ill-conditioned
               problems. In this paper we show how many non-differentiable
               regularization methods can be reformulated into smooth
               objectives using bilinear parameterization. This allows us to
               use standard second order methods, such as Levenberg- Marquardt
               (LM) and Variable Projection (VarPro), to achieve accurate
               solutions for ill-conditioned cases. We show on several real and
               synthetic experiments that our second order formulation
               converges to substantially more accurate solutions than
               competing state-of-the-art methods.1",
  pages     = "1416--1425",
  month     =  jun,
  year      =  2020,
  keywords  = "Convergence;Computer vision;Structure from
               motion;Robustness;Optimization;Standards;Deformable models"
}

@ARTICLE{Qian_undated-nx,
  title  = "Structures of Spurious Local Minima in k-means",
  author = "Qian, Wei and Zhang, Yuqian and Chen, Yudong"
}

@ARTICLE{Nesterov2012-ab,
  title     = "Efficiency of Coordinate Descent Methods on {Huge-Scale}
               Optimization Problems",
  author    = "Nesterov, Yu",
  abstract  = "In this paper we propose new methods for solving huge-scale
               optimization problems. For problems of this size, even the
               simplest full-dimensional vector operations are very expensive.
               Hence, we propose to apply an optimization technique based on
               random partial update of decision variables. For these methods,
               we prove the global estimates for the rate of convergence.
               Surprisingly, for certain classes of objective functions, our
               results are better than the standard worst-case bounds for
               deterministic algorithms. We present constrained and
               unconstrained versions of the method and its accelerated
               variant. Our numerical test confirms a high efficiency of this
               technique on problems of very big size.",
  journal   = "SIAM J. Optim.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  22,
  number    =  2,
  pages     = "341--362",
  month     =  jan,
  year      =  2012
}

@ARTICLE{Wang2015-vx,
  title         = "Graph Connectivity in Noisy Sparse Subspace Clustering",
  author        = "Wang, Yining and Wang, Yu-Xiang and Singh, Aarti",
  abstract      = "Subspace clustering is the problem of clustering data points
                   into a union of low-dimensional linear/affine subspaces. It
                   is the mathematical abstraction of many important problems
                   in computer vision, image processing and machine learning. A
                   line of recent work (4, 19, 24, 20) provided strong
                   theoretical guarantee for sparse subspace clustering (4),
                   the state-of-the-art algorithm for subspace clustering, on
                   both noiseless and noisy data sets. It was shown that under
                   mild conditions, with high probability no two points from
                   different subspaces are clustered together. Such guarantee,
                   however, is not sufficient for the clustering to be correct,
                   due to the notorious ``graph connectivity problem'' (15). In
                   this paper, we investigate the graph connectivity problem
                   for noisy sparse subspace clustering and show that a simple
                   post-processing procedure is capable of delivering
                   consistent clustering under certain ``general position'' or
                   ``restricted eigenvalue'' assumptions. We also show that our
                   condition is almost tight with adversarial noise
                   perturbation by constructing a counter-example. These
                   results provide the first exact clustering guarantee of
                   noisy SSC for subspaces of dimension greater then 3.",
  month         =  apr,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1504.01046"
}

@ARTICLE{Lipor2017-tn,
  title         = "Subspace Clustering using Ensembles of {$K$-Subspaces}",
  author        = "Lipor, John and Hong, David and Tan, Yan Shuo and Balzano,
                   Laura",
  abstract      = "Subspace clustering is the unsupervised grouping of points
                   lying near a union of low-dimensional linear subspaces.
                   Algorithms based directly on geometric properties of such
                   data tend to either provide poor empirical performance, lack
                   theoretical guarantees, or depend heavily on their
                   initialization. We present a novel geometric approach to the
                   subspace clustering problem that leverages ensembles of the
                   K-subspaces (KSS) algorithm via the evidence accumulation
                   clustering framework. Our algorithm, referred to as ensemble
                   K-subspaces (EKSS), forms a co-association matrix whose
                   (i,j)th entry is the number of times points i and j are
                   clustered together by several runs of KSS with random
                   initializations. We prove general recovery guarantees for
                   any algorithm that forms an affinity matrix with entries
                   close to a monotonic transformation of pairwise absolute
                   inner products. We then show that a specific instance of
                   EKSS results in an affinity matrix with entries of this
                   form, and hence our proposed algorithm can provably recover
                   subspaces under similar conditions to state-of-the-art
                   algorithms. The finding is, to the best of our knowledge,
                   the first recovery guarantee for evidence accumulation
                   clustering and for KSS variants. We show on synthetic data
                   that our method performs well in the traditionally
                   challenging settings of subspaces with large intersection,
                   subspaces with small principal angles, and noisy data.
                   Finally, we evaluate our algorithm on six common benchmark
                   datasets and show that unlike existing methods, EKSS
                   achieves excellent empirical performance when there are both
                   a small and large number of points per subspace.",
  month         =  sep,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1709.04744"
}

@INPROCEEDINGS{Nasihatkon2011-bq,
  title     = "Graph connectivity in sparse subspace clustering",
  booktitle = "{CVPR} 2011",
  author    = "Nasihatkon, Behrooz and Hartley, Richard",
  abstract  = "Sparse Subspace Clustering (SSC) is one of the recent approaches
               to subspace segmentation. In SSC a graph is constructed whose
               nodes are the data points and whose edges are inferred from the
               L1-sparse representation of each point by the others. It has
               been proved that if the points lie on a mixture of independent
               subspaces, the graphical structure of each subspace is
               disconnected from the others. However, the problem of
               connectivity within each subspace is still unanswered. This is
               important since the subspace segmentation in SSC is based on
               finding the connected components of the graph. Our analysis is
               built upon the connection between the sparse representation
               through L1-norm minimization and the geometry of convex
               poly-topes proposed by the compressed sensing community. After
               introduction of some assumptions to make the problem
               well-defined, it is proved that the connectivity within each
               subspace holds for 2- and 3-dimensional subspaces. The claim of
               connectivity for general d-dimensional case, even for generic
               configurations, is proved false by giving a counterexample in
               dimensions greater than 3.",
  pages     = "2137--2144",
  month     =  jun,
  year      =  2011,
  keywords  = "Image segmentation;Three dimensional displays;Motion
               segmentation;Geometry;Minimization;Vectors;Communities"
}

@ARTICLE{Yu2023-ud,
  title         = "{White-Box} Transformers via Sparse Rate Reduction",
  author        = "Yu, Yaodong and Buchanan, Sam and Pai, Druv and Chu, Tianzhe
                   and Wu, Ziyang and Tong, Shengbang and Haeffele, Benjamin D
                   and Ma, Yi",
  abstract      = "In this paper, we contend that the objective of
                   representation learning is to compress and transform the
                   distribution of the data, say sets of tokens, towards a
                   mixture of low-dimensional Gaussian distributions supported
                   on incoherent subspaces. The quality of the final
                   representation can be measured by a unified objective
                   function called sparse rate reduction. From this
                   perspective, popular deep networks such as transformers can
                   be naturally viewed as realizing iterative schemes to
                   optimize this objective incrementally. Particularly, we show
                   that the standard transformer block can be derived from
                   alternating optimization on complementary parts of this
                   objective: the multi-head self-attention operator can be
                   viewed as a gradient descent step to compress the token sets
                   by minimizing their lossy coding rate, and the subsequent
                   multi-layer perceptron can be viewed as attempting to
                   sparsify the representation of the tokens. This leads to a
                   family of white-box transformer-like deep network
                   architectures which are mathematically fully interpretable.
                   Despite their simplicity, experiments show that these
                   networks indeed learn to optimize the designed objective:
                   they compress and sparsify representations of large-scale
                   real-world vision datasets such as ImageNet, and achieve
                   performance very close to thoroughly engineered transformers
                   such as ViT. Code is at
                   \textbackslashurl\{https://github.com/Ma-Lab-Berkeley/CRATE\}.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2306.01129"
}

@ARTICLE{Sun2017-mk,
  title         = "Supervised Deep Sparse Coding Networks",
  author        = "Sun, Xiaoxia and Nasrabadi, Nasser M and Tran, Trac D",
  abstract      = "In this paper, we describe the deep sparse coding network
                   (SCN), a novel deep network that encodes intermediate
                   representations with nonnegative sparse coding. The SCN is
                   built upon a number of cascading bottleneck modules, where
                   each module consists of two sparse coding layers with
                   relatively wide and slim dictionaries that are specialized
                   to produce high dimensional discriminative features and low
                   dimensional representations for clustering, respectively.
                   During training, both the dictionaries and regularization
                   parameters are optimized with an end-to-end supervised
                   learning algorithm based on multilevel optimization.
                   Effectiveness of an SCN with seven bottleneck modules is
                   verified on several popular benchmark datasets. Remarkably,
                   with few parameters to learn, our SCN achieves 5.81\% and
                   19.93\% classification error rate on CIFAR-10 and CIFAR-100,
                   respectively.",
  month         =  jan,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1701.08349"
}

@ARTICLE{Zarka2019-jz,
  title         = "Deep Network Classification by Scattering and Homotopy
                   Dictionary Learning",
  author        = "Zarka, John and Thiry, Louis and Angles, Tom{\'a}s and
                   Mallat, St{\'e}phane",
  abstract      = "We introduce a sparse scattering deep convolutional neural
                   network, which provides a simple model to analyze properties
                   of deep representation learning for classification. Learning
                   a single dictionary matrix with a classifier yields a higher
                   classification accuracy than AlexNet over the ImageNet 2012
                   dataset. The network first applies a scattering transform
                   that linearizes variabilities due to geometric
                   transformations such as translations and small deformations.
                   A sparse $\ell^1$ dictionary coding reduces intra-class
                   variability while preserving class separation through
                   projections over unions of linear spaces. It is implemented
                   in a deep convolutional network with a homotopy algorithm
                   having an exponential convergence. A convergence proof is
                   given in a general framework that includes ALISTA.
                   Classification results are analyzed on ImageNet.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.03561"
}

@ARTICLE{Li2021-xw,
  title         = "From the simplex to the sphere: Faster constrained
                   optimization using the Hadamard parametrization",
  author        = "Li, Qiuwei and McKenzie, Daniel and Yin, Wotao",
  abstract      = "The standard simplex in R^n, also known as the probability
                   simplex, is the set of nonnegative vectors whose entries sum
                   up to 1. They frequently appear as constraints in
                   optimization problems that arise in machine learning,
                   statistics, data science, operations research, and beyond.
                   We convert the standard simplex to the unit sphere and thus
                   transform the corresponding constrained optimization problem
                   into an optimization problem on a simple, smooth manifold.
                   We show that KKT points and strict-saddle points of the
                   minimization problem on the standard simplex all correspond
                   to those of the transformed problem, and vice versa. So,
                   solving one problem is equivalent to solving the other
                   problem. Then, we propose several simple, efficient, and
                   projection-free algorithms using the manifold structure. The
                   equivalence and the proposed algorithm can be extended to
                   optimization problems with unit simplex, weighted
                   probability simplex, or `1-norm sphere constraints.
                   Numerical experiments between the new algorithms and
                   existing ones show the advantages of the new approach",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2112.05273"
}

@ARTICLE{Levin2022-gk,
  title         = "The effect of smooth parametrizations on nonconvex
                   optimization landscapes",
  author        = "Levin, Eitan and Kileel, Joe and Boumal, Nicolas",
  abstract      = "We develop new tools to study landscapes in nonconvex
                   optimization. Given one optimization problem, we pair it
                   with another by smoothly parametrizing the domain. This is
                   either for practical purposes (e.g., to use smooth
                   optimization algorithms with good guarantees) or for
                   theoretical purposes (e.g., to reveal that the landscape
                   satisfies a strict saddle property). In both cases, the
                   central question is: how do the landscapes of the two
                   problems relate? More precisely: how do desirable points
                   such as local minima and critical points in one problem
                   relate to those in the other problem? A key finding in this
                   paper is that these relations are often determined by the
                   parametrization itself, and are almost entirely independent
                   of the cost function. Accordingly, we introduce a general
                   framework to study parametrizations by their effect on
                   landscapes. The framework enables us to obtain new
                   guarantees for an array of problems, some of which were
                   previously treated on a case-by-case basis in the
                   literature. Applications include: optimization over low-rank
                   matrices and tensors by optimizing over a factorization; the
                   Burer--Monteiro approach to semidefinite programs; training
                   neural networks by optimizing over their weights and biases;
                   and quotienting out symmetries.",
  month         =  jul,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2207.03512"
}

@MISC{Yaras_undated-gb,
  title       = "law-of-parsimony: Experimental code for ``The Law of Parsimony
                 in Gradient Descent for Learning Deep Linear Networks''",
  author      = "Yaras, Can",
  abstract    = "Experimental code for ``The Law of Parsimony in Gradient
                 Descent for Learning Deep Linear Networks'' -
                 cjyaras/law-of-parsimony: Experimental code for ``The Law of
                 Parsimony in Gradient Descent for Learning Deep Linear
                 Networks''",
  institution = "Github",
  language    = "en"
}

@ARTICLE{Shen2023-xo,
  title         = "Computationally Efficient and Statistically Optimal Robust
                   {High-Dimensional} Linear Regression",
  author        = "Shen, Yinan and Li, Jingyang and Cai, Jian-Feng and Xia,
                   Dong",
  abstract      = "High-dimensional linear regression under heavy-tailed noise
                   or outlier corruption is challenging, both computationally
                   and statistically. Convex approaches have been proven
                   statistically optimal but suffer from high computational
                   costs, especially since the robust loss functions are
                   usually non-smooth. More recently, computationally fast
                   non-convex approaches via sub-gradient descent are proposed,
                   which, unfortunately, fail to deliver a statistically
                   consistent estimator even under sub-Gaussian noise. In this
                   paper, we introduce a projected sub-gradient descent
                   algorithm for both the sparse linear regression and low-rank
                   linear regression problems. The algorithm is not only
                   computationally efficient with linear convergence but also
                   statistically optimal, be the noise Gaussian or heavy-tailed
                   with a finite 1 + epsilon moment. The convergence theory is
                   established for a general framework and its specific
                   applications to absolute loss, Huber loss and quantile loss
                   are investigated. Compared with existing non-convex methods,
                   ours reveals a surprising phenomenon of two-phase
                   convergence. In phase one, the algorithm behaves as in
                   typical non-smooth optimization that requires gradually
                   decaying stepsizes. However, phase one only delivers a
                   statistically sub-optimal estimator, which is already
                   observed in the existing literature. Interestingly, during
                   phase two, the algorithm converges linearly as if minimizing
                   a smooth and strongly convex objective function, and thus a
                   constant stepsize suffices. Underlying the phase-two
                   convergence is the smoothing effect of random noise to the
                   non-smooth robust losses in an area close but not too close
                   to the truth. Numerical simulations confirm our theoretical
                   discovery and showcase the superiority of our algorithm over
                   prior methods.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "2305.06199"
}

@ARTICLE{Luo2022-pm,
  title         = "Understanding diffusion models: A unified perspective",
  author        = "Luo, Calvin",
  abstract      = "Diffusion models have shown incredible capabilities as
                   generative models; indeed, they power the current
                   state-of-the-art models on text-conditioned image generation
                   such as Imagen and DALL-E 2. In this work we review,
                   demystify, and unify the understanding of diffusion models
                   across both variational and score-based perspectives. We
                   first derive Variational Diffusion Models (VDM) as a special
                   case of a Markovian Hierarchical Variational Autoencoder,
                   where three key assumptions enable tractable computation and
                   scalable optimization of the ELBO. We then prove that
                   optimizing a VDM boils down to learning a neural network to
                   predict one of three potential objectives: the original
                   source input from any arbitrary noisification of it, the
                   original source noise from any arbitrarily noisified input,
                   or the score function of a noisified input at any arbitrary
                   noise level. We then dive deeper into what it means to learn
                   the score function, and connect the variational perspective
                   of a diffusion model explicitly with the Score-based
                   Generative Modeling perspective through Tweedie's Formula.
                   Lastly, we cover how to learn a conditional distribution
                   using diffusion models via guidance.",
  month         =  aug,
  year          =  2022,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2208.11970"
}

@INPROCEEDINGS{Costeira1995-nc,
  title     = "A multi-body factorization method for motion analysis",
  booktitle = "Proceedings of {IEEE} International Conference on Computer
               Vision",
  author    = "Costeira, J and Kanade, T",
  abstract  = "The structure from motion problem has been extensively studied
               in the field of computer vision. Yet, the bulk of the existing
               work assumes that the scene contains only a single moving
               object. The more realistic case where an unknown number of
               objects move in the scene has received little attention,
               especially for its theoretical treatment. We present a new
               method for separating and recovering the motion and shape of
               multiple independently moving objects in a sequence of images.
               The method does not require prior knowledge of the number of
               objects, nor is dependent on any grouping of features into an
               object at the image level. For this purpose, we introduce a
               mathematical construct of object shapes, called the shape
               interaction matrix, which is invariant to both the object
               motions and the selection of coordinate systems. This invariant
               structure is computable solely from the observed trajectories of
               image features without grouping them into individual objects.
               Once the structure is computed, it allows for segmenting
               features into objects by the process of transforming it into a
               canonical form, as well as recovering the shape and motion of
               each object.",
  pages     = "1071--1076",
  month     =  jun,
  year      =  1995,
  keywords  = "Motion analysis;Shape;Image segmentation;Layout;Image
               sequences;Constraint theory;Computer science;Computer
               vision;Feature extraction;Filters"
}

@ARTICLE{Tatli2022-xx,
  title         = "Polytopic Matrix Factorization: Determinant Maximization
                   Based Criterion and Identifiability",
  author        = "Tatli, Gokcan and Erdogan, Alper T",
  abstract      = "We introduce Polytopic Matrix Factorization (PMF) as a novel
                   data decomposition approach. In this new framework, we model
                   input data as unknown linear transformations of some latent
                   vectors drawn from a polytope. In this sense, the article
                   considers a semi-structured data model, in which the input
                   matrix is modeled as the product of a full column rank
                   matrix and a matrix containing samples from a polytope as
                   its column vectors. The choice of polytope reflects the
                   presumed features of the latent components and their mutual
                   relationships. As the factorization criterion, we propose
                   the determinant maximization (Det-Max) for the sample
                   autocorrelation matrix of the latent vectors. We introduce a
                   sufficient condition for identifiability, which requires
                   that the convex hull of the latent vectors contains the
                   maximum volume inscribed ellipsoid of the polytope with a
                   particular tightness constraint. Based on the Det-Max
                   criterion and the proposed identifiability condition, we
                   show that all polytopes that satisfy a particular symmetry
                   restriction qualify for the PMF framework. Having infinitely
                   many polytope choices provides a form of flexibility in
                   characterizing latent vectors. In particular, it is possible
                   to define latent vectors with heterogeneous features,
                   enabling the assignment of attributes such as nonnegativity
                   and sparsity at the subvector level. The article offers
                   examples illustrating the connection between polytope
                   choices and the corresponding feature representations.",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "2202.09638"
}

@MISC{Parikh2013-ts,
  title        = "Proximal Algorithms",
  author       = "Parikh, Neal and Boyd, Stephen",
  year         =  2013,
  howpublished = "\url{https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf}",
  note         = "Accessed: 2023-6-30"
}

@ARTICLE{Soudry2018-dw,
  title    = "The Implicit Bias of Gradient Descent on Separable Data",
  author   = "Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and
              Gunasekar, Suriya and Srebro, Nathan",
  journal  = "J. Mach. Learn. Res.",
  volume   =  19,
  number   =  70,
  pages    = "1--57",
  year     =  2018
}

@ARTICLE{Soudry2018-xa,
  title    = "The Implicit Bias of Gradient Descent on Separable Data",
  author   = "Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and
              Gunasekar, Suriya and Srebro, Nathan",
  journal  = "J. Mach. Learn. Res.",
  volume   =  19,
  number   =  70,
  pages    = "1--57",
  year     =  2018
}

@ARTICLE{Tariyal2016-lh,
  title    = "Deep Dictionary Learning",
  author   = "Tariyal, Snigdha and Majumdar, Angshul and Singh, Richa and
              Vatsa, Mayank",
  abstract = "Two popular representation learning paradigms are dictionary
              learning and deep learning. While dictionary learning focuses on
              learning ``basis'' and ``features'' by matrix factorization, deep
              learning focuses on extracting features via learning ``weights''
              or ``filter'' in a greedy layer by layer fashion. This paper
              focuses on combining the concepts of these two paradigms by
              proposing deep dictionary learning and show how deeper
              architectures can be built using the layers of dictionary
              learning. The proposed technique is compared with other deep
              learning approaches, such as stacked autoencoder, deep belief
              network, and convolutional neural network. Experiments on
              benchmark data sets show that the proposed technique achieves
              higher classification and clustering accuracies. On a real-world
              problem of electrical appliance classification, we show that deep
              dictionary learning excels where others do not yield at-par
              performance. We postulate that the proposed formulation can pave
              the path for a new class of deep learning tools.",
  journal  = "IEEE Access",
  volume   =  4,
  pages    = "10096--10109",
  year     =  2016,
  keywords = "Dictionaries;Feature extraction;Machine learning;Matrix
              decomposition;Sparse matrices;Neural networks;Deep
              learning;dictionary learning;feature representation"
}

@ARTICLE{Sulam2017-kz,
  title         = "{Multi-Layer} Convolutional Sparse Modeling: Pursuit and
                   Dictionary Learning",
  author        = "Sulam, Jeremias and Papyan, Vardan and Romano, Yaniv and
                   Elad, Michael",
  abstract      = "The recently proposed Multi-Layer Convolutional Sparse
                   Coding (ML-CSC) model, consisting of a cascade of
                   convolutional sparse layers, provides a new interpretation
                   of Convolutional Neural Networks (CNNs). Under this
                   framework, the computation of the forward pass in a CNN is
                   equivalent to a pursuit algorithm aiming to estimate the
                   nested sparse representation vectors -- or feature maps --
                   from a given input signal. Despite having served as a
                   pivotal connection between CNNs and sparse modeling, a
                   deeper understanding of the ML-CSC is still lacking: there
                   are no pursuit algorithms that can serve this model exactly,
                   nor are there conditions to guarantee a non-empty model.
                   While one can easily obtain signals that approximately
                   satisfy the ML-CSC constraints, it remains unclear how to
                   simply sample from the model and, more importantly, how one
                   can train the convolutional filters from real data. In this
                   work, we propose a sound pursuit algorithm for the ML-CSC
                   model by adopting a projection approach. We provide new and
                   improved bounds on the stability of the solution of such
                   pursuit and we analyze different practical alternatives to
                   implement this in practice. We show that the training of the
                   filters is essential to allow for non-trivial signals in the
                   model, and we derive an online algorithm to learn the
                   dictionaries from real data, effectively resulting in
                   cascaded sparse convolutional layers. Last, but not least,
                   we demonstrate the applicability of the ML-CSC model for
                   several applications in an unsupervised setting, providing
                   competitive results. Our work represents a bridge between
                   matrix factorization, sparse dictionary learning and sparse
                   auto-encoders, and we analyze these connections in detail.",
  month         =  aug,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1708.08705"
}

@ARTICLE{Meng2021-wt,
  title         = "Estimating high order gradients of the data distribution by
                   denoising",
  author        = "Meng, Chenlin and Song, Yang and Li, Wenzhe and Ermon,
                   Stefano",
  abstract      = "The first order derivative of a data density can be
                   estimated efficiently by denoising score matching, and has
                   become an important component in many applications, such as
                   image generation and audio synthesis. Higher order
                   derivatives provide additional local information about the
                   data distribution and enable new applications. Although they
                   can be estimated via automatic differentiation of a learned
                   density model, this can amplify estimation errors and is
                   expensive in high dimensional settings. To overcome these
                   limitations, we propose a method to directly estimate high
                   order derivatives (scores) of a data density from samples.
                   We first show that denoising score matching can be
                   interpreted as a particular case of Tweedie's formula. By
                   leveraging Tweedie's formula on higher order moments, we
                   generalize denoising score matching to estimate higher order
                   derivatives. We demonstrate empirically that models trained
                   with the proposed method can approximate second order
                   derivatives more efficiently and accurately than via
                   automatic differentiation. We show that our models can be
                   used to quantify uncertainty in denoising and to improve the
                   mixing speed of Langevin dynamics via Ozaki discretization
                   for sampling synthetic data and natural images.",
  month         =  nov,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2111.04726"
}

@MISC{Bengio_undated-kb,
  title        = "Group Sparse Coding",
  author       = "Bengio, Samy and Pereira, Fernando and Google, Yoram Singer
                  and {Google Mountain View} and {Mountain View} and {Mountain
                  View}",
  howpublished = "\url{https://proceedings.neurips.cc/paper_files/paper/2009/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf}",
  note         = "Accessed: 2023-7-28"
}

@ARTICLE{Karatsuba2001-er,
  title    = "On hypergeometric functions and generalizations of Legendre's
              relation",
  author   = "Karatsuba, E A and Vuorinen, M",
  abstract = "We prove some convexity properties for a sum of hypergeometric
              functions and obtain a generalization of Legendre's relation for
              complete elliptic integrals. We apply these results to prove some
              inequalities for hypergeometric functions, incomplete
              beta-functions, and Legendre functions. \copyright{} 2001
              Academic Press.",
  journal  = "J. Math. Anal. Appl.",
  volume   =  260,
  number   =  2,
  pages    = "623--640",
  year     =  2001,
  keywords = "Gaussian hypergeometric functions"
}

@ARTICLE{Soltanolkotabi2012-kp,
  title    = "A geometric analysis of subspace clustering with outliers",
  author   = "Soltanolkotabi, Mahdi and Cand{\'e}s, Emmanuel J",
  abstract = "This paper considers the problem of clustering a collection of
              unlabeled data points assumed to lie near a union of
              lower-dimensional planes. As is common in computer vision or
              unsupervised learning applications, we do not know in advance how
              many subspaces there are nor do we have any information about
              their dimensions. We develop a novel geometric analysis of an
              algorithm named sparse subspace clustering (SSC) [In IEEE
              Conference on Computer Vision and Pattern Recognition, 2009. CVPR
              2009 (2009) 2790-2797. IEEE], which significantly broadens the
              range of problems where it is provably effective. For instance,
              we show that SSC can recover multiple subspaces, each of
              dimension comparable to the ambient dimension. We also prove that
              SSC can correctly cluster data points even when the subspaces of
              interest intersect. Further, we develop an extension of SSC that
              succeeds when the data set is corrupted with possibly
              overwhelmingly many outliers. Underlying our analysis are clear
              geometric insights, which may bear on other sparse recovery
              problems. A numerical study complements our theoretical analysis
              and demonstrates the effectiveness of these methods.",
  journal  = "Annals of statistics",
  volume   =  40,
  number   =  4,
  pages    = "2195--2238",
  year     =  2012
}

@ARTICLE{Soltanolkotabi2014-rp,
  title     = "Robust subspace clustering",
  author    = "Soltanolkotabi, Mahdi and Elhamifar, Ehsan and Candes, Emmanuel
               J",
  journal   = "Annals of statistics",
  publisher = "Institute of Mathematical Statistics",
  volume    =  42,
  number    =  2,
  pages     = "669--699",
  year      =  2014
}

@ARTICLE{Segura2016-vr,
  title    = "Sharp bounds for cumulative distribution functions",
  author   = "Segura, Javier",
  abstract = "Ratios of integrals can be bounded in terms of ratios of
              integrands under certain monotonicity conditions. This result,
              related to L'H{\^o}pital's monotone rule, can be used to obtain
              sharp bounds for cumulative distribution functions. We consider
              the case of noncentral cumulative gamma and beta distributions.
              Three different types of sharp bounds for the noncentral gamma
              distributions (also called Marcum functions) are obtained in
              terms of modified Bessel functions and one additional type of
              function: a second modified Bessel function, two error functions
              or one incomplete gamma function. For the noncentral beta case
              the bounds are expressed in terms of Kummer functions and one
              additional Kummer function or an incomplete beta function. These
              bounds improve previous results with respect to their range of
              application and/or its sharpness.",
  journal  = "J. Math. Anal. Appl.",
  volume   =  436,
  number   =  2,
  pages    = "748--763",
  year     =  2016,
  keywords = "Bounds; Cumulative distribution functions; Incomplete beta
              functions; L'H{\^o}pital's rule; Marcum functions"
}

@ARTICLE{Tsakiris2018-vj,
  title    = "Dual Principal Component Pursuit",
  author   = "Tsakiris, Manolis C and Vidal, Rene",
  abstract = "We consider the problem of learning a linear subspace from data
              corrupted by outliers. Classical approaches are typically
              designed for the case in which the subspace dimension is small
              relative to the ambient dimension. Our approach works with a dual
              representation of the subspace and hence aims to find its
              orthogonal complement; as such, it is particularly suitable for
              subspaces whose dimension is close to the ambient dimension
              (subspaces of high relative dimension). We pose the problem of
              computing normal vectors to the inlier subspace as a non-convex
              $\ell_1$ minimization problem on the sphere, which we call Dual
              Principal Component Pursuit (DPCP) problem. We provide
              theoretical guarantees under which every global solution to DPCP
              is a vector in the orthogonal complement of the inlier subspace.
              Moreover, we relax the non-convex DPCP problem to a recursion of
              linear programs whose solutions are shown to converge in a finite
              number of steps to a vector orthogonal to the subspace. In
              particular, when the inlier subspace is a hyperplane, the
              solutions to the recursion of linear programs converge to the
              global minimum of the non-convex DPCP problem in a finite number
              of steps. We also propose algorithms based on alternating
              minimization and iteratively re-weighted least squares, which are
              suitable for dealing with large-scale data. Experiments on
              synthetic data show that the proposed methods are able to handle
              more outliers and higher relative dimensions than current
              state-of-the-art methods, while experiments in the context of the
              three-view geometry problem in computer vision suggest that the
              proposed methods can be a useful or even superior alternative to
              traditional RANSAC-based approaches for computer vision and other
              applications.",
  journal  = "Journal of Machine Learning Research",
  volume   =  19,
  pages    = "1--49",
  year     =  2018,
  keywords = "Computational modeling; Convex functions; Data models;
              Optimization; Principal component analysis; Robustness; Upper
              bound"
}

@ARTICLE{Li2020-pl,
  title    = "On the efficient computation of a generalized Jacobian of the
              projector over the Birkhoff polytope",
  author   = "Li, Xudong and Sun, Defeng and Toh, Kim Chuan",
  abstract = "We derive an explicit formula, as well as an efficient procedure,
              for constructing a generalized Jacobian for the projector of a
              given square matrix onto the Birkhoff polytope, i.e., the set of
              doubly stochastic matrices. To guarantee the high efficiency of
              our procedure, a semismooth Newton method for solving the dual of
              the projection problem is proposed and efficiently implemented.
              Extensive numerical experiments are presented to demonstrate the
              merits and effectiveness of our method by comparing its
              performance against other powerful solvers such as the commercial
              software Gurobi and the academic code PPROJ (Hager and Zhang in
              SIAM J Optim 26:1773--1798, 2016). In particular, our algorithm
              is able to solve the projection problem with over one billion
              variables and nonnegative constraints to a very high accuracy in
              less than 15 min on a modest desktop computer. More importantly,
              based on our efficient computation of the projections and their
              generalized Jacobians, we can design a highly efficient augmented
              Lagrangian method (ALM) for solving a class of convex quadratic
              programming (QP) problems constrained by the Birkhoff polytope.
              The resulted ALM is demonstrated to be much more efficient than
              Gurobi in solving a collection of QP problems arising from the
              relaxation of quadratic assignment problems.",
  journal  = "Math. Program.",
  volume   =  179,
  number   = "1-2",
  pages    = "419--446",
  year     =  2020,
  keywords = "Doubly stochastic matrix; Generalized Jacobian; Newton's method;
              Semismoothness"
}

@ARTICLE{Allred2020-vt,
  title    = "Controlled Forgetting: Targeted Stimulation and Dopaminergic
              Plasticity Modulation for Unsupervised Lifelong Learning in
              Spiking Neural Networks",
  author   = "Allred, Jason M and Roy, Kaushik",
  abstract = "Stochastic gradient descent requires that training samples be
              drawn from a uniformly random distribution of the data. For a
              deployed system that must learn online from an uncontrolled and
              unknown environment, the ordering of input samples often fails to
              meet this criterion, making lifelong learning a difficult
              challenge. We exploit the locality of the unsupervised Spike
              Timing Dependent Plasticity (STDP) learning rule to target local
              representations in a Spiking Neural Network (SNN) to adapt to
              novel information while protecting essential information in the
              remainder of the SNN from catastrophic forgetting. In our
              Controlled Forgetting Networks (CFNs), novel information triggers
              stimulated firing and heterogeneously modulated plasticity,
              inspired by biological dopamine signals, to cause rapid and
              isolated adaptation in the synapses of neurons associated with
              outlier information. This targeting controls the forgetting
              process in a way that reduces the degradation of accuracy for
              older tasks while learning new tasks. Our experimental results on
              the MNIST dataset validate the capability of CFNs to learn
              successfully over time from an unknown, changing environment,
              achieving 95.24\% accuracy, which we believe is the best
              unsupervised accuracy ever achieved by a fixed-size, single-layer
              SNN on a completely disjoint MNIST dataset.",
  journal  = "Front. Neurosci.",
  volume   =  14,
  number   =  1987,
  year     =  2020,
  keywords = "Spike Timing Dependent Plasticity; Spiking Neural Networks;
              catastrophic forgetting; continual learning; controlled
              forgetting; dopaminergic learning; lifelong learning;
              stability-plasticity dilemma"
}

@ARTICLE{Peng2018-zu,
  title     = "Structured autoencoders for subspace clustering",
  author    = "Peng, Xi and Feng, Jiashi and Xiao, Shijie and Yau, Wei Yun and
               Zhou, Joey Tianyi and Yang, Songfan",
  abstract  = "Existing subspace clustering methods typically employ shallow
               models to estimate underlying subspaces of unlabeled data points
               and cluster them into corresponding groups. However, due to the
               limited representative capacity of the employed shallow models,
               those methods may fail in handling realistic data without the
               linear subspace structure. To address this issue, we propose a
               novel subspace clustering approach by introducing a new deep
               model-Structured AutoEncoder (StructAE). The StructAE learns a
               set of explicit transformations to progressively map input data
               points into nonlinear latent spaces while preserving the local
               and global subspace structure. In particular, to preserve local
               structure, the StructAE learns representations for each data
               point by minimizing reconstruction error with respect to itself.
               To preserve global structure, the StructAE incorporates a prior
               structured information by encouraging the learned representation
               to preserve specified reconstruction patterns over the entire
               data set. To the best of our knowledge, StructAE is one of the
               first deep subspace clustering approaches. Extensive experiments
               show that the proposed StructAE significantly outperforms 15
               state-of-the-art subspace clustering approaches in terms of five
               evaluation metrics.",
  journal   = "IEEE Trans. Image Process.",
  publisher = "IEEE",
  volume    =  27,
  number    =  10,
  pages     = "5076--5086",
  year      =  2018,
  keywords  = "Unsupervised deep learning; globality preservation; locality
               preservation; spectral clustering"
}

@ARTICLE{Vinayak2016-bx,
  title    = "Similarity clustering in the presence of outliers: Exact recovery
              via convex program",
  author   = "Vinayak, Ramya Korlakai and Hassibi, Babak",
  abstract = "We study the problem of clustering a set of data points based on
              their similarity matrix, each entry of which represents the
              similarity between the corresponding pair of points. We propose a
              convex-optimization-based algorithm for clustering using the
              similarity matrix, which has provable recovery guarantees. It
              needs no prior knowledge of the number of clusters and it behaves
              in a robust way in the presence of outliers and noise. Using a
              generative stochastic model for the similarity matrix (which can
              be thought of as a generalization of the classical Stochastic
              Block Model) we obtain precise bounds (not orderwise) on the
              sizes of the clusters, the number of outliers, the noise
              variance, separation between the mean similarities inside and
              outside the clusters and the values of the regularization
              parameter that guarantee the exact recovery of the clusters with
              high probability. The theoretical findings are corroborated with
              extensive evidence from simulations.",
  journal  = "IEEE International Symposium on Information Theory - Proceedings",
  volume   = "2016-Augus",
  pages    = "91--95",
  year     =  2016
}

@ARTICLE{Adler2015-ys,
  title     = "{Linear-Time} Subspace Clustering via Bipartite Graph Modeling",
  author    = "Adler, Amir and Elad, Michael and Hel-Or, Yacov",
  abstract  = "We present a linear-time subspace clustering approach that
               combines sparse representations and bipartite graph modeling.
               The signals are modeled as drawn from a union of low-dimensional
               subspaces, and each signal is represented by a sparse
               combination of basis elements, termed atoms, which form the
               columns of a dictionary matrix. The sparse representation
               coefficients are arranged in a sparse affinity matrix, which
               defines a bipartite graph of two disjoint sets: 1) atoms and 2)
               signals. Subspace clustering is obtained by applying
               low-complexity spectral bipartite graph clustering that exploits
               the small number of atoms for complexity reduction. The
               complexity of the proposed approach is linear in the number of
               signals, thus it can rapidly cluster very large data
               collections. Performance evaluation of face clustering and
               temporal video segmentation demonstrates comparable clustering
               accuracies to state-of-the-art at a significantly lower
               computational load.",
  journal   = "IEEE Transactions on Neural Networks and Learning Systems",
  publisher = "IEEE",
  volume    =  26,
  number    =  10,
  pages     = "2234--2246",
  year      =  2015,
  keywords  = "Bipartite graph; dictionary; face clustering; sparse
               representation; subspace clustering; temporal video
               segmentation."
}

@ARTICLE{Aholt2012-gv,
  title    = "The ideal of the trifocal variety",
  author   = "Aholt, Chris and Oeding, Luke",
  abstract = "Techniques from representation theory, symbolic computational
              algebra, and numerical algebraic geometry are used to find the
              minimal generators of the ideal of the trifocal variety. An
              effective test for determining whether a given tensor is a
              trifocal tensor is also given.",
  journal  = "Arxiv preprint arXiv:1205.3776",
  pages    = "1--27",
  year     =  2012
}

@ARTICLE{Aholt2013-ct,
  title    = "A hilbert scheme in computer vision",
  author   = "Aholt, Chris and Sturmfels, Bernd and Thomas, Rekha",
  abstract = "Multiview geometry is the study of two-dimensional images of
              three-dimensional scenes, a foundational subject in computer
              vision. We determine a universal Groebner basis for the multiview
              ideal of n generic cameras. As the cameras move, the multiview
              varieties vary in a family of dimension 11n-15. This family is
              the distinguished component of a multigraded Hilbert scheme with
              a unique Borel-fixed point. We present a combinatorial study of
              ideals lying on that Hilbert scheme.",
  journal  = "Canad. J. Math.",
  volume   =  65,
  number   =  5,
  pages    = "961--988",
  year     =  2013,
  keywords = "Computer vision; Generic initial ideal; Groebner basis; Monomial
              ideal; Multigraded Hilbert Scheme"
}

@ARTICLE{Alzati2010-nj,
  title    = "A geometric approach to the trifocal tensor",
  author   = "Alzati, Alberto and Tortora, Alfonso",
  journal  = "J. Math. Imaging Vis.",
  volume   =  38,
  number   =  3,
  pages    = "159--170",
  year     =  2010,
  keywords = "Constraints; Multiple view geometry; Trifocal tensor"
}

@ARTICLE{An2012-br,
  title     = "Fast incremental {3D} plane extraction from a collection of {2D}
               line segments for {3D} mapping",
  author    = "An, Su Yong and Lee, Lae Kyoung and Oh, Se Young",
  abstract  = "Three-dimensional (3D) data processing has recently acquired
               greater importance in solving complex tasks such as object
               recognition, environment modeling, and robotic mapping and
               localization. Since using raw 3D data without preprocessing is
               very time-consuming, extraction of geometric features that
               describe the environment concisely is essential. A plane is a
               suitable geometric feature due to its richness and simplicity of
               extraction. This paper presents an online incremental plane
               extraction method using line segments. Since our system is based
               on a nodding laser scanner, we exploit the incremental nature of
               data acquisition in which physical rotation and algorithm
               implementation are conducted in parallel. In contrast to other
               plane extraction methods, line segments defined by two end
               points become supporting elements that comprise a plane, so we
               need not handle all the scan points once the line segments are
               extracted from each scan slice. This reduces the algorithms
               complexity and the computation time. Experimental validation and
               comparison with state of the art method were conducted using
               tens of complete scan data sets acquired from a typical indoor
               environment. \copyright{} 2012 IEEE.",
  journal   = "Rep. U. S.",
  publisher = "IEEE",
  pages     = "4530--4537",
  year      =  2012
}

@ARTICLE{Bailo2018-jq,
  title    = "Efficient adaptive non-maximal suppression algorithms for
              homogeneous spatial keypoint distribution",
  author   = "Bailo, Oleksandr and Rameau, Francois and Joo, Kyungdon and Park,
              Jinsun and Bogdan, Oleksandr and Kweon, In So",
  abstract = "Keypoint detection usually results in a large number of keypoints
              which are mostly clustered, redundant, and noisy. These keypoints
              often require special processing like Adaptive Non-Maximal
              Suppression (ANMS) to retain the most relevant ones. In this
              paper, we present three new efficient ANMS approaches which
              ensure a fast and homogeneous repartition of the keypoints in the
              image. For this purpose, a square approximation of the search
              range to suppress irrelevant points is proposed to reduce the
              computational complexity of the ANMS. To further speed up the
              proposed approaches, we also introduce a novel strategy to
              initialize the search range based on image dimension which leads
              to a faster convergence. An exhaustive survey and comparisons
              with already existing methods are provided to highlight the
              effectiveness and scalability of our methods and the
              initialization strategy.",
  journal  = "Pattern Recognit. Lett.",
  volume   =  106,
  number   = "February",
  pages    = "53--60",
  year     =  2018,
  keywords = "Adaptive non-maximal suppression; Point detection; SLAM"
}

@ARTICLE{Beauchemin2015-ms,
  title     = "On affinity matrix normalization for graph cuts and spectral
               clustering",
  author    = "Beauchemin, M",
  abstract  = "Graph-based spectral clustering algorithms involve the analysis
               of an affinity matrix. The latter defines the pairwise
               similarity relations among data points. Popular graph
               partitioning algorithms typically involve a normalization step
               that reflects itself onto an affinity matrix normalization step
               in spectral clustering algorithms. In this paper, we show that
               affinity matrix normalization with constant row/column sum
               guarantees the invariance of the size-weighted sum of the
               between- and within-cluster graph association; a property
               conceptually equivalent to the data variance decomposition
               exploited by the standard k-means algorithm. From this
               observation, we demonstrate that the solution of numerous
               spectral clustering methods can be obtained using the standard
               graph ratio cut objective function. We have identified in the
               literature 7 such affinity matrix normalization schemes relevant
               to spectral clustering. Clustering experiments performed with
               these 7 normalization schemes on 17 benchmark datasets are
               presented. As a general rule, it is observed that the
               appropriate normalization method depends on the dataset. A
               geometric interpretation in the feature space (FS) of such a
               normalization scheme for k-way spectral clustering is also
               presented.",
  journal   = "Pattern Recognit. Lett.",
  publisher = "Elsevier Ltd.",
  volume    =  68,
  pages     = "90--96",
  year      =  2015,
  keywords  = "Affinity matrix; Feature space; Kernel k-means; Spectral
               clustering"
}

@ARTICLE{Blondel2018-hu,
  title    = "Smooth and sparse optimal transport",
  author   = "Blondel, Mathieu and Seguy, Vivien and Rolet, Antoine",
  abstract = "Entropic regularization is quickly emerging as a new standard in
              optimal transport (OT). It enables to cast the OT computation as
              a differentiable and unconstrained convex optimization problem,
              which can be efficiently solved using the Sinkhorn algorithm.
              However, entropy keeps the transportation plan strictly positive
              and therefore completely dense, unlike unregularized OT. This
              lack of sparsity can be problematic in applications where the
              transportation plan itself is of interest. In this paper, we
              explore regularizing the primal and dual OT formulations with a
              strongly convex term, which corresponds to relaxing the dual and
              primal constraints with smooth approximations. We show how to
              incorporate squared 2-norm and group lasso regularizations within
              that framework, leading to sparse and group-sparse transportation
              plans. On the theoretical side, we bound the approximation error
              introduced by regularizing the primal and dual formulations. Our
              results suggest that, for the regularized primal, the
              approximation error can often be smaller with squared 2-norm than
              with entropic regularization. We showcase our proposed framework
              on the task of color transfer.",
  journal  = "International Conference on Artificial Intelligence and
              Statistics, AISTATS 2018",
  volume   =  84,
  pages    = "880--889",
  year     =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Blumensath2008-an,
  title    = "Iterative thresholding for saparse approximations",
  author   = "Blumensath, Thomas and Davies, Mike E",
  abstract = "Sparse signal expansions represent or approximate a signal using
              a small number of elements from a large collection of elementary
              waveforms. Finding the optimal sparse expansion is known to be NP
              hard in general and non-optimal strategies such as Matching
              Pursuit, Orthogonal Matching Pursuit, Basis Pursuit and Basis
              Pursuit De-noising are often called upon. These methods show good
              performance in practical situations, however, they do not operate
              on the ℓ0 penalised cost functions that are often at the heart of
              the problem. In this paper we study two iterative algorithms that
              are minimising the cost functions of interest. Furthermore, each
              iteration of these strategies has computational complexity
              similar to a Matching Pursuit iteration, making the methods
              applicable to many real world problems. However, the optimisation
              problem is non-convex and the strategies are only guaranteed to
              find local solutions, so good initialisation becomes paramount.
              We here study two approaches. The first approach uses the
              proposed algorithms to refine the solutions found with other
              methods, replacing the typically used conjugate gradient solver.
              The second strategy adapts the algorithms and we show on one
              example that this adaptation can be used to achieve results that
              lie between those obtained with Matching Pursuit and those found
              with Orthogonal Matching Pursuit, while retaining the
              computational complexity of the Matching Pursuit algorithm.
              \copyright{} 2008 Birkh{\"a}user Boston.",
  journal  = "J. Fourier Anal. Appl.",
  volume   =  14,
  number   = "5-6",
  pages    = "629--654",
  year     =  2008,
  keywords = "Iterative thresholding; Sparse approximations; Subset selection;
              ℓ0 regularisation"
}

@ARTICLE{Bravo_Ferreira2018-us,
  title    = "Semidefinite programming approach for the quadratic assignment
              problem with a sparse graph",
  author   = "Bravo Ferreira, Jos{\'e} F S and Khoo, Yuehaw and Singer, Amit",
  abstract = "The matching problem between two adjacency matrices can be
              formulated as the NP-hard quadratic assignment problem (QAP).
              Previous work on semidefinite programming (SDP) relaxations to
              the QAP have produced solutions that are often tight in practice,
              but such SDPs typically scale badly, involving matrix variables
              of dimension $n^2$ where n is the number of nodes. To achieve a
              speed up, we propose a further relaxation of the SDP involving a
              number of positive semidefinite matrices of dimension
              $\mathcal\{O\}(n)$ no greater than the number of edges in one of
              the graphs. The relaxation can be further strengthened by
              considering cliques in the graph, instead of edges. The dual
              problem of this novel relaxation has a natural three-block
              structure that can be solved via a convergent Augmented Direction
              Method of Multipliers (ADMM) in a distributed manner, where the
              most expensive step per iteration is computing the
              eigendecomposition of matrices of dimension $\mathcal\{O\}(n)$.
              The new SDP relaxation produces strong bounds on quadratic
              assignment problems where one of the graphs is sparse with
              reduced computational complexity and running times, and can be
              used in the context of nuclear magnetic resonance spectroscopy
              (NMR) to tackle the assignment problem.",
  journal  = "Comput. Optim. Appl.",
  volume   =  69,
  number   =  3,
  pages    = "677--712",
  year     =  2018,
  keywords = "Alternating direction method of multipliers; Convex relaxation;
              Graph matching; Quadratic assignment problem; Semidefinite
              programming"
}

@ARTICLE{Brown2007-wf,
  title    = "Automatic Panoramic Image Stitching Automatic {2D} Stitching",
  author   = "Brown, Matthew and Lowe, David G",
  abstract = "This paper concerns the problem of fully automated panoramic
              image stitching. Though the 1D problem (single axis of rotation)
              is well studied, 2D or multi-row stitching is more difficult.
              Previous approaches have used human input or restrictions on the
              image sequence in order to establish matching images. In this
              work, we formulate stitching as a multi-image matching problem,
              and use invariant local features to find matches between all of
              the images. Because of this our method is insensitive to the
              ordering, orientation, scale and illumination of the input
              images. It is also insensitive to noise images that are not part
              of a panorama, and can recognise multiple panoramas in an
              unordered image dataset. In addition to providing more detail,
              this paper extends our previous work in the area (Brown and Lowe,
              2003) by introducing gain compensation and automatic
              straightening steps.",
  journal  = "Int. J. Comput. Vis.",
  volume   =  74,
  number   =  1,
  pages    = "59--73",
  year     =  2007
}

@ARTICLE{Bruna2013-ru,
  title    = "Invariant scattering convolution networks",
  author   = "Bruna, Joan and Mallat, St{\'e}phane",
  abstract = "A wavelet scattering network computes a translation invariant
              image representation which is stable to deformations and
              preserves high-frequency information for classification. It
              cascades wavelet transform convolutions with nonlinear modulus
              and averaging operators. The first network layer outputs
              SIFT-type descriptors, whereas the next layers provide
              complementary invariant information that improves classification.
              The mathematical analysis of wavelet scattering networks explains
              important properties of deep convolution networks for
              classification. A scattering representation of stationary
              processes incorporates higher order moments and can thus
              discriminate textures having the same Fourier power spectrum.
              State-of-the-art classification results are obtained for
              handwritten digits and texture discrimination, with a Gaussian
              kernel SVM and a generative PCA classifier.",
  journal  = "IEEE transactions on pattern analysis and machine intelligence",
  volume   =  35,
  number   =  8,
  pages    = "1872--1886",
  month    =  aug,
  year     =  2013,
  keywords = "Classification; convolution networks; deformations; invariants;
              wavelets"
}

@ARTICLE{Cai2019-ez,
  title    = "Accelerated alternating projections for robust principal
              component analysis",
  author   = "Cai, Han Qin and Cai, Jian Feng and Wei, Ke",
  abstract = "We study robust PCA for the fully observed setting, which is
              about separating a low rank matrix L and a sparse matrix S from
              their sum D = L + S. In this paper, a new algorithm, dubbed
              accelerated alternating projections, is introduced for robust PCA
              which significantly improves the computational efficiency of the
              existing alternating projections proposed in (Netrapalli et al.,
              2014) when updating the low rank factor. The acceleration is
              achieved by first projecting a matrix onto some low dimensional
              subspace before obtaining a new estimate of the low rank matrix
              via truncated SVD. Exact recovery guarantee has been established
              which shows linear convergence of the proposed algorithm.
              Empirical performance evaluations establish the advantage of our
              algorithm over other state-of-theart algorithms for robust PCA.",
  journal  = "J. Mach. Learn. Res.",
  volume   =  20,
  pages    = "1--33",
  year     =  2019,
  keywords = "Alternating Projections; Matrix Manifold; Robust PCA; Subspace
              Projection; Tangent Space"
}

@ARTICLE{Candes2010-sk,
  title    = "Matrix completion with noise",
  author   = "Candes, Emmanuel J and Plan, Yaniv",
  abstract = "On the heels of compressed sensing, a new field has very recently
              emerged. This field addresses a broad range of problems of
              significant practical interest, namely, the recovery of a data
              matrix from what appears to be incomplete, and perhaps even
              corrupted, information. In its simplest form, the problem is to
              recover a matrix from a small sample of its entries. It comes up
              in many areas of science and engineering, including collaborative
              filtering, machine learning, control, remote sensing, and
              computer vision, to name a few. This paper surveys the novel
              literature on matrix completion, which shows that under some
              suitable conditions, one can recover an unknown low-rank matrix
              from a nearly minimal set of entries by solving a simple convex
              optimization problem, namely, nuclear-norm minimization subject
              to data constraints. Further, this paper introduces novel results
              showing that matrix completion is provably accurate even when the
              few observed entries are corrupted with a small amount of noise.
              A typical result is that one can recover an unknown n $\times$ n
              matrix of low rank r from just about nr log 2 noisy samples with
              an error that is proportional to the noise level. We present
              numerical results that complement our quantitative analysis and
              show that, in practice, nuclear-norm minimization accurately
              fills in the many missing entries of large low-rank matrices from
              just a few noisy samples. Some analogies between matrix
              completion and compressed sensing are discussed throughout.
              \copyright{} 2010 IEEE.",
  journal  = "Proc. IEEE",
  volume   =  98,
  number   =  6,
  pages    = "925--936",
  year     =  2010,
  keywords = "Compressed sensing; Duality in optimization; Low-rank matrices;
              Matrix completion; Nuclear-norm minimization; Oracle
              inequalities; Semidefinite programming"
}

@ARTICLE{Chan2015-un,
  title    = "{PCANet}: A Simple Deep Learning Baseline for Image
              Classification?",
  author   = "Chan, Tsung Han and Jia, Kui and Gao, Shenghua and Lu, Jiwen and
              Zeng, Zinan and Ma, Yi",
  abstract = "In this paper, we propose a very simple deep learning network for
              image classification that is based on very basic data processing
              components: 1) cascaded principal component analysis (PCA); 2)
              binary hashing; and 3) blockwise histograms. In the proposed
              architecture, the PCA is employed to learn multistage filter
              banks. This is followed by simple binary hashing and block
              histograms for indexing and pooling. This architecture is thus
              called the PCA network (PCANet) and can be extremely easily and
              efficiently designed and learned. For comparison and to provide a
              better understanding, we also introduce and study two simple
              variations of PCANet: 1) RandNet and 2) LDANet. They share the
              same topology as PCANet, but their cascaded filters are either
              randomly selected or learned from linear discriminant analysis.
              We have extensively tested these basic networks on many benchmark
              visual data sets for different tasks, including Labeled Faces in
              the Wild (LFW) for face verification; the MultiPIE, Extended Yale
              B, AR, Facial Recognition Technology (FERET) data sets for face
              recognition; and MNIST for hand-written digit recognition.
              Surprisingly, for all tasks, such a seemingly naive PCANet model
              is on par with the state-of-the-art features either prefixed,
              highly hand-crafted, or carefully learned [by deep neural
              networks (DNNs)]. Even more surprisingly, the model sets new
              records for many classification tasks on the Extended Yale B, AR,
              and FERET data sets and on MNIST variations. Additional
              experiments on other public data sets also demonstrate the
              potential of PCANet to serve as a simple but highly competitive
              baseline for texture classification and object recognition.",
  journal  = "IEEE Trans. Image Process.",
  volume   =  24,
  number   =  12,
  pages    = "5017--5032",
  year     =  2015,
  keywords = "Convolution Neural Network; Deep Learning; Face Recognition;
              Handwritten Digit Recognition; LDA Network; Object
              Classification; PCA Network; Random Network"
}

@INPROCEEDINGS{Chang2018-oo,
  title    = "{Matterport3D}: Learning from {RGB-D} data in indoor environments",
  author   = "Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber,
              Maciej and Niebner, Matthias and Savva, Manolis and Song, Shuran
              and Zeng, Andy and Zhang, Yinda",
  abstract = "Access to large, diverse RGB-D datasets is critical for training
              RGB-D scene understanding algorithms. However, existing datasets
              still cover only a limited number of views or a restricted scale
              of spaces. In this paper, we introduce Matterport3D, a
              large-scale RGB-D dataset containing 10,800 panoramic views from
              194,400 RGB-D images of 90 building-scale scenes. Annotations are
              provided with surface reconstructions, camera poses, and 2D and
              3D semantic segmentations. The precise global alignment and
              comprehensive, diverse panoramic set of views over entire
              buildings enable a variety of supervised and self-supervised
              computer vision tasks, including keypoint matching, view overlap
              prediction, normal prediction from color, semantic segmentation,
              and region classification.",
  pages    = "667--676",
  year     =  2018
}

@ARTICLE{Choi2017-nu,
  title    = "Range Sensors: Ultrasonic Sensors, Kinect, and {LiDAR}",
  author   = "Choi, Jongmoo",
  abstract = "We present an introductory summary of range sensing technologies
              including ultrasonic sensors, RGB-D cameras, time-of-flight (TOF)
              cameras, and LiDAR sensors. For each technology, we briefly
              introduce the principle of the range sensors, representative
              commercial products, comparisons, and main applications. We also
              provide key algorithmic methods to process range data, such as
              point cloud registration, and some useful software tools. As the
              detailed knowledge can easily be found from the literature or the
              Internet, we focus on the big picture of the sensing
              technologies.",
  journal  = "Humanoid Robotics: A Reference",
  pages    = "1--19",
  year     =  2017,
  keywords = "range sensor; rgb-d camera; time-of-flight camera; ultrasonic
              sensor"
}

@INPROCEEDINGS{Chuan2003-ws,
  title    = "A planar homography estimation method for camera calibration",
  author   = "Chuan, Zhou and Long, Tan Da and Feng, Zhu and Li, Dong Zai",
  abstract = "More and more calibration methods are proposed based on planar
              homography. However, result accuracy of intrinsic parameters
              depends closely on homography estimation, which may be sensitive
              to noise. In this paper, a planar homography estimation method is
              introduced for camera calibration. It exploits both total
              least-squares method and multi-view constraints in order to
              increase the computational accuracy of this algorithm. Applying
              this technique to camera calibration will provide reliability and
              robustness in the presence of noise. Simulations and experiments
              with real images validate this method.",
  volume   =  1,
  pages    = "424--429",
  year     =  2003
}

@ARTICLE{Chum2008-vd,
  title    = "Optimal randomized {RANSAC}",
  author   = "Chum, Ond{\v r}ej and Matas, Ji{\v r}{\'\i}",
  abstract = "A randomized model verification strategy for RANSAC is presented.
              The proposed method finds, like RANSAC, a solution that is
              optimal with user-specified probability. The solution is found in
              time that is (i) close to the shortest possible and (ii) superior
              to any deterministic verification strategy. A provably fastest
              model verification strategy is designed for the (theoretical)
              situation when the contamination of data by outliers is known. In
              this case, the algorithm is the fastest possible (on average) of
              all randomized RANSAC algorithms guaranteeing a confidence in the
              solution. The derivation of the optimality property is based on
              Wald's theory of sequential decision making, in particular a
              modified sequential probability ratio test (SPRT). Next, the
              R-RANSAC with SPRT algorithm is introduced. The algorithm removes
              the requirement for a priori knowledge of the fraction of
              outliers and estimates the quantity online. We show
              experimentally that on standard test data the method has
              performance close to the theoretically optimal and is 2 to 10
              times faster than standard RANSAC and is up to 4 times faster
              than previously published methods. \copyright{} 2008 IEEE.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  30,
  number   =  8,
  pages    = "1472--1482",
  year     =  2008,
  keywords = "RANSAC; Randomized RANSAC"
}

@INPROCEEDINGS{Chum2005-bi,
  title     = "Two-view geometry estimation unaffected by a dominant plane",
  booktitle = "{IEEE} Computer Society Conference on Computer Vision and
               Pattern Recognition, {CVPR} 2005",
  author    = "Chum, Ond{\v r}ej and Werner, Tom{\'a}{\v s} and Matas, Ji{\v
               r}{\'\i}",
  abstract  = "A RANSAC-based algorithm for robust estimation of epipolar
               geometry from point correspondences in the possible presence of
               a dominant scene plane is presented. The algorithm handles
               scenes with (i) all points in a single plane, (ii) majority of
               points in a single plane and the rest off the plane, (iii) no
               dominant plane. It is not required to know a priori which of the
               cases (i) - (iii) occurs. The algorithm exploits a theorem we
               proved, that if five or more of seven correspondences are
               related by a homography then there is an epipolar geometry
               consistent with the seven-tuple as well as with all
               correspondences related by the homography. This means that a
               seven point sample consisting of two outliers and five inliers
               lying in a dominant plane produces an epipolar geometry which is
               wrong and yet consistent with a high number of correspondences.
               The theorem explains why RANSAC often fails to estimate epipolar
               geometry in the presence of a dominant plane. Rather
               surprisingly, the theorem also implies that RANSAC-based
               homography estimation is faster when drawing non-minimal samples
               of seven correspondences than minimal samples of four
               correspondences. \copyright{} 2005 IEEE.",
  volume    = "I",
  pages     = "772--779",
  year      =  2005
}

@ARTICLE{Cvetkovic2007-bc,
  title    = "Signless Laplacians of finite graphs",
  author   = "Cvetkovi{\'c}, Drago{\v s} and Rowlinson, Peter and Simi{\'c},
              Slobodan K",
  abstract = "We survey properties of spectra of signless Laplacians of graphs
              and discuss possibilities for developing a spectral theory of
              graphs based on this matrix. For regular graphs the whole
              existing theory of spectra of the adjacency matrix and of the
              Laplacian matrix transfers directly to the signless Laplacian,
              and so we consider arbitrary graphs with special emphasis on the
              non-regular case. The results which we survey (old and new) are
              of two types: (a) results obtained by applying to the signless
              Laplacian the same reasoning as for corresponding results
              concerning the adjacency matrix, (b) results obtained indirectly
              via line graphs. Among other things, we present eigenvalue bounds
              for several graph invariants, an interpretation of the
              coefficients of the characteristic polynomial, a theorem on
              powers of the signless Laplacian and some remarks on star
              complements. \copyright{} 2007 Elsevier Inc. All rights reserved.",
  journal  = "Linear Algebra Appl.",
  volume   =  423,
  number   =  1,
  pages    = "155--171",
  year     =  2007,
  keywords = "Graph spectra; Graph theory; Line graph; Signless Laplacian; Star
              complement"
}

@ARTICLE{Dai2017-zy,
  title    = "{ScanNet}: Richly-annotated {3D} reconstructions of indoor scenes",
  author   = "Dai, Angela and Chang, Angel X and Savva, Manolis and Halber,
              Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias",
  abstract = "A key requirement for leveraging supervised deep learning methods
              is the availability of large, labeled datasets. Unfortunately, in
              the context of RGB-D scene understanding, very little data is
              available - current datasets cover a small range of scene views
              and have limited semantic annotations. To address this issue, we
              introduce ScanNet, an RGB-D video dataset containing 2.5M views
              in 1513 scenes annotated with 3D camera poses, surface
              reconstructions, and semantic segmentations. To collect this
              data, we designed an easy-to-use and scalable RGB-D capture
              system that includes automated surface reconstruction and
              crowd-sourced semantic annotation. We show that using this data
              helps achieve state-of-the-art performance on several 3D scene
              understanding tasks, including 3D object classification, semantic
              voxel labeling, and CAD model retrieval.",
  journal  = "Proceedings - 30th IEEE Conference on Computer Vision and Pattern
              Recognition, CVPR 2017",
  volume   = "2017-Janua",
  pages    = "2432--2443",
  year     =  2017
}

@ARTICLE{DeTone2016-ty,
  title    = "Deep Image Homography Estimation",
  author   = "DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew",
  abstract = "We present a deep convolutional neural network for estimating the
              relative homography between a pair of images. Our feed-forward
              network has 10 layers, takes two stacked grayscale images as
              input, and produces an 8 degree of freedom homography which can
              be used to map the pixels from the first image to the second. We
              present two convolutional neural network architectures for
              HomographyNet: a regression network which directly estimates the
              real-valued homography parameters, and a classification network
              which produces a distribution over quantized homographies. We use
              a 4-point homography parameterization which maps the four corners
              from one image into the second image. Our networks are trained in
              an end-to-end fashion using warped MS-COCO images. Our approach
              works without the need for separate local feature detection and
              transformation estimation stages. Our deep models are compared to
              a traditional homography estimator based on ORB features and we
              highlight the scenarios where HomographyNet outperforms the
              traditional technique. We also describe a variety of applications
              powered by deep homography estimation, thus showcasing the
              flexibility of a deep learning approach.",
  journal  = "Arxiv preprint arXiv:1606.03798",
  year     =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Elhamifar2013-de,
  title    = "Sparse subspace clustering: Algorithm, theory, and applications",
  author   = "Elhamifar, Ehsan and Vidal, Rene",
  abstract = "Many real-world problems deal with collections of
              high-dimensional data, such as images, videos, text, and web
              documents, DNA microarray data, and more. Often, such
              high-dimensional data lie close to low-dimensional structures
              corresponding to several classes or categories to which the data
              belong. In this paper, we propose and study an algorithm, called
              sparse subspace clustering, to cluster data points that lie in a
              union of low-dimensional subspaces. The key idea is that, among
              the infinitely many possible representations of a data point in
              terms of other points, a sparse representation corresponds to
              selecting a few points from the same subspace. This motivates
              solving a sparse optimization program whose solution is used in a
              spectral clustering framework to infer the clustering of the data
              into subspaces. Since solving the sparse optimization program is
              in general NP-hard, we consider a convex relaxation and show
              that, under appropriate conditions on the arrangement of the
              subspaces and the distribution of the data, the proposed
              minimization program succeeds in recovering the desired sparse
              representations. The proposed algorithm is efficient and can
              handle data points near the intersections of subspaces. Another
              key advantage of the proposed algorithm with respect to the state
              of the art is that it can deal directly with data nuisances, such
              as noise, sparse outlying entries, and missing entries, by
              incorporating the model of the data into the sparse optimization
              program. We demonstrate the effectiveness of the proposed
              algorithm through experiments on synthetic data as well as the
              two real-world problems of motion segmentation and face
              clustering. \copyright{} 1979-2012 IEEE.",
  journal  = "IEEE transactions on pattern analysis and machine intelligence",
  volume   =  35,
  number   =  11,
  pages    = "2765--2781",
  year     =  2013,
  keywords = "(ℓ1)-minimization; High-dimensional data; clustering; convex
              programming; face clustering; intrinsic low-dimensionality;
              motion segmentation; principal angles; sparse representation;
              spectral clustering; subspaces"
}

@ARTICLE{Faugeras1988-fj,
  title    = "Motion and Structure From Motion in a Piecewise Planar
              Environment",
  author   = "Faugeras, O D and Lustman, F",
  abstract = "We show in this article that when the environment is piecewise
              linear, it provides a powerful constraint on the kind of matches
              that exist between two images of the scene when the camera motion
              is unknown. For points and lines located in the same plane, the
              correspondence between the two cameras is a collineation. We show
              that the unknowns (the camera motion and the plane equation) can
              be recovered, in general, from an estimate of the matrix of this
              collineation. The two-fold ambiguity that remains can be removed
              by looking at a second plane, by taking a third view of the same
              plane, or by using a priori knowledge about the geometry of the
              plane being looked at. We then show how to combine the estimation
              of the matrix of collineation and the obtaining of point and line
              matches between the two images, by a strategy of Hypothesis
              Prediction and Testing guided by a Kalman filter. We finally show
              how our approach can be used to calibrate a system of cameras.",
  journal  = "Int. J. Pattern Recognit Artif Intell.",
  volume   =  02,
  number   =  03,
  pages    = "485--508",
  year     =  1988
}

@ARTICLE{Fabbri2019-vz,
  title    = "Trifocal Relative Pose from Lines at Points and its Efficient
              Solution",
  author   = "Fabbri, Ricardo and Duff, Timothy and Fan, Hongyi and Regan,
              Margaret and de Pinho, David and Tsigaridas, Elias and Wampler,
              Charles and Hauenstein, Jonathan and Kimia, Benjamin and Leykin,
              Anton and Pajdla, Tomas",
  abstract = "We present a new minimal problem for relative pose estimation
              mixing point features with lines incident at points observed in
              three views and its efficient homotopy continuation solver. We
              demonstrate the generality of the approach by analyzing and
              solving an additional problem with mixed point and line
              correspondences in three views. The minimal problems include
              correspondences of (i) three points and one line and (ii) three
              points and two lines through two of the points which is reported
              and analyzed here for the first time. These are difficult to
              solve, as they have 216 and - as shown here - 312 solutions, but
              cover important practical situations when line and point features
              appear together, e.g., in urban scenes or when observing curves.
              We demonstrate that even such difficult problems can be solved
              robustly using a suitable homotopy continuation technique and we
              provide an implementation optimized for minimal problems that can
              be integrated into engineering applications. Our simulated and
              real experiments demonstrate our solvers in the camera geometry
              computation task in structure from motion. We show that new
              solvers allow for reconstructing challenging scenes where the
              standard two-view initialization of structure from motion fails.",
  journal  = "arXiv",
  year     =  2019
}

@ARTICLE{Fathy2011-ce,
  title    = "Fundamental matrix estimation: A study of error criteria",
  author   = "Fathy, Mohammed E and Hussein, Ashraf S and Tolba, Mohammed F",
  abstract = "The fundamental matrix (FM) describes the geometric relations
              that exist between two images of the same scene. Different error
              criteria are used for estimating FMs from an input set of
              correspondences. In this paper, the accuracy and efficiency
              aspects of the different error criteria are studied. We
              mathematically and experimentally proved that the most popular
              error criterion, the symmetric epipolar distance, is biased. It
              was also shown that despite the similarity between the algebraic
              expressions of the symmetric epipolar distance and Sampson
              distance, they have different accuracy properties. In addition, a
              new error criterion, Kanatani distance, was proposed and proved
              to be the most effective for use during the outlier removal phase
              from accuracy and efficiency perspectives. To thoroughly test the
              accuracy of the different error criteria, we proposed a
              randomized algorithm for Reprojection Error-based Correspondence
              Generation (RE-CG). As input, RE-CG takes an FM and a desired
              reprojection error value d. As output, RE-CG generates a random
              correspondence having that error value. Mathematical analysis of
              this algorithm revealed that the success probability for any
              given trial is 1 - (2/3)2 at best and is 1 - (6/7)2 at worst
              while experiments demonstrated that the algorithm often succeeds
              after only one trial. \copyright{} 2010 Elsevier B.V. All rights
              reserved.",
  journal  = "Pattern Recognit. Lett.",
  volume   =  32,
  number   =  2,
  pages    = "383--391",
  year     =  2011,
  keywords = "Epipolar geometry; Fundamental matrix; Structure and motion"
}

@ARTICLE{Ferraz2014-eh,
  title     = "Very fast solution to the {PnP} problem with algebraic outlier
               rejection",
  author    = "Ferraz, Luis and Binefa, Xavier and Moreno-Noguer, Francesc",
  abstract  = "We propose a real-time, robust to outliers and accurate solution
               to the Perspective-n-Point (PnP) problem. The main advantages of
               our solution are twofold: first, it in- tegrates the outlier
               rejection within the pose estimation pipeline with a negligible
               computational overhead, and sec- ond, its scalability to
               arbitrarily large number of correspon- dences. Given a set of
               3D-to-2D matches, we formulate pose estimation problem as a
               low-rank homogeneous sys- tem where the solution lies on its 1D
               null space. Outlier correspondences are those rows of the linear
               system which perturb the null space and are progressively
               detected by projecting them on an iteratively estimated solution
               of the null space. Since our outlier removal process is based on
               an algebraic criterion which does not require computing the
               full-pose and reprojecting back all 3D points on the image plane
               at each step, we achieve speed gains of more than 100$\times$
               compared to RANSAC strategies. An extensive exper- imental
               evaluation will show that our solution yields accu- rate results
               in situations with up to 50\% of outliers, and can process more
               than 1000 correspondences in less than 5ms.",
  journal   = "IEEE Computer Society Conference on Computer Vision and Pattern
               Recognition",
  publisher = "IEEE",
  pages     = "501--508",
  year      =  2014,
  keywords  = "Camera pose estimation; Perspective-n-Point problem; RANSAC;
               outlier rejection"
}

@ARTICLE{Fragoso2013-sv,
  title    = "{EVSAC}: Accelerating hypotheses generation by modeling matching
              scores with extreme value theory",
  author   = "Fragoso, Victor and Sen, Pradeep and Rodriguez, Sergio and Turk,
              Matthew",
  abstract = "Algorithms based on RANSAC that estimate models using feature
              correspondences between images can slow down tremendously when
              the percentage of correct correspondences (inliers) is small. In
              this paper, we present a probabilistic parametric model that
              allows us to assign confidence values for each matching
              correspondence and therefore accelerates the generation of
              hypothesis models for RANSAC under these conditions. Our
              framework leverages Extreme Value Theory to accurately model the
              statistics of matching scores produced by a nearest-neighbor
              feature matcher. Using a new algorithm based on this model, we
              are able to estimate accurate hypotheses with RANSAC at low
              inlier ratios significantly faster than previous state-of-the-art
              approaches, while still performing comparably when the number of
              inliers is large. We present results of homography and
              fundamental matrix estimation experiments for both SIFT and SURF
              matches that demonstrate that our method leads to accurate and
              fast model estimations. \copyright{} 2013 IEEE.",
  journal  = "Proceedings of the IEEE International Conference on Computer
              Vision",
  pages    = "2472--2479",
  year     =  2013,
  keywords = "extreme value theory; ransac; robust estimation"
}

@ARTICLE{Gallo2011-ir,
  title     = "{CC-RANSAC}: Fitting planes in the presence of multiple surfaces
               in range data",
  author    = "Gallo, Orazio and Manduchi, Roberto and Rafii, Abbas",
  abstract  = "Range sensors, in particular time-of-flight and stereo cameras,
               are being increasingly used for applications such as robotics,
               automotive, human-machine interface and virtual reality. The
               ability to recover the geometrical structure of visible surfaces
               is critical for scene understanding. Typical structured indoor
               or urban scenes are often represented via compositional models
               comprising multiple planar surface patches. The RANSAC robust
               regression algorithm is the most popular technique to date for
               extracting individual planar patches from noisy data sets
               containing multiple surfaces. Unfortunately, RANSAC fails to
               produce reliable results in situations with two nearby patches
               of limited extent, where a single plane crossing through the two
               patches may contain more inliers than the ``correct'' models.
               This is the case of steps, curbs, or ramps, which represent the
               focus of our research for the impact they can have on cars' safe
               parking systems or robot navigation. In an effort to improve the
               quality of regression in these cases, we propose a modification
               of the RANSAC algorithm, dubbed CC-RANSAC, that only considers
               the largest connected components of inliers to evaluate the
               fitness of a candidate plane. We provide experimental evidence
               that CC-RANSAC may recover the planar patches composing a
               typical step or ramp with substantially higher accuracy than the
               traditional RANSAC algorithm. \copyright{} 2010 Elsevier B.V.
               All rights reserved.",
  journal   = "Pattern Recognit. Lett.",
  publisher = "Elsevier B.V.",
  volume    =  32,
  number    =  3,
  pages     = "403--410",
  year      =  2011,
  keywords  = "RANSAC; Range data processing; Robust fitting; Time-of-flight
               applications"
}

@ARTICLE{Gherardi2010-qo,
  title    = "Improving the efficiency of hierarchical structure-and-motion",
  author   = "Gherardi, Riccardo and Farenzena, Michela and Fusiello, Andrea",
  abstract = "We present a completely automated Structure and Motion pipeline
              capable of working with uncalibrated images with varying internal
              parameters and no ancillary information. The system is based on a
              novel hierarchical scheme which reduces the total complexity by
              one order of magnitude. We assess the quality of our approach
              analytically by comparing the recovered point clouds with laser
              scans, which serves as ground truth data.",
  journal  = "Proceedings of the IEEE Computer Society Conference on Computer
              Vision and Pattern Recognition",
  pages    = "1594--1600",
  year     =  2010
}

@ARTICLE{Girshick2014-nt,
  title    = "Rich feature hierarchies for accurate object detection and
              semantic segmentation",
  author   = "Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik,
              Jitendra",
  abstract = "Object detection performance, as measured on the canonical PASCAL
              VOC dataset, has plateaued in the last few years. The
              best-performing methods are complex ensemble systems that
              typically combine multiple low-level image features with
              high-level context. In this paper, we propose a simple and
              scalable detection algorithm that improves mean average precision
              (mAP) by more than 30\% relative to the previous best result on
              VOC 2012 - achieving a mAP of 53.3\%. Our approach combines two
              key insights: (1) one can apply high-capacity convolutional
              neural networks (CNNs) to bottom-up region proposals in order to
              localize and segment objects and (2) when labeled training data
              is scarce, supervised pre-training for an auxiliary task,
              followed by domain-specific fine-tuning, yields a significant
              performance boost. Since we combine region proposals with CNNs,
              we call our method R-CNN: Regions with CNN features. We also
              present experiments that provide insight into what the network
              learns, revealing a rich hierarchy of image features. Source code
              for the complete system is available at
              http://www.cs.berkeley.edu/~rbg/rcnn.",
  journal  = "Proceedings of the IEEE Computer Society Conference on Computer
              Vision and Pattern Recognition",
  pages    = "580--587",
  year     =  2014
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gitlin2018-sq,
  title    = "Improving {K-Subspaces} via Coherence Pursuit",
  author   = "Gitlin, Andrew and Tao, Biaoshuai and Balzano, Laura and Lipor,
              John",
  abstract = "Subspace clustering is a powerful generalization of clustering
              for high-dimensional data analysis, where low-rank cluster
              structure is leveraged for accurate inference. K-Subspaces (KSS),
              an alternating algorithm that mirrors K-means, is a classical
              approach for clustering with this model. Like K-means, KSS is
              highly sensitive to initialization, yet KSS has two major
              handicaps beyond this issue. First, unlike K-means, the KSS
              objective is NP-hard to approximate within any finite factor for
              a large enough subspace rank. Second, it is known that the ℓ2
              subspace estimation step is faulty when an estimated cluster has
              points from multiple subspaces. In this paper, we demonstrate
              both of these additional drawbacks, provide a proof for the
              former, and offer a solution to the latter through the use of a
              robust subspace recovery (RSR) method known as coherence pursuit
              (CoP). While many RSR methods have been developed in recent
              years, few can handle the case where the outliers are themselves
              low rank. We prove that CoP can handle low-rank outliers. This
              and its low computational complexity make it ideal to incorporate
              into the subspace estimation step of KSS. We demonstrate on
              synthetic data that CoP successfully rejects low-rank outliers
              and show that combining CoP with K-Subspaces yields
              state-of-the-art clustering performance on canonical benchmark
              datasets.",
  journal  = "IEEE J. Sel. Top. Sign. Proces.",
  volume   =  12,
  number   =  6,
  pages    = "1575--1588",
  year     =  2018,
  keywords = "K-Subspaces (KSS); Subspace clustering; coherence pursuit (CoP);
              robust principal component analysis (PCA); robust subspace
              recovery (RSR)"
}

@MISC{Hartley1997-wq,
  title    = "In defense of the eight-point algorithm",
  author   = "Hartley, Richard I",
  abstract = "The fundamental matrix is a basic tool in the analysis of
              scenes\textbackslashntaken with two uncalibrated cameras, and the
              eight-point algorithm is a\textbackslashnfrequently cited method
              for computing the fundamental matrix from a set\textbackslashnof
              eight or more point matches. It has the advantage of simplicity
              of\textbackslashnimplementation. The prevailing view is, however,
              that it is extremely\textbackslashnsusceptible to noise and hence
              virtually useless for most purposes. This\textbackslashnpaper
              challenges that view, by showing that by preceding the
              algorithm\textbackslashnwith a very simple normalization
              (translation and scaling) of the\textbackslashncoordinates of the
              matched points, results are obtained comparable
              with\textbackslashnthe best iterative algorithms. This improved
              performance is justified by\textbackslashntheory and verified by
              extensive experiments on real images",
  journal  = "IEEE Transactions on Pattern Analysis and Machine Intelligence",
  volume   =  19,
  number   =  6,
  pages    = "580--593",
  year     =  1997,
  keywords = "Condition number; Eight-point algorithm; Epipolar structure;
              Fundamental matrix; Stereo vision"
}

@ARTICLE{Heckel2015-hm,
  title    = "Robust Subspace Clustering via Thresholding",
  author   = "Heckel, Reinhard and B{\"o}lcskei, Helmut",
  abstract = "The problem of clustering noisy and incompletely observed
              high-dimensional data points into a union of low-dimensional
              subspaces and a set of outliers is considered. The number of
              subspaces, their dimensions, and their orientations are assumed
              unknown. We propose a simple low-complexity subspace clustering
              algorithm, which applies spectral clustering to an adjacency
              matrix obtained by thresholding the correlations between data
              points. In other words, the adjacency matrix is constructed from
              the nearest neighbors of each data point in spherical distance. A
              statistical performance analysis shows that the algorithm
              exhibits robustness to additive noise and succeeds even when the
              subspaces intersect. Specifically, our results reveal an explicit
              tradeoff between the affinity of the subspaces and the tolerable
              noise level. We furthermore prove that the algorithm succeeds
              even when the data points are incompletely observed with the
              number of missing entries allowed to be (up to a log-factor)
              linear in the ambient dimension. We also propose a simple scheme
              that provably detects outliers, and we present numerical results
              on real and synthetic data.",
  journal  = "IEEE transactions on information theory",
  volume   =  61,
  number   =  11,
  pages    = "6320--6342",
  year     =  2015,
  keywords = "Subspace clustering; concentration of measure; incomplete
              observations; order statistics; outlier detection; spectral
              clustering"
}

@ARTICLE{Hosseinzadeh2019-wq,
  title    = "Structure Aware {SLAM} Using Quadrics and Planes",
  author   = "Hosseinzadeh, Mehdi and Latif, Yasir and Pham, Trung and
              Suenderhauf, Niko and Reid, Ian",
  abstract = "Simultaneous Localization And Mapping (SLAM) is a fundamental
              problem in mobile robotics. While point-based SLAM methods
              provide accurate camera localization, the generated maps lack
              semantic information. On the other hand, state of the art object
              detection methods provide rich information about entities present
              in the scene from a single image. This work marries the two and
              proposes a method for representing generic objects as quadrics
              which allows object detections to be seamlessly integrated in a
              SLAM framework. For scene coverage, additional dominant planar
              structures are modeled as infinite planes. Experiments show that
              the proposed points-planes-quadrics representation can easily
              incorporate Manhattan and object affordance constraints, greatly
              improving camera localization and leading to semantically
              meaningful maps.",
  journal  = "Lect. Notes Comput. Sci.",
  volume   = "11363 LNCS",
  pages    = "410--426",
  year     =  2019,
  keywords = "Object SLAM; Planes; Quadrics; Visual semantic SLAM"
}

@ARTICLE{Huang2019-do,
  title    = "Deep Clustering via Weighted k-Subspace Network",
  author   = "Huang, Weitian and Yin, Ming and Li, Jianzhong and Xie, Shengli",
  abstract = "Subspace clustering aims to separate the data into clusters under
              the hypothesis that the samples within the same cluster will lie
              in the same low-dimensional subspace. Due to the tough pairwise
              constraints, k-subspace clustering is sensitive to outliers and
              initialization. In this letter, we present a novel deep
              architecture for k-subspace clustering to address this issue,
              called as Deep Weighted k-Subspace Clustering (DWSC).
              Specifically, our framework consists of autoencoder and weighted
              k-subsapce network. We first use the autoencoder to non-linearly
              compress the samples into the low-dimensional latent space. In
              the weighted k-subspace network, we feed the latent
              representation into the assignment network to output soft
              assignments which represent the probability of data belonging to
              the according subspace. Subsequently, the optimal k subspaces are
              identified by minimizing the projection residuals of the latent
              representations to all subspaces, using the learned soft
              assignments as a weighting vector. Finally, we jointly optimize
              the representation learning and clustering in a unified
              framework. Experimental results show that our approach
              outperforms the state-of-the-art subspace clustering methods on
              two benchmark datasets.",
  journal  = "IEEE Signal Process. Lett.",
  volume   =  26,
  number   =  11,
  pages    = "1628--1632",
  year     =  2019,
  keywords = "Deep clustering; autoencoder; subspace clustering; weighted"
}

@ARTICLE{Indelman2012-pb,
  title     = "{Real-Time} {Vision-Aided} Localization and Navigation Based on
               {Three-View} Geometry",
  author    = "Indelman, Vadim and Gurfil, Pini and Rivlin, Ehud and Rotstein,
               Hector",
  journal   = "IEEE Trans. Aerosp. Electron. Syst.",
  publisher = "IEEE",
  volume    =  48,
  number    =  3,
  pages     = "2239--2259",
  year      =  2012
}

@INPROCEEDINGS{Ji2017-yx,
  title     = "Deep subspace clustering networks",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Ji, Pan and Zhang, Tong and Li, Hongdong and Salzmann, Mathieu
               and Reid, Ian",
  abstract  = "We present a novel deep neural network architecture for
               unsupervised subspace clustering. This architecture is built
               upon deep auto-encoders, which non-linearly map the input data
               into a latent space. Our key idea is to introduce a novel
               self-expressive layer between the encoder and the decoder to
               mimic the ``self-expressiveness'' property that has proven
               effective in traditional subspace clustering. Being
               differentiable, our new self-expressive layer provides a simple
               but effective way to learn pairwise affinities between all data
               points through a standard back-propagation procedure. Being
               nonlinear, our neural-network based method is able to cluster
               data points having complex (often nonlinear) structures. We
               further propose pre-training and fine-tuning strategies that let
               us effectively learn the parameters of our subspace clustering
               networks. Our experiments show that our method significantly
               outperforms the state-of-the-art unsupervised subspace
               clustering techniques.",
  volume    = "2017-Decem",
  pages     = "24--33",
  year      =  2017
}

@ARTICLE{Jiang2018-pc,
  title     = "A nonconvex formulation for low rank subspace clustering:
               algorithms and convergence analysis",
  author    = "Jiang, Hao and Robinson, Daniel P and Vidal, Ren{\'e} and You,
               Chong",
  abstract  = "We consider the problem of subspace clustering with data that is
               potentially corrupted by both dense noise and sparse gross
               errors. In particular, we study a recently proposed low rank
               subspace clustering approach based on a nonconvex modeling
               formulation. This formulation includes a nonconvex spectral
               function in the objective function that makes the optimization
               task challenging, e.g., it is unknown whether the alternating
               direction method of multipliers (ADMM) framework proposed to
               solve the nonconvex model formulation is provably convergent. In
               this paper, we establish that the spectral function is
               differentiable and give a formula for computing the derivative.
               Moreover, we show that the derivative of the spectral function
               is Lipschitz continuous and provide an explicit value for the
               Lipschitz constant. These facts are then used to provide a lower
               bound for how the penalty parameter in the ADMM method should be
               chosen. As long as the penalty parameter is chosen according to
               this bound, we show that the ADMM algorithm computes iterates
               that have a limit point satisfying first-order optimality
               conditions. We also present a second strategy for solving the
               nonconvex problem that is based on proximal gradient
               calculations. The convergence and performance of the algorithms
               is verified through experiments on real data from face and digit
               clustering and motion segmentation.",
  journal   = "Comput. Optim. Appl.",
  publisher = "Springer US",
  volume    =  70,
  number    =  2,
  pages     = "395--418",
  year      =  2018,
  keywords  = "ADMM; Nonconvex; Subspace clustering"
}

@ARTICLE{Jiang2019-vr,
  title     = "A novel relative camera motion estimation algorithm with
               applications to visual odometry",
  author    = "Jiang, Yue and Kang, Mun Cheon and Fan, Ming and Chae, Sung Ho
               and Ko, Sung Jea",
  journal   = "Proceedings - 2018 IEEE International Symposium on Multimedia,
               ISM 2018",
  publisher = "IEEE",
  pages     = "215--216",
  year      =  2019,
  keywords  = "Eight-point algorithm; Trifocal tensor; Visual odometry"
}

@ARTICLE{Kalofolias2017-ba,
  title    = "Large scale graph learning from smooth signals",
  author   = "Kalofolias, Vassilis and Perraudin, Nathana{\"e}l",
  abstract = "Graphs are a prevalent tool in data science, as they model the
              inherent structure of data. Graphs have been used successfully in
              unsupervised and semi-supervised learning. Typically they are
              constructed either by connecting nearest samples, or by learning
              them from data, solving an optimization problem. While graph
              learning does achieve a better quality, it also comes with a
              higher computational cost. In particular, the previous
              state-of-the-art model cost is O ( n2) for n samples. In this
              paper, we show how to scale it, obtaining an approximation with
              leading cost of O(n log(n)), with quality that approaches the
              exact graph learning model. Our algorithm uses known approximate
              nearest neighbor techniques to reduce the number of variables,
              and automatically selects the correct parameters of the model,
              requiring a single intuitive input: The desired edge density.",
  journal  = "arXiv",
  pages    = "1--21",
  year     =  2017
}

@ARTICLE{Ke2017-sg,
  title    = "An efficient algebraic solution to the perspective-three-point
              problem",
  author   = "Ke, Tong and Roumeliotis, Stergios I",
  abstract = "In this work, we present an algebraic solution to the classical
              perspective-3-point (P3P) problem for determining the position
              and attitude of a camera from observations of three known
              reference points. In contrast to previous approaches, we first
              directly determine the camera's attitude by employing the
              corresponding geometric constraints to formulate a system of
              trigonometric equations. This is then efficiently solved,
              following an algebraic approach, to determine the unknown
              rotation matrix and subsequently the camera's position. As
              compared to recent alternatives, our method avoids computing
              unnecessary (and potentially numerically unstable) intermediate
              results, and thus achieves higher numerical accuracy and
              robustness at a lower computational cost. These benefits are
              validated through extensive Monte-Carlo simulations for both
              nominal and closeto-singular geometric configurations.",
  journal  = "Proceedings - 30th IEEE Conference on Computer Vision and Pattern
              Recognition, CVPR 2017",
  volume   = "2017-Janua",
  pages    = "4618--4626",
  year     =  2017
}

@ARTICLE{Kim2018-fd,
  title     = "Deep Monocular Depth Estimation via Integration of Global and
               Local Predictions",
  author    = "Kim, Youngjung and Jung, Hyungjoo and Min, Dongbo and Sohn,
               Kwanghoon",
  abstract  = "Recent works on machine learning have greatly advanced the
               accuracy of single image depth estimation. However, the
               resulting depth images are still over-smoothed and perceptually
               unsatisfying. This paper casts depth prediction from single
               image as a parametric learning problem. Specifically, we propose
               a deep variational model that effectively integrates
               heterogeneous predictions from two convolutional neural networks
               (CNNs), named global and local networks. They have contrasting
               network architecture and are designed to capture the depth
               information with complementary attributes. These intermediate
               outputs are then combined in the integration network based on
               the variational framework. By unrolling the optimization steps
               of Split Bregman iterations in the integration network, our
               model can be trained in an end-to-end manner. This enables one
               to simultaneously learn an efficient parameterization of the
               CNNs and hyper-parameter in the variational method. Finally, we
               offer a new data set of 0.22 million RGB-D images captured by
               Microsoft Kinect v2. Our model generates realistic and
               discontinuity-preserving depth prediction without involving any
               low-level segmentation or superpixels. Intensive experiments
               demonstrate the superiority of the proposed method in a range of
               RGB-D benchmarks, including both indoor and outdoor scenarios.",
  journal   = "IEEE Trans. Image Process.",
  publisher = "IEEE",
  volume    =  27,
  number    =  8,
  pages     = "4131--4144",
  year      =  2018,
  keywords  = "2D-to-3D conversion; Depth estimation; RGB-D database;
               convolutional neural networks; non-parametric sampling"
}

@ARTICLE{Kitt2010-ug,
  title     = "Visual Odometry based on Stereo Image Sequences with
               {RANSAC-based} Outlier Rejection Scheme",
  author    = "Kitt, Bernd and Geiger, Andreas and Lategahn, Henning",
  abstract  = "good news: we could replace the outlier rejection scheme by
               using trifocal constraints bad news: 3 data points needed in
               order to determine a model. it is unclear if we have advantage
               here",
  journal   = "2010 IEEE Intelligent Vehicles Symposium",
  publisher = "IEEE",
  pages     = "486--492",
  year      =  2010
}

@ARTICLE{Korman2018-ns,
  title    = "Latent {RANSAC}",
  author   = "Korman, Simon and Litman, Roee",
  abstract = "We present a method that can evaluate a RANSAC hypothesis in
              constant time, i.e. independent of the size of the data. A key
              observation here is that correct hypotheses are tightly clustered
              together in the latent parameter domain. In a manner similar to
              the generalized Hough transform we seek to find this cluster,
              only that we need as few as two votes for a successful detection.
              Rapidly locating such pairs of similar hypotheses is made
              possible by adapting the recent ``Random Grids'' range-search
              technique. We only perform the usual (costly) hypothesis
              verification stage upon the discovery of a close pair of
              hypotheses. We show that this event rarely happens for incorrect
              hypotheses, enabling a significant speedup of the RANSAC
              pipeline. The suggested approach is applied and tested on three
              robust estimation problems: camera localization, 3D rigid
              alignment and 2D-homography estimation. We perform rigorous
              testing on both synthetic and real datasets, demonstrating an
              improvement in efficiency without a compromise in accuracy.
              Furthermore, we achieve state-of-the-art 3D alignment results on
              the challenging ``Redwood'' loop-closure challenge.",
  journal  = "Proceedings of the IEEE Computer Society Conference on Computer
              Vision and Pattern Recognition",
  pages    = "6693--6702",
  year     =  2018
}

@ARTICLE{Kumar2004-zk,
  title    = "Discrete contours in multiple views: Approximation and
              recognition",
  author   = "Kumar, M Pawan and Goyal, Saurabh and Kuthirummal, Sujit and
              Jawahar, C V and Narayanan, P J",
  abstract = "Recognition of discrete planar contours under similarity
              transformations has received a lot of attention but little work
              has been reported on recognizing them under more general
              transformations. Planar object boundaries undergo projective or
              affine transformations across multiple views. We present two
              methods to recognize discrete curves in this paper. The first
              method computes a piecewise parametric approximation of the
              discrete curve that is projectively invariant. A polygon
              approximation scheme and a piecewise conic approximation scheme
              are presented here. The second method computes an invariant
              sequence directly from the sequence of discrete points on the
              curve in a Fourier transform space. The sequence is shown to be
              identical up to a scale factor in all affine related views of the
              curve. We present the theory and demonstrate its applications to
              several problems including numeral recognition, aircraft
              recognition, and homography computation. \copyright{} 2004
              Elsevier B.V. All rights reserved.",
  journal  = "Image Vis. Comput.",
  volume   =  22,
  number   =  14,
  pages    = "1229--1239",
  year     =  2004,
  keywords = "Fourier transform; Invariant; Piecewise conic approximation;
              Planar shape recognition; Polygonal approximation; Projective
              geometry"
}

@ARTICLE{Lee2015-lv,
  title    = "Membership representation for detecting block-diagonal structure
              in low-rank or sparse subspace clustering",
  author   = "Lee, Minsik and Lee, Jieun and Lee, Hyeogjin and Kwak, Nojun",
  abstract = "Recently, there have been many proposals with state-of-the-art
              results in subspace clustering that take advantages of the
              low-rank or sparse optimization techniques. These methods are
              based on self-expressive models, which have well-defined
              theoretical aspects. They produce matrices with (approximately)
              block-diagonal structure, which is then applied to spectral
              clustering. However, there is no definitive way to construct
              affinity matrices from these block-diagonal matrices and it is
              ambiguous how the performance will be affected by the
              construction method. In this paper, we propose an alternative
              approach to detect block-diagonal structures from these matrices.
              The proposed method shares the philosophy of the above subspace
              clustering methods, in that it is a self-expressive system based
              on a Hadamard product of a membership matrix. To resolve the
              difficulty in handling the membership matrix, we solve the convex
              relaxation of the problem and then transform the representation
              to a doubly stochastic matrix, which is closely related to
              spectral clustering. The result of our method has eigenvalues
              normalized in between zero and one, which is more reliable to
              estimate the number of clusters and to perform spectral
              clustering. The proposed method shows competitive results in our
              experiments, even though we simply count the number of
              eigenvalues larger than a certain threshold to find the number of
              clusters.",
  journal  = "Proceedings of the IEEE Computer Society Conference on Computer
              Vision and Pattern Recognition",
  volume   = "07-12-June",
  pages    = "1648--1656",
  year     =  2015
}

@ARTICLE{Leonardos2015-sh,
  title    = "A metric parametrization for trifocal tensors with non-colinear
              pinholes",
  author   = "Leonardos, Spyridon and Tron, Roberto and Daniilidis, Kostas",
  abstract = "The trifocal tensor, which describes the relation between
              projections of points and lines in three views, is a fundamental
              entity of geometric computer vision. In this work, we investigate
              a new parametrization of the trifocal tensor for calibrated
              cameras with non-colinear pinholes obtained from a quotient
              Riemannian manifold. We incorporate this formulation into
              state-of-the art methods for optimization on manifolds, and show,
              through experiments in pose averaging, that it produces a
              meaningful way to measure distances between trifocal tensors.",
  journal  = "Proceedings of the IEEE Computer Society Conference on Computer
              Vision and Pattern Recognition",
  volume   = "07-12-June",
  number   =  1,
  pages    = "259--267",
  year     =  2015
}

@ARTICLE{Lepetit2009-fm,
  title    = "{EPnP}: An accurate O(n) solution to the {PnP} problem",
  author   = "Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal",
  abstract = "We propose a non-iterative solution to the PnP problem-the
              estimation of the pose of a calibrated camera from n 3D-to-2D
              point correspondences-whose computational complexity grows
              linearly with n. This is in contrast to state-of-the-art methods
              that are O(n 5) or even O(n 8), without being more accurate. Our
              method is applicable for all n $\geq$ 4 and handles properly both
              planar and non-planar configurations. Our central idea is to
              express the n 3D points as a weighted sum of four virtual control
              points. The problem then reduces to estimating the coordinates of
              these control points in the camera referential, which can be done
              in O(n) time by expressing these coordinates as weighted sum of
              the eigenvectors of a 12 $\times$ 12 matrix and solving a small
              constant number of quadratic equations to pick the right weights.
              Furthermore, if maximal precision is required, the output of the
              closed-form solution can be used to initialize a Gauss-Newton
              scheme, which improves accuracy with negligible amount of
              additional time. The advantages of our method are demonstrated by
              thorough testing on both synthetic and real-data.",
  journal  = "Int. J. Comput. Vis.",
  volume   =  81,
  number   =  2,
  pages    = "155--166",
  year     =  2009,
  keywords = "Absolute orientation; Perspective-n-Point; Pose estimation"
}

@BOOK{Lerman2015-cg,
  title    = "Robust Computation of Linear Models by Convex Relaxation",
  author   = "Lerman, Gilad and McCoy, Michael B and Tropp, Joel A and Zhang,
              Teng",
  abstract = "Consider a dataset of vector-valued observations that consists of
              noisy inliers, which are explained well by a low-dimensional
              subspace, along with some number of outliers. This work describes
              a convex optimization problem, called REAPER, that can reliably
              fit a low-dimensional model to this type of data. This approach
              parameterizes linear subspaces using orthogonal projectors, and
              it uses a relaxation of the set of orthogonal projectors to reach
              the convex formulation. The paper provides an efficient algorithm
              for solving the REAPER problem, and it documents numerical
              experiments which confirm that REAPER can dependably find linear
              structure in synthetic and natural data. In addition, when the
              inliers lie near a low-dimensional subspace, there is a rigorous
              theory that describes when REAPER can approximate this subspace.",
  volume   =  15,
  year     =  2015,
  keywords = "Convex relaxation; Iteratively reweighted least squares; Robust
              linear models"
}

@ARTICLE{Lerman2018-lb,
  title   = "An Overview of Robust Subspace Recovery",
  author  = "Lerman, Gilad and Maunu, Tyler",
  journal = "Proc. IEEE",
  volume  =  106,
  pages   = "1380--1410",
  year    =  2018
}

@ARTICLE{Lerman2018-zt,
  title    = "Fast, robust and non-convex subspace recovery",
  author   = "Lerman, Gilad and Maunu, Tyler",
  abstract = "This work presents a fast and non-convex algorithm for robust
              subspace recovery. The datasets considered include inliers drawn
              around a low-dimensional subspace of a higher dimensional ambient
              space and a possibly large portion of outliers that do not lie
              nearby this subspace. The proposed algorithm, which we refer to
              as fast median subspace (FMS), is designed to robustly determine
              the underlying subspace of such datasets, while having lower
              computational complexity than existing accurate methods. We prove
              convergence of the FMS iterates to a stationary point. Further,
              under two special models of data, FMS converges to a point which
              is near to the global minimum with overwhelming probability.
              Under these models, we show that the iteration complexity is
              globally sublinear and locally r-linear. For one of the models,
              these results hold for any fixed fraction of outliers (< 1).
              Numerical experiments on synthetic and real data demonstrate its
              competitive speed and accuracy.",
  journal  = "Information and Inference",
  volume   =  7,
  number   =  2,
  pages    = "277--336",
  year     =  2018,
  keywords = "Dimension reduction; Iteratively reweighted least squares;
              Minimization on the Grassmannian; Non-convex optimization; Robust
              subspace recovery"
}

@ARTICLE{Li2017-gy,
  title    = "Structured Sparse Subspace Clustering: A Joint Affinity Learning
              and Subspace Clustering Framework",
  author   = "Li, Chun Guang and You, Chong and Vidal, Rene",
  abstract = "Subspace clustering refers to the problem of segmenting data
              drawn from a union of subspaces. State-of-the-art approaches for
              solving this problem follow a two-stage approach. In the first
              step, an affinity matrix is learned from the data using sparse or
              low-rank minimization techniques. In the second step, the
              segmentation is found by applying spectral clustering to this
              affinity. While this approach has led to the state-of-the-art
              results in many applications, it is suboptimal, because it does
              not exploit the fact that the affinity and the segmentation
              depend on each other. In this paper, we propose a joint
              optimization framework - Structured Sparse Subspace Clustering
              (S3C) - for learning both the affinity and the segmentation. The
              proposed S3C framework is based on expressing each data point as
              a structured sparse linear combination of all other data points,
              where the structure is induced by a norm that depends on the
              unknown segmentation. Moreover, we extend the proposed S3C
              framework into Constrained S3C (CS3C) in which available partial
              side-information is incorporated into the stage of learning the
              affinity. We show that both the structured sparse representation
              and the segmentation can be found via a combination of an
              alternating direction method of multipliers with spectral
              clustering. Experiments on a synthetic data set, the Extended
              Yale B face data set, the Hopkins 155 motion segmentation
              database, and three cancer data sets demonstrate the
              effectiveness of our approach.",
  journal  = "IEEE Trans. Image Process.",
  volume   =  26,
  number   =  6,
  pages    = "2988--3001",
  year     =  2017,
  keywords = "Structured sparse subspace clustering; cancer clustering;
              constrained subspace clustering; structured subspace clustering;
              subspace structured norm"
}

@ARTICLE{Liu2013-px,
  title    = "Bundled camera paths for video stabilization",
  author   = "Liu, Shuaicheng and Yuan, Lu and Tan, Ping and Sun, Jian",
  abstract = "We present a novel video stabilization method which models camera
              motion with a bundle of (multiple) camera paths. The proposed
              model is based on a mesh-based, spatially-variant motion
              representation and an adaptive, space-time path optimization. Our
              motion representation allows us to fundamentally handle parallax
              and rolling shutter effects while it does not require long
              feature trajectories or sparse 3D reconstruction. We introduce
              the 'as-similaras- possible' idea to make motion estimation more
              robust. Our space-time path smoothing adaptively adjusts
              smoothness strength by considering discontinuities, cropping size
              and geometrical distortion in a unified optimization framework.
              The evaluation on a large variety of consumer videos demonstrates
              the merits of our method. Copyright \copyright{} ACM 2013.",
  journal  = "ACM Trans. Graph.",
  volume   =  32,
  number   =  4,
  year     =  2013,
  keywords = "Camera paths; Image warping; Video stabilization"
}

@ARTICLE{Liu2013-sl,
  title    = "Robust recovery of subspace structures by low-rank representation",
  author   = "Liu, Guangcan and Lin, Zhouchen and Yan, Shuicheng and Sun, Ju
              and Yu, Yong and Ma, Yi",
  abstract = "In this paper, we address the subspace clustering problem. Given
              a set of data samples (vectors) approximately drawn from a union
              of multiple subspaces, our goal is to cluster the samples into
              their respective subspaces and remove possible outliers as well.
              To this end, we propose a novel objective function named Low-Rank
              Representation (LRR), which seeks the lowest rank representation
              among all the candidates that can represent the data samples as
              linear combinations of the bases in a given dictionary. It is
              shown that the convex program associated with LRR solves the
              subspace clustering problem in the following sense: When the data
              is clean, we prove that LRR exactly recovers the true subspace
              structures; when the data are contaminated by outliers, we prove
              that under certain conditions LRR can exactly recover the row
              space of the original data and detect the outlier as well; for
              data corrupted by arbitrary sparse errors, LRR can also
              approximately recover the row space with theoretical guarantees.
              Since the subspace membership is provably determined by the row
              space, these further imply that LRR can perform robust subspace
              clustering and error correction in an efficient and effective
              way.",
  journal  = "IEEE transactions on pattern analysis and machine intelligence",
  volume   =  35,
  number   =  1,
  pages    = "171--184",
  month    =  jan,
  year     =  2013,
  keywords = "Low-rank representation; outlier detection; segmentation;
              subspace clustering",
  language = "en"
}

@ARTICLE{Lorenz2021-qk,
  title     = "Quadratically regularized optimal transport",
  author    = "Lorenz, Dirk A and Manns, Paul and Meyer, Christian",
  abstract  = "We investigate the problem of optimal transport in the so-called
               Kantorovich form, i.e. given two Radon measures on two compact
               sets, we seek an optimal transport plan which is another Radon
               measure on the product of the sets that has these two measures
               as marginals and minimizes a certain cost function. We consider
               quadratic regularization of the problem, which forces the
               optimal transport plan to be a square integrable function rather
               than a Radon measure. We derive the dual problem and show strong
               duality and existence of primal and dual solutions to the
               regularized problem. Then we derive two algorithms to solve the
               dual problem of the regularized problem: A Gauss--Seidel method
               and a semismooth quasi-Newton method and investigate both
               methods numerically. Our experiments show that the methods
               perform well even for small regularization parameters. Quadratic
               regularization is of interest since the resulting optimal
               transport plans are sparse, i.e. they have a small support
               (which is not the case for the often used entropic
               regularization where the optimal transport plan always has full
               measure).",
  journal   = "Applied Mathematics \& Optimization",
  publisher = "Springer Science and Business Media LLC",
  volume    =  83,
  number    =  3,
  pages     = "1919--1949",
  month     =  jun,
  year      =  2021,
  keywords  = "Duality; Gauss--Seidel method; Optimal transport;
               Regularization; Semismooth Newton method",
  language  = "en"
}

@ARTICLE{Lourakis2009-uw,
  title    = "{SBA}: A software package for generic sparse bundle adjustment",
  author   = "Lourakis, Manolis I A and Argyros, Antonis A",
  abstract = "Bundle adjustment constitutes a large,
              nonlinear\textbackslashnleast-squares problem that is often
              solved as the\textbackslashnlast step of feature-based structure
              and motion\textbackslashnestimation computer vision algorithms to
              obtain\textbackslashnoptimal estimates. Due to the very large
              number of\textbackslashnparameters involved, a general purpose
              least-squares\textbackslashnalgorithm incurs high computational
              and memory\textbackslashnstorage costs when applied to
              bundle\textbackslashnadjustment. Fortunately, the lack of
              interaction\textbackslashnamong certain subgroups of parameters
              results in the\textbackslashncorresponding Jacobian being sparse,
              a fact that can\textbackslashnbe exploited to achieve
              considerable computational\textbackslashnsavings. This article
              presents sba, a publicly\textbackslashnavailable C/C++ software
              package for realizing\textbackslashngeneric bundle adjustment
              with high efficiency and\textbackslashnflexibility regarding
              parameterization.",
  journal  = "ACM Trans. Math. Softw.",
  volume   =  36,
  number   =  1,
  year     =  2009,
  keywords = "Bundle adjustment; Engineering applications; Levenberg-Marquardt;
              Multiple-view geometry; Nonlinear least squares; Sparse Jacobian;
              Structure and motion estimation; Unconstrained optimization"
}

@INPROCEEDINGS{Lu2012-lz,
  title     = "Robust and Efficient Subspace Segmentation via Least Squares
               Regression",
  booktitle = "European conference on computer vision",
  author    = "Lu, Can-Yi and Min, Hai and Zhao, Zhong-Qiu and Zhu, Lin and
               Huang, De-Shuang and Yan, Shuicheng",
  abstract  = "This paper studies the subspace segmentation problem which aims
               to segment data drawn from a union of multiple linear subspaces.
               Recent works by using sparse representation, low rank
               representation and their extensions attract much attention. If
               the subspaces from which the data drawn are independent or
               orthogonal, they are able to obtain a block diagonal affinity
               matrix, which usually leads to a correct segmentation. The main
               differences among them are their objective functions. We
               theoretically show that if the objective function satisfies some
               conditions, and the data are sufficiently drawn from independent
               subspaces, the obtained affinity matrix is always block
               diagonal. Furthermore, the data sampling can be insufficient if
               the subspaces are orthogonal. Some existing methods are all
               special cases. Then we present the Least Squares Regression
               (LSR) method for subspace segmentation. It takes advantage of
               data correlation, which is common in real data. LSR encourages a
               grouping effect which tends to group highly correlated data
               together. Experimental results on the Hopkins 155 database and
               Extended Yale Database B show that our method significantly
               outperforms state-of-the-art methods. Beyond segmentation
               accuracy, all experiments demonstrate that LSR is much more
               efficient.",
  publisher = "Springer",
  pages     = "347--360",
  year      =  2012
}

@ARTICLE{Lu2019-nh,
  title    = "Subspace Clustering by Block Diagonal Representation",
  author   = "Lu, Canyi and Feng, Jiashi and Lin, Zhouchen and Mei, Tao and
              Yan, Shuicheng",
  abstract = "This paper studies the subspace clustering problem. Given some
              data points approximately drawn from a union of subspaces, the
              goal is to group these data points into their underlying
              subspaces. Many subspace clustering methods have been proposed
              and among which sparse subspace clustering and low-rank
              representation are two representative ones. Despite the different
              motivations, we observe that many existing methods own the common
              block diagonal property, which possibly leads to correct
              clustering, yet with their proofs given case by case. In this
              work, we consider a general formulation and provide a unified
              theoretical guarantee of the block diagonal property. The block
              diagonal property of many existing methods falls into our special
              case. Second, we observe that many existing methods approximate
              the block diagonal representation matrix by using different
              structure priors, e.g., sparsity and low-rankness, which are
              indirect. We propose the first block diagonal matrix induced
              regularizer for directly pursuing the block diagonal matrix. With
              this regularizer, we solve the subspace clustering problem by
              Block Diagonal Representation (BDR), which uses the block
              diagonal structure prior. The BDR model is nonconvex and we
              propose an alternating minimization solver and prove its
              convergence. Experiments on real datasets demonstrate the
              effectiveness of BDR.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  41,
  number   =  2,
  pages    = "487--501",
  year     =  2019,
  keywords = "Subspace clustering; block diagonal regularizer; block diagonal
              representation; convergence analysis; nonconvex optimization;
              spectral clustering"
}

@ARTICLE{Ma2004-jw,
  title    = "Rank conditions on the multiple-view matrix",
  author   = "Ma, Yi and Huang, Kun and Vidal, Ren{\'e} and Ko{\v s}eck{\'a},
              Jana and Sastry, Shankar",
  abstract = "Geometric relationships governing multiple images of points and
              lines and associated algorithms have been studied to a large
              extent separately in multiple-view geometry. The previous studies
              led to a characterization based on multilinear constraints, which
              have been extensively used for structure and motion recovery,
              feature matching and image transfer. In this paper we present a
              universal rank condition on the so-called multiple-view matrix M
              for arbitrarily combined point and line features across multiple
              views. The condition gives rise to a complete set of constraints
              among multiple images. All previously known multilinear
              constraints become simple instantiations of the new condition. In
              particular, the relationship between bilinear, bilinear and
              quadrilinear constraints can be clearly revealed from this new
              approach. The theory enables us to carry out global geometric
              analysis for multiple images, as well as systematically
              characterize all degenerate configurations, without breaking
              image sequence into pairwise or triple-wise sets of views. This
              global treatment allows us to utilize all incidence conditions
              governing all features in all images simultaneously for a
              consistent recovery of motion and structure from multiple views.
              In particular, a rank-based multiple-view factorization algorithm
              for motion and structure recovery is derived from the rank
              condition. Simulation results are presented to validate the
              multiple-view matrix based approach.",
  journal  = "Int. J. Comput. Vis.",
  volume   =  59,
  number   =  2,
  pages    = "115--137",
  year     =  2004,
  keywords = "Factorization; Multiple-view matrix; Rank conditions; Structure
              from motion"
}

@ARTICLE{Ma2007-dx,
  title    = "Segmentation of multivariate mixed data via lossy data coding and
              compression",
  author   = "Ma, Yi and Derksen, Harm and Hong, Wei and Wright, John",
  abstract = "In this paper, based on ideas from lossy data coding and
              compression, we present a simple but effective technique for
              segmenting multivariate mixed data that are drawn from a mixture
              of Gaussian distributions, which are allowed to be almost
              degenerate. The goal is to find the optimal segmentation that
              minimizes the overall coding length of the segmented data,
              subject to a given distortion. By analyzing the coding
              length/rate of mixed data, we formally establish some strong
              connections of data segmentation to many fundamental concepts in
              lossy data compression and rate distortion theory. We show that a
              deterministic segmentation is approximately the (asymptotically)
              optimal solution for compressing mixed data. We propose a very
              simple and effective algorithm which depends on a single
              parameter, the allowable distortion. At any given distortion, the
              algorithm automatically determines the corresponding number and
              dimension of the groups and does not involve any parameter
              estimation. Simulation results reveal intriguing
              phase-transition-like behaviors of the number of segments when
              changing the level of distortion or the amount of outliers.
              Finally, we demonstrate how this technique can be readily applied
              to segment real imagery and bioinformatic data. \copyright{} 2007
              IEEE.",
  journal  = "IEEE transactions on pattern analysis and machine intelligence",
  volume   =  29,
  number   =  9,
  pages    = "1546--1562",
  year     =  2007,
  keywords = "Data clustering; Data segmentation; Image segmentation; Lossy
              coding; Lossy compression; Microarray data clustering;
              Multivariate mixed data; Rate distortion"
}

@ARTICLE{Matas2005-bt,
  title    = "Randomized {RANSAC} with sequential probability ratio test",
  author   = "Matas, Ji{\v r}{\'\i} and Chum, Ond{\v r}ej",
  abstract = "A randomized model verification strategy for RANSAC is presented.
              The proposed method finds, like RANSAC, a solution that is
              optimal with user-controllable probability $\eta$. A provably
              optimal model verification strategy is designed for the situation
              when the contamination of data by outliers is known, i.e. the
              algorithm is the fastest possible (on average) of all randomized
              RANSAC algorithms guaranteeing 1 - $\eta$ confidence in the
              solution. The derivation of the optimality property is based on
              Wald's theory of sequential decision making. The R-RANSAC with
              SPRT, which does not require the a priori knowledge of the
              fraction of outliers and has results close to the optimal
              strategy, is introduced. We show experimentally that on standard
              test data the method is 2 to 10 times faster than the standard
              RANSAC and up to 4 times faster than previously published
              methods. \copyright{} 2005 IEEE.",
  journal  = "Proceedings of the IEEE International Conference on Computer
              Vision",
  volume   = "II",
  pages    = "1727--1732",
  year     =  2005
}

@ARTICLE{Maunu2019-jn,
  title    = "A well-tempered landscape for non-convex robust subspace recovery",
  author   = "Maunu, Tyler and Zhang, Teng and Lerman, Gilad",
  abstract = "We present a mathematical analysis of a non-convex energy
              landscape for robust subspace recovery. We prove that an
              underlying subspace is the only stationary point and local
              minimizer in a specified neighborhood under a deterministic
              condition on a dataset. If the deterministic condition is
              satisfied, we further show that a geodesic gradient descent
              method over the Grassmannian manifold can exactly recover the
              underlying subspace when the method is properly initialized.
              Proper initialization by principal component analysis is
              guaranteed with a simple deterministic condition. Under slightly
              stronger assumptions, the gradient descent method with a
              piecewise constant step-size scheme achieves linear convergence.
              The practicality of the deterministic condition is demonstrated
              on some statistical models of data, and the method achieves
              almost state-of-the-art recovery guarantees on the Haystack Model
              for different regimes of sample size and ambient dimension. In
              particular, when the ambient dimension is fixed and the sample
              size is large enough, we show that our gradient method can
              exactly recover the underlying subspace for any fixed fraction of
              outliers (less than 1).",
  journal  = "J. Mach. Learn. Res.",
  volume   =  20,
  pages    = "1--59",
  year     =  2019,
  keywords = "Dimension reduction; Non-convex optimization; Optimization on the
              Grassmannian; Robust subspace recovery"
}

@ARTICLE{Moisan2012-rl,
  title    = "Automatic Homographic Registration of a Pair of Images, with A
              Contrario Elimination of Outliers",
  author   = "Moisan, Lionel and Moulon, Pierre and Monasse, Pascal",
  abstract = "The RANSAC algorithm (RANdom SAmple Consensus) is a robust method
              to\textbackslashr\textbackslashnestimate parameters of a model
              fitting the data, in presence
              of\textbackslashr\textbackslashnoutliers among the data. Its
              random nature is due only to
              complexity\textbackslashr\textbackslashnconsiderations. It
              iteratively extracts a random sample out of
              all\textbackslashr\textbackslashndata, of minimal size sufficient
              to estimate the parameters. At
              each\textbackslashr\textbackslashnsuch trial, the number of
              inliers (data that fits the model within
              an\textbackslashr\textbackslashnacceptable error threshold) is
              counted. In the end, the set
              of\textbackslashr\textbackslashnparameters maximizing the number
              of inliers is
              accepted.\textbackslashr\textbackslashn\textbackslashr\textbackslashnThe
              variant proposed by Moisan and Stival consists in introducing an
              a\textbackslashr\textbackslashncontrario criterion to avoid the
              hard thresholds for
              inlier/outlier\textbackslashr\textbackslashndiscrimination. It
              has three consequences: The threshold
              for\textbackslashr\textbackslashninlier/outlier discrimination is
              adaptive, it does not need to
              be\textbackslashr\textbackslashnfixed. It gives a decision on the
              adequacy of the final model: it does
              not\textbackslashr\textbackslashnprovide a wrong set of
              parameters if it does not have
              enough\textbackslashr\textbackslashnconfidence. The procedure to
              draw a new sample can be amended as
              soon\textbackslashr\textbackslashnas one set of parameters is
              deemed meaningful: the new sample can
              be\textbackslashr\textbackslashndrawn among the inliers of this
              model.\textbackslashr\textbackslashn\textbackslashr\textbackslashnIn
              this particular instantiation, we apply it to the estimation of
              the\textbackslashr\textbackslashnhomography registering two
              images of the same scene. The homography
              is\textbackslashr\textbackslashnan 8-parameter model arising in
              two situations when using a
              pinhole\textbackslashr\textbackslashncamera: the scene is planar
              (a painting, a facade, etc.) or
              the\textbackslashr\textbackslashnviewpoint location is fixed
              (pure rotation around the
              optical\textbackslashr\textbackslashncenter). When the homography
              is found, it is used to stitch the
              images\textbackslashr\textbackslashnin the coordinate frame of
              the second image and build a panorama.
              The\textbackslashr\textbackslashnpoint correspondences between
              images are computed by the
              SIFT\textbackslashr\textbackslashnalgorithm.",
  journal  = "Image Processing On Line",
  volume   =  2,
  pages    = "56--73",
  year     =  2012
}

@ARTICLE{Moisan2016-mh,
  title    = "Fundamental Matrix of a Stereo Pair, with A Contrario Elimination
              of Outliers",
  author   = "Moisan, Lionel and Moulon, Pierre and Monasse, Pascal",
  abstract = "In a stereo image pair, the fundamental matrix encodes the
              rigidity constraint of the scene. It combines the internal
              parameters of both cameras (which can be the same) and their
              relative position and orientation. It associates to image points
              in one view the so-called epipolar line in the other view, which
              is the locus of projection of the same 3D point, whose particular
              position on the straight line is determined by its depth.
              Reducing the correspondence search to a 1D line instead of the 2D
              image is a large benefit enabling the computation of the dense 3D
              scene. The estimation of the matrix depends on at least seven
              pairs of corresponding points in the images. The
              algorithmdiscarding outliers presented here is a variant of the
              classical RANSAC (RANdom SAmple Consensus) based on a contrario
              methodology and proposed first by Moisan and Stival in 2004 under
              the name ORSA. The distinguishing feature of this algorithm
              compared to other RANSAC variants is that the measure of validity
              of a set of point pairs is not its sheer number, but a
              combination of this number and the geometric precision of the
              points.",
  journal  = "Image Processing On Line",
  volume   =  5,
  pages    = "89--113",
  year     =  2016
}

@ARTICLE{Moulon2013-mt,
  title    = "Global fusion of relative motions for robust, accurate and
              scalable structure from motion",
  author   = "Moulon, Pierre and Monasse, Pascal and Marlet, Renaud",
  abstract = "Multi-view structure from motion (SfM) estimates the position and
              \textbackslashnorientation of pictures in a common 3D coordinate
              frame. When views are treated \textbackslashnincrementally, this
              external calibration can be subject to drift, contrary to
              \textbackslashnglobal methods that distribute residual errors
              evenly. We propose a new global \textbackslashncalibration
              approach based on the fusion of relative motions between image
              \textbackslashnpairs. We improve an existing method for robustly
              computing global rotations. We \textbackslashnpresent an
              efficient a contrario trifocal tensor estimation method, from
              which \textbackslashnstable and precise translation directions
              can be extracted. We also define an \textbackslashnefficient
              translation registration method that recovers accurate camera
              \textbackslashnpositions. These components are combined into an
              original SfM pipeline. Our \textbackslashnexperiments show that,
              on most datasets, it outperforms in accuracy other
              \textbackslashnexisting incremental and global pipelines. It also
              achieves strikingly good \textbackslashnrunning times: it is
              about 20 times faster than the other global method we could
              \textbackslashncompare to, and as fast as the best incremental
              method. More importantly, it \textbackslashnfeatures better
              scalability properties.",
  journal  = "Proc. IEEE Int. Conf. Comput. Vis.",
  pages    = "3248--3255",
  year     =  2013,
  keywords = "Calibration; Structure-from-Motion; robust estimation"
}

@ARTICLE{Mur-Artal2015-nl,
  title    = "{ORB-SLAM}: A Versatile and Accurate Monocular {SLAM} System",
  author   = "Mur-Artal, Raul and Montiel, J M M and Tardos, Juan D",
  abstract = "This paper presents ORB-SLAM, a feature-based monocular
              simultaneous localization and mapping (SLAM) system that operates
              in real time, in small and large indoor and outdoor environments.
              The system is robust to severe motion clutter, allows wide
              baseline loop closing and relocalization, and includes full
              automatic initialization. Building on excellent algorithms of
              recent years, we designed from scratch a novel system that uses
              the same features for all SLAM tasks: tracking, mapping,
              relocalization, and loop closing. A survival of the fittest
              strategy that selects the points and keyframes of the
              reconstruction leads to excellent robustness and generates a
              compact and trackable map that only grows if the scene content
              changes, allowing lifelong operation. We present an exhaustive
              evaluation in 27 sequences from the most popular datasets.
              ORB-SLAM achieves unprecedented performance with respect to other
              state-of-the-art monocular SLAM approaches. For the benefit of
              the community, we make the source code public.",
  journal  = "IEEE Trans. Rob.",
  volume   =  31,
  number   =  5,
  pages    = "1147--1163",
  year     =  2015,
  keywords = "Lifelong mapping; Simultaneous localization and mapping (SLAM);
              localization; monocular vision; recognition"
}

@ARTICLE{Mur-Artal2017-ur,
  title    = "{ORB-SLAM2}: An {Open-Source} {SLAM} System for Monocular,
              Stereo, and {RGB-D} Cameras",
  author   = "Mur-Artal, Raul and Tardos, Juan D",
  abstract = "We present ORB-SLAM2, a complete simultaneous localization and
              mapping (SLAM) system for monocular, stereo and RGB-D cameras,
              including map reuse, loop closing, and relocalization
              capabilities. The system works in real time on standard central
              processing units in a wide variety of environments from small
              hand-held indoors sequences, to drones flying in industrial
              environments and cars driving around a city. Our back-end, based
              on bundle adjustment with monocular and stereo observations,
              allows for accurate trajectory estimation with metric scale. Our
              system includes a lightweight localization mode that leverages
              visual odometry tracks for unmapped regions and matches with map
              points that allow for zero-drift localization. The evaluation on
              29 popular public sequences shows that our method achieves
              state-of-the-art accuracy, being in most cases the most accurate
              SLAM solution. We publish the source code, not only for the
              benefit of the SLAM community, but with the aim of being an
              out-of-the-box SLAM solution for researchers in other fields.",
  journal  = "IEEE Trans. Rob.",
  volume   =  33,
  number   =  5,
  pages    = "1255--1262",
  year     =  2017,
  keywords = "Localization; RGB-D; mapping; simultaneous localization and
              mapping (SLAM); stereo"
}

@INPROCEEDINGS{Newell2016-ty,
  title    = "Stacked Hourglass Networks for Human Pose Estimation",
  author   = "Newell, Alejandro and Yang, Kaiyu and Deng, Jia",
  abstract = "This work introduces a novel Convolutional Network architecture
              for the task of human pose estimation. Features are processed
              across all scales and consolidated to best capture the various
              spatial relationships associated with the body. We show how
              repeated bottom-up, top-down processing used in conjunction with
              intermediate supervision is critical to improving the performance
              of the network. We refer to the architecture as a 'stacked
              hourglass' network based on the successive steps of pooling and
              upsampling that are done to produce a final set of estimates.
              State-of-the-art results are achieved on the FLIC and MPII
              benchmarks outcompeting all recent methods.",
  pages    = "483--499",
  year     =  2016,
  keywords = "1; estimation consists of two; fig; human pose estimation; our
              network for pose; stacked hourglass models; top-down inference;
              which allow repeated bottom-up"
}

@ARTICLE{Nguyen2018-ad,
  title    = "Unsupervised Deep Homography: A Fast and Robust Homography
              Estimation Model",
  author   = "Nguyen, Ty and Chen, Steven W and Shivakumar, Shreyas S and
              Taylor, Camillo Jose and Kumar, Vijay",
  abstract = "Homography estimation between multiple aerial images can provide
              relative pose estimation for collaborative autonomous exploration
              and monitoring. The usage on a robotic system requires a fast and
              robust homography estimation algorithm. In this letter, we
              propose an unsupervised learning algorithm that trains a deep
              convolutional neural network to estimate planar homographies. We
              compare the proposed algorithm to traditional-feature-based and
              direct methods, as well as a corresponding supervised learning
              algorithm. Our empirical results demonstrate that compared to
              traditional approaches, the unsupervised algorithm achieves
              faster inference speed, while maintaining comparable or better
              accuracy and robustness to illumination variation. In addition,
              our unsupervised method has superior adaptability and performance
              compared to the corresponding supervised deep learning method.
              Our image dataset and a Tensorflow implementation of our work are
              available at
              https://github.com/tynguyen/unsupervisedDeepHomographyRAL2018.",
  journal  = "IEEE Robotics and Automation Letters",
  volume   =  3,
  number   =  3,
  pages    = "2346--2353",
  year     =  2018,
  keywords = "Computer vision for automation; computer vision for other robotic
              applications; deep homography; deep learning in robotics and
              automation; image alignment"
}

@ARTICLE{Nikiforov2017-ah,
  title    = "Merging the {A-} and Q-spectral theories",
  author   = "Nikiforov, V",
  abstract = "Let G be a graph with adjacency matrix A(G), and let D(G) be the
              diagonal matrix of the degrees of G: The signless Laplacian Q(G)
              of G is defined as Q(G) := A(G) +D(G). Cvetkovi{\'c} called the
              study of the adjacency matrix the A-spectral theory, and the
              study of the signless Laplacian-the Q-spectral theory. To track
              the gradual change of A(G) into Q(G), in this paper it is
              suggested to study the convex linear combinations A$\alpha$ (G)
              of A(G) and D(G) defined by A$\alpha$ (G) := $\alpha$D(G) + (1 -
              $\alpha$)A(G) , 0 $\leq$ $\alpha$ $\leq$ 1: This study sheds new
              light on A(G) and Q(G), and yields, in particular, a novel
              spectral Tur{\'a}n theorem. A number of open problems are
              discussed.",
  journal  = "Appl. Anal. Discrete Math.",
  volume   =  11,
  number   =  1,
  pages    = "81--107",
  year     =  2017,
  keywords = "Adjacency matrix; Signless Laplacian; Spectral Tur{\'a}n theorem;
              Spectral extremal problems; Spectral radius"
}

@ARTICLE{Nister2006-fj,
  title    = "Four points in two or three calibrated views: Theory and practice",
  author   = "Nist{\'e}r, David and Schaffalitzky, Frederik",
  abstract = "Suppose two perspective views of four world points are given and
              that the intrinsic parameters are known but the camera poses and
              the world point positions are not. We prove that the epipole in
              each view is then constrained to lie on a curve of degree ten. We
              derive the equation for the curve and establish many of the
              curve's properties. For example, we show that the curve has four
              branches through each of the image points and that it has four
              additional points on each conic of the pencil of conics through
              the four image points. We show how to compute the four curve
              points on each conic in closed form. We show that orientation
              constraints allow only parts of the curve and find that there are
              impossible configurations of four corresponding point pairs. We
              give a novel algorithm that solves for the essential matrix given
              three corresponding points and one of the epipoles. We then use
              the theory to create the most efficient solution yet to the
              notoriously difficult problem of solving for the pose of three
              views given four corresponding points. The solution is a search
              over a one-dimensional parameter domain, where each point in the
              search can be evaluated in closed form. The intended use for the
              solution is in a hypothesise-and-test architecture to solve for
              structure and motion. ABSTRACT FROM AUTHOR Copyright of
              International Journal of Computer Vision is the property of
              Springer Science \& Business Media B.V. and its content may not
              be copied or emailed to multiple sites or posted to a listserv
              without the copyright holder's express written permission.
              However, users may print, download, or email articles for
              individual use. This abstract may be abridged. No warranty is
              given about the accuracy of the copy. Users should refer to the
              original published version of the material for the full abstract.
              (Copyright applies to all Abstracts); Suppose two perspective
              views of four world points are given and that the intrinsic
              parameters are known but the camera poses and the world point
              positions are not. We prove that the epipole in each view is then
              constrained to lie on a curve of degree ten. We derive the
              equation for the curve and establish many of the curve's
              properties. For example, we show that the curve has four branches
              through each of the image points and that it has four additional
              points on each conic of the pencil of conics through the four
              image points. We show how to compute the four curve points on
              each conic in closed form. We show that orientation constraints
              allow only parts of the curve and find that there are impossible
              configurations of four corresponding point pairs. We give a novel
              algorithm that solves for the essential matrix given three
              corresponding points and one of the epipoles. We then use the
              theory to create the most efficient solution yet to the
              notoriously difficult problem of solving for the pose of three
              views given four corresponding points. The solution is a search
              over a one-dimensional parameter domain, where each point in the
              search can be evaluated in closed form. The intended use for the
              solution is in a hypothesise-and-test architecture to solve for
              structure and motion. ABSTRACT FROM AUTHOR Copyright of
              International Journal of Computer Vision is the property of
              Springer Science \& Business Media B.V. and its content may not
              be copied or emailed to multiple sites or posted to a listserv
              without the copyright holder's express written permission.
              However, users may print, download, or email articles for
              individual use. This abstract may be abridged. No warranty is
              given about the accuracy of the copy. Users should refer to the
              original published version of the material for the full abstract.
              (Copyright applies to all Abstracts)",
  journal  = "Int. J. Comput. Vis.",
  volume   =  67,
  number   =  2,
  pages    = "211--231",
  year     =  2006,
  keywords = "Algebraic curves; Kruppa constraint; Minimal methods; Multi-view
              geometry; RANSAC; Structure-from-motion"
}

@ARTICLE{Nordberg2011-dc,
  title    = "The key to three-view geometry",
  author   = "Nordberg, Klas",
  abstract = "In this article we describe a set of canonical transformations of
              the image spaces that make the description of three-view geometry
              very simple. The transformations depend on the three-view
              geometry and the canonically transformed trifocal tensor T' takes
              the form of a sparse array where 17 elements in well-defined
              positions are zero, it has a linear relation to the camera
              matrices and to two of the fundamental matrices, a third order
              relation to the third fundamental matrix, a second order relation
              to the other two trifocal tensors, and first order relations to
              the 10 three-view all-point matching constraints. In this
              canonical form, it is also simple to determine if the
              corresponding camera configuration is degenerate or co-linear. An
              important property of the three canonical transformations of the
              images spaces is that they are in SO(3). The 9 parameters needed
              to determine these transformations and the 9 parameters that
              determine the elements of T' together provide a minimal
              parameterization of the tensor. It does not have problems with
              multiple maps or multiple solutions that other parameterizations
              have, and is therefore simple to use. It also provides an
              implicit representation of the trifocal internal constraints: the
              sparse canonical representation of the trifocal tensor can be
              determined if and only if it is consistent with its internal
              constraints. In the non-ideal case, the canonical transformation
              can be determined by solving a minimization problem and a simple
              algorithm for determining the solution is provided. This allows
              us to extend the standard linear method for estimation of the
              trifocal tensor to include a constraint enforcement as a final
              step, similar to the constraint enforcement of the fundamental
              matrix. Experimental evaluation of this extended linear
              estimation method shows that it significantly reduces the
              geometric error of the resulting tensor, but on average the
              algebraic estimation method is even better. For a small
              percentage of cases, however, the extended linear method gives a
              smaller geometric error, implying that it can be used as a
              complement to the algebraic method for these cases. \copyright{}
              2011 Springer Science+Business Media, LLC.",
  journal  = "Int. J. Comput. Vis.",
  volume   =  94,
  number   =  3,
  pages    = "282--294",
  year     =  2011,
  keywords = "Internal constraints; Minimal parameterization; Three-view
              geometry; Trifocal tensor; Trifocal tensor estimation"
}

@ARTICLE{Oeding2017-ly,
  title     = "The quadrifocal variety",
  author    = "Oeding, Luke",
  abstract  = "Multi-view Geometry is reviewed from an Algebraic Geometry
               perspective and multi-focal tensors are constructed as
               equivariant projections of the Grassmannian. A connection to the
               principal minor assignment problem is made by considering
               several flatlander cameras. The ideal of the quadrifocal variety
               is computed up to degree 8 (and partially in degree 9) using the
               representations of GL(3)$\times$4 in the polynomial ring on the
               space of 3$\times$3$\times$3$\times$3 tensors. Further
               representation-theoretic analysis gives a lower bound for the
               number of minimal generators.",
  journal   = "Linear Algebra Appl.",
  publisher = "Elsevier Inc.",
  volume    =  512,
  pages     = "306--330",
  year      =  2017,
  keywords  = "Computer vision; Quadrifocal tensors; Representation theory"
}

@ARTICLE{Ohlsson2013-kv,
  title    = "Nonlinear basis pursuit",
  author   = "Ohlsson, Henrik and Yang, Allen Y and Dong, Roy and Sastry, S
              Shankar",
  abstract = "In compressive sensing, the basis pursuit algorithm aims to find
              the sparsest solution to an underdetermined linear equation
              system. In this paper, we generalize basis pursuit to finding the
              sparsest solution to higher order nonlinear systems of equations,
              called nonlinear basis pursuit. In contrast to the existing
              nonlinear compressive sensing methods, the new algorithm is based
              on convex relaxation and is not a greedy method. The novel
              algorithm enables the compressive sensing approach to be used for
              a broader range of applications where there are nonlinear
              relationships between the measurements and the unknowns.
              \copyright{} 2013 IEEE.",
  journal  = "Conference Record - Asilomar Conference on Signals, Systems and
              Computers",
  pages    = "315--319",
  year     =  2013
}

@ARTICLE{Olsson2011-vo,
  title    = "Stable structure from motion for unordered image collections",
  author   = "Olsson, Carl and Enqvist, Olof",
  abstract = "We present a non-incremental approach to structure from motion.
              Our solution is based on robustly computing global rotations from
              relative geometries and feeding these into the known-rotation
              framework to create an initial solution for bundle adjustment. To
              increase robustness we present a new method for constructing
              reliable point tracks from pairwise matches. We show that our
              method can be seen as maximizing the reliability of a point track
              if the quality of the weakest link in the track is used to
              evaluate reliability. To estimate the final geometry we alternate
              between bundle adjustment and a robust version of the
              known-rotation formulation. The ability to compute both structure
              and camera translations independent of initialization makes our
              algorithm insensitive to degenerate epipolar geometries. We
              demonstrate the performance of our system on a number of image
              collections. \copyright{} 2011 Springer-Verlag.",
  journal  = "Lect. Notes Comput. Sci.",
  volume   = "6688 LNCS",
  pages    = "524--535",
  year     =  2011
}

@ARTICLE{Oskarsson2004-od,
  title    = "Minimal projective reconstruction for combinations of points and
              lines in three views",
  author   = "Oskarsson, Magnus and Zisserman, Andrew and {\AA}str{\"o}m, Kalle",
  abstract = "In this article we address the problem of projective
              reconstruction of structure and motion given only image data. In
              particular we investigate three novel minimal combinations of
              points and lines over three views, and give complete solutions
              and reconstruction methods for two of these cases: 'four points
              and three lines in three views', and 'two points and six lines in
              three views'. We show that in general there are three and seven
              solutions, respectively, to these cases. The reconstruction
              methods are tested on real and simulated data. We also give
              tentative results for the case of nine lines in correspondence
              over three views, where experiments indicate that there may be up
              to 36 complex solutions. \copyright{} 2004 Elsevier B.V. All
              rights reserved.",
  journal  = "Image Vis. Comput.",
  volume   =  22,
  number   = "10 SPEC. ISS.",
  pages    = "777--785",
  year     =  2004,
  keywords = "Conics; Points and lines; Projective reconstruction"
}

@ARTICLE{Papadopoulo1998-va,
  title    = "A new characterization of the trifocal tensor",
  author   = "Papadopoulo, Th{\'e}odore and Faugeras, Olivier",
  abstract = "This paper deals with the problem of characterizing and
              parametrizing the manifold of trifocal tensors that describe the
              geometry of three views like the fundamental matrix characterizes
              the geometry of two. The paper contains two new results. First a
              new, simpler, set of algebraic constraints that characterizes the
              set of trifocal tensors is presented. Second, we give a new
              parametrization of the trifocal tensor based upon those
              constraints which is also simpler than previously known
              parametrizations. Some preliminary experimental results of the
              use of these constraints and parametrization to estimate the
              trifocal tensor from image correspondences are presented.",
  journal  = "Lect. Notes Comput. Sci.",
  volume   =  1406,
  number   = "March 2014",
  pages    = "109--123",
  year     =  1998
}

@ARTICLE{Pritchett1998-cn,
  title   = "Matching and reconstruction from widely separated views",
  author  = "Pritchett, Philip and Zisserman, Andrew",
  journal = "Lect. Notes Comput. Sci.",
  volume  =  1506,
  pages   = "78--92",
  year    =  1998
}

@INBOOK{Qu2014-tn,
  title  = "Finding a sparse vector in a subspace: Linear sparsity using
            alternating directions",
  author = "Qu, Qing and Sun, Ju and Wright, John",
  pages  = "3401--3409",
  year   =  2014
}

@ARTICLE{Quan1995-wa,
  title    = "Invariants of Six Points and Projective Reconstruction From Three
              Uncalibrated Images",
  author   = "Quan, Long",
  abstract = "There are three projective invariants of a set of six points in
              general position in space. It is well known that these invariants
              cannot be recovered from one image, however an invariant
              relationship does exist between space invariants and image
              invariants. This invariant relationship is first derived for a
              single image. Then this invariant relationship is used to derive
              the space invariants, when multiple images are available. This
              paper establishes that the minimum number of images for computing
              these invariants is three, and the computation of invariants of
              six points from three images can have as many as three solutions.
              Algorithms are presented for computing these invariants in closed
              form. The accuracy and stability with respect to image noise,
              selection of the triplets of images and distance between viewing
              positions are studied both through real and simulated images.
              Applications of these invariants are also presented. Both the
              results of Faugeras [1] and Hartley et al. [2] for projective
              reconstruction and Sturm's method [3] for epipolar geometry
              determination from two uncalibrated images with at least seven
              points are extended to the case of three uncalibrated images with
              only six points. \copyright{} 1995 IEEE",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  17,
  number   =  1,
  pages    = "34--46",
  year     =  1995,
  keywords = "This invariant; epipolar geometry; projective geometry;
              projective reconstruction; self-calibration; uncalibrated images"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Quan2006-yw,
  title    = "Some results on minimal euclidean reconstruction from four points",
  author   = "Quan, Long and Triggs, Bill and Mourrain, Bernard",
  abstract = "Methods for reconstruction and camera estimation from miminal
              data\textbackslashnare oftenused to boot-strap robust (RANSAC and
              LMS) and optimal (bundle\textbackslashnadjustment)structure and
              motion estimates. Minimal methods are known\textbackslashnfor
              projectivereconstruction from two or more uncalibrated
              images,\textbackslashnand for �5 point�relative orientation and
              Euclidean reconstruction\textbackslashnfrom two calibrated
              parameters,but we know of no efficient
              minimal\textbackslashnmethod for three or more calibratedcameras
              except the uniqueness\textbackslashnproof by Holt and Netravali.
              We reformulatethe problem of
              Euclidean\textbackslashnreconstruction from minimal data of four
              pointsin three or more calibrated\textbackslashnimages, and
              develop a random rational simulationmethod to show
              some\textbackslashnnew results on this problem. In addition to an
              alternativeproof of\textbackslashnthe uniqueness of the solutions
              in general cases, we further showthat\textbackslashnunknown
              coplanar configurations are not singular, but the true
              solutionis\textbackslashna double root. The solution from a known
              coplanar configuration is\textbackslashnalsogenerally unique.
              Some especially symmetric point-camera
              configurationslead\textbackslashnto multiple solutions, but only
              symmetry of points or the camerasgives\textbackslashna unique
              solution.",
  journal  = "J. Math. Imaging Vis.",
  volume   =  24,
  number   =  3,
  pages    = "341--348",
  year     =  2006,
  keywords = "3D reconstruction; Algebraic geometry; Polynomial methods;
              Relative orientation; Structure from motion"
}

@ARTICLE{Raguram2013-mj,
  title    = "{USAC}: A universal framework for random sample consensus",
  author   = "Raguram, Rahul and Chum, Ondrej and Pollefeys, Marc and Matas,
              Jiri and Frahm, Jan Michael",
  abstract = "A computational problem that arises frequently in computer vision
              is that of estimating the parameters of a model from data that
              have been contaminated by noise and outliers. More generally, any
              practical system that seeks to estimate quantities from noisy
              data measurements must have at its core some means of dealing
              with data contamination. The random sample consensus (RANSAC)
              algorithm is one of the most popular tools for robust estimation.
              Recent years have seen an explosion of activity in this area,
              leading to the development of a number of techniques that improve
              upon the efficiency and robustness of the basic RANSAC algorithm.
              In this paper, we present a comprehensive overview of recent
              research in RANSAC-based robust estimation by analyzing and
              comparing various approaches that have been explored over the
              years. We provide a common context for this analysis by
              introducing a new framework for robust estimation, which we call
              Universal RANSAC (USAC). USAC extends the simple
              hypothesize-and-verify structure of standard RANSAC to
              incorporate a number of important practical and computational
              considerations. In addition, we provide a general-purpose C++
              software library that implements the USAC framework by leveraging
              state-of-the-art algorithms for the various modules. This
              implementation thus addresses many of the limitations of standard
              RANSAC within a single unified package. We benchmark the
              performance of the algorithm on a large collection of estimation
              problems. The implementation we provide can be used by
              researchers either as a stand-alone tool for robust estimation or
              as a benchmark for evaluating new techniques.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  35,
  number   =  8,
  pages    = "2022--2038",
  year     =  2013,
  keywords = "RANSAC; robust estimation"
}

@ARTICLE{Raguram2008-fx,
  title    = "A comparative analysis of {RANSAC} techniques leading to adaptive
              real-time random sample consensus",
  author   = "Raguram, Rahul and Frahm, Jan Michael and Pollefeys, Marc",
  abstract = "The Random Sample Consensus (RANSAC) algorithm is a popular tool
              for robust estimation problems in computer vision, primarily due
              to its ability to tolerate a tremendous fraction of outliers.
              There have been a number of recent efforts that aim to increase
              the efficiency of the standard RANSAC algorithm. Relatively fewer
              efforts, however, have been directed towards formulating RANSAC
              in a manner that is suitable for real-time implementation. The
              contributions of this work are two-fold: First, we provide a
              comparative analysis of the state-of-the-art RANSAC algorithms
              and categorize the various approaches. Second, we develop a
              powerful new framework for real-time robust estimation. The
              technique we develop is capable of efficiently adapting to the
              constraints presented by a fixed time budget, while at the same
              time providing accurate estimation over a wide range of inlier
              ratios. The method shows significant improvements in accuracy and
              speed over existing techniques. \copyright{} 2008 Springer Berlin
              Heidelberg.",
  journal  = "Lect. Notes Comput. Sci.",
  volume   = "5303 LNCS",
  number   = "PART 2",
  pages    = "500--513",
  year     =  2008
}

@INBOOK{Ren2015-nn,
  title     = "Faster {R-CNN}: Towards {Real-Time} Object Detection with Region
               Proposal Networks",
  author    = "Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian",
  editor    = "Cortes, C and Lawrence, N D and Lee, D D and Sugiyama, M and
               Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "91--99",
  year      =  2015
}

@ARTICLE{Ros2017-yz,
  title    = "Motion Estimation via Robust Decomposition With Constrained Rank",
  author   = "Ros, German and Alvarez, Jose M and Guerrero, Julio",
  abstract = "In this work, we address the problem of outlier detection for
              robust motion estimation by using modern sparse-low-rank
              decompositions, i.e., Robust PCA-like methods, to impose global
              rank constraints. Robust decompositions have shown to be good at
              splitting a corrupted matrix into an uncorrupted low-rank matrix
              and a sparse matrix, containing outliers. However, this process
              only works when matrices have relatively low rank with respect to
              their ambient space, a property not met in motion estimation
              problems. As a solution, we propose to exploit the partial
              information present in the decomposition to decide which matches
              are outliers. We provide evidences showing that even when it is
              not possible to recover an uncorrupted low-rank matrix, the
              resulting information can be exploited for outlier detection. To
              this end we propose the Robust Decomposition with Constrained
              Rank (RD-CR), a proximal gradient based method that enforces the
              rank constraints inherent to motion estimation. We also present a
              general framework to perform robust estimation for stereo Visual
              Odometry, based on our RD-CR and a simple but effective
              compressed optimization method that achieves high performance.
              Our evaluation on synthetic data and on the KITTI dataset
              demonstrates the applicability of our approach in complex
              scenarios and it yields state-of-the-art performance.",
  journal  = "IEEE Transactions on Intelligent Vehicles",
  volume   =  1,
  number   =  4,
  pages    = "346--357",
  year     =  2017
}

@ARTICLE{Rousseeuw1984-ao,
  title     = "Least median of squares regression",
  author    = "Rousseeuw, Peter J",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  79,
  number    =  388,
  pages     = "871--880",
  year      =  1984
}

@ARTICLE{Sarkar2015-jd,
  title    = "Role of normalization in spectral clustering for stochastic
              blockmodels",
  author   = "Sarkar, Purnamrita and Bickel, Peter J",
  abstract = "Spectral Clustering clusters elements using the top few
              eigenvectors of their (possibly normalized) similarity matrix.
              The quality of Spectral Clustering is closely tied to the
              convergence properties of these principal eigenvectors. This rate
              of convergence has been shown to be identical for both the
              normalized and unnormalized variants ([17]). However
              normalization for Spectral Clustering is the common practice
              ([16], [19]). Indeed, our experiments also show that
              normalization improves prediction accuracy. In this paper, for
              the popular Stochastic Blockmodel, we theoretically show that
              under spectral embedding, normalization shrinks the variance of
              points in a class by a constant fraction. As a byproduct of our
              work, we also obtain sharp deviation bounds of empirical
              principal eigenvalues of graphs generated from a Stochastic
              Blockmodel.",
  journal  = "Ann. Stat.",
  volume   =  43,
  number   =  3,
  pages    = "962--990",
  year     =  2015,
  keywords = "clustering"
}

@ARTICLE{Schiebinger2015-yv,
  title    = "The geometry of kernelized spectral clustering",
  author   = "Schiebinger, Geoffrey and Wainwright, Martin J and Yu, Bin",
  abstract = "Clustering of data sets is a standard problem in many areas of
              science and engineering. The method of spectral clustering is
              based on embedding the data set using a kernel function, and
              using the top eigenvectors of the normalized Laplacian to recover
              the connected components. We study the performance of spectral
              clustering in recovering the latent labels of i.i.d. samples from
              a finite mixture of nonparametric distributions. The difficulty
              of this label recovery problem depends on the overlap between
              mixture components and how easily a mixture component is divided
              into two nonoverlapping components. When the overlap is small
              compared to the indivisibility of the mixture components, the
              principal eigenspace of the population-level normalized Laplacian
              operator is approximately spanned by the square-root kernelized
              component densities. In the finite sample setting, and under the
              same assumption, embedded samples from different components are
              approximately orthogonal with high probability when the sample
              size is large. As a corollary we control the fraction of samples
              mislabeled by spectral clustering under finite mixtures with
              nonparametric components.",
  journal  = "Ann. Stat.",
  volume   =  43,
  number   =  2,
  pages    = "819--846",
  year     =  2015,
  keywords = "Kernel function; Mixture model; Normalized Laplacian; Spectral
              clustering"
}

@ARTICLE{Schweighofer2006-vj,
  title    = "Robust pose estimation from a planar target",
  author   = "Schweighofer, Gerald and Pinz, Axel",
  abstract = "In theory, the pose of a calibrated camera can be uniquely
              determined from a minimum of four coplanar but noncollinear
              points. In practice, there are many applications of camera pose
              tracking from planar targets and there is also a number of recent
              pose estimation algorithms which perform this task in real-time,
              but all of these algorithms suffer from pose ambiguities. This
              paper investigates the pose ambiguity for planar targets viewed
              by a perspective camera. We show that pose ambiguities - two
              distinct local minima of the according error function - exist
              even for cases with wide angle lenses and close range targets. We
              give a comprehensive interpretation of the two minima and derive
              an analytical solution that locates the second minimum. Based on
              this solution, we develop a new algorithm for unique and robust
              pose estimation from a planar target. In the experimental
              evaluation, this algorithm outperforms four state-of-the-art pose
              estimation algorithms. \copyright{} 2006 IEEE.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  28,
  number   =  12,
  pages    = "2024--2030",
  year     =  2006,
  keywords = "Camera pose ambiguity; Pose tracking"
}

@ARTICLE{Schonemann1970-ns,
  title    = "Fitting one matrix to another under choice of a central dilation
              and a rigid motion",
  author   = "Sch{\"o}nemann, Peter H and Carroll, Robert M",
  abstract = "A least squares method is presented for fitting a given matrix A
              to another given matrix B under choice of an unknown rotation, an
              unknown translation, and an unknown central dilation. The
              procedure may be useful to investigators who wish to compare
              results obtained with nonmetric scaling techniques across samples
              or who wish to compare such results with those obtained by
              conventional factor analytic techniques on the same sample.
              \copyright{} 1970 Psychometric Society.",
  journal  = "Psychometrika",
  volume   =  35,
  number   =  2,
  pages    = "245--255",
  year     =  1970
}

@ARTICLE{Seo2004-ic,
  title    = "Two quantitative measures of inlier distributions for precise
              fundamental matrix estimation",
  author   = "Seo, Jung Kak and Hong, Hyun Ki and Jho, Cheung Woon and Choi,
              Min Hyung",
  abstract = "Because the estimation of a fundamental matrix is much dependent
              on the correspondence, it is important to select a proper inlier
              set that represents variation of the image due to camera motion.
              Previous studies showed that a more precise fundamental matrix
              can be obtained if the evenly distributed points are selected.
              When the inliers are detected, however, no previous methods have
              taken into account their distribution. This paper presents two
              novel approaches to estimate the fundamental matrix by
              considering the inlier distribution. The proposed algorithms
              divide an entire image into several sub-regions, and then examine
              the number of the inliers in each sub-region and the area of each
              region. In our method, the standard deviations are used as
              quantitative measures to select a proper inlier set. The
              simulation results on synthetic and real images show that our
              consideration of the inlier distribution can achieve a more
              precise estimation of the fundamental matrix. \copyright{} 2004
              Elsevier B.V. All rights reserved.",
  journal  = "Pattern Recognit. Lett.",
  volume   =  25,
  number   =  6,
  pages    = "733--741",
  year     =  2004,
  keywords = "Correspondence; Epipolar geometry; Fundamental matrix; Inlier
              set; Stereo vision"
}

@ARTICLE{Sermanet2013-og,
  title   = "Overfeat: Integrated recognition, localization and detection using
             convolutional networks",
  author  = "Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu,
             Micha{\"e}l and Fergus, Rob and LeCun, Yann",
  journal = "arXiv preprint arXiv:1312.6229",
  year    =  2013
}

@ARTICLE{Shelhamer2017-ag,
  title    = "Fully Convolutional Networks for Semantic Segmentation",
  author   = "Shelhamer, Evan and Long, Jonathan and Darrell, Trevor",
  abstract = "Convolutional networks are powerful visual models that yield
              hierarchies of features. We show that convolutional networks by
              themselves, trained end-to-end, pixels-to-pixels, exceed the
              state-of-the-art in semantic segmentation. Our key insight is to
              build ``fully convolutional'' networks that take input of
              arbitrary size and produce correspondingly-sized output with
              efficient inference and learning. We define and detail the space
              of fully convolutional networks, explain their application to
              spatially dense prediction tasks, and draw connections to prior
              models. We adapt contemporary classification networks (AlexNet,
              the VGG net, and GoogLeNet) into fully convolutional networks and
              transfer their learned representations by fine-tuning to the
              segmentation task. We then define a novel architecture that
              combines semantic information from a deep, coarse layer with
              appearance information from a shallow, fine layer to produce
              accurate and detailed segmentations. Our fully convolutional
              network achieves state-of-the-art segmentation of PASCAL VOC
              (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2,
              and SIFT Flow, while inference takes one third of a second for a
              typical image.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  39,
  number   =  4,
  pages    = "640--651",
  year     =  2017,
  keywords = "Convolutional Networks; Deep Learning; Semantic Segmentation;
              Transfer Learning"
}

@ARTICLE{Sim2006-vk,
  title    = "Recovering camera motion using L-infinity minimization",
  author   = "Sim, Kristy and Hartley, Richard",
  abstract = "Recently, there has been interest in formulating various
              geometric problems in Computer Vision as L\textbackslashinfty
              optimization problems. The advantage of this approach is that
              under L\textbackslashinfty norm, such problems typically have a
              single minimum, and may be efficiently solved using Second-Order
              Cone Programming (SOCP). This paper shows that such techniques
              may be used effectively on the problem of determining the track
              of a camera given observations of features in the environment.
              The approach to this problem involves two steps: determination of
              the orientation of the camera by estimation of relative
              orientation between pairs of views, followed by determination of
              the translation of the camera. This paper focusses on the second
              step, that of determining the motion of the camera. It is shown
              that it may be solved effectively by using SOCP to reconcile
              translation estimates obtained for pairs or triples of views. In
              addition, it is observed that the individual translation
              estimates are not known with equal certainty in all directions.
              To account for this anisotropy in uncertainty, we introduce the
              use of covariances into the L\textbackslashinfty optimization
              framework.",
  journal  = "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.",
  volume   =  1,
  pages    = "1230--1237",
  year     =  2006
}

@INPROCEEDINGS{Simon2000-ts,
  title    = "Markerless tracking using planar structures in the scene",
  author   = "Simon, Gilles and Fitzgibbon, Andrew W and Zisserman, Andrew",
  abstract = "We describe a markerless camera tracking system for augmented
              reality that operates in environments which contain one or more
              planes. This is a common special case, which we show
              significantly simplifies tracking. The result is a practical,
              reliable, vision-based tracker. Furthermore, the tracked plane
              imposes a natural reference frame, so that the alignment of the
              real and virtual coordinate systems is rather simpler than would
              be the case with a general structure-and-motion system. Multiple
              planes can be tracked, and additional data such as 2D point
              tracks are easily incorporated.",
  pages    = "120--128",
  year     =  2000
}

@INPROCEEDINGS{Spielman2012-jk,
  title    = "Exact Recovery of {Sparsely-Used} Dictionaries",
  author   = "Spielman, Daniel A and Wang, Huan and Wright, John",
  abstract = "We consider the problem of learning sparsely used dictionaries
              with an arbitrary square dictionary and a random, sparse
              coefficient matrix. We prove that \textbackslashemphO(n log
              \textbackslashemphn) samples are sufficient to uniquely determine
              the coefficient matrix. Based on this proof, we design a
              polynomial-time algorithm, called Exact Recovery of Sparsely-Used
              Dictionaries (ER-SpUD), and prove that it probably recovers the
              dictionary and coefficient matrix when the coefficient matrix is
              sufficiently sparse. Simulation results show that ER-SpUD reveals
              the true dictionary as well as the coefficients with probability
              higher than many state-of-the-art algorithms.",
  volume   =  23,
  pages    = "37.1--37.18",
  year     =  2012,
  location = "Edinburgh, Scotland"
}

@ARTICLE{Stewart1997-oi,
  title    = "Bias in robust estimation caused by discontinuities and multiple
              structures",
  author   = "Stewart, Charles V",
  abstract = "When fitting models to data containing multiple structures, such
              as when fitting surface patches to data taken from a neighborhood
              that includes a range discontinuity, robust estimators must
              tolerate both gross outliers and pseudo outliers. Pseudo outliers
              are outliers to the structure of interest, but inliers to a
              different structure. They differ from gross outliers because of
              their coherence. Such data occurs frequently in computer vision
              problems, including motion estimation, model fitting, and range
              data analysis. The focus in this paper is the problem of fitting
              surfaces near discontinuities in range data. To characterize the
              performance of least median of the squares, least trimmed
              squares, M-estimators, Hough transforms, RANSAC, and MINPRAN on
              this type of data, the ``pseudo outlier bias'' metric is
              developed using techniques from the robust statistics literature,
              and it is used to study the error in robust fits caused by
              distributions modeling various types of discontinuities. The
              results show each robust estimator to be biased at small, but
              substantial, discontinuities. They also show the circumstances
              under which different estimators are most effective. Most
              importantly, the results imply present estimators should be used
              with care, and new estimators should be developed. \copyright{}
              1997 IEEE.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  19,
  number   =  8,
  pages    = "818--833",
  year     =  1997,
  keywords = "Bias; Discontinuities; Multiple structures; Outliers; Parameter
              estimation; Robust estimation"
}

@ARTICLE{Sun2019-qc,
  title    = "Oriented Point Sampling for Plane Detection in Unorganized Point
              Clouds",
  author   = "Sun, Bo and Mordohai, Philippos",
  abstract = "Plane detection in 3D point clouds is a crucial pre-processing
              step for applications such as point cloud segmentation, semantic
              mapping and SLAM. In contrast to many recent plane detection
              methods that are only applicable on organized point clouds, our
              work is targeted to unorganized point clouds that do not permit a
              2D parametrization. We compare three methods for detecting planes
              in point clouds efficiently. One is a novel method proposed in
              this paper that generates plane hypotheses by sampling from a set
              of points with estimated normals. We named this method Oriented
              Point Sampling (OPS) to contrast with more conventional
              techniques that require the sampling of three unoriented points
              to generate plane hypotheses. We also implemented an efficient
              plane detection method based on local sampling of three
              unoriented points and compared it with OPS and the 3D-KHT
              algorithm, which is based on octrees, on the detection of planes
              on 10,000 point clouds from the SUN RGB-D dataset.",
  journal  = "IEEE Int. Conf. Robot. Autom.",
  volume   = "2019-May",
  pages    = "2917--2923",
  month    =  may,
  year     =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sun2017-cr,
  title    = "Complete Dictionary Recovery Over the Sphere I: Overview and the
              Geometric Picture",
  author   = "Sun, Ju and Qu, Qing and Wright, John",
  abstract = "We consider the problem of recovering a complete (i.e., square
              and invertible) matrix A0, from Y $\in$ ℝn $\times$ p with Y = A0
              X0, provided X0 is sufficiently sparse. This recovery problem is
              central to theoretical understanding of dictionary learning,
              which seeks a sparse representation for a collection of input
              signals and finds numerous applications in modern signal
              processing and machine learning. We give the first efficient
              algorithm that provably recovers A0 when X0 has O (n) nonzeros
              per column, under suitable probability model for X0. In contrast,
              prior results based on efficient algorithms either only guarantee
              recovery when X0 has O($\surd$n) zeros per column, or require
              multiple rounds of semidefinite programming relaxation to work
              when X0 has O(n) nonzeros per column. Our algorithmic pipeline
              centers around solving a certain nonconvex optimization problem
              with a spherical constraint. In this paper, we provide a
              geometric characterization of the objective landscape. In
              particular, we show that the problem is highly structured with
              high probability: 1) there are no ``spurious'' local minimizers
              and 2) around all saddle points the objective has a negative
              directional curvature. This distinctive structure makes the
              problem amenable to efficient optimization algorithms. In a
              companion paper, we design a second-order trust-region algorithm
              over the sphere that provably converges to a local minimizer from
              arbitrary initializations, despite the presence of saddle points.",
  journal  = "IEEE Trans. Inf. Theory",
  volume   =  63,
  number   =  2,
  pages    = "853--884",
  year     =  2017,
  keywords = "Dictionary learning; escaping saddle points; function landscape;
              inverse problems; manifold optimization; nonconvex optimization;
              nonlinear approximation; second-order geometry; spherical
              constraint; structured signals; trust-region method"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sun2017-xz,
  title    = "Complete Dictionary Recovery Over the Sphere {II}: Recovery by
              Riemannian {Trust-Region} Method",
  author   = "Sun, Ju and Qu, Qing and Wright, John",
  abstract = "We consider the problem of recovering a complete (i.e., square
              and invertible) matrix A0, from Y $\in$ ℝn $\times$ p with Y = A0
              X0, provided X0 is sufficiently sparse. This recovery problem is
              central to theoretical understanding of dictionary learning,
              which seeks a sparse representation for a collection of input
              signals and finds numerous applications in modern signal
              processing and machine learning. We give the first efficient
              algorithm that provably recovers A0 when X0 has O (n) nonzeros
              per column, under suitable probability model for X0. Our
              algorithmic pipeline centers around solving a certain nonconvex
              optimization problem with a spherical constraint, and hence is
              naturally phrased in the language of manifold optimization. In a
              companion paper, we have showed that with high probability, our
              nonconvex formulation has no ``spurious'' local minimizers and
              around any saddle point, the objective function has a negative
              directional curvature. In this paper, we take advantage of the
              particular geometric structure and describe a Riemannian trust
              region algorithm that provably converges to a local minimizer
              with from arbitrary initializations. Such minimizers give
              excellent approximations to the rows of X0. The rows are then
              recovered by a linear programming rounding and deflation.",
  journal  = "IEEE Trans. Inf. Theory",
  volume   =  63,
  number   =  2,
  pages    = "885--914",
  year     =  2017,
  keywords = "Dictionary learning; escaping saddle points; function landscape;
              inverse problems; manifold optimization; nonconvex optimization;
              nonlinear approximation; second-order geometry; spherical
              constraint; structured signals; trust-region method"
}

@ARTICLE{Tan2015-fd,
  title     = "Feature matching in stereo images encouraging uniform spatial
               distribution",
  author    = "Tan, Xiao and Sun, Changming and Sirault, Xavier and Furbank,
               Robert and Pham, Tuan D",
  abstract  = "Abstract Finding feature correspondences between a pair of
               stereo images is a key step in computer vision for 3D
               reconstruction and object recognition. In practice, a larger
               number of correct correspondences and a higher percentage of
               correct matches are beneficial. Previous researches show that
               the spatial distribution of correspondences are also very
               important especially for fundamental matrix estimation. So far,
               no existing feature matching method considers the spatial
               distribution of correspondences. In our research, we develop a
               new algorithm to find good correspondences in all the three
               aspects mentioned, i.e., larger number of correspondences,
               higher percentage of correct correspondences, and better spatial
               distribution of correspondences. Our method consists of two
               processes: an adaptive disparity smoothing filter to remove
               false correspondences based on the disparities of neighboring
               correspondences and a matching exploration algorithm to find
               more correspondences according to the spatial distribution of
               correspondences so that the correspondences are as uniformly
               distributed as possible in the images. To find correspondences
               correctly and efficiently, we incorporate the cheirality
               constraint under an epipole polar transformation together with
               the epipolar constraint to predict the potential location of
               matching point. Experiments demonstrate that our method performs
               very well on both the number of correct correspondences and the
               percentage of correct correspondences; and the obtained
               correspondences are also well distributed over the image space.",
  journal   = "Pattern Recognit.",
  publisher = "Elsevier",
  volume    =  48,
  number    =  8,
  pages     = "2530--2542",
  year      =  2015,
  keywords  = "3D/stereo scene analysis; Feature correspondences; Stereo image
               processing"
}

@ARTICLE{Tennakoon2016-xu,
  title    = "Robust Model Fitting Using Higher Than Minimal Subset Sampling",
  author   = "Tennakoon, Ruwan B and Bab-Hadiashar, Alireza and Cao, Zhenwei
              and Hoseinnezhad, Reza and Suter, David",
  abstract = "Identifying the underlying model in a set of data contaminated by
              noise and outliers is a fundamental task in computer vision. The
              cost function associated with such tasks is often highly complex,
              hence in most cases only an approximate solution is obtained by
              evaluating the cost function on discrete locations in the
              parameter (hypothesis) space. To be successful at least one
              hypothesis has to be in the vicinity of the solution. Due to
              noise hypotheses generated by minimal subsets can be far from the
              underlying model, even when the samples are from the said
              structure. In this paper we investigate the feasibility of using
              higher than minimal subset sampling for hypothesis generation.
              Our empirical studies showed that increasing the sample size
              beyond minimal size (p), in particular up to p + 2, will
              significantly increase the probability of generating a hypothesis
              closer to the true model when subsets are selected from inliers.
              On the other hand, the probability of selecting an all inlier
              sample rapidly decreases with the sample size, making direct
              extension of existing methods unfeasible. Hence, we propose a new
              computationally tractable method for robust model fitting that
              uses higher than minimal subsets. Here, one starts from an
              arbitrary hypothesis (which does not need to be in the vicinity
              of the solution) and moves until either a structure in data is
              found or the process is re-initialized. The method also has the
              ability to identify when the algorithm has reached a hypothesis
              with adequate accuracy and stops appropriately, thereby saving
              computational time. The experimental analysis carried out using
              synthetic and real data shows that the proposed method is both
              accurate and efficient compared to the state-of-the-art robust
              model fitting techniques.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  38,
  number   =  2,
  pages    = "350--362",
  year     =  2016,
  keywords = "Model fitting; Robust Statistics; data segmentation; higher than
              minimal subset sampling; hypothesis generation"
}

@ARTICLE{Torr1997-gs,
  title    = "Assessment of information criteria for motion model selection",
  author   = "Torr, P H S",
  abstract = "Rigid motion imposes constraints on the motion of image points
              between the two images. The matched points must conform to one of
              several possible constraints, such as that given by the
              fundamental matrix or image-image homography, and it is essential
              to know which model to fit to the data before recovery of
              structure, matching or segmentation can be performed
              successfully. This paper compares several model selection methods
              with a particular emphasis on providing a method that will work
              fully automatically on real imagery.",
  journal  = "Proceedings of the IEEE Computer Society Conference on Computer
              Vision and Pattern Recognition",
  pages    = "47--52",
  year     =  1997
}

@ARTICLE{Trager2016-wi,
  title    = "Trinocular Geometry Revisited",
  author   = "Trager, Matthew and Ponce, Jean and Hebert, Martial",
  abstract = "When do the visual rays associated with triplets of point
              correspondences converge, that is, intersect in a common point?
              Classical models of trinocular geometry based on the fundamental
              matrices and trifocal tensor associated with the corresponding
              cameras only provide partial answers to this fundamental
              question, in large part because of underlying, but seldom
              explicit, general configuration assumptions. This paper uses
              elementary tools from projective line geometry to provide
              necessary and sufficient geometric and analytical conditions for
              convergence in terms of transversals to triplets of visual rays,
              without any such assumptions. In turn, this yields a novel and
              simple minimal parameterization of trinocular geometry for
              cameras with non-collinear or collinear pinholes, which can be
              used to construct a practical and efficient method for trinocular
              geometry parameter estimation. We present numerical experiments
              using synthetic and real data.",
  journal  = "Int. J. Comput. Vis.",
  volume   =  120,
  number   =  2,
  pages    = "134--152",
  year     =  2016,
  keywords = "Camera parameter estimation; Minimal parameterizations of camera
              geometry; Trilinearities; Trinocular geometry"
}

@ARTICLE{Tsakiris2017-sj,
  title    = "Filtrated algebraic subspace clustering",
  author   = "Tsakiris, Manolis C and Vidal, Ren{\'e}",
  abstract = "Subspace clustering is the problem of clustering data that lie
              close to a union of linear subspaces. Existing algebraic subspace
              clustering methods are based on fitting the data with an
              algebraic variety and decomposing this variety into its
              constituent subspaces. Such methods are well suited to the case
              of a known number of subspaces of known and equal dimensions,
              where a single polynomial vanishing in the variety is sufficient
              to identify the subspaces. While subspaces of unknown and
              arbitrary dimensions can be handled using multiple vanishing
              polynomials, current approaches are not robust to corrupted data
              due to the difficulty of estimating the number of polynomials. As
              a consequence, the current practice is to use a single polynomial
              to fit the data with a union of hyperplanes containing the union
              of subspaces, an approach that works well only when the
              dimensions of the subspaces are high enough. In this paper, we
              propose a new algebraic subspace clustering algorithm, which can
              identify the subspace S passing through a point $\chi$ by
              constructing a descending filtration of subspaces containing S.
              First, a single polynomial vanishing in the variety is identified
              and used to find a hyperplane containing S. After intersecting
              this hyperplane with the variety to obtain a subvariety, a new
              polynomial vanishing in the subvariety is found, and so on, until
              no nontrivial vanishing polynomial exists. In this case, our
              algorithm identifies S as the intersection of the hyperplanes
              identified thus far. By repeating this procedure for other
              points, our algorithm eventually identifies all the subspaces.
              Alternatively, by constructing a filtration at each data point
              and comparing any two filtrations using a suitable affinity, we
              propose a spectral version of our algebraic procedure based on
              spectral clustering, which is suitable for computations with
              noisy data. We show by experiments on synthetic and real data
              that the proposed algorithm outperforms state-of-the-art methods
              on several occasions, thus demonstrating the merit of the idea of
              filtrations.",
  journal  = "SIAM J. Imag. Sci.",
  volume   =  10,
  number   =  1,
  pages    = "372--415",
  year     =  2017,
  keywords = "Algebraic subspace clustering; Generalized principal component
              analysis; Spectral clustering; Subspace arrangements; Subspace
              clustering; Transversal subspaces"
}

@ARTICLE{Urban2016-de,
  title    = "{MLPNP} - A {REAL-TIME} {MAXIMUM} {LIKELIHOOD} {SOLUTION} to the
              {PERSPECTIVE-N-POINT} {PROBLEM}",
  author   = "Urban, S and Leitloff, J and Hinz, S",
  abstract = "In this paper, a statistically optimal solution to the
              Perspective-n-Point (PnP) problem is presented. Many solutions to
              the PnP problem are geometrically optimal, but do not consider
              the uncertainties of the observations. In addition, it would be
              desirable to have an internal estimation of the accuracy of the
              estimated rotation and translation parameters of the camera pose.
              Thus, we propose a novel maximum likelihood solution to the PnP
              problem, that incorporates image observation uncertainties and
              remains real-time capable at the same time. Further, the
              presented method is general, as is works with 3D direction
              vectors instead of 2D image points and is thus able to cope with
              arbitrary central camera models. This is achieved by projecting
              (and thus reducing) the covariance matrices of the observations
              to the corresponding vector tangent space.",
  journal  = "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial
              Information Sciences",
  volume   =  3,
  pages    = "131--138",
  year     =  2016,
  keywords = "computer vision; maximum-likelihood estimation;
              perspective-n-point; photogrammetry; pose estimation"
}

@ARTICLE{Vakhitov2018-zx,
  title    = "Stereo relative pose from line and point feature triplets",
  author   = "Vakhitov, Alexander and Lempitsky, Victor and Zheng, Yinqiang",
  abstract = "Stereo relative pose problem lies at the core of stereo visual
              odometry systems that are used in many applications. In this work
              we present two minimal solvers for the stereo relative pose. We
              specifically consider the case when a minimal set consists of
              three point or line features and each of them has three known
              projections on two stereo cameras. We validate the importance of
              this formulation for practical purposes in our experiments with
              motion estimation. We then present a complete classification of
              minimal cases with three point or line correspondences each
              having three projections, and present two new solvers that can
              handle all such cases. We demonstrate a considerable effect from
              the integration of the new solvers into a visual SLAM system.",
  journal  = "Lect. Notes Comput. Sci.",
  volume   = "11212 LNCS",
  pages    = "662--677",
  year     =  2018,
  keywords = "Generalized camera; Line features; Minimal solver; Relative pose;
              Stereo visual odometry"
}

@BOOK{Vandenberghe2009-uh,
  title  = "Convex Optimization",
  author = "Vandenberghe, Lieven",
  year   =  2009
}

@ARTICLE{Vidal2014-lr,
  title    = "Low rank subspace clustering ({LRSC})",
  author   = "Vidal, Ren{\'e} and Favaro, Paolo",
  abstract = "We consider the problem of fitting a union of subspaces to a
              collection of data points drawn from one or more subspaces and
              corrupted by noise and/or gross errors. We pose this problem as a
              non-convex optimization problem, where the goal is to decompose
              the corrupted data matrix as the sum of a clean and
              self-expressive dictionary plus a matrix of noise and/or gross
              errors. By self-expressive we mean a dictionary whose atoms can
              be expressed as linear combinations of themselves with low-rank
              coefficients. In the case of noisy data, our key contribution is
              to show that this non-convex matrix decomposition problem can be
              solved in closed form from the SVD of the noisy data matrix. The
              solution involves a novel polynomial thresholding operator on the
              singular values of the data matrix, which requires minimal
              shrinkage. For one subspace, a particular case of our framework
              leads to classical PCA, which requires no shrinkage. For multiple
              subspaces, the low-rank coefficients obtained by our framework
              can be used to construct a data affinity matrix from which the
              clustering of the data according to the subspaces can be obtained
              by spectral clustering. In the case of data corrupted by gross
              errors, we solve the problem using an alternating minimization
              approach, which combines our polynomial thresholding operator
              with the more traditional shrinkage-thresholding operator.
              Experiments on motion segmentation and face clustering show that
              our framework performs on par with state-of-the-art techniques at
              a reduced computational cost. \copyright{} 2013 Elsevier B.V. All
              rights reserved.",
  journal  = "Pattern Recognit. Lett.",
  volume   =  43,
  number   =  1,
  pages    = "47--61",
  year     =  2014,
  keywords = "Face clustering; Low-rank and sparse methods; Motion
              segmentation; Principal component analysis; Subspace clustering"
}

@ARTICLE{Vidal2008-ip,
  title    = "Three-view multibody structure from motion",
  author   = "Vidal, Ren{\'e} and Hartley, Richard",
  abstract = "We propose a geometric approach to 3-D motion segmentation from
              point correspondences in three perspective views. We demonstrate
              that after applying a polynomial embedding to the point
              correspondences they become related by the socalled multibody
              trilinear constraint and its associated multibody trifocal
              tensor, which are natural generalizations of the trilinear
              constraint and the trifocal tensor to multiple motions. We derive
              a rank constraint on the embedded correspondences, from which one
              can estimate the number of independent motions as well as
              linearly solve for the multibody trifocal tensor. We then show
              how to compute the epipolar lines associated with each image
              point from the common root of a set of univariate polynomials and
              the epipoles by solving a pair of plane clustering problems using
              Generalized PCA (GPCA). The individual trifocal tensors are then
              obtained from the second order derivatives of the multibody
              trilinear constraint. Given epipolar lines and epipoles, or
              trifocal tensors, one can immediately obtain an initial
              clustering of the correspondences. We use this clustering to
              initialize an iterative algorithm that alternates between the
              computation of the trifocal tensors and the segmentation of the
              correspondences. We test our algorithm on various synthetic and
              real scenes, and compare with other algebraic and iterative
              algorithms.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  30,
  number   =  2,
  pages    = "214--227",
  year     =  2008,
  keywords = "3-D motion segmentation; Generalized PCA (GPCA); Multibody
              structure from motion; Multibody trifocal tensor; Multibody
              trilinear constraint"
}

@ARTICLE{Vidal2006-kl,
  title    = "A unified algebraic approach to {2-D} and {3-D} motion
              segmentation and estimation",
  author   = "Vidal, Ren{\'e} and Ma, Yi",
  abstract = "In this paper, we present an analytic solution to the problem of
              estimating an unknown number of 2-D and 3-D motion models from
              two-view point correspondences or optical flow. The key to our
              approach is to view the estimation of multiple motion models as
              the estimation of a single multibody motion model. This is
              possible thanks to two important algebraic facts. First, we show
              that all the image measurements, regardless of their associated
              motion model, can be fit with a single real or complex
              polynomial. Second, we show that the parameters of the individual
              motion model associated with an image measurement can be obtained
              from the derivatives of the polynomial at that measurement. This
              leads to an algebraic motion segmentation and estimation
              algorithm that applies to most of the two-view motion models that
              have been adopted in computer vision. Our experiments show that
              the proposed algorithm out-performs existing algebraic and
              factorization-based methods in terms of efficiency and
              robustness, and provides a good initialization for iterative
              techniques, such as Expectation Maximization, whose performance
              strongly depends on good initialization.",
  journal  = "J. Math. Imaging Vis.",
  volume   =  25,
  number   =  3,
  pages    = "403--421",
  year     =  2006,
  keywords = "Generalized PCA (GPCA); Motion segmentation; Multibody epipolar
              constraint; Multibody fundamental matrix; Multibody homography;
              Multibody structure from motion"
}

@BOOK{Vidal2016-zp,
  title  = "Generalized principal component analysis",
  author = "Vidal, Ren{\'e} and Ma, Yi and Sastry, S Shankar",
  volume =  40,
  pages  = "1--566",
  year   =  2016
}

@BOOK{Wang2012-hi,
  title    = "Improving clustering by learning a bi-stochastic data similarity
              matrix",
  author   = "Wang, Fei and Li, Ping and K{\"o}nig, Arnd Christian and Wan,
              Muting",
  abstract = "An idealized clustering algorithm seeks to learn a
              cluster-adjacency matrix such that, if two data points belong to
              the same cluster, the corresponding entry would be 1; otherwise,
              the entry would be 0. This integer (1/0) constraint makes it
              difficult to find the optimal solution. We propose a relaxation
              on the cluster-adjacency matrix, by deriving a bi-stochastic
              matrix from a data similarity (e. g., kernel) matrix according to
              the Bregman divergence. Our general method is named the
              Bregmanian Bi-Stochastication (BBS) algorithm. We focus on two
              popular choices of the Bregman divergence: the Euclidean distance
              and the Kullback-Leibler (KL) divergence. Interestingly, the BBS
              algorithm using the KL divergence is equivalent to the
              Sinkhorn-Knopp (SK) algorithm for deriving bi-stochastic
              matrices. We show that the BBS algorithm using the Euclidean
              distance is closely related to the relaxed k-means clustering and
              can often produce noticeably superior clustering results to the
              SK algorithm (and other algorithms such as Normalized Cut),
              through extensive experiments on public data sets. \copyright{}
              2011 Springer-Verlag London Limited.",
  volume   =  32,
  year     =  2012,
  keywords = "Bi-stochastic matrix; Bregman divergence; Clustering;clustering"
}

@ARTICLE{Wang2020-rt,
  title   = "Robust Bi-stochastic Graph Regularized Matrix Factorization for
             Data Clustering",
  author  = "Wang, Qi and He, Xiang and Jiang, Xu and Li, Xuelong",
  journal = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume  = "XXX",
  number  = "XXX",
  pages   = "1--1",
  year    =  2020
}

@ARTICLE{Wang2016-of,
  title    = "Structured doubly stochastic matrix for graph based clustering",
  author   = "Wang, Xiaoqian and Nie, Feiping and Huang, Heng",
  abstract = "As one of the most significant machine learning topics,
              clustering has been extensively employed in various kinds of
              area. Its prevalent application in scientific research as well as
              industrial practice has drawn high attention in this day and age.
              A multitude of clustering methods have been developed, among
              which the graph based clustering method using the affinity matrix
              has been laid great emphasis on. Recent research work used the
              doubly stochastic matrix to normalize the input affinity matrix
              and enhance the graph based clustering models. Although the
              doubly stochastic matrix can improve the clustering performance,
              the clustering structure in the doubly stochastic matrix is not
              clear as expected. Thus, postprocessing step is required to
              extract the final clustering results, which may not be optimal.
              To address this problem, in this paper, we propose a novel convex
              model to learn the structured doubly stochastic matrix by
              imposing low-rank constraint on the graph Laplacian matrix. Our
              new structured doubly stochastic matrix can explicitly uncover
              the clustering structure and encode the probabilities of
              pair-wise data points to be connected, such that the clustering
              results are enhanced. An efficient optimization algorithm is
              derived to solve our new objective. Also, we provide theoretical
              discussions that when the input differs, our method possesses
              interesting connections with K-means and spectral graph cut
              models respectively. We conduct experiments on both synthetic and
              benchmark datasets to validate the performance of our proposed
              method. The empirical results demonstrate that our model provides
              an approach to better solving the K-mean clustering problem. By
              using the cluster indicator provided by our model as
              initialization, Kmeans converges to a smaller objective function
              value with better clustering performance. Moreover, we compare
              the clustering performance of our model with spectral clustering
              and related double stochastic model. On all datasets, our method
              performs equally or better than the related methods.",
  journal  = "Proceedings of the ACM SIGKDD International Conference on
              Knowledge Discovery and Data Mining",
  volume   = "13-17-Augu",
  pages    = "1245--1254",
  year     =  2016,
  keywords = "Doubly stochastic matrix; Graph laplacian; K-means clustering;
              Spectral clustering"
}

@ARTICLE{Wang2013-vx,
  title   = "Reliable {RANSAC} using a novel preprocessing model",
  author  = "Wang, Xiaoyan and Zhang, Hui and Liu, Sheng",
  journal = "Comput. Math. Methods Med.",
  volume  =  2013,
  year    =  2013
}

@ARTICLE{Weng1992-rx,
  title    = "Motion and structure from line correspondences: Closed-form
              solution, uniqueness, and optimization",
  author   = "Weng, Juyang and Huang, Thomas S and Ahuja, Narendra",
  abstract = "This work discusses estimating motion and structure parameters
              from line correspondences of a rigid scene. The authors present a
              closed-form solution to motion and structure parameters from line
              correspondences through three monocular perspective views. The
              algorithm makes use of redundancy in the data to improve the
              accuracy of the solutions. The uniqueness of the solution is
              established, and necessary and sufficient conditions for
              degenerate spatial line configurations are given. Optimization
              has been employed to further improve the accuracy of the
              estimates in the presence of noise. Simulations have shown that
              the errors of the optimized estimates are close to the
              theoretical lower error bound",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  14,
  number   =  3,
  pages    = "318--336",
  month    =  mar,
  year     =  1992,
  keywords = "Cameras; Closed-form solution; Computer errors; Image
              segmentation; Layout; Motion analysis; Motion estimation; Pixel;
              Redundancy; Sufficient conditions; closed-form solution; line
              correspondences; motion parameters; necessary conditions;
              optimisation; optimization; parameter estimation; pattern
              recognition; picture processing; rigid scene; structure
              parameters; sufficient conditions; uniqueness"
}

@ARTICLE{Wu2020-ns,
  title    = "Incremental learning via rate reduction",
  author   = "Wu, Ziyang and Baek, Christina and You, Chong and Ma, Yi",
  abstract = "Current deep learning architectures suffer from catastrophic
              forgetting, a failure to retain knowledge of previously learned
              classes when incrementally trained on new classes. The
              fundamental roadblock faced by deep learning methods is that deep
              learning models are optimized as ``black boxes'', making it
              difficult to properly adjust the model parameters to preserve
              knowledge about previously seen data. To overcome the problem of
              catastrophic forgetting, we propose utilizing an alternative
              ``white box'' architecture derived from the principle of rate
              reduction, where each layer of the network is explicitly computed
              without back propagation. Under this paradigm, we demonstrate
              that, given a pretrained network and new data classes, our
              approach can provably construct a new network that emulates joint
              training with all past and new classes. Finally, our experiments
              show that our proposed learning algorithm observes significantly
              less decay in classification performance, outperforming state of
              the art methods on MNIST and CIFAR-10 by a large margin and
              justifying the use of ``white box'' algorithms for incremental
              learning even for sufficiently complex image data.",
  journal  = "arXiv",
  year     =  2020
}

@ARTICLE{Xiao2010-me,
  title     = "New fundamental matrix estimation method using global
               optimization",
  author    = "Xiao, Xuelian",
  abstract  = "The estimation of fundamental matrix is one of the most crucial
               steps in many computer vision applications such as 3D
               reconstruction, autocalibration and motion segmentation. In this
               paper, we give a new method for nonlinearly estimating the
               fundamental matrix from point correspondences using global
               optimization. We firstly parameterize the fundamental matrix in
               7 unknowns in a way that the rank-two constraint is satisfied.
               Then, the fundamental matrix is estimated by globally minimize
               non-convex formulation in term of convex (linear matrix
               inequality) LMI relaxation and standard LMI techniques. In order
               to obtain robustness, we perform the computation in a RANSAC
               framework and consider nonlinear criteria minimizing meaningful
               geometric distances. The iterate process leads the estimation to
               a more accurate level. Experimental results show the
               effectiveness of the proposed method.",
  journal   = "ICCASM 2010 - 2010 International Conference on Computer
               Application and System Modeling, Proceedings",
  publisher = "IEEE",
  volume    =  7,
  number    = "Iccasm",
  pages     = "V7--400--V7--404",
  year      =  2010,
  keywords  = "Fundamental matrix; Global minimization; Linear matrix
               inequality; Pipolar structure; Stereo vision"
}

@ARTICLE{Xu2012-hy,
  title    = "Robust {PCA} via outlier pursuit",
  author   = "Xu, Huan and Caramanis, Constantine and Sanghavi, Sujay",
  abstract = "Singular-value decomposition (SVD) [and principal component
              analysis (PCA)] is one of the most widely used techniques for
              dimensionality reduction: successful and efficiently computable,
              it is nevertheless plagued by a well-known, well-documented
              sensitivity to outliers. Recent work has considered the setting
              where each point has a few arbitrarily corrupted components. Yet,
              in applications of SVD or PCA, such as robust collaborative
              filtering or bioinformatics, malicious agents, defective genes,
              or simply corrupted or contaminated experiments may effectively
              yield entire points that are completely corrupted. We present an
              efficient convex optimization-based algorithm that we call
              outlier pursuit, which under some mild assumptions on the
              uncorrupted points (satisfied, e.g., by the standard generative
              assumption in PCA problems) recovers the exact optimal
              low-dimensional subspace and identifies the corrupted points.
              Such identification of corrupted points that do not conform to
              the low-dimensional approximation is of paramount interest in
              bioinformatics, financial applications, and beyond. Our
              techniques involve matrix decomposition using nuclear norm
              minimization; however, our results, setup, and approach
              necessarily differ considerably from the existing line of work in
              matrix completion and matrix decomposition, since we develop an
              approach to recover the correct column space of the uncorrupted
              matrix, rather than the exact matrix itself. In any problem where
              one seeks to recover a structure rather than the exact initial
              matrices, techniques developed thus far relying on certificates
              of optimality will fail. We present an important extension of
              these methods, which allows the treatment of such problems.
              \copyright{} 2011 IEEE.",
  journal  = "IEEE Trans. Inf. Theory",
  volume   =  58,
  number   =  5,
  pages    = "3047--3064",
  year     =  2012
}

@ARTICLE{Yan2014-dj,
  title    = "Efficient semidefinite spectral clustering via lagrange duality",
  author   = "Yan, Yan and Shen, Chunhua and Wang, Hanzi",
  abstract = "We propose an efficient approach to semidefinite spectral
              clustering (SSC), which addresses the Frobenius normalization
              with the positive semidefinite (p.s.d.) constraint for spectral
              clustering. Compared with the original Frobenius norm
              approximation-based algorithm, the proposed algorithm can more
              accurately find the closest doubly stochastic approximation to
              the affinity matrix by considering the p.s.d. constraint. In this
              paper, SSC is formulated as a semidefinite programming (SDP)
              problem. In order to solve the high computational complexity of
              SDP, we present a dual algorithm based on the Lagrange dual
              formalization. Two versions of the proposed algorithm are
              proffered: one with less memory usage and the other with faster
              convergence rate. The proposed algorithm has much lower time
              complexity than that of the standard interior-point-based SDP
              solvers. Experimental results on both the UCI data sets and
              real-world image data sets demonstrate that: 1) compared with the
              state-of-the-art spectral clustering methods, the proposed
              algorithm achieves better clustering performance and 2) our
              algorithm is much more efficient and can solve larger-scale SSC
              problems than those standard interior-point SDP solvers.
              \copyright{} 1992-2012 IEEE.",
  journal  = "IEEE Trans. Image Process.",
  volume   =  23,
  number   =  8,
  pages    = "3522--3534",
  year     =  2014,
  keywords = "Lagrange duality; Spectral clustering; doubly stochastic
              normalization; semidefinite programming"
}

@ARTICLE{Yang2020-eg,
  title    = "Robust Plane Clustering Based on {L1-Norm} Minimization",
  author   = "Yang, Hongxin and Yang, Xubing and Zhang, Fuquan and Ye, Qiaolin",
  abstract = "Plane clustering methods, typically, $k$ plane clustering ( $k$
              PC), play conclusive roles in the family of data clustering.
              Instead of point-prototype, they aim to seek multiple
              plane-prototype fitting planes as centers to group the given data
              into their corresponding clusters based on L2 norm metric.
              However, they are usually sensitive to outliers because of square
              operation on the L2 norm. In this paper, we focus on robust plane
              clustering and propose a L1 norm plane clustering method, termed
              as $\text\{L\}1k$ PC. The leading problem is optimized on the L1
              ball hull, a non-convex feasible domain. To handle the problem,
              we provide a new strategy and its related mathematical proofs for
              L1 norm optimization. Compared to state-of-the-art methods, the
              advantages of our proposed lie in 4 folds: 1) similar to $k$ PC,
              it has clear geometrical interpretation; 2) it is more capable of
              resisting to outlier; 3) theoretically, it is proved that the
              leading non-convex problem is equivalent to several convex
              sub-problems. To our best knowledge, this opens up a new way for
              L1 norm optimization; 4) the $k$ fitting planes are solved by $k$
              individual linear programming problems, rather than higher
              time-consuming eigenvalue equations or quadratic programming
              problems used in the conventional plane clustering methods.
              Experiments on some artificial, benchmark UCI and human face
              datasets show its superiorities in robustness, training time, and
              clustering accuracy.",
  journal  = "IEEE Access",
  volume   =  8,
  pages    = "29489--29500",
  year     =  2020,
  keywords = "L1 norm; eigenvalue problem; linear programming; plane clustering"
}

@ARTICLE{Yang2018-jk,
  title    = "Recovering {3D} planes from a single image via convolutional
              neural networks",
  author   = "Yang, Fengting and Zhou, Zihan",
  abstract = "In this paper, we study the problem of recovering 3D planar
              surfaces from a single image of man-made environment. We show
              that it is possible to directly train a deep neural network to
              achieve this goal. A novel plane structure-induced loss is
              proposed to train the network to simultaneously predict a plane
              segmentation map and the parameters of the 3D planes. Further, to
              avoid the tedious manual labeling process, we show how to
              leverage existing large-scale RGB-D dataset to train our network
              without explicit 3D plane annotations, and how to take advantage
              of the semantic labels come with the dataset for accurate planar
              and non-planar classification. Experiment results demonstrate
              that our method significantly outperforms existing methods, both
              qualitatively and quantitatively. The recovered planes could
              potentially benefit many important visual tasks such as
              vision-based navigation and human-robot interaction.",
  journal  = "Lect. Notes Comput. Sci.",
  volume   = "11214 LNCS",
  pages    = "87--103",
  year     =  2018,
  keywords = "3D reconstruction; Deep learning; Plane segmentation"
}

@ARTICLE{Yao2007-zw,
  title    = "Robust multi-view feature matching from multiple unordered views",
  author   = "Yao, Jian and Cham, Wai Kuen",
  abstract = "This paper explores the problem of multi-view feature matching
              from an unordered set of widely separated views. A set of local
              invariant features is extracted independently from each view.
              First we propose a new view-ordering algorithm that organizes all
              the unordered views into clusters of related (i.e. the same
              scene) views by efficiently computing the view-similarity values
              of all view pairs by reasonably selecting part of extracted
              features to match. Second a robust two-view matching algorithm is
              developed to find initial matches, then detect the outliers and
              finally incrementally find more reliable feature matches under
              the epipolar constraint between two views from dense to sparse
              based on an assumption that changes of both motion and feature
              characteristics of one match are consistent with those of
              neighbors. Third we establish the reliable multi-view matches
              across related views by reconstructing missing matches in a
              neighboring triple of views and efficiently determining the
              states of matches between view pairs. Finally, the reliable
              multi-view matches thus obtained are used to automatically track
              all the views by using a self-calibration method. The proposed
              methods were tested on several sets of real images. Experimental
              results show that it is efficient and can track a large set of
              multi-view feature matches across multiple widely separated
              views. \copyright{} 2007 Pattern Recognition Society.",
  journal  = "Pattern Recognit.",
  volume   =  40,
  number   =  11,
  pages    = "3081--3099",
  year     =  2007,
  keywords = "Organization of unordered views; Two-view matching; multi-view
              matching"
}

@INPROCEEDINGS{You2016-ob,
  title     = "Oracle Based Active Set Algorithm for Scalable Elastic Net
               Subspace Clustering",
  booktitle = "the {IEEE} conference on computer vision and pattern recognition",
  author    = "You, Chong and Li, Chun Guang and Robinson, Daniel P and Vidal,
               Rene",
  abstract  = "State-of-the-art subspace clustering methods are based on
               expressing each data point as a linear combination of other data
               points while regularizing the matrix of coefficients with l1, l2
               or nuclear norms. l1 regularization is guaranteed to give a
               subspace-preserving affinity (i.e., there are no connections
               between points from different subspaces) under broad theoretical
               conditions, but the clusters may not be connected. l2 and
               nuclear norm regularization often improve connectivity, but give
               a subspace-preserving affinity only for independent subspaces.
               Mixed l1, l2 and nuclear norm regularizations offer a balance
               between the subspacepreserving and connectedness properties, but
               this comes at the cost of increased computational complexity.
               This paper studies the geometry of the elastic net regularizer
               (a mixture of the l1 and l2 norms) and uses it to derive a
               provably correct and scalable active set method for finding the
               optimal coefficients. Our geometric analysis also provides a
               theoretical justification and a geometric interpretation for the
               balance between the connectedness (due to l2 regularization) and
               subspace-preserving (due to l1 regularization) properties for
               elastic net subspace clustering. Our experiments show that the
               proposed active set method not only achieves state-of-the-art
               clustering performance, but also efficiently handles large-scale
               datasets.",
  pages     = "3928--3937",
  year      =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{You2016-na,
  title     = "Scalable Sparse Subspace Clustering by Orthogonal Matching
               Pursuit",
  booktitle = "{IEEE} Conference on Computer Vision and Pattern Recognition",
  author    = "You, Chong and Robinson, Daniel P and Vidal, Ren{\'e}",
  abstract  = "Subspace clustering methods based on ℓ1, ℓ2 or nuclear norm
               regularization have become very popular due to their simplicity,
               theoretical guarantees and empirical success. However, the
               choice of the regularizer can greatly impact both theory and
               practice. For instance, ℓ1 regularization is guaranteed to give
               a subspace-preserving affinity (i.e., there are no connections
               between points from different subspaces) under broad conditions
               (e.g., arbitrary subspaces and corrupted data). However, it
               requires solving a large scale convex optimization problem. On
               the other hand, ℓ2 and nuclear norm regularization provide
               efficient closed form solutions, but require very strong
               assumptions to guarantee a subspace-preserving affinity, e.g.,
               independent subspaces and uncorrupted data. In this paper we
               study a subspace clustering method based on orthogonal matching
               pursuit. We show that the method is both computationally
               efficient and guaranteed to give a subspace-preserving affinity
               under broad conditions. Experiments on synthetic data verify our
               theoretical analysis, and applications in handwritten digit and
               face clustering show that our approach achieves the best trade
               off between accuracy and efficiency. Moreover, our approach is
               the first one to handle 100,000 data points.",
  month     =  jun,
  year      =  2016,
  keywords  = "Matching pursuit algorithms;Silicon;Clustering algorithms;Sparse
               matrices;Computer vision;Clustering methods;Optimization"
}

@INPROCEEDINGS{Yu2020-mx,
  title     = "Learning Diverse and Discriminative Representations via the
               Principle of Maximal Coding Rate Reduction",
  booktitle = "Neural Information Processing Systems",
  author    = "Yu, Yaodong and Chan, Kwan Ho Ryan and You, Chong and Song,
               Chaobing and Ma, Yi",
  abstract  = "To learn intrinsic low-dimensional structures from
               high-dimensional data that most discriminate between classes, we
               propose the principle of Maximal Coding Rate Reduction (MCR2),
               an information-theoretic measure that maximizes the coding rate
               difference between the whole dataset and the sum of each
               individual class. We clarify its relationships with most
               existing frameworks such as cross-entropy, information
               bottleneck, information gain, contractive and contrastive
               learning, and provide theoretical guarantees for learning
               diverse and discriminative features. The coding rate can be
               accurately computed from finite samples of degenerate
               subspace-like distributions and can learn intrinsic
               representations in supervised, self-supervised, and unsupervised
               settings in a unified manner. Empirically, the representations
               learned using this principle alone are significantly more robust
               to label corruptions in classification than those using
               cross-entropy, and can lead to state-of-the-art results in
               clustering mixed data from self-learned invariant features.",
  month     =  jun,
  year      =  2020
}

@ARTICLE{Zeng2008-zu,
  title    = "A new normalized method on line-based homography estimation",
  author   = "Zeng, Hui and Deng, Xiaoming and Hu, Zhanyi",
  abstract = "It is a conventional belief that line-based approaches perform
              better than point-based ones for homography estimation, as the
              line-fitting is generally more noise resistant than point
              detection. In this note, we show that blithely using line-based
              estimation is a risky business. More specifically, we show that
              when the image line(s) is (are) passing through or close to the
              origin, the line-based homography estimation could become wildly
              unstable whereas the point-based estimation performs normally. To
              tackle this problem, a new normalized method specially designed
              for line-based homography estimation is proposed and validated by
              extensive experiments. \copyright{} 2008 Elsevier B.V. All rights
              reserved.",
  journal  = "Pattern Recognit. Lett.",
  volume   =  29,
  number   =  9,
  pages    = "1236--1244",
  year     =  2008,
  keywords = "Data normalization; Line-based homography estimation; Point-based
              homography estimation; Visual metrology"
}

@ARTICLE{Zhang2000-yb,
  title    = "A Flexible New Technique for Camera Calibration",
  author   = "Zhang, Zhengyou",
  abstract = "We propose a flexible new technique to easily calibrate a camera.
              It is well suited for use without specialized knowledge of 3D
              geometry or computer vision. The technique only requires the
              camera to observe a planar pattern shown at a few (at least two)
              different orientations. Either the camera or the planar pattern
              can be freely moved. The motion need not be known. Radial lens
              distortion is modeled. The proposed procedure consists of a
              closed-form solution, followed by a nonlinear refinement based on
              the maximum likelihood criterion. Both computer simulation and
              real data have been used to test the proposed technique, and very
              good results have been obtained. Compared with classical
              techniques which use expensive equipments such as two or three
              orthogonal planes, the proposed technique is easy to use and
              flexible. It advances 3D computer vision one step from laboratory
              environments to real world use.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  22,
  number   =  11,
  pages    = "1330--1334",
  year     =  2000
}

@ARTICLE{Zhang2018-kw,
  title    = "Scalable Deep k-Subspace Clustering",
  author   = "Zhang, Tong and Ji, Pan and Harandi, Mehrtash and Hartley,
              Richard and Reid, Ian",
  abstract = "Subspace clustering algorithms are notorious for their
              scalability issues because building and processing large affinity
              matrices are demanding. In this paper, we introduce a method that
              simultaneously learns an embedding space along subspaces within
              it to minimize a notion of reconstruction error, thus addressing
              the problem of subspace clustering in an end-to-end learning
              paradigm. To achieve our goal, we propose a scheme to update
              subspaces within a deep neural network. This in turn frees us
              from the need of having an affinity matrix to perform clustering.
              Unlike previous attempts, our method can easily scale up{\^A} to
              large datasets, making it unique in the context of unsupervised
              learning with deep architectures. Our experiments show that our
              method significantly improves the clustering accuracy while
              enjoying cheaper memory footprints.",
  journal  = "Asian Conference on Computer Vision",
  volume   = "11365 LNCS",
  pages    = "466--481",
  year     =  2018,
  keywords = "Deep learning; Scalable; Subspace clustering"
}

@ARTICLE{Zhang2019-il,
  title    = "Neural collaborative subspace clustering",
  author   = "Zhang, Tong and Ji, Pan and Harandi, Mehrtash and Huang, Wenbing
              and Li, Hongdong",
  abstract = "We introduce the Neural Collaborative Subspace Clustering, a
              neural model that discovers clusters of data points drawn from a
              union of low-dimensional subspaces. In contrast to previous
              attempts, our model runs without the aid of spectral clustering.
              This makes our algorithm one of the kinds that can gracefully
              scale to large datasets. At its heart, our neural model benefits
              from a classifier which determines whether a pair of points lies
              on the same subspace or not. Essential to our model is the
              construction of two affinity matrices, one from the classifier
              and one based on a notion of subspace self-expressiveness, to
              supervise training in a collaborative scheme. We thoroughly
              assess and contrast the performance of our model against various
              state-of-the-art clustering algorithms including deep
              subspace-based ones.",
  journal  = "36th International Conference on Machine Learning, ICML 2019",
  volume   = "2019-June",
  pages    = "12777--12786",
  year     =  2019
}

@ARTICLE{Zhang2014-fq,
  title     = "Structure and motion from line correspondences: Representation,
               projection, initialization and sparse bundle adjustment",
  author    = "Zhang, Lilian and Koch, Reinhard",
  abstract  = "We address the problem of structure and motion from line
               correspondences, which ranges from the representation of lines,
               their projections and the initialization procedure to the final
               adjustment. The Cayley representation of spatial lines is
               developed, which is a nonlinear minimal parametrization
               circumventing the tiresome Pl{\"u}cker constraint. The
               relationships between different line representations are given.
               Based on these relationships, we derive a novel line projection
               function which is consistent with the previous results. After
               building the line observation model, we employ a closed-form
               solution for the first image triplet, then develop an
               incremental initialization approach to initialize the motion and
               structure parameters. Finally, the sparse bundle adjustment
               (SBA) is applied to refine the parameters, which updates the
               spatial lines by using the Cayley representation with an
               unconstrained optimization engine. The experiments show that the
               proposed algorithm outperforms the previous works both in
               efficiency and accuracy. \copyright{} 2014 Elsevier Inc. All
               rights reserved.",
  journal   = "J. Vis. Commun. Image Represent.",
  publisher = "Elsevier Inc.",
  volume    =  25,
  number    =  5,
  pages     = "904--915",
  year      =  2014,
  keywords  = "Cayley representation; Closed-form solution; Incremental
               initialization; Line projection; Line representation; Sparse
               bundle adjustment; Structure and motion; Unconstrained
               optimization"
}

@ARTICLE{Zhang2004-gv,
  title    = "Principal manifolds and nonlinear dimensionality reduction via
              tangent space alignment",
  author   = "Zhang, Zhen Yue and Zha, Hong Yuan",
  abstract = "We present a new algorithm for manifold learning and nonlinear
              dimensionality reduction. Based on a set of unorganized data
              points sampled with noise from a parameterized manifold, the
              local geometry of the manifold is learned by constructing an
              approximation for the tangent space at each point, and those
              tangent spaces are then aligned to give the global coordinates of
              the data points with respect to the underlying manifold. We also
              present an error analysis of our algorithm showing that
              reconstruction errors can be quite small in some cases. We
              illustrate our algorithm using curves and surfaces both in 2D/3D
              Euclidean spaces and higher dimensional Euclidean spaces. We also
              address several theoretical and algorithmic issues for further
              research and improvements. \copyright{} 2004 Shanghai University.",
  journal  = "J. Shanghai Univ.",
  volume   =  8,
  number   =  4,
  pages    = "406--424",
  year     =  2004,
  keywords = "nonlinear dimensionality reduction; principal manifold; singular
              value decomposition; subspace alignment; tangent space"
}

@ARTICLE{Zhou2016-fu,
  title    = "Factorized Graph Matching",
  author   = "Zhou, Feng and De La Torre, Fernando",
  abstract = "Graph matching plays a central role in solving correspondence
              problems \textbackslashnin computer vision. Graph matching
              problems that incorporate pair-wise \textbackslashnconstraints
              can be cast as a quadratic assignment problem (QAP).
              Unfortunately, \textbackslashnQAP is NP-hard and many algorithms
              have been proposed to solve different \textbackslashnrelaxations.
              This paper presents factorized graph matching (FGM), a novel
              \textbackslashnframework for interpreting and optimizing graph
              matching problems. In this work \textbackslashnwe show that the
              affinity matrix can be factorized as a Kronecker product of
              \textbackslashnsmaller matrices. There are three main benefits of
              using this factorization in \textbackslashngraph matching: (1)
              There is no need to compute the costly (in space and time)
              \textbackslashnpair-wise affinity matrix; (2) The factorization
              provides a taxonomy for graph \textbackslashnmatching and reveals
              the connection among several methods; (3) Using the
              \textbackslashnfactorization we derive a new approximation of the
              original problem that \textbackslashnimproves state-of-the-art
              algorithms in graph matching. Experimental results in
              \textbackslashnsynthetic and real databases illustrate the
              benefits of FGM. The code is \textbackslashnavailable at
              http://humansensing.cs.cmu.edu/fgm.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  year     =  2016,
  keywords = "Graph matching; feature matching; iterative closet point method;
              quadratic assignment problem"
}

@ARTICLE{Zhou2013-as,
  title    = "Plane-based content preserving warps for video stabilization",
  author   = "Zhou, Zihan and Jin, Hailin and Ma, Yi",
  abstract = "Recently, a new image deformation technique called
              content-preserving warping (CPW) has been successfully employed
              to produce the state-of-the-art video stabilization results in
              many challenging cases. The key insight of CPW is that the true
              image deformation due to viewpoint change can be well
              approximated by a carefully constructed warp using a set of
              sparsely constructed 3D points only. However, since CPW solely
              relies on the tracked feature points to guide the warping, it
              works poorly in large texture less regions, such as ground and
              building interiors. To overcome this limitation, in this paper we
              present a hybrid approach for novel view synthesis, observing
              that the texture less regions often correspond to large planar
              surfaces in the scene. Particularly, given a jittery video, we
              first segment each frame into piecewise planar regions as well as
              regions labeled as non-planar using Markov random fields. Then, a
              new warp is computed by estimating a single homography for
              regions belong to the same plane, while inheriting results from
              CPW in the non-planar regions. We demonstrate how the
              segmentation information can be efficiently obtained and
              seamlessly integrated into the stabilization framework.
              Experimental results on a variety of real video sequences verify
              the effectiveness of our method. \copyright{} 2013 IEEE.",
  journal  = "IEEE Conference on Computer Vision and Pattern Recognition",
  number   =  1,
  pages    = "2299--2306",
  year     =  2013
}

@ARTICLE{Zhou2015-ff,
  title     = "A Revisit of Methods for Determining the Fundamental Matrix with
               Planes",
  author    = "Zhou, Yi and Kneip, Laurent and Li, Hongdong",
  abstract  = "Determining the fundamental matrix from a collection of
               inter-frame homographies (more than two) is a classical problem.
               The compatibility relationship between the fundamental matrix
               and any of the ideally consistent homographies can be used to
               compute the fundamental matrix. Using the direct linear
               transformation (DLT), the compatibility equation can be
               translated into a least squares problem and can be easily solved
               via SVD decomposition. However, this solution is extremely
               susceptible to noise and motion inconsistencies, hence rarely
               used. Inspired by the normalized eight-point algorithm, we show
               that a relatively simple but non-Trivial two-step normalization
               of the input homographies achieves the desired effect, and the
               results are at last comparable to the less attractive
               hallucinated points method. The algorithm is theoretically
               justified and verified by experiments on both synthetic and real
               data.",
  journal   = "2015 International Conference on Digital Image Computing:
               Techniques and Applications, DICTA 2015",
  publisher = "IEEE",
  pages     = "1--7",
  year      =  2015
}
