\begin{table}[t]
\small
\centering
\caption{\textbf{Scene understanding with end-to-end finetuning.} After pretraining, models are equipped with task-specific decoders and finetuned for that task on the entire downstream dataset. 
\textsuperscript{\textdagger}indicates results are taken from \cite{he2021masked}, using UperNet \cite{xiao2018unified} as the decoder. 
Results for all other baselines are taken from \cite{caron2022location} and use the linear decoder from \cite{strudel2021segmenter}. For ViT-L results, see Appendix \ref{sec:app-vitl-finetuning}.
}
\vspace{-0.5em}
\begin{tabularx}{\columnwidth}{l *{6}{Y}}

\multicolumn{3}{c}{} & \multicolumn{2}{c}{Fine-tuned accuracy (mIoU)} \\
\cmidrule(l){4-5}
Method & Encoder & Dataset & PASCAL $\uparrow$ & ADE20K $\uparrow$ \\
\midrule										
Random	&	ViT-B	&	IN1K	&	29.1	&	21.1	\\
Supervised \cite{touvron2022deit}	&	ViT-B	&	IN1K	&	76.1	&	47.3	\\
DINO \cite{caron2021emerging}	&	ViT-B	&	IN1K	&	74.1	&	44.1	\\
MoCo-v3 \cite{chen2021empirical}	&	ViT-B	&	IN1K	&	74.5 &	\hspace{0.4em}47.3\textsuperscript{\textdagger}	\\
BEiT \cite{bao2021beit}	&	ViT-B	&	\mbox{IN1K+DALLE \cite{ramesh2021zeroshot}}	&	-	&	\hspace{0.4em}47.1\textsuperscript{\textdagger}	\\
MAE \cite{he2021masked}	&	ViT-B	&	IN1K	&	75.0	&	\hspace{0.4em}48.1\textsuperscript{\textdagger}	\\
LOCA \cite{caron2022location}	&	ViT-B	&	IN1K	&	76.7	&	47.9	\\
\rowcolor{DnCBG}\oursb	&	ViT-B	&	IN1K	&	80.0	&	44.9	\\
\rowcolor{DnCBG}\oursupb	&	ViT-B	&	IN1K	&	81.2	&	44.9	\\
\rowcolor{DnCBG}\oursb	&	ViT-B	&	IN22K	&	81.6	&	46.9	\\
\rowcolor{DnCBG}\oursupb	&	ViT-B	&	IN22K	&	\textbf{82.1}	&	\textbf{48.2}	\\
\end{tabularx}
\vspace{-0.3em}
\label{tab:finetuning}
\end{table}