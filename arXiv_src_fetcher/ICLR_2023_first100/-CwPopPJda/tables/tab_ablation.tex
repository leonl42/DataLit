\begin{table}[t]
% \vspace{1.em}
\small
\centering
\caption{\textbf{Ablation of pretraining components.} Effect of training with spatial attention pooling (as opposed to mean pooling or a \texttt{[CLS]} token) and memory contextualization ("Cont.") on performance. All models were pretrained with ViT-B on ImageNet-1k.}
\begin{tabularx}{\columnwidth}{l *{5}{Y}}

& & & \multicolumn{2}{c}{Semantic segmentation} & Depth pred. \\
\cmidrule(l){4-5}  \cmidrule(l){6-6}
Method & Pool. & Cont.  & PASCAL $\uparrow$ & ADE20K $\uparrow$ & NYUv2 $\downarrow$ \\
\midrule
\vspace{-0.5em} \\
MoCLR \cite{tian2021divide}                           & mean  & \xmark    & 38.6 &   4.9  & 1.01\\
\hspace{0.5em} + cont.     &      mean     & \checkmark      & 55.6 & 15.3 & .901 \\ 
\hspace{0.5em} + \texttt{[CLS]} & \texttt{[CLS]}  & \xmark    & 64.5 &   23.9  & .741\\
\hspace{0.5em} + \texttt{[CLS]} + cont. & \texttt{[CLS]}  & \checkmark  & 65.6  & 25.1 & .731  \\
\hspace{0.5em} + QK att. \cite{parthasarathy2022self} + cont.  &   QK att.    & \checkmark      & 68.7 & 26.3 & .728 \\ 
\hspace{0.5em} + QKV att.  &       QKV att.     & \xmark      & 68.0 & 27.4 & .742 \\ 
\rowcolor{DnCBG} \oursb  & QKV att.  & \checkmark & \textbf{70.5} & \textbf{28.3} & \textbf{.718}\\
\end{tabularx}
\vspace{-0.3em}
\label{tab:ablation}
\end{table}
