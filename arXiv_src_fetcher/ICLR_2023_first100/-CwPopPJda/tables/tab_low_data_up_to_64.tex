\begin{table}[t]
\vspace{1.em}
\small
\centering
\caption{\textbf{Data-efficient scene understanding.} After pretraining, models are adapted to downstream tasks on small amounts of data
with end-to-end fine-tuning with a linear head (\evalft) or with nearest neighbor retrieval (\nneval). \(n\) refers to the number of images a fraction represents. All runs are averaged over five different seeds, with standard deviation of the order of 0.04 / 0.10\% for NN and 0.36 / 1.35\% for \evalft on PASCAL / ADE20K. Each method is trained with ViT-B on ImageNet-1k.}
\begin{tabularx}{\columnwidth}{l *{7}{Y}} &
                    & \multicolumn{2}{c}{PASCAL $\uparrow$}                                   & \multicolumn{2}{c}{ADE20K $\uparrow$}                                           \\ \cmidrule(l){3-4} \cmidrule(l){5-6} 
                    \multicolumn{1}{l}{Method} & \multicolumn{1}{c}{Decoder} & \multicolumn{1}{c}{1/128 (\(n\)=83)} & \multicolumn{1}{c}{1/64 (\(n\)=165)} & \multicolumn{1}{c}{1/128 (\(n\)=158)} & \multicolumn{1}{c}{1/64 (\(n\)=316)} \\ \hline
Supervised \cite{touvron2022deit} & \evalft	&	41.8	&	53.8	&	10.8	&	14.3	\\
DINO \cite{caron2021emerging} & \evalft	&	36.1	&	44.3	&	11.7	&	14.4	\\
MoCo-v3 \cite{chen2021empirical} & \evalft	&	19.9	&	33.4	&	4.6	&	7.9	\\
MAE \cite{he2021masked} & \evalft	&	34.2	&	44.1	&	8.2	&	12.2	\\
LOCA \cite{caron2022location} & \evalft	&	40.1	&	53.9	&	11.2	&	15.5	\\
\rowcolor{DnCBG}\oursb   & \nneval	&	50.5	&	57.2	&	11.7	&	15.1	\\
\rowcolor{DnCBG}\oursupb & \nneval	&	\textbf{52.4}	&	\textbf{57.3}	&	\textbf{12.7}	&	\textbf{16.4}      
\end{tabularx}
\label{tab:data_eff_up_to_64}
\end{table}