\begin{table}[ht]
\small
\centering
\caption{\textbf{PASCAL VOC data efficiency analysis.} After pretraining, models are applied to downstream tasks with the indicated fraction of the dataset size. Models perform the task either with end-to-end fine-tuning with a linear head (\evalft) or with our mechanism for in-context scene understanding using nearest neighbors at evaluation time (\nneval). All fine-tuning runs are averaged over five different seeds. The metric reported is mean IoU (higher numbers are better). \textsuperscript{\textdagger} denotes models trained on ImageNet-22k; all other models were trained on ImageNet-1k.}
\begin{tabular}{@{}lcccccccccc@{}}
       & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{8}{c}{Fraction of dataset}            \\ \cmidrule(l){4-11} 
Method & Decoder              & Backbone                                 & 1/128 & 1/64 & 1/32 & 1/16 & 1/8 & 1/4 & 1/2 & 1/1 \\ \midrule
DeiT-III \cite{touvron2022deit}	&	\evalft	&	\mbox{ViT-B}	&	41.8	&	53.8	&	63.1	&	67.7	&	70.7	&	72.2	&	73.4	&	75.2	\\
DINO \cite{caron2021emerging}	&	\evalft	&	\mbox{ViT-B}	&	36.1	&	44.3	&	54.3	&	57.8	&	61.7	&	64.8	&	68.2	&	72.2	\\
MoCo-v3 \cite{chen2021empirical}	&	\evalft	&	\mbox{ViT-B}	&	19.9	&	33.4	&	47.0	&	54.8	&	61.5	&	67.1	&	70.7	&	73.4	\\
MAE \cite{he2021masked}	&	\evalft	&	\mbox{ViT-B}	&	34.2	&	44.1	&	53.0	&	58.7	&	62.7	&	67.4	&	70.8	&	73.5	\\
LOCA \cite{caron2022location}	&	\evalft	&	\mbox{ViT-B}	&	40.1	&	53.9	&	63.1	&	67.8	&	70.7	&	72.8	&	74.4	&	75.5	\\
\rowcolor{DnCBG}\oursb	&	\nneval	&	\mbox{ViT-B}	&	50.5	&	57.2	&	60.1	&	62.6	&	64.3	&	65.9	&	68.9	&	71.8	\\
\rowcolor{DnCBG}\oursupb	&	\nneval	&	\mbox{ViT-B}	&	52.4	&	57.3	&	61.5	&	64.6	&	66.2	&	67.9	&	70.5	&	73.2	\\
\rowcolor{DnCBG}\oursupb\textsuperscript{\textdagger}	&	\nneval	&	\mbox{ViT-L}	&	\textbf{61.8}	&	\textbf{65.3}	&	\textbf{68.0}	&	\textbf{70.7}	&	\textbf{71.4}	&	\textbf{73.2}	&	\textbf{75.3}	&	\textbf{77.2}	\\
\end{tabular}
\label{tab:appendix_pascal_data_eff}
\end{table}

\begin{table}[ht]
\small
\centering
\caption{\textbf{ADE20K data efficiency analysis.} After pretraining, models are applied to downstream tasks with the indicated fraction of the dataset size. Models perform the task either with end-to-end fine-tuning with a linear head (\evalft) or with our mechanism for in-context scene understanding using nearest neighbors at evaluation time (\nneval). All fine-tuning runs are averaged over five different seeds. The metric reported is mean IoU (higher numbers are better). The results for other techniques between \(1/32\) and \(1/1\) are sourced directly from \cite{caron2022location}, the rest are reproductions. \textsuperscript{\textdagger} denotes models trained on ImageNet-22k; all other models were trained on ImageNet-1k.}
\begin{tabular}{@{}lcccccccccc@{}}
       & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{8}{c}{Fraction of dataset}            \\ \cmidrule(l){4-11} 
Method & Decoder              & Backbone                                 & 1/128 & 1/64 & 1/32 & 1/16 & 1/8 & 1/4 & 1/2 & 1/1 \\ \midrule
DeiT-III \cite{touvron2022deit}	&	\evalft	&	\mbox{ViT-B}	&	10.8	&	14.3	&	20.9	&	27.1	&	32.7	&	38.3	&	42.0	&	47.3	\\
DINO \cite{caron2021emerging}	&	\evalft	&	\mbox{ViT-B}	&	11.7	&	14.4	&	18.4	&	24.5	&	29.5	&	35.2	&	39.5	&	44.1	\\
MoCo-v3 \cite{chen2021empirical}	&	\evalft	&	\mbox{ViT-B}	&	4.6	&	7.9	&	17.7	&	25.2	&	30.8	&	36.5	&	40.7	&	45.4	\\
MAE \cite{he2021masked}	&	\evalft	&	\mbox{ViT-B}	&	8.2	&	12.2	&	18.4	&	25.3	&	30.5	&	36.1	&	40.6	&	45.5	\\
LOCA \cite{caron2022location}	&	\evalft	&	\mbox{ViT-B}	&	11.2	&	15.5	&	22.2	&	\textbf{30.0}	&	\textbf{34.4}	&	\textbf{39.1}	&	\textbf{42.8}	&	\textbf{47.9}	\\
\rowcolor{DnCBG}\oursb	&	\nneval	&	\mbox{ViT-B}	&	11.7	&	15.1	&	17.3	&	20.0	&	22.3	&	24.9	&	27.9	&	29.6	\\
\rowcolor{DnCBG}\oursupb	&	\nneval	&	\mbox{ViT-B}	&	12.7	&	16.4	&	18.9	&	21.5	&	24.0	&	26.8	&	29.9	&	32.0	\\
\rowcolor{DnCBG}\oursupb\textsuperscript{\textdagger}	&	\nneval	&	\mbox{ViT-L}	&	\textbf{16.6}	&	\textbf{20.5}	&	\textbf{24.0}	&	27.4	&	30.2	&	33.1	&	36.0	&	37.8	\\
\end{tabular}
\label{tab:appendix_ade20k_data_eff}
\end{table}