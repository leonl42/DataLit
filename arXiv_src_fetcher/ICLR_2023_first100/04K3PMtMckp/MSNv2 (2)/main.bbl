\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Khandeparkar, Khodak, Plevrakis, and
  Saunshi]{arora2019theoretical}
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and
  Nikunj Saunshi.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock \emph{arXiv preprint arXiv:1902.09229}, 2019.

\bibitem[Asano et~al.(2019)Asano, Rupprecht, and Vedaldi]{asano2019self}
Yuki~Markus Asano, Christian Rupprecht, and Andrea Vedaldi.
\newblock Self-labelling via simultaneous clustering and representation
  learning.
\newblock \emph{arXiv preprint arXiv:1911.05371}, 2019.

\bibitem[Assran et~al.(2020)Assran, Ballas, Castrejon, and
  Rabbat]{assran2020supervision}
Mahmoud Assran, Nicolas Ballas, Lluis Castrejon, and Michael Rabbat.
\newblock Supervision accelerates pre-training in contrastive semi-supervised
  learning of visual representations.
\newblock \emph{arXiv preprint arXiv:2006.10803}, 2020.

\bibitem[Assran et~al.(2021)Assran, Caron, Misra, Bojanowski, Joulin, Ballas,
  and Rabbat]{assran2021semi}
Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin,
  Nicolas Ballas, and Michael Rabbat.
\newblock Semi-supervised learning of visual features by non-parametrically
  predicting view assignments with support samples.
\newblock \emph{arXiv preprint arXiv:2104.13963}, 2021.

\bibitem[Assran et~al.(2022)Assran, Caron, Misra, Bojanowski, Bordes, Vincent,
  Joulin, Rabbat, and Ballas]{assran2022masked}
Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes,
  Pascal Vincent, Armand Joulin, Michael Rabbat, and Nicolas Ballas.
\newblock Masked siamese networks for label-efficient learning.
\newblock \emph{arXiv preprint arXiv:2204.07141}, 2022.

\bibitem[Awasthi et~al.(2015)Awasthi, Bandeira, Charikar, Krishnaswamy, Villar,
  and Ward]{awasthi2015relax}
Pranjal Awasthi, Afonso~S Bandeira, Moses Charikar, Ravishankar Krishnaswamy,
  Soledad Villar, and Rachel Ward.
\newblock Relax, no need to round: Integrality of clustering formulations.
\newblock In \emph{Proceedings of the 2015 Conference on Innovations in
  Theoretical Computer Science}, pp.\  191--200, 2015.

\bibitem[Bachman et~al.(2019)Bachman, Hjelm, and
  Buchwalter]{bachman2019learning}
Philip Bachman, R~Devon Hjelm, and William Buchwalter.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Baevski et~al.(2022)Baevski, Hsu, Xu, Babu, Gu, and
  Auli]{baevski2022data2vec}
Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael
  Auli.
\newblock Data2vec: A general framework for self-supervised learning in speech,
  vision and language.
\newblock \emph{arXiv preprint arXiv:2202.03555}, 2022.

\bibitem[Balestriero \& LeCun(2022)Balestriero and
  LeCun]{balestriero2022contrastive}
Randall Balestriero and Yann LeCun.
\newblock Contrastive and non-contrastive self-supervised learning recover
  global and local spectral embedding methods.
\newblock \emph{arXiv preprint arXiv:2205.11508}, 2022.

\bibitem[Bao et~al.(2021)Bao, Dong, and Wei]{bao2021beit}
Hangbo Bao, Li~Dong, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock \emph{arXiv preprint arXiv:2106.08254}, 2021.

\bibitem[Bardes et~al.(2021)Bardes, Ponce, and LeCun]{bardes2021vicreg}
Adrien Bardes, Jean Ponce, and Yann LeCun.
\newblock Vicreg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2105.04906}, 2021.

\bibitem[Bordes et~al.(2022{\natexlab{a}})Bordes, Balestriero, Garrido, Bardes,
  and Vincent]{bordes2022guillotine}
Florian Bordes, Randall Balestriero, Quentin Garrido, Adrien Bardes, and Pascal
  Vincent.
\newblock Guillotine regularization: Improving deep networks generalization by
  removing their head.
\newblock \emph{arXiv preprint arXiv:2206.13378}, 2022{\natexlab{a}}.

\bibitem[Bordes et~al.(2022{\natexlab{b}})Bordes, Balestriero, and
  Vincent]{bordes2022high}
Florian Bordes, Randall Balestriero, and Pascal Vincent.
\newblock High fidelity visualization of what your self-supervised
  representation knows about.
\newblock \emph{Transactions on Machine Learning Research}, 2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=urfWb7VjmL}.

\bibitem[Bradley et~al.(2000)Bradley, Bennett, and
  Demiriz]{bradley2000constrained}
Paul~S Bradley, Kristin~P Bennett, and Ayhan Demiriz.
\newblock Constrained k-means clustering.
\newblock \emph{Microsoft Research, Redmond}, 20, 2000.

\bibitem[Bridle et~al.(1991)Bridle, Heading, and
  MacKay]{bridle1991unsupervised}
John Bridle, Anthony Heading, and David MacKay.
\newblock Unsupervised classifiers, mutual information and'phantom targets.
\newblock \emph{Advances in neural information processing systems}, 4, 1991.

\bibitem[Bromley et~al.(1993)Bromley, Bentz, Bottou, Guyon, LeCun, Moore,
  S{\"a}ckinger, and Shah]{bromley1993signature}
Jane Bromley, James~W Bentz, L{\'e}on Bottou, Isabelle Guyon, Yann LeCun, Cliff
  Moore, Eduard S{\"a}ckinger, and Roopak Shah.
\newblock Signature verification using a “siamese” time delay neural
  network.
\newblock \emph{International Journal of Pattern Recognition and Artificial
  Intelligence}, 7\penalty0 (04):\penalty0 669--688, 1993.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock \emph{arXiv preprint arXiv:2006.09882}, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal,
  Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock \emph{arXiv preprint arXiv:2104.14294}, 2021.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Radford, Child, Wu, Jun, Luan,
  and Sutskever]{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1691--1703. PMLR, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock \emph{preprint arXiv:2002.05709}, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Luo, and Li]{chen2021intriguing}
Ting Chen, Calvin Luo, and Lala Li.
\newblock Intriguing properties of contrastive losses.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 11834--11845, 2021.

\bibitem[Chen \& He(2020)Chen and He]{chen2020exploring}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock \emph{arXiv preprint arXiv:2011.10566}, 2020.

\bibitem[Chen et~al.(2020{\natexlab{c}})Chen, Fan, Girshick, and
  He]{chen2020mocov2}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{arXiv preprint arXiv:2003.04297}, 2020{\natexlab{c}}.

\bibitem[Chen et~al.(2022)Chen, Bardes, Li, and LeCun]{chen2022intra}
Yubei Chen, Adrien Bardes, Zengyi Li, and Yann LeCun.
\newblock Intra-instance vicreg: Bag of self-supervised image patch embedding.
\newblock \emph{arXiv preprint arXiv:2206.08954}, 2022.

\bibitem[Cuturi(2013)]{cuturi2013sinkhorn}
Marco Cuturi.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[El-Nouby et~al.(2021)El-Nouby, Izacard, Touvron, Laptev, Jegou, and
  Grave]{el2021large}
Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv{\'e}
  Jegou, and Edouard Grave.
\newblock Are large-scale datasets necessary for self-supervised pre-training?
\newblock \emph{arXiv preprint arXiv:2112.10740}, 2021.

\bibitem[Garrido et~al.(2022)Garrido, Chen, Bardes, Najman, and
  Lecun]{garrido2022duality}
Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, and Yann Lecun.
\newblock On the duality between contrastive and non-contrastive
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2206.02574}, 2022.

\bibitem[Geiger et~al.(2013)Geiger, Lenz, Stiller, and
  Urtasun]{geiger2013vision}
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.
\newblock Vision meets robotics: The kitti dataset.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1231--1237, 2013.

\bibitem[Gidaris et~al.(2020)Gidaris, Bursuc, Komodakis, P{\'e}rez, and
  Cord]{gidaris2020learning}
Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P{\'e}rez, and Matthieu
  Cord.
\newblock Learning representations by predicting bags of visual words.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  6928--6938, 2020.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Goyal et~al.(2021)Goyal, Duval, Reizenstein, Leavitt, Xu, Lefaudeux,
  Singh, Reis, Caron, Bojanowski, Joulin, and Misra]{goyal2021vissl}
Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew Leavitt, Min Xu,
  Benjamin Lefaudeux, Mannat Singh, Vinicius Reis, Mathilde Caron, Piotr
  Bojanowski, Armand Joulin, and Ishan Misra.
\newblock Vissl.
\newblock \url{https://github.com/facebookresearch/vissl}, 2021.

\bibitem[Goyal et~al.(2022)Goyal, Duval, Seessel, Caron, Singh, Misra, Sagun,
  Joulin, and Bojanowski]{goyal2022vision}
Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan
  Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski.
\newblock Vision models are more robust and fair when pretrained on uncurated
  images without supervision.
\newblock \emph{arXiv preprint arXiv:2202.08360}, 2022.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, et~al.]{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec,
  Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2006.07733}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2019)He, Fan, Wu, Xie, and Girshick]{he2019moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock \emph{arXiv preprint arXiv:1911.05722}, 2019.

\bibitem[He et~al.(2021)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2021masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock \emph{arXiv preprint arXiv:2111.06377}, 2021.

\bibitem[Hjelm et~al.(2018)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman,
  Trischler, and Bengio]{hjelm2018learning}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
  Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock \emph{arXiv preprint arXiv:1808.06670}, 2018.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ddpm_2020}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin (eds.), \emph{Advances in Neural
  Information Processing Systems 33: Annual Conference on Neural Information
  Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html}.

\bibitem[Hoffman \& Johnson(2016)Hoffman and Johnson]{hoffman2016elbo}
Matthew~D Hoffman and Matthew~J Johnson.
\newblock Elbo surgery: yet another way to carve up the variational evidence
  lower bound.
\newblock In \emph{Workshop in Advances in Approximate Bayesian Inference,
  NIPS}, volume~1, 2016.

\bibitem[Hornik et~al.(2012)Hornik, Feinerer, Kober, and
  Buchta]{hornik2012spherical}
Kurt Hornik, Ingo Feinerer, Martin Kober, and Christian Buchta.
\newblock Spherical k-means clustering.
\newblock \emph{Journal of statistical software}, 50:\penalty0 1--22, 2012.

\bibitem[Hu et~al.(2017)Hu, Miyato, Tokui, Matsumoto, and
  Sugiyama]{hu2017learning}
Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama.
\newblock Learning discrete representations via information maximizing
  self-augmented training.
\newblock In \emph{International conference on machine learning}, pp.\
  1558--1567. PMLR, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and
  Szegedy]{https://doi.org/10.48550/arxiv.1502.03167}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift, 2015.
\newblock URL \url{https://arxiv.org/abs/1502.03167}.

\bibitem[Johnson et~al.(2017)Johnson, Hariharan, Van Der~Maaten, Fei-Fei,
  Lawrence~Zitnick, and Girshick]{johnson2017clevr}
Justin Johnson, Bharath Hariharan, Laurens Van Der~Maaten, Li~Fei-Fei,
  C~Lawrence~Zitnick, and Ross Girshick.
\newblock Clevr: A diagnostic dataset for compositional language and elementary
  visual reasoning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2901--2910, 2017.

\bibitem[Kleindessner et~al.(2019)Kleindessner, Awasthi, and
  Morgenstern]{kleindessner2019fair}
Matth{\"a}us Kleindessner, Pranjal Awasthi, and Jamie Morgenstern.
\newblock Fair k-center clustering for data summarization.
\newblock In \emph{ICML}, pp.\  3448--3457. PMLR, 2019.

\bibitem[Krause et~al.(2010)Krause, Perona, and
  Gomes]{krause2010discriminative}
Andreas Krause, Pietro Perona, and Ryan Gomes.
\newblock Discriminative clustering by regularized information maximization.
\newblock \emph{Advances in neural information processing systems}, 23, 2010.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun(2022)]{lecun2022path}
Yann LeCun.
\newblock A path towards autonomous machine intelligence version 0.9. 2,
  2022-06-27.
\newblock 2022.

\bibitem[LeCun \& Cortes(2010)LeCun and
  Cortes]{lecun-mnisthandwrittendigit-2010}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Liang et~al.(2012)Liang, Bai, Dang, and Cao]{liang2012k}
Jiye Liang, Liang Bai, Chuangyin Dang, and Fuyuan Cao.
\newblock The $ k $-means-type algorithms versus imbalanced data distributions.
\newblock \emph{IEEE Transactions on Fuzzy Systems}, 20\penalty0 (4):\penalty0
  728--745, 2012.

\bibitem[Linsker(1988)]{linsker1988self}
Ralph Linsker.
\newblock Self-organization in a perceptual network.
\newblock \emph{Computer}, 21\penalty0 (3):\penalty0 105--117, 1988.

\bibitem[Ma et~al.(2022)Ma, Tsao, and Shum]{ma2022principles}
Yi~Ma, Doris Tsao, and Heung-Yeung Shum.
\newblock On the principles of parsimony and self-consistency for the emergence
  of intelligence.
\newblock \emph{Frontiers of Information Technology \& Electronic Engineering},
  pp.\  1--26, 2022.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and Van Der~Maaten]{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  181--196, 2018.

\bibitem[Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and
  Dean]{https://doi.org/10.48550/arxiv.1310.4546}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Distributed representations of words and phrases and their
  compositionality, 2013.
\newblock URL \url{https://arxiv.org/abs/1310.4546}.

\bibitem[Misra \& van~der Maaten(2020)Misra and van~der Maaten]{misra2020self}
Ishan Misra and Laurens van~der Maaten.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  6707--6717, 2020.

\bibitem[Mitrovic et~al.(2020)Mitrovic, McWilliams, Walker, Buesing, and
  Blundell]{mitrovic2020representation}
Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, and Charles
  Blundell.
\newblock Representation learning via invariant causal mechanisms.
\newblock \emph{arXiv preprint arXiv:2010.07922}, 2020.

\bibitem[Newman(2005)]{newman2005power}
Mark~EJ Newman.
\newblock Power laws, pareto distributions and zipf's law.
\newblock \emph{Contemporary physics}, 46\penalty0 (5):\penalty0 323--351,
  2005.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and Chen]{dale2}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents,
  2022.
\newblock URL \url{https://arxiv.org/abs/2204.06125}.

\bibitem[Rujeerapaiboon et~al.(2019)Rujeerapaiboon, Schindler, Kuhn, and
  Wiesemann]{rujeerapaiboon2019size}
Napat Rujeerapaiboon, Kilian Schindler, Daniel Kuhn, and Wolfram Wiesemann.
\newblock Size matters: Cardinality-constrained clustering and outlier
  detection via conic optimization.
\newblock \emph{SIAM J. Optimization}, 29\penalty0 (2):\penalty0 1211--1239,
  2019.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and
  Fei-Fei]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton,
  Ghasemipour, Ayan, Mahdavi, Lopes, Salimans, Ho, Fleet, and Norouzi]{imagen}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
  Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, S.~Sara Mahdavi,
  Rapha~Gontijo Lopes, Tim Salimans, Jonathan Ho, David~J Fleet, and Mohammad
  Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.11487}.

\bibitem[Tian et~al.(2021{\natexlab{a}})Tian, Henaff, and van~den
  Oord]{tian2021divide}
Yonglong Tian, Olivier~J Henaff, and A{\"a}ron van~den Oord.
\newblock Divide and contrast: Self-supervised learning from uncurated data.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  10063--10074, 2021{\natexlab{a}}.

\bibitem[Tian et~al.(2021{\natexlab{b}})Tian, Chen, and
  Ganguli]{tian2021understanding}
Yuandong Tian, Xinlei Chen, and Surya Ganguli.
\newblock Understanding self-supervised learning dynamics without contrastive
  pairs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10268--10278. PMLR, 2021{\natexlab{b}}.

\bibitem[Tschannen et~al.(2019)Tschannen, Djolonga, Rubenstein, Gelly, and
  Lucic]{tschannen2019mutual}
Michael Tschannen, Josip Djolonga, Paul~K Rubenstein, Sylvain Gelly, and Mario
  Lucic.
\newblock On mutual information maximization for representation learning.
\newblock \emph{arXiv preprint arXiv:1907.13625}, 2019.

\bibitem[Van~Horn et~al.(2018)Van~Horn, Mac~Aodha, Song, Cui, Sun, Shepard,
  Adam, Perona, and Belongie]{van2018inaturalist}
Grant Van~Horn, Oisin Mac~Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
  Hartwig Adam, Pietro Perona, and Serge Belongie.
\newblock The inaturalist species classification and detection dataset.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  8769--8778, 2018.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Vincent et~al.(2010)Vincent, Larochelle, Lajoie, Bengio, Manzagol, and
  Bottou]{vincent2010stacked}
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine
  Manzagol, and L{\'e}on Bottou.
\newblock Stacked denoising autoencoders: Learning useful representations in a
  deep network with a local denoising criterion.
\newblock \emph{Journal of machine learning research}, 11\penalty0 (12), 2010.

\bibitem[Wang et~al.(2010)Wang, Li, and Konig]{wang2010learning}
Fei Wang, Ping Li, and Arnd~Christian Konig.
\newblock Learning a bi-stochastic data similarity matrix.
\newblock In \emph{2010 IEEE International Conference on Data Mining}, pp.\
  551--560. IEEE, 2010.

\bibitem[Wang \& Isola(2020)Wang and Isola]{wang2020understanding}
Tongzhou Wang and Phillip Isola.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9929--9939. PMLR, 2020.

\bibitem[Wei et~al.(2021)Wei, Fan, Xie, Wu, Yuille, and
  Feichtenhofer]{wei2021masked}
Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph
  Feichtenhofer.
\newblock Masked feature prediction for self-supervised visual pre-training.
\newblock \emph{arXiv preprint arXiv:2112.09133}, 2021.

\bibitem[Wu et~al.(2009)Wu, Xiong, and Chen]{wu2009adapting}
Junjie Wu, Hui Xiong, and Jian Chen.
\newblock Adapting the right measures for k-means clustering.
\newblock In \emph{Proceedings of the 15th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  877--886, 2009.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{wu2018unsupervised}
Zhirong Wu, Yuanjun Xiong, Stella~X Yu, and Dahua Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  3733--3742, 2018.

\bibitem[Xie et~al.(2019)Xie, Dai, Hovy, Luong, and Le]{xie2019unsupervised}
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc~V Le.
\newblock Unsupervised data augmentation.
\newblock \emph{arXiv preprint arXiv:1904.12848}, 2019.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock \emph{arXiv preprint arXiv:2103.03230}, 2021.

\bibitem[Zha et~al.(2001)Zha, He, Ding, Gu, and Simon]{zha2001spectral}
Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, and Horst~D Simon.
\newblock Spectral relaxation for k-means clustering.
\newblock In \emph{NeurIPS}, pp.\  1057--1064, 2001.

\bibitem[Zhou et~al.(2014)Zhou, Lapedriza, Xiao, Torralba, and
  Oliva]{zhou2014learning}
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva.
\newblock Learning deep features for scene recognition using places database.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Zhou et~al.(2021)Zhou, Wei, Wang, Shen, Xie, Yuille, and
  Kong]{zhou2021ibotyes}
Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao
  Kong.
\newblock Ibot: Image bert pre-training with online tokenizer.
\newblock \emph{arXiv preprint arXiv:2111.07832}, 2021.

\end{thebibliography}
