\section{Related work}
\label{sec:relatedwork}

We now discuss how our results relate to phenomena that have been observed or proven in the literature before.

\paragraph{Robust and non-robust useful features}
In the words of \citet{ilyas19, springer21}, for
directed attacks, all robust features become less useful, but adversarial
training uses robust features more.  In the small sample-size regime
$n<d-1$ in particular, robust learning assigns so much weight
on the robust (possibly non-useful) features, that the signal in the non-robust
features is drowned. This leads to an unavoidable and large increase
in standard error that dominates the decrease in susceptibility and
hence ultimately leads to an increase of the robust error.

\paragraph{Small sample size and robustness}
A direct consequence of Theorem~\ref{thm:linlinf} is that in order to
achieve the same robust error as standard training, adversarial
training requires more samples. This statement might remind the reader
of sample complexity results for robust generalization in
\citet{schmidt18, Yin19, Khim18}. While those results compare sample
complexity bounds for standard vs. robust error, our theorem
statement compares two algorithms, standard vs. adversarial training,
with respect to the robust error.


\paragraph{Trade-off between standard and robust error} 

Many papers observed that even though adversarial training decreases robust error compared to standard training, it may lead
to an increase in standard test error \cite{madry18, zhang19}.  
%This
%trade-off phenomenon has been theoretically studied under different
%assumptions: \cite{nakkiran19, madry18} show that small model capacity
%could result in the poor generalization performance. 
For example, \citet{tsipras19, zhang19, javanmard20, dobriban20, chen20} study settings where the Bayes optimal robust classifier is not equal to the Bayes optimal (standard)
classifier (i.e. the perturbations are inconsistent or the dataset is non-separable).
%% However, these settings inherently assume inconsistent3
%% $\ell_p$-ball perturbations, i.e. where perturbations may also change
%% the ground truth label.
%% By definition however, adversarial attacks
%% are “only useful” when they do not change the class for the ground
%% truth classifier such as the human (like for traffic sign stickers or
%% imperceptible perturbation).  Also, neural networks are known to be
%% complex enough to even fit random labels and inputs.
\cite{raghunathan20} study consistent perturbations, as in our paper,
and prove that for small sample size, fitting adversarial
examples can increase standard error even in the absence of
noise. In contrast to aforementioned works, which do not refute that
adversarial training decreases robust error, we prove that for
\nameofattacks perturbations, in the small sample regime adversarial training may also increase \emph{robust error}.

%As a reaction to the trade-off phenomenon,
\paragraph{Mitigation of the trade-off} 
A long line of work has proposed procedures to 
mitigate the trade-off phenomenon.  For example \citet{alayrac19,
  Carmon19, zhai20, raghunathan20} study robust self training, which
leverages a large set of unlabelled data, while \citet{lee20, lamb19,
  xu20} use data augmentation by interpolation. \citet{Ding20,
  balaji19, Cheng20} on the other hand propose to use adaptive
perturbation budgets $\epstrain$ that vary across inputs. 
%Their
%approach is closest to the story of this paper, although it is
%motivated by inconsistent perturbations. 
%Some of these mitigation approaches also result in an increase in robust accuracy against
%$\ell_p$-perturbations and hence might be candidates to successfully increase robust accuracy for \nameofattacks perturbations in the small sample size regime.
Our intuition from the theoretical analysis suggests that the standard
mitigation procedures for imperceptible perturbations may not work for
perceptible \nameofattacks, because all relevant features are non-robust.
%% robustness inherently requires
%% using the signal component in the data less. 
%% less weight on
%% the signal component in the data.
We leave a thorough empirical study
as interesting future work.

%\paragraph{Perceptible adversarial perturbations} \jc{doubting}
%% Multiple works have considered perceptible perturbations in the form of adversarial perceptible perturbations or common image corruptions. For example stickers on traffic signs \cite{Eykholt18}, adversarially coloured glasses or face-stickers \cite{Wu20} and adversarial watermarks \cite{Hayes18}. The common defences against these attacks are data-augmentation techniques such as adversarial training and variants thereof. 

 %adaptive perturbation budgets $\epstrain$ are proposed.
%% In many cases we observe that adversarial training, while gaining some robust accuracy, can cause a drop in clean test accuracy \cite{madry18, zhang19}. In \cite{tsipras19, zhang19} they advocate that the drop in standard accuracy stems from a general trade-off between the standard and robust accuracy objectives. In contrast, \cite{yang20} shows that there exist solutions in commonly used function spaces that achieve both objectives on common datasets. We show that adversarial training with signal-attacking perturbations can also hurt robust generalization. \jc{would need to get this in there aboout sample complexity} In \cite{schmidt18}, they show that finding robust classifiers may require more samples whereas in \cite{nakkiran19, madry18} they find that robustness may require a larger model capacity.

%% Recent work has tried to come up with different approaches to mitigate
%% the drop in standard accuracy. For example \cite{alayrac19, Carmon19, zhai20, raghunathan20} propose robust self training, which leverages unlabelled data, the works \cite{lee20, lamb19, xu20} use data augmentation by interpolation and in the works \cite{Ding20, balaji19, Cheng20} adaptive perturbation budgets $\epstrain$ are proposed. We expect the methods that are able to increase standard accuracy or fully mitigate the drop, to also help adversarial training in the small sample size regime. In particular, robust self-training may help, but is because of the large unlabelled data set recourse and time expensive. 
%% In \cite{ilyas19, springer21} they characterize the existence of robust useful features, useful features and non-useful features. Our work aligns with this perspective. In the low sample regime, adversarial training causes the classifier to rely heavier on non-useful features to classify the dataset. This in contrast to the high sample regime where adversarial training gains robust accuracy by causing the classifier to comparatively rely less on non-robust useful features instead then more on non-useful features. 

%% In \cite{bhattacharjee20} they note that adversarial training can hurt robust generalisation one sample at the time. We characterize the phenomenon and give statistical bounds. In \cite{Bornschein20, Brigato21} they consider deep learning in the low sample regime. Both works find that smaller models may outperform larger commonly used models in the low sample regime. Our results comply with the results published there, but we note the high dependence on the specific dataset.



%% \paragraph{Non-perfect adversarial training}
%% On the other hand, recent empirical and theoretical papers have noted that test
%% robust accuracy might decrease with robust training accuracy and hence regularizing model complexity could benefit robust generalization \cite{rice20, Sanyal20, Donhauser}, a phenomenon referred to as robust overfitting. 
%% \fy{whats this adversarial feature overfitting - are there other overfitting things}
%% \fy{dunno if that stuff on non-robust features is any relevant}

%% Here we have perfectly certified models and still have that it hurts.
%% \fy{certification is in some sense uber perfect fitting attack models - should be even worse than just interpolating on adversarial examples?}

%% Attack-model overfitting
%% We want to emphasize that usually

%% Recent work has tried to come up with different approaches to mitigate
%% the drop in standard accuracy 
%% some of which show final improvement in ....
%% We expect them to work well 
