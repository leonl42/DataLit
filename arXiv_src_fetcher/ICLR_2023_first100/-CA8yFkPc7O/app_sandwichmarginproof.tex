\subsection{Proof of Lemma~\ref{lem:maxmargin}}
\label{sec:maxmarginproof}


We start by proving that $\thetahat{}$ is of the form
\begin{equation}
\label{Eq:max_margin_param_form_total_D}
\thetahat{} = \left[\constone, \constwo \thetatilde \right],
\end{equation}
for $\constone, \constwo > 0$. Denote by $\decplanegen{\theta}$ the plane through the origin with normal $\theta$. We define $d\left((x,y), \decplanegen{\theta} \right)$ as the signed euclidean distance from the point $(x,y) \in \data \sim \prob_{\sigsep}$ to the plane $\decplanegen{\theta}$. The signed euclidean distance is the defined as the euclidean distance from x to the plane if the point $(x,y)$ is correctly predicted by $\theta$, and the negative euclidean distance from $x$ to the plane otherwise. We rewrite the definition of the max $l_2$-margin classifier. It is the classifier induced by the  normalized vector $\thetahat{}$, such that 
\begin{equation*}
\max_{\theta \in \mathbb{R}^{\dims}} \min_{(x,y) \in \data}d\left( \left(x,y\right),\decplanegen{\theta}\right)  = \min_{(x,y) \in \data} d\left( \left(x,y \right),\decplanegen{\thetahat }\right).
\end{equation*}
We use that $\data$ is deterministic in its first coordinate and get
\begin{equation*}
\begin{split}
	\max_{\theta}\min_{(x,y) \in \data}d\left(\left(x,y\right), \decplanegen{\theta} \right) &= \max_{\theta}\min_{(x,y) \in \data} y (\thetaind{1} \xind{1} + \thetatilde^{\top} \xnonsig)\\
	&= \max_{\theta}  \theta_1  \frac{r}{2} + \min_{(x,y) \in \data}  y \thetatilde^{\top} \Tilde{x}.
	\end{split}
\end{equation*}
Because $\sigsep >0$, the maximum over all $\theta$ has $\thetahatind{}{1} \geq 0$. Take any $a > 0$ such that $\|\thetatilde\|_2 = a$.  By definition the max $l_2$-margin classifier, $\thetatilde$, maximizes $\min_{(x,y) \in \data} d\left(\left(x,y\right), \decplanegen{\theta} \right)$. Therefore, $\thetahat{}$ is of the form of Equation \eqref{Eq:max_margin_param_form_total_D}. 

Note that all classifiers induced by vectors of the form of Equation \eqref{Eq:max_margin_param_form_total_D} classify $\data$ correctly.  Next, we aim to find expressions for $\constone$ and $\constwo$ such that Equation \eqref{Eq:max_margin_param_form_total_D} is the normalized max $l_2$-margin classifier. The distance from any $x \in \data$ to $\decplanegen{\thetahat{}}$ is
\begin{equation*}
d\left(x,\decplanegen{\thetahat{}} \right) = \left| \constone \xind{1}  + \constwo \thetatilde^{\top} \xnonsig \right|.
\end{equation*}
Using that $\xind{1} = y \frac{\sigsep}{2}$ and that the second term equals $\constwo d\left(x, \decplanegen{\thetatilde}\right)$, we get
\begin{equation}
\label{eq:distance_to_opt_intermidate}
d\left(x,  \decplanegen{\thetahat{}}\right) =  \left| \constone \frac{\sigsep}{2}  + \constwo d\left(x, \decplanegen{\thetatilde}\right) \right| = \constone \frac{\sigsep}{2}  + \sqrt{1-\constone^2} d\left(x, \decplanegen{\thetatilde}\right).
\end{equation}
Let $(\xnonsig,y) \in \datanonsig$ be the point closest in Euclidean
distance to $\thetatilde$. This point is also the closest point in
Euclidean distance to $\decplanegen{\thetahat{}}$, because by Equation
\eqref{eq:distance_to_opt_intermidate} $d\left(x,
\decplanegen{\thetahat{}}\right)$ is strictly decreasing for
decreasing $d\left(x, \decplanegen{\thetatilde}\right)$. We maximize
the minimum margin $d\left(x, \decplanegen{\thetahat{}} \right)$ with
respect to $\constone$. Define the vectors $a = \left[\constone,
  \constwo\right]$ and $v = \left[\frac{\sigsep}{2}, d\left(x,
  \decplanegen{\thetatilde}\right)\right]$. We find using the dual
norm that
\begin{equation*}
a = \frac{v}{\|v\|_2}.
\end{equation*}
Plugging the expression of $a$ into Equation
\eqref{Eq:max_margin_param_form_total_D} yields that $\thetahat{}$ is
given by
\begin{equation*}
	\thetahat{} = \frac{1}{\sqrt{\sigsep^2 + 4 \marginnonsig^2}}\left[\sigsep,  2 \marginnonsig\thetatilde \right].
\end{equation*}

For the second part of the lemma we first decompose
\begin{equation}
  \label{eq:jacob}
\prob_{\sigseptest} (Y\thetahat{\top} X >0 ) = \frac{1}{2}\prob_{\sigseptest} \left[ \thetahat{\top} X >0 \mid Y=1 \right]  +\frac{1}{2}\prob_{\sigseptest} \left[\thetahat{\top} X <0 \mid Y=-1\right]\nonumber
\end{equation}
%The expected robust accuracy over $\prob_\sigsep$ we then obtain
%Since $\|\thetatilde\|_2 = 1$ and $\indof{\thetahat{}}{i} = \indof{\thetatilde}{i}$ with $\thetatilde = (\indof{\thetahat{}}{2}, \dots, \indof{\thetahat{}}{\dims})$,
We can further write 
\begin{align}
  \label{eq:cumul1}
\prob_{\sigseptest} \left[\thetahat{\top} X > 0 \mid
  Y = 1\right] &=\prob_{\sigseptest} \left[\sum_{i=2}^{\dims}\indof{\thetahat{}}{i} \indof{X}{i} > -
  \indof{\thetahat{}}{1} \: \indof{X}{1} \mid Y=1\right]\\
&= \prob_{\sigseptest} \left[2 \marginnonsig \sum_{i=1}^{\dims-1}\indof{\thetatilde}{i} \indof{X}{i} > -
  \sigsep \: \frac{\sigseptest}{2} \mid Y=1\right]\nonumber\\
&= 1-\Phi\left(-\frac{\sigsep\: \sigseptest}{4\mixvar \marginnonsig} \right) =
\Phi\left(\frac{\sigsep \: \sigseptest}{4\mixvar \marginnonsig} \right) \nonumber
\end{align}
where $\Phi$ is the cumulative distribution function. The second equality
follows by multiplying by the normalization constant on both sides and the
third equality is due to the fact that $\sum_{i=1}^{\dims-1}\indof{\thetatilde}{i} \indof{X}{i}$ is
a zero-mean Gaussian with variance $\sigma^2\|\thetatilde\|^2_2 = \sigma^2$ since $\thetatilde$ is normalized.
Correspondingly we can write
\begin{align}
  \label{eq:cumul2}
\prob_{\sigseptest} \left[\thetahat{\top} X < 0 \mid
  Y = -1\right] &=\prob_{\sigseptest} \left[2\marginnonsig
  \sum_{i=1}^{\dims-1}\indof{\thetatilde}{i} \indof{X}{i} < -
  \sigsep \left(- \frac{\sigseptest}{2}\right) \mid Y=-1\right] = \Phi\left(\frac{\sigsep \:\sigseptest}{4\mixvar \marginnonsig}\right) 
\end{align}
so that we can
combine~\eqref{eq:jacob} and~\eqref{eq:cumul1} and \eqref{eq:cumul2} to obtain
$\prob_{\sigseptest} (Y\thetahat{\top} X >0 ) = \Phi \left(\frac{\sigsep \:\sigseptest}{4\mixvar \marginnonsig}\right)$. This concludes the proof of the lemma.


%% \subsection{Proof of Lemma~\ref{lem:sandwichmargin}}
%% \label{sec:sandwichmarginproof}

%% For the proof of Lemma~\ref{lem:sandwichmargin} we use a lemma that gives bounds on the max $l_2$-margin of a dataset drawn from the marginal distribution of the $\dims - 1$ last coordinates of the covariates of $\prob_{\sigsep}$. We denote the marginal distribution by $\margprob$. We state the lemma here and give the proof in Subsection \label{sec:boundsmaxmargin}.

%% \begin{lemma}
%% \label{lem:boundsmaxmargin}
%% Define 
%% Let $\datanonsig=\{(\Tilde{x}_i,y_i)\}_{i=1}^{\numsamp} \sim \margprob$ and define $\mathcal{V} \subset \mathbb{R}^{\dims-1}$ as the set of vectors inducing classifiers that achieve perfect training accuracy on $\data$. We recall
%% \begin{equation*}
%% \marginnonsig= \max_{v \in \mathcal{V}, \|v\|=1} \min_{j \in [\numsamp]} | v^{\top} \Tilde{x}_j |.
%% \end{equation*}
%% Then, for any $t \geq 0$, with a probability greater than $1-2e^{-\frac{t^2}{2}}$,
%% \begin{equation}
%% \begin{split}
%% 	&\marginnonsig \leq \mixvar\left(1+ \frac{t+\sqrt{\dims-1}}{\sqrt{\numsamp}}\right)\\
%% 	&\marginnonsig \geq \mixvar\left( \frac{\sqrt{\dims-1}-t}{\sqrt{\numsamp}}-1\right).
%% \end{split}
%% \end{equation}
%% Moreover, 
%% \begin{equation}
%% \begin{split}
%% 	&\mathbb{E}_{\datanonsig \sim\margprob}\left[ \marginnonsig\right] \leq \mixvar\left(1+ \frac{\sqrt{\dims-1}}{\sqrt{\numsamp}}\right),\\
%% 	&\mathbb{E}_{\datanonsig \sim \margprob}\left[ \marginnonsig \right] \geq \mixvar\left( \frac{\sqrt{\dims-1}}{\sqrt{\numsamp}}-1\right).
%% \end{split}
%% \end{equation} 
%% \end{lemma}

%% Let $\data \sim \prob_{\sigsep}$ with $\numsamp< \dims$. We compute the standard accuracy of the max $l_2-$margin classifier of $\data$ evaluated on $\prob_{\sigsep_{test}}$. We first use Lemma \ref{lem:maxmargin}  to get an explicit expression of the max $l_2$-margin classifier. Then, we compute the distribution of the random variable $\thetahattop{} x$, where $(x,y) \sim \prob_{\sigsep_{test}}$. Lastly, using the expression of the distribution, we can compute the accuracy. 

%% Because $\numsamp < \dims$, the dataset $\data$ is linearly separable in its last $\dims-1$ coordinates. As a consequence of linear separability in the last $\dims-1$ coordinates and the deterministic first coordinate of $\prob_{\sigsep}$, it follows from Lemma \ref{lem:maxmargin} that the max $l_2-$margin classifier of $\data$ is given by Equation \ref{eq:maxmargin}. Since the accuracy of a binary classifier induced by $\theta$ is invariant to positive scaling of $\theta$,  we can scale the max $l_2$-margin classifier induced by $\thetahat{}$ as follows
%% \begin{equation}
%% \label{eq:maxl2scaled}
%% \thetahat{} = \left[\frac{\sigsep}{2},   \marginnonsig \thetatilde\right].
%% \end{equation}
%% For ease of notation, we retain the notation $\thetahat{}$ for the scaled variant of the max $l_2$-margin classifier. 

%% We identify the distribution of $\thetahattop{} x$. Recall that the sum of two independent Gaussian distributed random variables with means $\mu_1$ and $\mu_2$ and variances $\sigma_1^2$ and $\sigma_2^2$ is again a Gaussian distributed random variable with mean $\mu = \mu_1+\mu_2$ and variance $\sigma^2 = \sigma_1^2+\sigma_2^2$. Moreover, recall that for any $(x,y) \in \prob_{\sigsep_{test}}$ the first coordinate of $x$ equals $\xind{0} = y \frac{\sigsep_{test}}{2}$ deterministically and the last $\dims-1$ coordinates are independent Gaussian distributed. Hence,  $\thetahattop{} x$ is Gaussian distributed with mean $y \frac{\sigsep_{test}}{2}\thetahatind{0}{1}$ and variance $\mixvar^2 \sum_{i=2}^{\dims-1}\thetahatsqind{0}{i} = \mixvar^2  \marginnonsig^2$.

%% Having identified the distribution of $\thetahattop{} x$ as a Gaussian distribution and with the explicit expression for $\thetahat{}$ given in Equation \ref{eq:maxl2scaled}, we find
%% \begin{equation}
%% \label{eq:prob_dataset}
%% \stdacc{\thetahat{}} = P\left[ y \thetahattop{} x > 0\right] = 1-\Phi\left(-\frac{\sigsep \sigsep_{test}}{4\mixvar  \marginnonsig }\right),
%% \end{equation}
%% where $\Phi$ denotes the cumulative probability distribution function of the normal distribution. By Equation \ref{eq:prob_dataset} part $1$ of Lemma~\ref{lem:sandwichmargin} is proven. Indeed, the cumulative probability distribution function of the normal distribution is a strictly increasing function, which implies that the robust accuracy function is also a strictly increasing function with $\sigsep$. Moreover, plugging in the bounds on $ \marginnonsig $ provided in Lemma~\ref{lem:sandwichmargin} into Equation \ref{eq:prob_dataset} yields part 2 of Lemma~\ref{lem:sandwichmargin} and concludes the proof.

\subsection{Proof of Lemma \ref{lem:boundsmaxmargin}}
\label{sec:boundsmaxmargin}

The proof plan is as follows. We start from the definition of the max
$\ell_2$-margin of a dataset. Then, we rewrite the
max $\ell_2$-margin as an expression that includes a random matrix with independent
standard normal entries. This allows us to prove the upper and lower bounds for the
max-$\ell_2$-margin in Sections~\ref{sec:gammaupperbound} and ~\ref{sec:gammalowerbound}
respectively, using non-asymptotic estimates on the singular values of
Gaussian random matrices.
%% theory on random normal distributed matrices
%% y non-asymptotic estimates on the singular values of
%% the random matrix. Using these we come to the result with a direct
%% computation.

Given the dataset $\datanonsig =  \{(\xnonsig_i, y_i)\}_{i=1}^{\numsamp}$, we define the random matrix
\begin{equation}
\label{eq:randmatrixsamples}
\randdatamatr = \begin{pmatrix}
\xnonsig_1^{\top}\\
\xnonsig_2^{\top}\\
...\\
\xnonsig_{\numsamp}^{\top}
\end{pmatrix}.
\end{equation}
where $\xnonsig_i \sim \Normal(0,\sigma I_{d-1})$. 
Let $\mathcal{V}$ be the class of all perfect predictors of $\datanonsig$. For a matrix $A$ and vector $b$ we also denote by $|Ab|$ the vector whose entries correspond to the absolute values of the entries of $Ab$. 
Then, by definition
\begin{equation}
\label{maxmargindefgammaproof}
\marginnonsig = \max_{v \in \mathcal{V}, \|v\|_2=1} \min_{j \in [\numsamp]} \indof{|\randdatamatr v|}{j} = \max_{v \in \mathcal{V}, \|v\|_2=1} \min_{j \in [\numsamp]} \mixvar \indof{|\gaussianmatrix v|}{j},
\end{equation}
where $\gaussianmatrix = \frac{1}{\sigma} \randdatamatr$ is the scaled data matrix.
%% Denote by $\gaussianvec$ an i.i.d. standard Gaussian distributed vector of dimension $\dims-1$. Note that for any row of $\randdatamatr_j$, we can write
%% \begin{equation*}
%% \randdatamatr_j = \mixvar \gaussianvec
%% \end{equation*}
%% Denote by $\gaussianmatrix \in \mathbb{R}^{\numsamp \times (\dims-1)}$
%% an i.i.d. standard Gaussian distributed matrix. We can write
%% \begin{equation*}
%% \marginnonsig = \max_{v \in \mathcal{V}, \|v\|_2=1} \min_{j \in [\numsamp]} \mixvar \indof{|\gaussianmatrix v|}{j}.
%% \end{equation*}

In the sequel we will use the operator norm of a matrix $A \in \mathbb{R}^{\numsamp \times \dims-1}$.
\begin{equation*}
\| A\|_2 = \sup_{v \in \mathbb{R}^{\dims-1} \mid \|v\|_2=1} \|A v \|_2
\end{equation*}
and denote the maximum singular value of a matrix $A$ as $s_{\text{max}} (A)$ and the minimum singular value as $s_{\text{min}}(A)$.

\subsubsection{Upper bound}
\label{sec:gammaupperbound}


Given the maximality of the
operator norm and since the minimum entry of the vector
$|\gaussianmatrix v|$ must be smaller than $\frac{\|\gaussianmatrix\|_2}{\sqrt{\numsamp}}$, we can upper bound
$\marginnonsig$ by
\begin{equation*}
\marginnonsig  \leq \mixvar  \frac{1}{\sqrt{\numsamp}} \|\gaussianmatrix{}\|_2.
\end{equation*}
%% Recall that the operator norm of a matrix is bounded by the maximal
%% singular value of that matrix, i.e. $\| A\|_2\leq
%% s_{\text{max}}\left(A\right)$.
Taking the expectation on both sides with respect to the draw of
$\datanonsig$ and noting $\|\gaussianmatrix\|_2 \leq
s_{\text{max}}\left(\gaussianmatrix\right)$,
it follows from
%% Theorem 5.32 in \cite{vershynin12} that
%% \begin{equation*}
%% \EE \left[ \marginnonsig\right] \leq \mixvar\left(1 +  \frac{\sqrt{\dims-1}}{\sqrt{\numsamp}}\right).
%% \end{equation*}
%% Moreover, in
Corollary 5.35  of \cite{vershynin12} %they state and proof following upper bound that holds
that for all $t\geq 0$:
\begin{equation*}
\prob \left[\sqrt{\dims-1}+\sqrt{\numsamp}+t \geq s_{\text{max}}\left(\gaussianmatrix\right) \right] \geq 1-2e^{-\frac{t^2}{2}}.
\end{equation*}
Therefore, with a probability greater than $1-2e^{-\frac{t^2}{2}}$,
\begin{equation*}
\marginnonsig \leq  \mixvar \left(1+ \frac{t+\sqrt{\dims-1}}{\sqrt{\numsamp}}\right).
\end{equation*}

\subsubsection{Lower bound}
\label{sec:gammalowerbound}
By the definition in Equation \eqref{maxmargindefgammaproof}, if we
find a vector $v \in \mathcal{V}$ with $\|v\|_2=1$ such that for an
$a>0$, it holds that $\Hquad \min_{j \in \numsamp} \sigma
\indof{|\randdatamatr v|}{j} > a$, then $\marginnonsig > a$.

%% We
%% distinguish two cases for the lower bound. Case $1$ assumes $\numsamp
%% < \dims-1$, which results in estimating singular values of a random
%% wide matrix and case $2$ assumes $\numsamp = \dims-1$, which needs
%% estimation of extreme eigenvalues of a random square matrix.

%\textbf{Case 1: $\numsamp < \dims-1$.}
Recall the definition of the max-$\ell_2$-margin
%, where we use the random matrix $\gaussianmatrix$ induced by the random matrix defined by the samples $\randdatamatr$
as in Equation \ref{eq:randmatrixsamples}.
%% \begin{equation*}
%% \maxMargindm1{} = \max_{v \in \mathcal{V}, \|v\|_2=1} \min_{j \in [n]}\mixvar |\gaussianmatrix v|_j.
%% \end{equation*}
As $\numsamp < \dims-1$, the random matrix $\gaussianmatrix$ is a wide
matrix, i.e. there are more columns than rows and therefore the
minimal singular value is $0$.
Furthermore, $\gaussianmatrix$ has rank $\numsamp$ almost surely and hence 
%% Note that a wide random normal
%% matrix has almost surely $\dims-1$ independent eigenvectors
%% corresponding to $\dims-1$ non-zero singular values. Hence,
for all $\cst >0$, there exists a $v \in \mathbb{R}^{\dims-1}$ such that
\begin{equation}
\label{eq:existencerhs}
 \mixvar \gaussianmatrix v= 1_{\{ \numsamp\}}\cst> 0,
\end{equation}
where $ 1_{\{ \numsamp \}}$ denotes the all ones vector of dimension $\numsamp$. The smallest non-zero singular value of $\gaussianmatrix$, $s_{\text{min, nonzero}}(\gaussianmatrix)$, equals the smallest non-zero singular value of its transpose $\gaussianmatrix^{\top}$. Therefore, there also exists a $v \in \mathcal{V}$ with $\|v\|_2=1$ such that
\begin{equation}
\label{minimum_step_gamma}
\marginnonsig \geq  \min_{j \in [n]} \mixvar \indof{|\gaussianmatrix v|}{j} \geq \mixvar s_{\text{min,nonzeros}}\left(\gaussianmatrix^{\top}\right)\frac{1}{\sqrt{\numsamp}},
\end{equation}
where we used the fact that any vector $v$ in the span of non-zero eigenvectors satisfies $\|\gaussianmatrix  v \|_2 \geq s_{\text{min, nonzeros}}(\gaussianmatrix)$ and the existence of a solution $v$ for any right-hand side as in Equation \ref{eq:existencerhs}.
Taking the expectation on both sides,
%% with Theorem 5.32 of \cite{vershynin12} we conclude
%% \begin{equation}
%% \mathbb{E}_{\datanonsig \sim  \margprob} \left[\marginnonsig \right] \geq \mixvar \left( \sqrt{\frac{\dims-1}{\numsamp}}-1\right).
%% \end{equation}
%% Similarly
Corollary 5.35 of \cite{vershynin12} yields that with a probability greater than $1-2e^{-\frac{t^2}{2}}, t\geq 0$ we have
\begin{equation}
\marginnonsig \geq \mixvar\left( \frac{\sqrt{\dims-1}-t}{\sqrt{\numsamp}}-1\right).
\end{equation}

%% \textbf{Case 2: $\numsamp = \dims-1$}

%% The bound above is trivial for $\numsamp = \dims-1$. It turns out that estimating the minimal singular value of a squared matrix $A$, is a highly non-trivial problem \cite{Rudelson09, Edelman88, Terence09}.
%% We have the following non-asymptotic result in Theorem 3.1 of \cite{Rudelson10}:
%% \begin{equation}
%% P\left[s_{\text{min}}(\gaussianmatrix) < \xi \numsamp^{-\frac{1}{2}} \right] < \xi, \xi > 0. 
%% \end{equation}
%% Plugging in Equation \ref{minimum_step_gamma} results in
%% \begin{equation}
%% \mathbb{E}_{\datanonsig \sim  \margprob} \left[\marginnonsig \right] \geq \frac{\mixvar }{2 \numsamp}.
%% \end{equation}


