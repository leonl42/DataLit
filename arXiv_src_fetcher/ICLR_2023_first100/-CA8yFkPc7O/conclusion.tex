
\section{Future work}


This paper aims to caution the practitioner against blindly following
current widespread practices to increase the robust
performance of machine learning models.
% Specifically, we study how
Specifically, adversarial training is currently recognized to be one
of the most effective defense mechanisms for $\ell_p$-perturbations,
%\fy{but also others like manifold adv. ex?}
significantly outperforming robust performance of standard training.  However, we prove that
this common wisdom is not applicable for directed attacks -- that are perceptible (albeit consistent) but efficiently focus their
attack budget to target ground truth class information -- in the low-sample size regime.
In particular, in such settings adversarial training can in fact yield worse accuracy than standard training.

% On a high level, our paper reveals fundamental and provable
% differences in robustness behavior between perceptible and imperceptible 
% perturbations. In particular, it underlines the necessity
% of future work that targets general understanding of adversarial robustness, to study a broader scope of perturbation
%types.  %(both empirical and theoretically) 

%% In particular, we show
%% theoretically and experimentally that it is critical to consider the
%% relationship between the attack transformation type and the believed
%% ground truth signal direction.


%detrimental to finding the ground truth, since the structural bias is
%actively worsened.

%% In the overparameterized and small-sample regime, many
%% classifiers can fit the training data perfectly.  Key is to have the
%% right inductive bias.  If the perturbation attacks the signal,
%% adversarial training is very detrimental to finding the ground truth,
%% since the structural bias is actively worsened.  Hence standard
%% error increases severely.

In terms of follow-up work on directed attacks in the low-sample
regime, there are some concrete questions that would be interesting to
explore.  For example, as discussed in Section~\ref{sec:relatedwork},
it would be useful to test whether some methods to mitigate the
standard accuracy vs. robustness trade-off would also relieve the
perils of adversarial training for directed attacks. Further, we
hypothesize, independent of the attack during test time, it is
important in the small sample-size regime to choose perturbation sets
during training that align with
the ground truth signal (such as rotations for data with inherent
rotation). If this hypothesis were to be confirmed, it would break
with yet another general rule that the best defense perturbation type
should always match the attack during evaluation.  The insights from
this study might also be helpful in the context of searching for
good defense perturbations.

%% help even when
%% the type of robustness during evaluation is a \nameofattack .  In
%% other words, in the overparameterized small sample regime, different
%% sets may be better.


