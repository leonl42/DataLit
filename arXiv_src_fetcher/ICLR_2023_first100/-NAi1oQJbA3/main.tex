\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}
% \usepackage[natbib]{neurips_2023}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{pgfplots} 
    \usepgfplotslibrary{groupplots}
\usepackage{siunitx}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usetikzlibrary{patterns}
\usepackage{amsmath, amssymb}
\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{setspace}
% defined command
\newcommand{\comment}[1]{[\textcolor{red}{#1}]}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\renewcommand{\algorithmicrequire}{\textbf{Input:}}

%%%%%%%%% recover this if we want to do iclr format
% \def\setstretch#1{\renewcommand{\baselinestretch}{#1}}
% \setstretch{0.97}
% \addtolength{\parskip}{-1pt}
\usepackage[compact]{titlesec}
\titlespacing{\subsection}{0pt}{*1}{*0}
\setcitestyle{square}
\setcitestyle{citesep={,}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Efficiently Adapting Pretrained Language Models to New Languages}
% \title{Tool Manipulation with Open-source Large Language Models}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Zoltan Csaki, Pian Pawakapan, Urmish Thakker, Qiantong Xu \\
  SambaNova Systems, Inc. \\
  Palo Alto, CA, USA \\
  \texttt{zoltan.csaki@sambanovasystems.com} \\
  % \texttt{\{qiantong.xu,fenglu.hong,bo.li,changran.hu,edison.chen,jian.zhang\}@sambanovasystems.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\vspace{-1.5em}
\begin{abstract}

% State-of-the-art large language models are predominantly trained on English text and exhibit sub-optimal performance on other language, especially low resource languages such as Hungarian and Thai. 
% Pretraining a monolingual LLM from scratch requires a vast amount of text available in the language, but most languages do not have this amount of text available. 
% One way to mitigate that is to train a multi-lingual model, where low resource languages are trained with high resource languages. However, this requires a large amount of compute.
% In this paper introduce we introduce an approach to adapt an existing LLM pretrained on primarily English text to a new language that the model was not trained on. This paper shows how to adapt the tokenizer to have good fertility on the new language, the best way to mix the original language and the low resource language to maximize language transfer, and explores instruction tuning the model with a minimal amount of data in the low-resource language. 

Recent large language models (LLM) exhibit sub-optimal performance on low-resource languages, as the training data of these models is usually dominated by English and other high-resource languages. 
Furthermore, it is challenging to train models for low-resource languages, especially from scratch, due to a lack of high quality training data. 
Adapting pretrained LLMs reduces the need for data in the new language while also providing cross lingual transfer capabilities. However, naively adapting to new languages leads to catastrophic forgetting and poor tokenizer efficiency.
In this work, we study how to efficiently adapt any existing pretrained LLM to a new language without running into these issues.
In particular, we improve the encoding efficiency of the tokenizer by adding new tokens from the target language and study the data mixing recipe to mitigate forgetting.
Our experiments on adapting an English LLM to Hungarian and Thai show that our recipe can reach better performance than open source models on the target language, with minimal regressions on English. 


\end{abstract}

% \vspace{-0.5em}
\section{Introduction \& Related work}
% \vspace{-0.25em}
\label{sec:intro}
\input{texts/intro}


\section{Implementation Details}
\label{sec:methods}
\input{texts/methods}


\section{Experiments}
\label{sec:experiments}
\input{texts/experiments}

\section{Conclusion}
\label{sec:conclusion}
\input{texts/conclusion}


% \begin{ack}
% We sincerely appreciate
% \end{ack}


\bibliographystyle{IEEEtran}
\bibliography{ref}

\newpage
\appendix

\section{Tokenizer Details}
\label{sec:tokenizer}
\input{texts/apdx_tokenizer}

\section{Base Model}
\label{base_model}
\input{texts/apdx_base_model}

\section{Datasets}
\label{sec:dataset_details}
\input{texts/apdx_dataset_details}

\section{Training Details}
\label{sec:training_details}
\input{texts/apdx_training_details}

% \section{Baselines}
% \label{sec:app_ext_baselines}
% \input{texts/appendix_extended_baselines}

% \section{Model Alignment Details}
% \label{sec:app_training_details}
% \input{texts/appendix_training_details}

% \section{API Selection Complexity Score}
% \label{sec:app_complexity_metrics}
% \input{texts/appendix_3m_model}

\section{Evaluation}
\label{sec:evaluation}
\input{texts/apdx_evaluation}

\end{document}
