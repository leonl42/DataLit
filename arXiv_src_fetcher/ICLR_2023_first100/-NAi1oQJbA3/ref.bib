@misc{workshop2023bloom,
      title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
      author={BigScience Workshop},
      year={2023},
      eprint={2211.05100},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{muennighoff2023crosslingual,
      title={Crosslingual Generalization through Multitask Finetuning}, 
      author={Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},
      year={2023},
      eprint={2211.01786},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xue2021mt5,
      title={mT5: A massively multilingual pre-trained text-to-text transformer}, 
      author={Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},
      year={2021},
      eprint={2010.11934},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{conneau2020unsupervised,
      title={Unsupervised Cross-lingual Representation Learning at Scale}, 
      author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
      year={2020},
      eprint={1911.02116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{goyal2021largerscale,
      title={Larger-Scale Transformers for Multilingual Masked Language Modeling}, 
      author={Naman Goyal and Jingfei Du and Myle Ott and Giri Anantharaman and Alexis Conneau},
      year={2021},
      eprint={2105.00572},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shliazhko2022mgpt,
      title={mGPT: Few-Shot Learners Go Multilingual}, 
      author={Oleh Shliazhko and Alena Fenogenova and Maria Tikhonova and Vladislav Mikhailov and Anastasia Kozlova and Tatiana Shavrina},
      year={2022},
      eprint={2204.07580},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{lin2022fewshot,
      title={Few-shot Learning with Multilingual Language Models}, 
      author={Xi Victoria Lin and Todor Mihaylov and Mikel Artetxe and Tianlu Wang and Shuohui Chen and Daniel Simig and Myle Ott and Naman Goyal and Shruti Bhosale and Jingfei Du and Ramakanth Pasunuru and Sam Shleifer and Punit Singh Koura and Vishrav Chaudhary and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Zornitsa Kozareva and Mona Diab and Veselin Stoyanov and Xian Li},
      year={2022},
      eprint={2112.10668},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2021bert,
      title={BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation}, 
      author={Haoran Xu and Benjamin Van Durme and Kenton Murray},
      year={2021},
      eprint={2109.04588},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yong2023bloom1,
      title={BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting}, 
      author={Zheng-Xin Yong and Hailey Schoelkopf and Niklas Muennighoff and Alham Fikri Aji and David Ifeoluwa Adelani and Khalid Almubarak and M Saiful Bari and Lintang Sutawika and Jungo Kasai and Ahmed Baruwa and Genta Indra Winata and Stella Biderman and Edward Raff and Dragomir Radev and Vassilina Nikoulina},
      year={2023},
      eprint={2212.09535},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{phang2020english,
      title={English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too}, 
      author={Jason Phang and Iacer Calixto and Phu Mon Htut and Yada Pruksachatkun and Haokun Liu and Clara Vania and Katharina Kann and Samuel R. Bowman},
      year={2020},
      eprint={2005.13013},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ebrahimi2021adapt,
      title={How to Adapt Your Pretrained Multilingual Model to 1600 Languages}, 
      author={Abteen Ebrahimi and Katharina Kann},
      year={2021},
      eprint={2106.02124},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ogueji2021small,
  title={Small data? no problem! exploring the viability of pretrained multilingual language models for low-resourced languages},
  author={Ogueji, Kelechi and Zhu, Yuxin and Lin, Jimmy},
  booktitle={Proceedings of the 1st Workshop on Multilingual Representation Learning},
  pages={116--126},
  year={2021}
}

@misc{ye2023language,
      title={Language Versatilists vs. Specialists: An Empirical Revisiting on Multilingual Transfer Ability}, 
      author={Jiacheng Ye and Xijia Tao and Lingpeng Kong},
      year={2023},
      eprint={2306.06688},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{armengolestapé2021multilingual,
      title={On the Multilingual Capabilities of Very Large-Scale English Language Models}, 
      author={Jordi Armengol-Estapé and Ona de Gibert Bonet and Maite Melero},
      year={2021},
      eprint={2108.13349},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{pires2023sabia,
      title={Sabi\'a: Portuguese Large Language Models}, 
      author={Ramon Pires and Hugo Abonizio and Thales Sales Almeida and Rodrigo Nogueira},
      year={2023},
      eprint={2304.07880},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{sengupta2023jais,
  title={Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models},
  author={Sengupta, Neha and Sahu, Sunil Kumar and Jia, Bokang and Katipomu, Satheesh and Li, Haonan and Koto, Fajri and Afzal, Osama Mohammed and Kamboj, Samta and Pandit, Onkar and Pal, Rahul and others},
  journal={arXiv preprint arXiv:2308.16149},
  year={2023}
}



@misc{cahyawijaya2023instructalign,
      title={Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction}, 
      author={Samuel Cahyawijaya and Holy Lovenia and Tiezheng Yu and Willy Chung and Pascale Fung},
      year={2023},
      eprint={2305.13627},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{french1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in cognitive sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{pfeiffer-etal-2020-mad,
    title = "{MAD-X}: {A}n {A}dapter-{B}ased {F}ramework for {M}ulti-{T}ask {C}ross-{L}ingual {T}ransfer",
    author = "Pfeiffer, Jonas  and
      Vuli{\'c}, Ivan  and
      Gurevych, Iryna  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = {"2020"},
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.617",
    doi = "10.18653/v1/2020.emnlp-main.617",
    pages = "7654--7673",
    abstract = "The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.",
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}




@article{rust2021good,
  title={How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
  author={Phillip Rust and Jonas Pfeiffer and Ivan Vulić and Sebastian Ruder and Iryna Gurevych},
  year={2021},
  eprint={2012.15613},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{stollenwerk2023training,
      title={Training and Evaluation of a Multilingual Tokenizer for GPT-SW3}, 
      author={Felix Stollenwerk},
      year={2023},
      eprint={2304.14780},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gage1994new,
  title={A New Algorithm for Data Compression},
  author={Philip Gage},
  journal={The C Users Journal Archive},
  year={1994},
  volume={12},
  pages={23-38},
  url={https://api.semanticscholar.org/CorpusID:59804030}
}

@online{acs2019,
  title={Exploring BERT's Vocabulary},
  author={Judit Ács},
  year={2019},
  month={February},
  url={https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html}
}

@misc{chalkidis2021multieurlex,
      title={MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer}, 
      author={Ilias Chalkidis and Manos Fergadiotis and Ion Androutsopoulos},
      year={2021},
      eprint={2109.00904},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vu2022overcoming,
      title={Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation}, 
      author={Tu Vu and Aditya Barua and Brian Lester and Daniel Cer and Mohit Iyyer and Noah Constant},
      year={2022},
      eprint={2205.12647},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{raffel2023exploring,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@PhDThesis{ Nemeskey:2020,                                                 
  author = {Nemeskey, Dávid Márk},
  title  = {Natural Language Processing Methods for Language Modeling},
  year   = {2020},
  school = {E\"otv\"os Lor\'and University},
  url  ={https://hlt.bme.hu/media/pdf/nemeskey_thesis.pdf}
}

@article{longpre2023flan,
  title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@misc{Nguyen_2023, title={The OIG dataset}, url={https://laion.ai/blog/oig-dataset/}, journal={LAION}, author={Nguyen, Huu}, year={2023}, month={Mar}} 

@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = {2021},
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}

@misc{abadji2022cleaner,
      title={Towards a Cleaner Document-Oriented Multilingual Crawled Corpus}, 
      author={Julien Abadji and Pedro Ortiz Suarez and Laurent Romary and Benoît Sagot},
      year={2022},
      eprint={2201.06642},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wenzek2019ccnet,
      title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data}, 
      author={Guillaume Wenzek and Marie-Anne Lachaux and Alexis Conneau and Vishrav Chaudhary and Francisco Guzmán and Armand Joulin and Edouard Grave},
      year={2019},
      eprint={1911.00359},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@software{chenghao_mou_2023_8364980,
  author       = {Chenghao Mou and
                  Chris Ha and
                  Kenneth Enevoldsen and
                  Peiyuan Liu},
  title        = {ChenghaoMou/text-dedup: Reference Snapshot},
  month        = sep,
  year         = {2023},
  publisher    = {Zenodo},
  version      = {2023.09.20},
  doi          = {10.5281/zenodo.8364980},
  url          = {https://doi.org/10.5281/zenodo.8364980}
}

@inproceedings{ligetinagy2022hulu,
  title={HuLU: magyar nyelvű benchmark adatbázis kiépítése a neurális nyelvmodellek kiértékelése céljából},
  author={Ligeti-Nagy, N. and Ferenczi, G. and Héja, E. and Jelencsik-Mátyus, K. and Laki, L. J. and Vadász, N. and Yang, Z. Gy. and Vadász, T.},
  booktitle={XVIII. Magyar Számítógépes Nyelvészeti Konferencia},
  year={2022},
  pages = {431--446}
}

@misc{narayan2018dont,
      title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization}, 
      author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
      year={2018},
      eprint={1808.08745},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{clark2019boolq,
      title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}, 
      author={Christopher Clark and Kenton Lee and Ming-Wei Chang and Tom Kwiatkowski and Michael Collins and Kristina Toutanova},
      year={2019},
      eprint={1905.10044},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{47761,
title	= {Natural Questions: a Benchmark for Question Answering Research},
author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
year	= {2019},
journal	= {Transactions of the Association of Computational Linguistics}
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@inproceedings{MultiRC2018,
    author = {Daniel Khashabi and Snigdha Chaturvedi and Michael Roth and Shyam Upadhyay and Dan Roth},
    title = {Looking Beyond the Surface:A Challenge Set for Reading Comprehension over Multiple Sentences},
    booktitle = {Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL)},
    year = {2018}
}

@article{wang2019superglue,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1905.00537},
  year={2019}
}

@article{ponti2020xcopa,
  title={{XCOPA: A} Multilingual Dataset for Causal Commonsense Reasoning},
  author={Edoardo M. Ponti and Goran Glava
{s} and Olga Majewska and Qianchu Liu and Ivan Vuli'{c} and Anna Korhonen},
  journal={arXiv preprint},
  year={2020},
  url={https://ducdauge.github.io/files/xcopa.pdf}
}

@software{bact_2019_3457447,
  author       = {Suriyawongkul, Arthit and
                  Chuangsuwanich, Ekapol and
                  Chormai, Pattarawat and
                  Polpanumas, Charin},
  title        = {PyThaiNLP/wisesight-sentiment: First release},
  month        = sep,
  year         = {2019},
  publisher    = {Zenodo},
  version      = {v1.0},
  doi          = {10.5281/zenodo.3457447},
  url          = {https://doi.org/10.5281/zenodo.3457447}
}

@article{Zhang2018ReCoRDBT,
  title={ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension},
  author={Sheng Zhang and Xiaodong Liu and Jingjing Liu and Jianfeng Gao and Kevin Duh and Benjamin Van Durme},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.12885},
  url={https://api.semanticscholar.org/CorpusID:53116244}
}

@article{Artetxe:etal:2019,
      author    = {Mikel Artetxe and Sebastian Ruder and Dani Yogatama},
      title     = {On the cross-lingual transferability of monolingual representations},
      journal   = {CoRR},
      volume    = {abs/1910.11856},
      year      = {2019},
      archivePrefix = {arXiv},
      eprint    = {1910.11856}
}

@mastersthesis{chumpolsathien_2020,
    title={Using Knowledge Distillation from Keyword Extraction to Improve the Informativeness of Neural Cross-lingual Summarization},
    author={Chumpolsathien, Nakhun},
    year={2020},
    school={Beijing Institute of Technology}
}

@inproceedings{cettolo-etal-2012-wit3,
    title = "{WIT}3: Web Inventory of Transcribed and Translated Talks",
    author = "Cettolo, Mauro  and
      Girardi, Christian  and
      Federico, Marcello",
    booktitle = "Proceedings of the 16th Annual conference of the European Association for Machine Translation",
    month = may # " 28{--}30",
    year = {"2012"},
    address = "Trento, Italy",
    publisher = "European Association for Machine Translation",
    url = "https://www.aclweb.org/anthology/2012.eamt-1.60",
    pages = "261--268",
}

@inproceedings{nivre-etal-2016-universal,
    title = "{U}niversal {D}ependencies v1: A Multilingual Treebank Collection",
    author = "Nivre, Joakim  and
      de Marneffe, Marie-Catherine  and
      Ginter, Filip  and
      Goldberg, Yoav  and
      Haji{\v{c}}, Jan  and
      Manning, Christopher D.  and
      McDonald, Ryan  and
      Petrov, Slav  and
      Pyysalo, Sampo  and
      Silveira, Natalia  and
      Tsarfaty, Reut  and
      Zeman, Daniel",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1262",
    pages = "1659--1666",
    abstract = "Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.",
}

@inproceedings{zeman-etal-2017-conll,
    title = "{C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to {U}niversal {D}ependencies",
    author = {Zeman, Daniel  and
      Popel, Martin  and
      Straka, Milan  and
      Haji{\v{c}}, Jan  and
      Nivre, Joakim  and
      Ginter, Filip  and
      Luotolahti, Juhani  and
      Pyysalo, Sampo  and
      Petrov, Slav  and
      Potthast, Martin  and
      Tyers, Francis  and
      Badmaeva, Elena  and
      Gokirmak, Memduh  and
      Nedoluzhko, Anna  and
      Cinkov{\'a}, Silvie  and
      Haji{\v{c}} jr., Jan  and
      Hlav{\'a}{\v{c}}ov{\'a}, Jaroslava  and
      Kettnerov{\'a}, V{\'a}clava  and
      Ure{\v{s}}ov{\'a}, Zde{\v{n}}ka  and
      Kanerva, Jenna  and
      Ojala, Stina  and
      Missil{\"a}, Anna  and
      Manning, Christopher D.  and
      Schuster, Sebastian  and
      Reddy, Siva  and
      Taji, Dima  and
      Habash, Nizar  and
      Leung, Herman  and
      de Marneffe, Marie-Catherine  and
      Sanguinetti, Manuela  and
      Simi, Maria  and
      Kanayama, Hiroshi  and
      de Paiva, Valeria  and
      Droganova, Kira  and
      Mart{\'\i}nez Alonso, H{\'e}ctor  and
      {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}  and
      Sulubacak, Umut  and
      Uszkoreit, Hans  and
      Macketanz, Vivien  and
      Burchardt, Aljoscha  and
      Harris, Kim  and
      Marheinecke, Katrin  and
      Rehm, Georg  and
      Kayadelen, Tolga  and
      Attia, Mohammed  and
      Elkahky, Ali  and
      Yu, Zhuoran  and
      Pitler, Emily  and
      Lertpradit, Saran  and
      Mandl, Michael  and
      Kirchner, Jesse  and
      Alcalde, Hector Fernandez  and
      Strnadov{\'a}, Jana  and
      Banerjee, Esha  and
      Manurung, Ruli  and
      Stella, Antonio  and
      Shimada, Atsuko  and
      Kwak, Sookyoung  and
      Mendon{\c{c}}a, Gustavo  and
      Lando, Tatiana  and
      Nitisaroj, Rattima  and
      Li, Josie},
    booktitle = "Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-3001",
    doi = "10.18653/v1/K17-3001",
    pages = "1--19",
    abstract = "The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, the task was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe how the data sets were prepared, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.",
}


@inproceedings{vincze-etal-2010-hungarian,
    title = "{H}ungarian Dependency Treebank",
    author = {Vincze, Veronika  and
      Szauter, D{\'o}ra  and
      Alm{\'a}si, Attila  and
      M{\'o}ra, Gy{\"o}rgy  and
      Alexin, Zolt{\'a}n  and
      Csirik, J{\'a}nos},
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/465_Paper.pdf",
    abstract = "Herein, we present the process of developing the first Hungarian Dependency TreeBank. First, short references are made to dependency grammars we considered important in the development of our Treebank. Second, mention is made of existing dependency corpora for other languages. Third, we present the steps of converting the Szeged Treebank into dependency-tree format: from the originally phrase-structured treebank, we produced dependency trees by automatic conversion, checked and corrected them thereby creating the first manually annotated dependency corpus for Hungarian. We also go into detail about the two major sets of problems, i.e. coordination and predicative nouns and adjectives. Fourth, we give statistics on the treebank: by now, we have completed the annotation of business news, newspaper articles, legal texts and texts in informatics, at the same time, we are planning to convert the entire corpus into dependency tree format. Finally, we give some hints on the applicability of the system: the present database may be utilized ― among others ― in information extraction and machine translation as well.",
}

@inproceedings{silveira14gold,
  year = {2014},
  author = {Natalia Silveira and Timothy Dozat and Marie-Catherine de
	  Marneffe and Samuel Bowman and Miriam Connor and John Bauer and
	  Christopher D. Manning},
  title = {A Gold Standard Dependency Corpus for {E}nglish},
  booktitle = {Proceedings of the Ninth International Conference on Language
    Resources and Evaluation (LREC-2014)}
}

@misc{paperno2016lambada,
      title={The LAMBADA dataset: Word prediction requiring a broad discourse context}, 
      author={Denis Paperno and Germán Kruszewski and Angeliki Lazaridou and Quan Ngoc Pham and Raffaella Bernardi and Sandro Pezzelle and Marco Baroni and Gemma Boleda and Raquel Fernández},
      year={2016},
      eprint={1606.06031},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mihaylov2018suit,
      title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}, 
      author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
      year={2018},
      eprint={1809.02789},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{clark2018think,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, 
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{bisk2019piqa,
      title={PIQA: Reasoning about Physical Commonsense in Natural Language}, 
      author={Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
      year={2019},
      eprint={1911.11641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{nie2020adversarial,
      title={Adversarial NLI: A New Benchmark for Natural Language Understanding}, 
      author={Yixin Nie and Adina Williams and Emily Dinan and Mohit Bansal and Jason Weston and Douwe Kiela},
      year={2020},
      eprint={1910.14599},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sakaguchi2019winogrande,
      title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
      author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1907.10641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{iyer2023optiml,
      title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization}, 
      author={Srinivasan Iyer and Xi Victoria Lin and Ramakanth Pasunuru and Todor Mihaylov and Daniel Simig and Ping Yu and Kurt Shuster and Tianlu Wang and Qing Liu and Punit Singh Koura and Xian Li and Brian O'Horo and Gabriel Pereyra and Jeff Wang and Christopher Dewan and Asli Celikyilmaz and Luke Zettlemoyer and Ves Stoyanov},
      year={2023},
      eprint={2212.12017},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{666900,
  author={Broder, A.Z.},
  booktitle={Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171)}, 
  title={On the resemblance and containment of documents}, 
  year={1997},
  volume={},
  number={},
  pages={21-29},
  doi={10.1109/SEQUEN.1997.666900}}

@INPROCEEDINGS{9567250,
  author={Prabhakar, Raghu and Jairath, Sumti},
  booktitle={2021 IEEE Hot Chips 33 Symposium (HCS)}, 
  title={SambaNova SN10 RDU:Accelerating Software 2.0 with Dataflow}, 
  year={2021},
  volume={},
  number={},
  pages={1-37},
  doi={10.1109/HCS52781.2021.9567250}}

@inproceedings{Nomoto2019InterpersonalMA,
  title={Interpersonal meaning annotation for Asian language corpora: The case of TUFS Asian Language Parallel Corpus (TALPCo)},
  author={Hiroki Nomoto},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:209441604}
}

@inproceedings{BuschbeckWolf2020APE,
  title={A Parallel Evaluation Data Set of Software Documentation with Document Structure Annotation},
  author={Bianka Buschbeck-Wolf and Miriam Exel},
  booktitle={Workshop on Asian Translation},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221095497}
}

@article{Riza2016IntroductionOT,
  title={Introduction of the Asian Language Treebank},
  author={Hammam Riza and Michael Purwoadi and Gunarso and Teduh Uliniansyah and Aw Ai Ti and Sharifah Mahani Aljunied and Luong Chi Mai and Vu Tat Thang and Nguyen Phuong Thai and Vichet Chea and Rapid Sun and Sethserey Sam and Sopheap Seng and Khin Mar Soe and Khin Thandar Nwet and Masao Utiyama and Chenchen Ding},
  journal={2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA)},
  year={2016},
  pages={1-6},
  url={https://api.semanticscholar.org/CorpusID:45848332}
}

@article{Ladhak2020WikiLinguaAN,
  title={WikiLingua: A New Benchmark Dataset for Multilingual Abstractive Summarization},
  author={Faisal Ladhak and Esin Durmus and Claire Cardie and Kathleen McKeown},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.03093},
  url={https://api.semanticscholar.org/CorpusID:222177239}
}

@article{team2022NoLL,
  title={No Language Left Behind: Scaling Human-Centered Machine Translation},
  author={Nllb team and Marta Ruiz Costa-juss{\`a} and James Cross and Onur cCelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Alison Youngblood and Bapi Akula and Lo{\"i}c Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon L. Spruit and C. Tran and Pierre Yves Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzm'an and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.04672},
  url={https://api.semanticscholar.org/CorpusID:250425961}
}

@inproceedings{Conneau2018XNLIEC,
  title={XNLI: Evaluating Cross-lingual Sentence Representations},
  author={Alexis Conneau and Guillaume Lample and Ruty Rinott and Adina Williams and Samuel R. Bowman and Holger Schwenk and Veselin Stoyanov},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:52271711}
}

@dataset{kobkrit_viriyayudhakorn_2021_4539916,
  author       = {Kobkrit Viriyayudhakorn and
                  Charin Polpanumas},
  title        = {iapp\_wiki\_qa\_squad},
  month        = feb,
  year         = {2021},
  publisher    = {Zenodo},
  version      = 1,
  doi          = {10.5281/zenodo.4539916},
  url          = {https://doi.org/10.5281/zenodo.4539916}
}

@inproceedings{hasan-etal-2021-xl,
    title = "{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages",
    author = "Hasan, Tahmid  and
      Bhattacharjee, Abhik  and
      Islam, Md. Saiful  and
      Mubasshir, Kazi  and
      Li, Yuan-Fang  and
      Kang, Yong-Bin  and
      Rahman, M. Sohel  and
      Shahriyar, Rifat",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.413",
    pages = "4693--4703",
}


@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@online{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}

@article{guo-etal-2023-hc3,
    title = "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
    author = "Guo, Biyang  and
      Zhang, Xin  and
      Wang, Ziyuan  and
      Jiang, Minqi  and
      Nie, Jinran  and
      Ding, Yuxuan  and
      Yue, Jianwei  and
      Wu, Yupeng",
    journal={arXiv preprint arxiv:2301.07597},
    year = "2023",
}

@article{Kopf2023OpenAssistantC,
  title={OpenAssistant Conversations - Democratizing Large Language Model Alignment},
  author={Andreas Kopf and Yannic Kilcher and Dimitri von Rutte and Sotiris Anagnostidis and Zhi Rui Tam and Keith Stevens and Abdullah Barhoum and Nguyen Minh Duc and Oliver Stanley and Rich'ard Nagyfi and ES Shahul and Sameer Suri and David Glushkov and Arnav Dantuluri and Andrew Maguire and Christoph Schuhmann and Huu Nguyen and Alexander Mattick},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.07327},
  url={https://api.semanticscholar.org/CorpusID:258179434}
}