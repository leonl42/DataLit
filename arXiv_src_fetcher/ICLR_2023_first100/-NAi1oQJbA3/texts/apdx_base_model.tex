We train our base model with the same tokenizer and architecture as GPT-2 model \cite{radford2019language}. The model has 40 layers of transformer blocks with hidden dimension 5120 and 13 billion parameters in total. The vocabulary size is 50260. The base model was pretrained on 300B English tokens from the PILE\cite{gao2020pile} and C4\cite{raffel2023exploring} datasets, filtered for only natural language English text.