
\subsection{Pretraining Hyperparameters}\label{pretraining hyperparameters}
The training process utilized cross-entropy loss to optimize the CLM objective. All training runs shared the same hyperparameters to ensure a fair comparison and to avoid hyperparameter searches. When comparing two pre-training ablations, the runs were trained to token parity, training on the same number of tokens regardless of available data or tokenizer efficiency

The hyperparameters used were batch size = 512, fixed learning rate = 0.000015 and weight decay = 0.1. All the tokens were packed into the training sequences, if they did not fit in a sequence then they would be placed in the next sequence so no training tokens are lost.\footnote{\label{note}\href{https://github.com/sambanova/generative_data_prep}{https://github.com/sambanova/generative\_data\_prep}} An attention mask was applied so that only tokens from the same article attend to each other.
 

\subsection{Instruction Tuning Hyperparameters}\label{instruction tuning hyperparameters}
All instruction tuning studies share the same hyper-parameters. But ablations comparing runs are not run to step parity, rather they are all trained to 1 epoch to ensure they see all the data.

The hyperparameters used are batch size = 128, fixed learning rate = 0.000015, weight decay = 0.1, grad norm clip = 1.0, and prompt loss weight = 0.0 to ensure that prompts are attended to but not trained on. All the tokens were packed into sequences in a greedy fashion, if they did not fit in a sequence then they would be discarded.\footnotemark[1] An attention mask was applied so that only tokens from the same article attend to each other.

\subsection{Hardware Configuration}\label{hardware}
All training is run on SambaNova's Reconfigurable Data Units (RDU) \cite{9567250}.