\subsection{English pretraining data} \label{english pretraining data}
For the continuous pretraining phase, we often mix English data with either Hungarian or Thai. The English data we used is a 100 gigabyte sample of data from the base model pretraining corpus introduced in section \ref{base_model}.

\subsection{English instruction tuning data} \label{english instruction tuning data}
To construct our English instruction tuning dataset, we sample each constituent task from FlanV2 \cite{longpre2023flan} and OIG \cite{Nguyen_2023} equally by raw text size with a fixed dataset size budget. This creates an instruction tuning dataset that is task diverse and reasonably sized. The benefit of sampling instruction tuning data at the task level is shown in  \cite{iyer2023optiml}, and provides a compute efficient alternative to training on all the data. The dataset is 2.6 gigabytes of raw text and about 1.9 million samples.

\subsection{Hungarian pretraining tuning data}\label{hungarian pretraining data}
The dataset used for Hungarian pretraining is the Hungarian Webcorpus 2.0 \cite{Nemeskey:2020}. Our dataset is 96 gigabytes and 11,152,900 documents.

\subsection{Hungarian instruction tuning tuning data}\label{hungarian instruction tuning data}
 There is a lack of naturally written Hungarian instruction tuning datasets, so we use google translate to translate our English Instruction tuning corpus \ref{english instruction tuning data} to Hungarian. During translation the prompt and completion are translated separately and only concatenated during training. 

\subsection{Thai pretraining data}\label{thai pretraining corpus}
For the Thai pre-training corpus, we combine the Thai subsets of OSCAR \cite{abadji2022cleaner}, MC4 \cite{xue-etal-2021-mt5}, and CCNet \cite{wenzek2019ccnet}, which are all derived from Common Crawl. The entire combined corpus was processed with MinHash deduplication \cite{chenghao_mou_2023_8364980, 666900} with 1-grams and a Jaccard similarity of 0.6, with sentence level n-grams (split by whitespace), and totals 15.32 million documents.

\subsection{Thai instruction tuning data}\label{thai instruction tuning corpus}
For Thai instruction tuning data, we use a mixture of manually templated Thai datasets, as well as existing IT datasets translated from English. 

We take various Thai NLP datasets and create a variety of prompting templates for each task to form an instruction tuning dataset. These consist of translation \cite{Nomoto2019InterpersonalMA} \cite{BuschbeckWolf2020APE} \cite{Riza2016IntroductionOT} \cite{Ladhak2020WikiLinguaAN} \cite{cettolo-etal-2012-wit3} \cite{team2022NoLL}, NLI \cite{Conneau2018XNLIEC}, QA \cite{Artetxe:etal:2019} \cite{kobkrit_viriyayudhakorn_2021_4539916}, text categorization, sentiment analysis \cite{bact_2019_3457447}, and summarization \cite{chumpolsathien_2020} \cite{hasan-etal-2021-xl} tasks. This collection of datasets totals 6.26 million instruction tuning examples.

The English-translated IT datasets consist of traditional instruction tuning, multi-turn conversation, and domain-specific QA (i.e. general knowledge, finance, science, mathematics) sourced from collections like FLAN \cite{longpre2023flan}, OIG \cite{Nguyen_2023}, Alpaca \cite{alpaca}, Dolly \cite{DatabricksBlog2023DollyV2}, HC3 \cite{guo-etal-2023-hc3}, and OpenAssistant \cite{Kopf2023OpenAssistantC}, totalling 1.06 million examples.