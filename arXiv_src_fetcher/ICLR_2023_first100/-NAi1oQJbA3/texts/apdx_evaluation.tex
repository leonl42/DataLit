The EAI evaluation harness\cite{eval-harness} is used for all benchmarking. The code\footnote{our open source eval suite link} was adapted for new tasks to evaluate the models on non-English languages. We categorize all the tasks into 4 categories:


\paragraph{Multiple-choice} In this category of tasks, we append each candidate option after the prompt and let the model pick answer with the highest probability. We report average accuracy on each tasks.

\begin{table}[!h]
\centering
\caption{Multiple Choice Evaluation Benchmarks}
\label{tab:table-label}
\begin{tabular}{|c|m{0.8\textwidth}|}
\hline
\textbf{Language} & \textbf{Evaluation Tasks} \\
\hline
English & Lambada \cite{paperno2016lambada}, HellaSwag \cite{zellers2019hellaswag}, Openbookqa \cite{mihaylov2018suit}, Boolq \cite{clark2019boolq}, Arc Easy and Challenge \cite{clark2018think}, PiQA \cite{bisk2019piqa}, ANLI R1 \cite{nie2020adversarial} and Winogrande \cite{sakaguchi2019winogrande}. \\
\hline
Hungarian & HULU evaluation suite \cite{ligetinagy2022hulu}, which is composed of human translated tasks HuCB, HuSST, HuWNLI, HuCOPA, HuCOLA and HuRTE. \\
\hline
Thai &  XCOPA \cite{ponti2020xcopa} and WiseSight Sentiment Analysis \cite{bact_2019_3457447} corpus. Translated versions of HellaSwag \cite{zellers2019hellaswag}, MultiRC \cite{MultiRC2018}, RTE \cite{wang2019superglue}. \\
\hline
% Add more rows as needed
\end{tabular}
\end{table}


\paragraph{Open-ended Question Answering} In this category of tasks, we let the model to freely generate completions for each question prompt and we report the average F1 score between the model output and the ground truth answer. 

\begin{table}[!h]
\centering
\caption{Open-ended Question Answering Evaluation Benchmarks}
\label{tab:table-label}
\begin{tabular}{|c|m{0.8\textwidth}|}
\hline
\textbf{Language} & \textbf{Evaluation Tasks} \\
\hline
Hungarian & translated versions of BoolQ \cite{clark2019boolq} and Natural Questions \cite{47761} \\
\hline
Thai &  XQuAD \cite{Artetxe:etal:2019} and a translated version of ReCoRD \cite{Zhang2018ReCoRDBT} \\
\hline
% Add more rows as needed
\end{tabular}
\end{table}







\paragraph{Summarization} In this category of tasks, we let the model freely generate a summary for each prompt and we report average ROUGE-2 score between the model output and ground truth. 

\begin{table}[!h]
\centering
\caption{Summarization Evaluation Benchmarks}
\label{tab:table-label}
\begin{tabular}{|c|m{0.8\textwidth}|}
\hline
\textbf{Language} & \textbf{Evaluation Tasks} \\
\hline
Hungarian & Translated version of XSum \cite{narayan2018dont}. \\
\hline
Thai &  ThaiSum \cite{chumpolsathien_2020} \\
\hline
% Add more rows as needed
\end{tabular}
\end{table}


\paragraph{Translation} In this category of tasks, we let the model to freely generate translated text and we report BLEU score between the model output and the ground truth answer. 

\begin{table}[!h]
% \vspace{-42pt}
\centering
\caption{Translation Evaluation Benchmarks}
\label{tab:table-label}
\begin{tabular}{|c|m{0.8\textwidth}|}
\hline
\textbf{Language} & \textbf{Evaluation Tasks} \\
\hline
Hungarian & wmt 2009 en-hu and wmt 2009 hu-en \footnote{\href{https://www.statmt.org/wmt09/}{https://www.statmt.org/wmt09/}} datasets \\
\hline
Thai & WIT3 Ted Talks Corpus \cite{cettolo-etal-2012-wit3} \\
\hline
% Add more rows as needed
\end{tabular}
\end{table}
