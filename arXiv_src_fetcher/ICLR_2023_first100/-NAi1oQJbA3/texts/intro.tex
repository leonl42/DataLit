Multilingual large language models have become prevalent recently \cite{workshop2023bloom, muennighoff2023crosslingual, xue2021mt5, conneau2020unsupervised, goyal2021largerscale, shliazhko2022mgpt}, and have shown strong cross lingual knowledge and capability transfer \cite{lin2022fewshot, xu2021bert, yong2023bloom1, phang2020english, ebrahimi2021adapt, ye2023language, armengolestapé2021multilingual}. However, these multilingual models tend to perform poorly on low-resource languages. On top of this, training models for low-resource languages from scratch is also challenging due to a lack of training data and prohibitive computational requirements. These challenges, along with the prevalence open sourced English models creates an interesting opportunity to see how they can be adapted to new languages quickly, without wasting resources by pretraining from scratch. While prior work \cite{yong2023bloom1, phang2020english, ebrahimi2021adapt, ogueji2021small, armengolestapé2021multilingual} has studied this concept, there are two important questions that warrant further investigation.

\textbf{\emph{How to efficiently encode the new language?}} Byte Pair Encoding (BPE) \cite{gage1994new} tokenizers are commonly used in LLMs including GPT\cite{radford2019language, brown2020language}, Llama \cite{touvron2023llama, touvron2023llama2} and BLOOM \cite{workshop2023bloom, muennighoff2023crosslingual}. These tokenizers are able to encode text at the byte level so that they can generalize to characters that are outside of their vocabulary; this means that any BPE tokenizer can be used for all languages. However, the BPE tokenizer has poor tokenization efficiency if it was not trained on a given language. For example, the original English-centric GPT2 tokenizer with a vocabulary size of 50k needs to use 3.8 times more tokens to encode Thai compared to a smaller tokenizer with a vocabulary size of 5k that is trained on Thai. This will inevitably cost us 3.8 times more compute in both training and inference. Furthermore, it has been shown that models with sub-optimal tokenizers can also have worse evaluation results \cite{rust2021good, stollenwerk2023training}.
In our work, we show how to improve tokenizer fertility\cite{acs2019} by replacing the least frequent tokens in the base model with tokens from the new language.

\textbf{\emph{How to avoid catastrophic forgetting?}} Many works have shown that when continuing to train a LLM on data from a new domain, it undergoes catastrophic forgetting of the original domain it was trained on \cite{french1999catastrophic}, and similar issues appear when training on a new language \cite{french1999catastrophic, yong2023bloom1, cahyawijaya2023instructalign, muennighoff2023crosslingual, chalkidis2021multieurlex, phang2020english, vu2022overcoming}. Different training paradigms including instruction-align\cite{cahyawijaya2023instructalign}, MAD-X \cite{pfeiffer-etal-2020-mad}, (IA)$^3$ \cite{liu2022few} are proposed to alleviate this issue, 
while mixing the training corpus from different languages \cite{yong2023bloom1, ebrahimi2021adapt, ogueji2021small, pires2023sabia, ye2023language, armengolestapé2021multilingual} is an approach shared among all the methods above. 
% Aside from instruction tuning, we also confirm the same forgetting issue during continuous pretraining stage, as this is a necessary step for us after we introduced new tokens. 
Thus, in order to avoid forgetting, we study how to use the minimum amount of mixed training data in both continuous pretraining and instruction tuning stages.

We adapt an English-centric model to Hungarian and Thai, and our evaluations show that adding new tokens and mixing training data from both languages can retain the model's English capabilities in addition to improving the models ability to learn the new language. Some contemporary works explore similar, but far less efficient methods of training LLMs on low resource languages. \cite{sengupta2023jais} builds an English-Arabic bilingual LLM, but they train it from scratch; while \cite{pires2023sabia} builds one for English-Portuguese, but it does not optimize the tokenizer or mix the training data.  