% As large language models are becoming more popular, it will never be feasible for the model to perform well on all existing languages or domains. 
% Our work explores a general recipe for how to take an existing model and efficiently adapt it to a language that the model has not seen before. 
% We introduce a novel method for improving the tokenizer efficiency of the model on the new language, with minimal impact on the previous language the model was trained on. 
% We also demonstrate that mixing data from the the model's original training set with new data helps maintain the model's knowledge, and even improves the model's ability to learn the new language. Lastly, we explore the instruction tuning or alginment stage of training LLMs and find that some data is required from the new language to avoid catastrophic forgetting, but as little as 1\% mixed with English IT data is enough.

% \comment{@zoli: Can you please redo the conclusion section by filling the details + numbers in the following 5 sentences?}

% Recent large language models (LLM) usually exhibit sub-optimal performance on certain low-resource languages, as the training data of those models are usually dominated by the high-resource languages, or even not covering the target language at all. 
% Besides, it is challenging to train models for a certain low-resource language, especially from scratch, due to the high resource consumption and lack of high quality training data.
% In this work, we study the recipe to efficiently adapt an existing pretrained LLM to a new language without catastrophic forgetting of its original knowledge.
% In particular, we improve the encoding efficiency of the tokenizer by adding new tokens from the target language and study the data mixing recipe to mitigate forgetting.
% Our experiments on adapting an English LLM to Hungarian and Thai show that our recipe can greatly improve the model performance on the target languages with little regression on English tasks. 

In our paper, we study the recipe to efficiently adapt an existing pretrained LLM to a new language, with better tokenizer efficiency and without catastrophic forgetting of its original knowledge.
With only 10\% of tokens in the tokenizer replaced by the the new ones from the target language, it can drop the fertility by 50\% and 70\% on Hungarian and Thai respectively, with limited regression on English. This can greatly improve the efficiency of both training and inference on the new language by 2x and 3x.
In addition, with mixing training data from both languages in pretraining and IT stages, we show that this can improve the model performance on the new language while retaining the model capability on the original language.