\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andriushchenko et~al.(2020)Andriushchenko, Croce, Flammarion, and
  Hein]{andriushchenko2020square}
Andriushchenko, M., Croce, F., Flammarion, N., and Hein, M.
\newblock Square attack: a query-efficient black-box adversarial attack via
  random searchg.
\newblock \emph{In ECCV}, 2020.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{carlini2017evaluating}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock \emph{IEEE Symposium on Security and Privacy}, 2017.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Ludwig, C~Duchi, and
  Liang]{carmon2019unlabeled}
Carmon, Y., Raghunathan, A., Ludwig, S., C~Duchi, J., and Liang, P.~S.
\newblock Unlabeled data improves adversarial robustness.
\newblock \emph{In Conference on Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Chen et~al.(2017)Chen, Zhang, Sharma, Yi, and Hsieh]{chen2017zoo}
Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural
  networks without training substitute models.
\newblock \emph{In ACM}, 2017.
\newblock \doi{10.1145/3128572.3140448}.
\newblock URL \url{http://dx.doi.org/10.1145/3128572.3140448}.

\bibitem[Chen et~al.(2021)Chen, Zhang, Liu, Chang, and Wang]{chen2021robust}
Chen, T., Zhang, Z., Liu, S., Chang, S., and Wang, Z.
\newblock Robust overfitting may be mitigated by properly learned smoothening.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Croce \& Hein(2020{\natexlab{a}})Croce and Hein]{croce2020minimally}
Croce, F. and Hein, M.
\newblock Minimally distorted adversarial examples with a fast adaptive
  boundary attack.
\newblock \emph{In The European Conference on Computer Vision(ECCV)},
  2020{\natexlab{a}}.

\bibitem[Croce \& Hein(2020{\natexlab{b}})Croce and Hein]{croce2020reliable}
Croce, F. and Hein, M.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks, 2020{\natexlab{b}}.

\bibitem[Deng et~al.(2020)Deng, Zheng, Zhang, Chen, Lou, and
  Kim]{deng2020analysis}
Deng, Y., Zheng, X., Zhang, T., Chen, C., Lou, G., and Kim, M.
\newblock An analysis of adversarial attacks and defenses on autonomous driving
  models.
\newblock \emph{IEEE International Conference on Pervasive Computing and
  Communications(PerCom)}, 2020.

\bibitem[Ding et~al.(2020)Ding, Sharma, Lui, and Huang]{ding2020mma}
Ding, G.~W., Sharma, Y., Lui, K. Y.~C., and Huang, R.
\newblock Mma training: Direct input space margin maximization through
  adversarial training.
\newblock \emph{In International Conference on Learning
  Representataions(ICLR)}, 2020.

\bibitem[Finlayson et~al.(2019)Finlayson, Chung, Kohane, and
  Beam]{finlayson2019adversarial}
Finlayson, S.~G., Chung, H.~W., Kohane, I.~S., and Beam, A.~L.
\newblock Adversarial attacks against medical deep learning systems.
\newblock \emph{In Science}, 2019.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2015explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017on}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock \emph{In International Conference on Machine Learning (ICML)}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{In CVPR}, 2016.

\bibitem[Hitaj et~al.(2021)Hitaj, Pagnotta, Masi, and
  Mancini]{hitaj2021evaluating}
Hitaj, D., Pagnotta, G., Masi, I., and Mancini, L.~V.
\newblock Evaluating the robustness of geometry-aware instance-reweighted
  adversarial training.
\newblock \emph{archive}, 2021.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{In Conference on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Ilyas et~al.(2018)Ilyas, Engstrom, Athalye, and
  Lin]{ilyas2018blackbox}
Ilyas, A., Engstrom, L., Athalye, A., and Lin, J.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock \emph{In International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{Proceedings of the international conference on Uncertainty in
  Artificial Intelligence}, 2018.

\bibitem[Jantre et~al.(2022)Jantre, Madireddy, Bhattacharya, Maiti, and
  Balaprakash]{jantre2022sequential}
Jantre, S., Madireddy, S., Bhattacharya, S., Maiti, T., and Balaprakash, P.
\newblock Sequential bayesian neural subnetwork ensembles.
\newblock \emph{arXiv}, 2022.

\bibitem[Jiang et~al.(2019)Jiang, Ma, Chen, Bailey, and
  Jiang]{jiang2019blackbox}
Jiang, L., Ma, X., Chen, S., Bailey, J., and Jiang, Y.-G.
\newblock Black-box adversarial attacks on video recognition models.
\newblock \emph{In ACM}, 2019.

\bibitem[Krizhevsky(2009)]{krizhevsky09learningmultiple}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Kurakin et~al.(2017)Kurakin, J~Goodfellow, and
  Bengio]{kurakin2016adversarial}
Kurakin, A., J~Goodfellow, I., and Bengio, S.
\newblock Adversarial examples in the physical world.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Li et~al.(2020)Li, Xu, Xiao, Li, and Shen]{li2020adaptive}
Li, Y., Xu, X., Xiao, J., Li, S., and Shen, H.~T.
\newblock Adaptive square attack: Fooling autonomous cars with adversarial
  traffic signs.
\newblock \emph{IEEE Internet of Things Journal}, 2020.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Ma et~al.(2020)Ma, Niu, Gu, Wang, Zhao, Bailey, and
  Lu]{ma2021understanding}
Ma, X., Niu, Y., Gu, L., Wang, Y., Zhao, Y., Bailey, J., and Lu, F.
\newblock Understanding adversarial attacks on deep learning based medical
  image analysis systems.
\newblock \emph{Pattern Recognition}, 2020.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Morgulis et~al.(2019)Morgulis, Kreines, Mendelowitz, and
  Weisglass]{morgulis2019fooling}
Morgulis, N., Kreines, A., Mendelowitz, S., and Weisglass, Y.
\newblock Fooling a real car with adversarial traffic signs.
\newblock \emph{ArXiv}, 2019.

\bibitem[M{\"{u}}ller et~al.(2019)M{\"{u}}ller, Kornblith, and
  Hinton]{muller2019when}
M{\"{u}}ller, R., Kornblith, S., and Hinton, G.~E.
\newblock When does label smoothing help?
\newblock \emph{In Conference on Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011svhn}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning 2011}, 2011.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, and
  Goodfellow]{papernot2016transferability}
Papernot, N., McDaniel, P., and Goodfellow, I.
\newblock Transferability in machine learning: from phenomena to black-box
  attacks using adversarial samples.
\newblock \emph{arXiv}, 2016.

\bibitem[Papernot et~al.(2017)Papernot, McDaniel, Goodfellow, Jha, Celik, and
  Swami]{papernot2017practical}
Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z.~B., and Swami,
  A.
\newblock Practical black-box attacks against machine learning.
\newblock \emph{In ACM}, 2017.

\bibitem[Papernot et~al.(2018)Papernot, McDaniel, Sinha, and
  Wellman]{papernot2016science}
Papernot, N., McDaniel, P., Sinha, A., and Wellman, M.
\newblock Towards the science of security and privacy in machine learning.
\newblock \emph{2018 IEEE European Symposium on Security and Privacy
  (EuroS\&P)}, 2018.

\bibitem[Pereyra et~al.(2017)Pereyra, Tucker, Chorowski, Kaiser, and
  Hinton]{pereyra2017regularzing}
Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L., and Hinton, G.~E.
\newblock Regularizing neural networks by penalizing confident output
  distributions.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Rade \& Moosavi-Dezfolli(2022)Rade and
  Moosavi-Dezfolli]{rade2022reducing}
Rade, R. and Moosavi-Dezfolli, S.-M.
\newblock Recuding excessive margin to achieve a better accuracy vs. robustness
  trade-off.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Rebuffi et~al.(2021)Rebuffi, Gowal, Calian, Stimberg, Wiles, and
  Mann]{rebuffi2021data}
Rebuffi, S.-A., Gowal, S., Calian, D.~A., Stimberg, F., Wiles, O., and Mann,
  T.~A.
\newblock Data augmentation can improve robustness.
\newblock \emph{In Conference on Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{rice2020overfitting}
Rice, L., Wong, E., and Kolter, J.~Z.
\newblock Overfitting in adversarially robust deep learning.
\newblock \emph{In International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2014intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2014.

\bibitem[Wang et~al.(2020)Wang, Zou, Yi, Bailey, Ma, and Gu]{wang2020improving}
Wang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., and Gu, Q.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Wu, D., Xia, S.-T., and Wang, Y.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock \emph{In Conference on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fahsion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{archive}, 2017.

\bibitem[Xu et~al.(2021)Xu, Liu, Li, Jain, and Tang]{xu2021to}
Xu, H., Liu, X., Li, Y., Jain, A.~K., and Tang, J.
\newblock To be robust or to be fair: Towards fairness in adversarial training.
\newblock \emph{In International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019cutmix}
Yun, S., Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock \emph{In CVPR}, 2019.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{Proceedings of the British Machine Vision Conference 2016},
  2016.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, P~Xing, El~Ghaoui, and
  I~Jordan]{zhang2019theoretically}
Zhang, H., Yu, Y., Jiao, J., P~Xing, E., El~Ghaoui, L., and I~Jordan, M.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock \emph{In International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Xu, Han, Niu, Cui, Sugiyama, and
  Kankanhalli]{zhang2020attacks}
Zhang, J., Xu, X., Han, B., Niu, G., Cui, L., Sugiyama, M., and Kankanhalli,
  M.~S.
\newblock Attacks which do not kill training make adversarial learning
  stronger.
\newblock \emph{In International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Zhu, Niu, Han, Sugiyama, and
  Kankanhalli]{zhang2021geometry}
Zhang, J., Zhu, J., Niu, G., Han, B., Sugiyama, M., and Kankanhalli, M.~S.
\newblock Geometry-aware instance-reweighted adversarial training.
\newblock \emph{In International Conference on Learning Representations
  (ICLR)}, 2021.

\end{thebibliography}
