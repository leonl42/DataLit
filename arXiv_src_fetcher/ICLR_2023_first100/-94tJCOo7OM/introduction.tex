\section{Introduction}

The success of reinforcement learning (RL) typically relies on large amounts of interactions with the environment. However, in real-world scenarios, such interactions can be unsafe or costly. As an alternative, offline RL (\cite{DBLP:journals/corr/abs-2005-01643}) leverages offline datasets of transitions, collected by a behavior policy, to train a policy that can transfer to an online task. To avoid overestimation of the value function for some (out-of-sample) states in the environment, which can mislead policy learning, model-free offline RL methods (\cite{DBLP:conf/nips/KumarZTL20, DBLP:journals/corr/abs-1911-11361}) often constrain the learned policy to remain close to the behavior policy or within the support of the offline dataset. However, collecting transitions that comprehensively cover possible task scenarios, or acquiring a large volume of demonstrations from a high-quality behavior policy, can be expensive. This challenge has led to the development of offline model-based reinforcement learning (MBRL) approaches, such as  (\cite{DBLP:conf/iclr/LuBPOR22, DBLP:conf/nips/GuoSG22}). These methods train dynamics models from offline data and optimize policies using imaginary rollouts generated by the models. Notably, the dynamics modeling is independent of the behavior policy, making it possible to achieve high returns even with data collected from a random policy. Furthermore, with careful dynamics modeling and thorough simulation, the learned policy can more effectively handle the environmental stochasticity and generalize to states beyond the support of the offline dataset.

Given a dataset, there may be various potential MDPs that behave identically on the limited set of states and actions, but their dynamics and reward functions could differ, especially on out-of-sample states and actions. This implies that we are dealing with a distribution of possible world models underlying the dataset. A common strategy in offline MBRL is to learn an ensemble of world models and treat them equally. For instance, when determining the next state, a world model is uniformly sampled from the ensemble and generate its prediction. However, different ensemble members may perform better in different regions of the state-action space, making it necessary to adapt the belief over each ensemble member based on the experience accumulated since the start of the episode. The Bayes Adaptive Markov Decision Process (BAMDP, \cite{duff2002optimal}) provides a principled framework for modelling such an adaptive process. We show in Section \ref{DeepEns} that, despite the need for Bayesian posterior updates, BAMDPs can still be efficiently simulated using deep ensembles. BAMCP (\cite{DBLP:journals/jair/GuezSD13}) is an efficient online planning method for solving BAMDPs. However, BAMCP has several limitations: (1) it relies on a ground-truth world model for planning; (2) it is restricted to discrete state and action spaces; and (3) its outcome is an action choice at a particular state, rather than a policy function. To address these challenges: (1) we apply a reward penalty (defined with the adapting belief) to construct a pessimistic BAMDP, preventing overexploitation of inaccurate world models (learned from the offline dataset); (2) we propose a novel planning algorithm to solve BAMDPs in continuous state and action spaces by extending BAMCP with double progressive widening (\cite{DBLP:conf/pkdd/AugerCT13}); and (3) we integrate the planning component as a policy improvement operator within policy iteration RL methods (\cite{Sutton1998}), enabling the derivation of a policy suitable for real-time execution from the planning results. Specifically, the planning process is carried out through Monte Carlo Tree Search on a BAMDP. Integrating search with RL allows for the use of significantly more computation input, thereby improving policy learning performance. Grounded in the ``scaling law", this paradigm has seen tremendous success in sophisticated policy learning, as demonstrated in (\cite{DBLP:journals/nature/SilverSSAHGHBLB17, DBLP:journals/nature/SchrittwieserAH20, deepmind2024ai}). Its application to offline MBRL, particularly in continuous control tasks, is a promising area for exploration. 

To summarize, the main contributions of this work include: 
(1) Introducing BAMDPs to handle model uncertainties in offline MBRL;
(2) Proposing an efficient Bayes Adaptive Monte Carlo Tree Search method for planning in continuous, stochastic BAMDPs;
(3) Developing the first algorithm to successfully integrate Bayesian RL, offline MBRL, and deep search for sophisticated policy learning in continuous control under highly stochastic environments;
(4) Demonstrating the improvements brought by Bayesian RL and deep search across twelve D4RL MuJoCo tasks and three target tracking tasks in a stochastic tokamak control scenario (for nuclear fusion), highlighting the potential of our algorithm to tackle challenging, real-world problems.