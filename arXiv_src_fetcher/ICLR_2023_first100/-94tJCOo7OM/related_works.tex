\section{Related Works} \label{rel_works}

Offline RL (\cite{DBLP:journals/corr/abs-2005-01643, DBLP:journals/corr/abs-2402-13777}) enables an agent to learn control policies from datasets of environment transitions pre-collected by a behavior policy $\mu$, i.e., $\mathcal{D}_\mu = \{[(s_t^i, a_t^i, r_t^i)_{t=1}^T]_{i=1}^N\}$, circumventing the need for potentially expensive or unsafe online interactions. Model-free offline RL methods directly learn value/policy functions from \(\mathcal{D}_\mu\) but restrict the policy to remain close to \(\mu\) or to behave within the support of \(\mathcal{D}_\mu\). Offline Model-based RL (MBRL) methods, on the other hand, explicitly learn world models $\mathcal{M}_\theta$ from $\mathcal{D}_\mu$ (through supervised learning) and adopt $\mathcal{M}_\theta$ as a surrogate simulator, enabling the learned policy to possibly generalize to states beyond $\mathcal{D}_\mu$. Specifically, both planning methods (\cite{DBLP:conf/iclr/ArgensonD21, DBLP:conf/ijcai/ZhanZX22, DBLP:journals/ral/DiehlSKHB23}), such as MPC (\cite{garcia1989model}), and RL methods (\cite{DBLP:conf/nips/YuTYEZLFM20, DBLP:conf/nips/KidambiRNJ20, DBLP:conf/iclr/LuBPOR22, DBLP:conf/nips/YuKRRLF21, DBLP:conf/nips/GuoSG22}) can be applied on top of the learned $\mathcal{M}_\theta$. However, since $\mathcal{D}_\mu$ may not span the entire state-action space, $\mathcal{M}_\theta$ is unlikely to be globally accurate. Learning/Planning without any safeguards against such model inaccuracy can yield poor results. In this case, the authors of (\cite{DBLP:conf/nips/YuTYEZLFM20, DBLP:conf/nips/KidambiRNJ20, DBLP:conf/iclr/LuBPOR22}) propose learning an ensemble of world models, using ensemble-based uncertainty estimations to construct a pessimistic MDP (P-MDP), and learning a near-optimal policy atop it. Ideally, for any policy, the performance in the real environment is lower-bounded by the performance in the corresponding P-MDP (with high probability), thus avoiding being overly optimistic about an inaccurate model. For practical implementation of the P-MDP, \cite{DBLP:conf/iclr/LuBPOR22} conduct a thorough empirical study to compare a series of heuristics. However, none of these offline MBRL methods have modeled the problem as a BAMDP, even though Bayesian RL/planning provides a principled framework for handling model uncertainty. Additionally, with the learned model \(\mathcal{M}_\theta\), deep and broad search in the policy learning process is possible, but the offline RL methods introduced above have not attempted such search-based policy improvement.

Monte-Carlo Tree Search (MCTS, \cite{browne2012survey}) has been successfully integrated with RL, as exemplified by AlphaZero (\cite{DBLP:journals/corr/abs-1712-01815}) and MuZero (\cite{DBLP:journals/nature/SchrittwieserAH20}). These methods have achieved superhuman performance in domains requiring highly precise and sophisticated decision-making processes. 
AlphaZero relies on given world models, whereas MuZero learns the world model and policy simultaneously by interacting with the environment. Although there have been various extensions of MuZero (\cite{DBLP:conf/icml/HubertSABSS21, DBLP:conf/nips/SchrittwieserHM21, DBLP:conf/nips/YeLKAG21, DBLP:conf/iclr/DanihelkaGSS22, DBLP:conf/iclr/AntonoglouSOHS22, oren2022mcts, DBLP:journals/corr/abs-2305-17327, zhao2024a}), most algorithms are designed for online MBRL. According to \cite{DBLP:conf/nips/NiuPYLZRHLL23}, the applications of MuZero in offline learning, especially for continuous control in highly stochastic environments, which is our focus, still require significant improvement. Although search-based, MuZero learns a dynamics model and a reward model in a latent state space rather than the real state space, does not apply uncertainty estimation of the learned models as in usual offline MBRL methods, and does not adopt the Bayesian RL framework, making it fundamentally different from our algorithm design.

As mentioned in Section \ref{back}, Bayes-optimal planning is typically intractable. Approximate methods, such as (\cite{DBLP:conf/uai/AsmuthLLNW09, DBLP:conf/uai/SorgSL10, DBLP:conf/pkdd/CastroP10, asmuth2011approaching, DBLP:conf/icml/WangWHL12, DBLP:conf/adprl/FonteneauBM13, DBLP:journals/jair/GuezSD13, DBLP:journals/iet-cps/SladeSK20}) have been developed. As a representative work, BAMCP (\cite{DBLP:journals/jair/GuezSD13}) adopts MCTS for Bayes Adaptive planning and is shown to converge in probability to a near Bayes-optimal policy at the root node of the search tree. However, all these methods cannot be directly applied to large-scale MDPs with continuous state and action spaces. \cite{DBLP:conf/nips/GuezHSD14} and \cite{DBLP:journals/corr/abs-2010-15948} attempt to extend the use of BAMCP to scenarios with continuous states and actions using value/policy function approximations, but they replace MCTS with simple Monte Carlo rollouts (\cite{DBLP:journals/heuristics/BertsekasC99}) and do not utilize UCT (\cite{DBLP:conf/ecml/KocsisS06}). Tree search makes branches at each node in the tree, providing a more thorough look-ahead search than simple rollouts, and UCT, as an efficient exploration algorithm, is the basis for the convergence guarantee of BAMCP. Moreover, these planning algorithms are not designed for offline MBRL. How to incorporate search-based planning in the policy improvement stage of RL and how to handle the uncertainty of the learned world model require further exploration, which are shown in the following section.
