\section{Background} \label{back}

A Markov Decision Process (MDP, \cite{puterman2014markov}) is described as a tuple $\mathcal{M}=\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$. $\mathcal{S}$ and $\mathcal{A}$ are the state space and action space, respectively. $\mathcal{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta_\mathcal{S}$ is the dynamics function and $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta_{[0, 1]}$ is the reward function, where $\Delta_\mathcal{X}$ denotes the set of possible probability distributions on $\mathcal{X}$. $\gamma \in [0, 1)$ is a discount factor. The goal of solving an MDP is to find a policy $\pi: \mathcal{S} \rightarrow \Delta_\mathcal{A}$ that maximizes the expected return, defined as the cumulative discounted reward over time. 

A Bayes Adaptive MDP (BAMDP, \cite{duff2002optimal}) can model scenarios where the precise MDP $\mathcal{M}_\theta=\langle \mathcal{S}, \mathcal{A}, \mathcal{P}_\theta, \mathcal{R}_\theta, \gamma \rangle$ is uncertain but is known to follow a prior distribution $b_0(\theta)$. During planning, a Bayes-optimal agent would update its belief over the MDP based on experience. Formally, a BAMDP can be described as a tuple $\mathcal{M}^+=\langle \mathcal{S}^+, \mathcal{A}, \mathcal{P}^+, \mathcal{R}^+, \gamma \rangle$. $\mathcal{S}^+$ denotes the space of information states $(s, b)$, which is a composition of the physical state and the current belief over the MDP. After each transition $(s, a, r, s')$, the belief is updated to the corresponding Bayesian posterior: $b'(\theta) \propto b(\theta) P((s, a, r, s') | \theta) = b(\theta) \mathcal{P}_\theta(s' | s, a) \mathcal{R}_\theta(r | s, a)$. Accordingly, $\mathcal{P}^+$ and $\mathcal{R}^+$ can be defined as follows:
\begin{equation} \label{bamdp-mdp}
    \mathcal{P}^+((s', b'') | (s, b), a) = \mathds{1}(b''=b')\int_{\theta} \mathcal{P}_\theta(s' | s, a) b(\theta) d\theta,\ \mathcal{R}^+((s, b), a) = \int_{\theta} \mathcal{R}_\theta(s, a) b(\theta) d\theta
\end{equation}
The Q-function that satisfies the Bellman optimality equations: $(\forall x=(s, b) \in \mathcal{S}^+, a \in \mathcal{A})$
\begin{equation}
    Q^*(x, a) = \mathcal{R}^+(x, a) + \gamma \int_{x'} V^*(x') \mathcal{P}^+(x'|x, a) d x',\ V^*(x') = \max_a Q^*(x', a)
\end{equation}
is the Bayes-optimal Q-function and $\pi^*(s, b) = \arg \max_a Q^*((s, b), a)$ is the Bayes-optimal policy. Actions derived from $\pi^*$ are executed in the real MDP and constitute the best course of action for a Bayesian agent with respect to its prior belief $b_0$ over the environment (\cite{DBLP:conf/nips/GuezHSD14}). A BAMDP can be cast into a partially observable MDP (POMDP)\footnote{Such a POMDP can be defined as a tuple $\langle \mathcal{S}^+, \mathcal{A}, \mathcal{P}^+, \mathcal{R}^+, \mathcal{Z}, \mathcal{S}, \gamma \rangle$, according to its original definition in (\cite{littman2009tutorial}).} by viewing $\mathcal{S}^+$ and $\mathcal{S}$ as the state and observation spaces, respectively, and setting the observation function as $\mathcal{Z}((s', b'), a, o) = \mathds{1}(s'=o)$. ($\mathcal{Z}$ specifies the probability of observing $o$ under the previous action $a$ and current state $(s', b')$.) As a result, approaches developed for POMDPs can potentially be used to solve BAMDPs. 

Bayesian Reinforcement Learning (BRL, \cite{DBLP:journals/ftml/GhavamzadehMPT15}), as introduced above, is a principled approach to dealing with uncertainty in the world model $\mathcal{M}_\theta$ and has two main advantages: (1) Domain knowledge can be injected by defining a proper prior belief; (2) A Bayes Adaptive policy solves the exploration-exploitation dilemma by explicitly including the belief in its state representation and incorporating belief updates into the planning process (\cite{DBLP:conf/uai/SorgSL10}). However, Bayes-optimal planning is generally intractable, for which we introduce some approximate methods in the next section.