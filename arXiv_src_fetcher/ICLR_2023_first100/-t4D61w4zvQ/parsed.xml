<?xml version="1.0" encoding="UTF-8"?>
<?latexml searchpaths="/home/miri/Documents/DataLit/arXiv_src_fetcher/ICLR_2023_first100/-t4D61w4zvQ"?>
<?latexml class="article"?>
<?latexml package="iclr2023_conference,times"?>
<!--  %Optional math commands from https://github.com/goodfeli/dlbook˙notation. --><!--  %%%%% NEW MATH DEFINITIONS %%%%% --><?latexml package="amsmath,amsfonts,bm"?>
<!--  %Laplace distribution --><!--  %See usage in notation.tex. Chosen to match Daphne’s book. --><?latexml package="caption" options="font=small,labelfont=bf,tableposition=top"?>
<?latexml package="wrapfig"?>
<?latexml package="tabularx,booktabs,subcaption"?>
<?latexml package="multirow"?>
<?latexml package="hyperref"?>
<?latexml package="url"?>
<?latexml package="graphicx"?>
<!--  %Authors must not appear in the submitted version. They should be hidden --><!--  %as long as the “iclrfinalcopy macro remains commented out below. --><!--  %Non-anonymous submissions will be rejected without review. --><!--  %The “author macro works with any number of authors. There are two commands --><!--  %used to separate the names and addresses of multiple authors: “And and “AND. --><!--  %Using “And between authors leaves it to “LaTeX–˝ to determine where to break --><!--  %the lines. Using “AND forces a linebreak at that point. So, if “LaTeX–˝ --><!--  %puts 3 of 4 authors names on the first line, and the last on the second --><!--  %line, try using “AND instead of “And before the third author name. --><?latexml RelaxNGSchema="LaTeXML"?>
<document xmlns="http://dlmf.nist.gov/LaTeXML" class="ltx_authors_1line">
  <resource src="LaTeXML.css" type="text/css"/>
  <resource src="ltx-article.css" type="text/css"/>
  <para xml:id="p1">
    <ERROR class="undefined">\iclrfinalcopy</ERROR>
<!--  %Uncomment for camera-ready version, but NOT for submission. 
     %**** iclr2023˙conference.tex Line 50 **** -->  </para>
  <title>Temporal Coherent Test-Time Optimization for Robust Video Classification</title>
  <creator role="author">
    <personname>Chenyu Yi<sup><text font="italic">1</text></sup>  Siyuan Yang<sup><text font="italic">1,2</text></sup>  Yufei Wang <sup><text font="italic">1</text></sup>  Haoliang Li<sup>3</sup>  Yap-Peng Tan<sup>1</sup>  Alex C. Kot<sup>1</sup>  <break/><sup>1</sup>School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore <break/><sup>2</sup>Interdisciplinary Graduate Programme, Nanyang Technological University, Singapore <break/><sup>3</sup>Department of Electrical Engineering, City University of Hong Kong, China<break/>{yich0003,siyuan005,yufei001}@e.ntu.edu.sg  haoliang.li@cityu.edu.hk  {eyptan,eackot}@ntu.edu.sg</personname>
    <contact role="thanks">Corresponding Author</contact>
  </creator>
  <abstract name="Abstract">
    <p>Deep neural networks are likely to fail when the test data is corrupted in real-world deployment (e.g., blur, weather, etc.). Test-time optimization is an effective way that adapts models to generalize to corrupted data during testing, which has been shown in the image domain. However, the techniques for improving video classification corruption robustness remain few. In this work, we propose a <text font="bold">Te</text>mporal <text font="bold">Co</text><!--  %TeCo uses global content from video clips and optimizes models for entropy minimization. -->herent Test-time Optimization framework (TeCo) to utilize spatio-temporal information in test-time optimization for robust video classification. To exploit information in video with self-supervised learning,
TeCo minimizes the entropy of the prediction based on the global content from video clips.
Meanwhile, it also feeds local content to regularize the temporal coherence at the feature level. TeCo retains the generalization ability of various video classification models and achieves significant improvements in corruption robustness across Mini Kinetics-C and Mini SSV2-C. Furthermore, TeCo sets a new baseline in video classification corruption robustness via test-time optimization.</p>
  </abstract>
  <section inlist="toc" xml:id="S1">
    <tags>
      <tag>1</tag>
      <tag role="autoref">section 1</tag>
      <tag role="refnum">1</tag>
      <tag role="typerefnum">§1</tag>
    </tags>
    <title><tag close=" ">1</tag>Introduction</title>
    <para xml:id="S1.p1">
      <p>Deep neural networks have achieved tremendous success in many computer vision tasks when the training and test data are identically and independently distributed (i.i.d). However, the mismatch between them is common when the model is deployed in the real world (<cite class="ltx_citemacro_cite"><bibref bibrefs="wang2022variational,li2020domain" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>). For example, weather changes like rain and fog, and data pre-processing like saturate adjustment and compression can corrupt the test data. Many works show that the common corruptions arising in nature can degrade the performance of models at test time significantly <cite class="ltx_citemacro_citep">(<bibref bibrefs="hendrycks2019benchmarking,yi2021benchmarking,kar20223d,geirhos2018generalisation,yu2022towards" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>. In video classification, <cite class="ltx_citemacro_cite"><bibref bibrefs="yi2021benchmarking" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite> demonstrates the vulnerability of models against corruptions like noise, blur, and weather variations.</p>
    </para>
<!--  %creates a benchmark to study the impact of corruptions in test video data. It -->    <para xml:id="S1.p2">
      <p>Various techniques have been proposed to improve the robustness of models against common corruptions (a.k.a.corruption robustness). The most popular direction is increasing the diversity of input training data via data augmentations and applying regularization at training time <cite class="ltx_citemacro_citep">(<bibref bibrefs="zheng2016improving,wang2021augmax,hendrycks2019augmix,rusak2020increasing,wang2020heterogeneous" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite><!--  %The trained model will be tested in a ‘one-for-all’ manner. -->.
It trains one model and evaluates on all types of corruption.
However, the training data is often unavailable because of privacy concerns and policy constraints, making it more challenging to deploy models in the real world.
In this work, we focus on the direction of test-time optimization.
Under such a scheme, the parameters of one model will be optimized by one type of corrupted test data specifically.
Test-time optimization updates the model to fit into the deployment environment at test time, without access to training data.
There are several test-time optimization techniques emerging in image-based tasks <!--  %In corruption robustness, “cite–hendrycks2019benchmarking˝ make an assumption that the tested model has no prior knowledge of the test data. 
     %However, we can update the model to fit into the deployment environment at test-time. 
     %**** iclr2023˙conference.tex Line 75 **** --><cite class="ltx_citemacro_citep">(<bibref bibrefs="wang2020tent,schneider2020improving,pmlr-v119-liang20a,sun2020test" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>.
However, we find these techniques are not able to generalize to video-based corruption robustness tasks well from empirical analysis.
We hypothesize the gap between image and video-based tasks comes from several aspects.
Firstly, the corruptions in the video can change temporal information, which requires the model to be both generalizable and robust <cite class="ltx_citemacro_citep">(<bibref bibrefs="yi2021benchmarking" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>.
Hence, improving model robustness against the corruptions in video like bit error, and frame rate conversion is more challenging.
Secondly, video input data has a different format from image data. For example, video data has a much larger size than image data.
It is impractical to use a similar batch size as image-based tasks (e.g., batch size of 256 in Tent <cite class="ltx_citemacro_citep">(<bibref bibrefs="wang2020tent" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>), though the batch size is an important hyper-parameter in test-time optimization <cite class="ltx_citemacro_citep">(<bibref bibrefs="wang2020tent,schneider2020improving" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>.
Lastly, these techniques ignore the huge information hidden in the temporal dimension.</p>
    </para>
    <para xml:id="S1.p3">
      <p>To improve the video classification model robustness against corruptions consistently, we propose a temporal coherent test-time optimization framework <text font="bold">TeCo</text>.
TeCo is a test-time optimization technique with two self-supervised objectives.
We propose to build our method upon the test-time optimization which updates all the parameters in shallow layers and only normalization layers in the deep layers.
It can benefit from both training and test data, and such an optimization strategy remains part of model parameters and statistics obtained at training time and updates the unfrozen parameters with test information.
Besides, we utilize global and local spatio-temporal information for self-supervision.
We use uniform sampling to ensure global information in input video data and optimize the model parameters via entropy minimization.
By dense sampling, we extract another local stream that has a smaller time gap between consecutive frames.
Due to the smooth and continuous nature of adjacent frames in the video <cite class="ltx_citemacro_citep">(<bibref bibrefs="li2008unsupervised,wood2016development" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>, we apply a temporal coherence regularization as a self-supervisor in test-time optimization on the local pathway.
As such, our proposed technique enables the model to learn more invariant features against corruption.
As a result, TeCo achieves promising robustness improvement on two large-scale video corruption robustness datasets, Mini Kinetics-C and Mini SSV2-C.
Its performance is superior across various video classification backbones.
TeCo increases average accuracy by 6.5% across backbones on Mini Kinetics-C and by 4.1% on Mini SSV2-C, which is better than the baseline methods Tent (1.9% and 0.9%) and SHOT (1.9% and 2.0%).
Additionally, We show that TeCo can guarantee the smoothness between consequent frames at the feature level, which indicates the effectiveness of temporal coherence regularization.</p>
    </para>
    <para xml:id="S1.p4">
      <p>We summarize our contributions as follows:</p>
      <itemize xml:id="S1.I1">
        <item xml:id="S1.I1.i1">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">1st item</tag>
          </tags>
          <para xml:id="S1.I1.i1.p1">
            <p>To the best of our knowledge, we make the first attempt to study the test-time optimization techniques for video classification corruption robustness across datasets and model architectures.</p>
          </para>
        </item>
        <item xml:id="S1.I1.i2">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">2nd item</tag>
          </tags>
          <para xml:id="S1.I1.i2.p1">
            <p>We propose a novel test-time optimization framework TeCo for video classification, which utilizes spatio-temporal information in training and test data to improve corruption robustness.</p>
          </para>
        </item>
        <item xml:id="S1.I1.i3">
          <tags>
            <tag>•</tag>
            <tag role="autoref">item </tag>
            <tag role="typerefnum">3rd item</tag>
          </tags>
          <para xml:id="S1.I1.i3.p1">
            <p>For video corruption robustness, TeCo outperforms other baseline test-time optimization techniques significantly and consistently on Mini Kinetics-C and Mini SSV2-C datasets.</p>
          </para>
        </item>
      </itemize>
    </para>
  </section>
  <section inlist="toc" xml:id="S2">
    <tags>
      <tag>2</tag>
      <tag role="autoref">section 2</tag>
      <tag role="refnum">2</tag>
      <tag role="typerefnum">§2</tag>
    </tags>
    <title><tag close=" ">2</tag>Related Works</title>
<!--  %“begin–wrapfigure˝–r˝–0.35“textwidth˝ 
     %“begin–center˝ 
     %“includegraphics[width=0.35“textwidth]–./images/kinetics-c˙sample.pdf˝ 
     %“end–center˝ 
     %“caption–Example of corruptions in Mini Kinetics-C.˝ 
     %“end–wrapfigure˝ -->    <subsection inlist="toc" xml:id="S2.SS1">
      <tags>
        <tag>2.1</tag>
        <tag role="autoref">subsection 2.1</tag>
        <tag role="refnum">2.1</tag>
        <tag role="typerefnum">§2.1</tag>
      </tags>
      <title><tag close=" ">2.1</tag>Problem Setting</title>
      <para xml:id="S2.SS1.p1">
        <p><text font="bold"><!--  %“begin–wrapfigure˝ % ¡========================================== -->Corruption Robustness.<!--  %%“framebox[4.0in]–$“;$˝ 
     %“subfloat– 
     %“includegraphics[width=0.4“linewidth]–./images/kinetics-c˙sample.pdf˝ 
     %˝ 
     %“caption–Full results on Mini Kinetics-C, with a backbone of 3D ResNet18.˝ 
     %**** iclr2023˙conference.tex Line 125 **** 
     %“label–fig:full-kinetics-c˝ 
     %“end–wrapfigure˝ --></text>
Deep neural networks are vulnerable to common corruptions generated in real-world deployment <cite class="ltx_citemacro_citep">(<bibref bibrefs="geirhos2017comparing,hendrycks2019benchmarking" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.
<cite class="ltx_citemacro_cite"><bibref bibrefs="hendrycks2019benchmarking" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> firstly propose ImageNet-C to benchmark the common corruption robustness of deep learning models.
It assumes the tested model has no prior knowledge of the corruption arising during test time.
The model is trained with clean data while tested on corrupted data.
Under such a setting, we are able to estimate the overall robustness of models against corruption.
In the following, benchmark studies and techniques across various computer vision tasks are booming <cite class="ltx_citemacro_citep">(<bibref bibrefs="kar20223d,michaelis2019benchmarking,kamann2020benchmarking" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.
These studies on corruption robustness bridge the gap between research in well-setup lab environments and deployment in the field.
Recently, studies have emerged on corruption robustness in video classification <cite class="ltx_citemacro_citep">(<bibref bibrefs="yi2021benchmarking,Wu_2020_CVPR" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite><!--  %The temporal dimension introduced in video data brings more types of corruptions like bit error, and frame rate change, but contains more information for obtaining robust video classification models. -->.
In this work, we tap the potential of spatio-temporal information in video data and improve the corruption robustness of video classification models during testing.</p>
      </para>
      <para xml:id="S2.SS1.p2">
        <p><text font="bold">Video Classification.</text><!--  %Video classification has been one of fast developing areas in computer vision. -->
Recently, with the introduction of a number of large-scale video datasets such as Kinetics <cite class="ltx_citemacro_citep">(<bibref bibrefs="carreira2017quo" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, Something-Something V2 (SSV2) <cite class="ltx_citemacro_citep">(<bibref bibrefs="goyal2017something" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, and Sports1M <cite class="ltx_citemacro_citep">(<bibref bibrefs="karpathy2014large" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, video classification has attracted increasing attention.
Most existing video classification works mainly focus on two perspectives: improving accuracy <cite class="ltx_citemacro_citep">(<bibref bibrefs="simonyan2014two,wang2016temporal,carreira2017quo,hara2017learning,xie2018rethinking,ECCV2020ysy,bertasius2021space,li2022mvitv2" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> and model efficiency <cite class="ltx_citemacro_citep">(<bibref bibrefs="feichtenhofer2019slowfast,lin2019tsm,feichtenhofer2020x3d,kondratyuk2021movinets,wang2021tdn" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.
However, as mentioned above, techniques for improving video classification corruption robustness remain few.
In this paper, we focus on the optimization of video corruption robustness and conduct experiments on Mini Kinetics-C and Mini SSV2-C datasets.
The Kinetics dataset relies on spatial semantic information for video classification, while SSV2 contains more temporal information.
It enables us to evaluate the effectiveness of our proposed optimization method on these two different types of video data.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="S2.SS2">
      <tags>
        <tag>2.2</tag>
        <tag role="autoref">subsection 2.2</tag>
        <tag role="refnum">2.2</tag>
        <tag role="typerefnum">§2.2</tag>
      </tags>
      <title><tag close=" ">2.2</tag>Test-time Optimization</title>
      <para xml:id="S2.SS2.p1">
        <p>Optimizing the model with unlabeled test data has been used in many machine learning tasks <cite class="ltx_citemacro_citep">(<bibref bibrefs="shu2022,huang2020neural,shocher2018zero,azimi2022self,niu2022efficient,wang2022continual" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite><!--  %In this work, we focus on improving the robustness of video classification models against corruption with test-time optimization techniques. Tent~“citep–wang2020tent˝ updates the transformation parameters in batchnorm layers with the objective of entropy minimization. Under the same setting of fully test-time optimization, BN~“citep–schneider2020improving˝ adapts the batchnorm statistics with test data on the fly; SHOT~“citep–pmlr-v119-liang20a˝ integrates entropy minimization and pseudo labeling for self-supervised optimization. -->.
These methods only require the pretrained model and test data for optimization in the inference.
Test-Time Training (TTT) <cite class="ltx_citemacro_citep">(<bibref bibrefs="sun2020test" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> and TTT++ <cite class="ltx_citemacro_citep">(<bibref bibrefs="NEURIPS2021_b618c321" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> update models at test-time, but they jointly optimize the supervised loss and the self-supervised auxiliary loss in training.
Though these methods demonstrate promising improvement in image-based corruption robustness, there is little progress made in video-based test-time optimization.
We make the first attempt to utilize the temporal information for model robustness under the test-time optimization setting.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="S2.SS3">
      <tags>
        <tag>2.3</tag>
        <tag role="autoref">subsection 2.3</tag>
        <tag role="refnum">2.3</tag>
        <tag role="typerefnum">§2.3</tag>
      </tags>
      <title><tag close=" ">2.3</tag>Temporal Coherence in Video</title>
      <para xml:id="S2.SS3.p1">
        <p>The concept of temporal coherence makes an assumption that adjacent frames in video data have semantically similar information <cite class="ltx_citemacro_citep">(<bibref bibrefs="goroshin2015unsupervised" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.
The assumption is based on the phenomenon that the visual world is smoothly varying and continuous.
The stability of the visual world in the temporal dimension is significant for many studies in biological vision <cite class="ltx_citemacro_citep">(<bibref bibrefs="li2008unsupervised,wood2016smoothness,wood2016development" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.
With the assumption, the invariance between neighboring frames in the video can provide self-supervision for many computer vision tasks <cite class="ltx_citemacro_citep">(<bibref bibrefs="jayaraman2015learning,wiskott2002slow,Dwibedi_2019_CVPR,wang2019learning" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.
For example, <cite class="ltx_citemacro_cite"><bibref bibrefs="jayaraman2015learning" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> exploits motion signal in the egocentric video to regularize image recognition task; <cite class="ltx_citemacro_cite"><bibref bibrefs="wang2019learning" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
              <bibrefphrase>(</bibrefphrase>
              <bibrefphrase>)</bibrefphrase>
            </bibref></cite> utilizes the cycle consistency in time to learn visual temporal correspondence.
In the corruption robustness problem, we use temporal coherence as a self-supervisor to regularize models to be more invariant against corruption.</p>
      </para>
    </subsection>
  </section>
  <section inlist="toc" xml:id="S3">
    <tags>
      <tag>3</tag>
      <tag role="autoref">section 3</tag>
      <tag role="refnum">3</tag>
      <tag role="typerefnum">§3</tag>
    </tags>
    <title><tag close=" ">3</tag>Method</title>
    <para xml:id="S3.p1">
      <p>We propose an end-to-end training framework to improve the robustness of video classification models against corruption at test time.
In such a framework, we optimize pre-trained deep learning models during the test time and minimize two objectives without label information.
These two objectives correspond to two pathways.
The first pathway uses a global stream as input and minimizes the entropy of prediction; the second pathway leverages a local stream as input and regularizes the temporal coherence among consecutive video frames.
We integrate these two pathways and named our framework as <text font="bold">TeCo</text>.
Figure <ref labelref="LABEL:framework-fig"/> outlines our temporal coherent test-time optimization framework.
The backbone in Figure <ref labelref="LABEL:framework-fig"/> is initialized by a pre-trained model. Any deep video classification model pretrained under a supervised learning scheme can fit into it.</p>
    </para>
    <figure inlist="lof" labels="LABEL:framework-fig" placement="t" xml:id="S3.F1">
      <tags>
        <tag><text fontsize="90%">Figure 1</text></tag>
        <tag role="autoref">Figure 1</tag>
        <tag role="refnum">1</tag>
        <tag role="typerefnum">Figure 1</tag>
      </tags>
      <graphics candidates="images/framework.pdf" class="ltx_centering" graphic="./images/framework.pdf" options="width=368.57964pt,keepaspectratio=true" xml:id="S3.F1.g1"/>
<!--  %˝ -->      <toccaption><tag close=" ">1</tag>The proposed framework TeCo at the test-time optimization stage. TeCo consists of two pathways: the local pathway uses dense video stream <Math mode="inline" tex="I_{d}" text="I _ d" xml:id="S3.F1.m1">
          <XMath>
            <XMApp>
              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
              <XMTok font="italic" role="UNKNOWN">I</XMTok>
              <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
            </XMApp>
          </XMath>
        </Math> as input. It passes through the shallow layers <Math mode="inline" tex="f" text="f" xml:id="S3.F1.m2">
          <XMath>
            <XMTok font="italic" role="UNKNOWN">f</XMTok>
          </XMath>
        </Math> and attention module <Math mode="inline" tex="h" text="h" xml:id="S3.F1.m3">
          <XMath>
            <XMTok font="italic" role="UNKNOWN">h</XMTok>
          </XMath>
        </Math> to generate an attention-based feature map <Math mode="inline" tex="z" text="z" xml:id="S3.F1.m4">
          <XMath>
            <XMTok font="italic" role="UNKNOWN">z</XMTok>
          </XMath>
        </Math>. Then it applies temporal coherence regularization on the feature map. We update all the parameters in the shallow layers in this pathway; the global pathway uses a sparse video stream <Math mode="inline" tex="I_{u}" text="I _ u" xml:id="S3.F1.m5">
          <XMath>
            <XMApp>
              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
              <XMTok font="italic" role="UNKNOWN">I</XMTok>
              <XMTok font="italic" fontsize="70%" role="UNKNOWN">u</XMTok>
            </XMApp>
          </XMath>
        </Math>, which captures the long-term information. TeCo minimizes the prediction entropy in the global pathway by updating all the parameters in the shallow layers and only normalization parameters in the deep layers.
</toccaption>
      <caption><tag close=": "><text fontsize="90%">Figure 1</text></tag><text fontsize="90%">The proposed framework TeCo at the test-time optimization stage. TeCo consists of two pathways: the local pathway uses dense video stream <Math mode="inline" tex="I_{d}" text="I _ d" xml:id="S3.F1.m6">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">I</XMTok>
                <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
              </XMApp>
            </XMath>
          </Math> as input. It passes through the shallow layers <Math mode="inline" tex="f" text="f" xml:id="S3.F1.m7">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">f</XMTok>
            </XMath>
          </Math> and attention module <Math mode="inline" tex="h" text="h" xml:id="S3.F1.m8">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">h</XMTok>
            </XMath>
          </Math> to generate an attention-based feature map <Math mode="inline" tex="z" text="z" xml:id="S3.F1.m9">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">z</XMTok>
            </XMath>
          </Math>. Then it applies temporal coherence regularization on the feature map. We update all the parameters in the shallow layers in this pathway; the global pathway uses a sparse video stream <Math mode="inline" tex="I_{u}" text="I _ u" xml:id="S3.F1.m10">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">I</XMTok>
                <XMTok font="italic" fontsize="70%" role="UNKNOWN">u</XMTok>
              </XMApp>
            </XMath>
          </Math>, which captures the long-term information. TeCo minimizes the prediction entropy in the global pathway by updating all the parameters in the shallow layers and only normalization parameters in the deep layers.
</text></caption>
    </figure>
    <subsection inlist="toc" xml:id="S3.SS1">
      <tags>
        <tag>3.1</tag>
        <tag role="autoref">subsection 3.1</tag>
        <tag role="refnum">3.1</tag>
        <tag role="typerefnum">§3.1</tag>
      </tags>
      <title><tag close=" ">3.1</tag>Test-Time Optimization Strategy</title>
      <para xml:id="S3.SS1.p1">
        <p><!--  %TeCo makes no assumption or modification to the training of the deep learning models. It only optimizes the model at test time. -->Though BN <cite class="ltx_citemacro_citep">(<bibref bibrefs="schneider2020improving" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> and Tent <cite class="ltx_citemacro_citep">(<bibref bibrefs="wang2020tent" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> achieve promising performance on image-based test-time optimization tasks, they only show minor improvements on video data and sometimes degrade the robustness on corruptions in our empirical analysis.
BN only adapts the mean and variance of normalization layers by test data partially,
it limits the searching space of optimization.
Tent removes the training time batch normalization statistics fully, while video classification tasks usually have smaller batch size.
It can only capture sub-optimal statistics when just relying on test-time data.</p>
      </para>
      <para xml:id="S3.SS1.p2">
        <p><!--  %, and we reserve the training time batch normalization statistics at test time. -->We perform a simple but effective mechanism by grafting the advantage of these two classic techniques.
Following <!--  %**** iclr2023˙conference.tex Line 200 **** --><cite class="ltx_citemacro_citep">(<bibref bibrefs="schneider2020improving" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>, we initialize mean <Math mode="inline" tex="\mu=\alpha\mu_{s}+(1-\alpha)\mu_{t}" text="mu = alpha * mu _ s + (1 - alpha) * mu _ t" xml:id="S3.SS1.p2.m1">
            <XMath>
              <XMApp>
                <XMTok meaning="equals" role="RELOP">=</XMTok>
                <XMTok font="italic" name="mu" role="UNKNOWN">μ</XMTok>
                <XMApp>
                  <XMTok meaning="plus" role="ADDOP">+</XMTok>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" name="mu" role="UNKNOWN">μ</XMTok>
                      <XMTok font="italic" fontsize="70%" role="UNKNOWN">s</XMTok>
                    </XMApp>
                  </XMApp>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMDual>
                      <XMRef idref="S3.SS1.p2.m1.1"/>
                      <XMWrap>
                        <XMTok role="OPEN" stretchy="false">(</XMTok>
                        <XMApp xml:id="S3.SS1.p2.m1.1">
                          <XMTok meaning="minus" role="ADDOP">-</XMTok>
                          <XMTok meaning="1" role="NUMBER">1</XMTok>
                          <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
                        </XMApp>
                        <XMTok role="CLOSE" stretchy="false">)</XMTok>
                      </XMWrap>
                    </XMDual>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" name="mu" role="UNKNOWN">μ</XMTok>
                      <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                    </XMApp>
                  </XMApp>
                </XMApp>
              </XMApp>
            </XMath>
          </Math> and variance <Math mode="inline" tex="\sigma^{2}=\alpha\sigma_{s}^{2}+(1-\alpha)\sigma_{t}^{2}" text="sigma ^ 2 = alpha * (sigma _ s) ^ 2 + (1 - alpha) * (sigma _ t) ^ 2" xml:id="S3.SS1.p2.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="equals" role="RELOP">=</XMTok>
                <XMApp>
                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                  <XMTok font="italic" name="sigma" role="UNKNOWN">σ</XMTok>
                  <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                </XMApp>
                <XMApp>
                  <XMTok meaning="plus" role="ADDOP">+</XMTok>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
                    <XMApp>
                      <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                      <XMApp>
                        <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" name="sigma" role="UNKNOWN">σ</XMTok>
                        <XMTok font="italic" fontsize="70%" role="UNKNOWN">s</XMTok>
                      </XMApp>
                      <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                    </XMApp>
                  </XMApp>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMDual>
                      <XMRef idref="S3.SS1.p2.m2.1"/>
                      <XMWrap>
                        <XMTok role="OPEN" stretchy="false">(</XMTok>
                        <XMApp xml:id="S3.SS1.p2.m2.1">
                          <XMTok meaning="minus" role="ADDOP">-</XMTok>
                          <XMTok meaning="1" role="NUMBER">1</XMTok>
                          <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
                        </XMApp>
                        <XMTok role="CLOSE" stretchy="false">)</XMTok>
                      </XMWrap>
                    </XMDual>
                    <XMApp>
                      <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                      <XMApp>
                        <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" name="sigma" role="UNKNOWN">σ</XMTok>
                        <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                      </XMApp>
                      <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                    </XMApp>
                  </XMApp>
                </XMApp>
              </XMApp>
            </XMath>
          </Math>, where <Math mode="inline" tex="s" text="s" xml:id="S3.SS1.p2.m3">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">s</XMTok>
            </XMath>
          </Math> stands for training, and <Math mode="inline" tex="t" text="t" xml:id="S3.SS1.p2.m4">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">t</XMTok>
            </XMath>
          </Math> stands for testing. <Math mode="inline" tex="\alpha" text="alpha" xml:id="S3.SS1.p2.m5">
            <XMath>
              <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
            </XMath>
          </Math> is the hyper-parameter that controls the weights of training and test time statistics.
At the same time, we update the affine transformation parameters in the normalization layers, which helps the model utilize both training and test-time statistics after adaptation.</p>
      </para>
      <para xml:id="S3.SS1.p3">
        <p>Apart from normalization layers, we also fine-tune other network parameters (e.g., Conv Layers) in the shallow layers, which are the initial (lower) layers in the network. The shallow layers of robust models generate smoother feature tensors that are better aligned with human perception <cite class="ltx_citemacro_citep">(<bibref bibrefs="xie2019feature,tsipras2018robustness" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. They play the role of denoiser when encountering natural corruptions arising in input data.
Inspired by this phenomenon, we update all the parameters in the shallow layers and only normalization layers in the deep layers, as shown in Figure <ref labelref="LABEL:framework-fig"/>.
It gives more flexibility to the shallow layers while not interrupting the higher-level semantic information.
The effect of different divisions of the shallow and deep layers is explored in the ablation study.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="S3.SS2">
      <tags>
        <tag>3.2</tag>
        <tag role="autoref">subsection 3.2</tag>
        <tag role="refnum">3.2</tag>
        <tag role="typerefnum">§3.2</tag>
      </tags>
      <title><tag close=" ">3.2</tag>Global and Local Pathways</title>
      <para xml:id="S3.SS2.p1">
        <p>In the test-time optimization of TeCo, we use two pathways (i.e, global pathway and local pathway) and two objectives to optimize model.
The global pathway uses uniform sampling to extract frames from the input video <Math mode="inline" tex="I_{u}" text="I _ u" xml:id="S3.SS2.p1.m1">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">I</XMTok>
                <XMTok font="italic" fontsize="70%" role="UNKNOWN">u</XMTok>
              </XMApp>
            </XMath>
          </Math>.
The uniform sampling splits the video into multiple clips with equal length. Then we randomly take one frame from each clip.
As a result, the selected frames by uniform sampling can capture the global information effectively <cite class="ltx_citemacro_citep">(<bibref bibrefs="chen2020deep" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.
In this pathway, we optimize the overall parameters of shallow layers and the transformation parameters in deep normalization layers by minimizing the entropy of prediction.
The objective is:</p>
        <equation labels="LABEL:ent-equ" xml:id="S3.E1">
          <tags>
            <tag>(1)</tag>
            <tag role="autoref">Equation 1</tag>
            <tag role="refnum">1</tag>
          </tags>
          <Math class="ltx_math_unparsed" mode="display" tex="L_{ent}=H(Softmax(g(f(I_{u})))" xml:id="S3.E1.m1">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">L</XMTok>
                <XMApp>
                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                </XMApp>
              </XMApp>
              <XMTok meaning="equals" role="RELOP">=</XMTok>
              <XMTok font="italic" role="UNKNOWN">H</XMTok>
              <XMWrap>
                <XMTok role="OPEN" stretchy="false">(</XMTok>
                <XMTok font="italic" role="UNKNOWN">S</XMTok>
                <XMTok font="italic" role="UNKNOWN">o</XMTok>
                <XMTok font="italic" role="UNKNOWN">f</XMTok>
                <XMTok font="italic" role="UNKNOWN">t</XMTok>
                <XMTok font="italic" role="UNKNOWN">m</XMTok>
                <XMTok font="italic" role="UNKNOWN">a</XMTok>
                <XMTok font="italic" role="UNKNOWN">x</XMTok>
                <XMWrap>
                  <XMTok role="OPEN" stretchy="false">(</XMTok>
                  <XMTok font="italic" role="UNKNOWN">g</XMTok>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMTok font="italic" role="UNKNOWN">f</XMTok>
                    <XMWrap>
                      <XMTok role="OPEN" stretchy="false">(</XMTok>
                      <XMApp>
                        <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" role="UNKNOWN">I</XMTok>
                        <XMTok font="italic" fontsize="70%" role="UNKNOWN">u</XMTok>
                      </XMApp>
                      <XMTok role="CLOSE" stretchy="false">)</XMTok>
                    </XMWrap>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                  <XMTok role="CLOSE" stretchy="false">)</XMTok>
                </XMWrap>
              </XMWrap>
            </XMath>
          </Math>
        </equation>
      </para>
      <para xml:id="S3.SS2.p2">
        <p>where <Math mode="inline" tex="g(\cdot)" text="g * cdot" xml:id="S3.SS2.p2.m1">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">g</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS2.p2.m1.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMTok name="cdot" role="MULOP" xml:id="S3.SS2.p2.m1.1">⋅</XMTok>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> represents the deep layers, <Math mode="inline" tex="f(\cdot)" text="f * cdot" xml:id="S3.SS2.p2.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">f</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS2.p2.m2.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMTok name="cdot" role="MULOP" xml:id="S3.SS2.p2.m2.1">⋅</XMTok>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> stands for the shallow layers, <Math mode="inline" tex="H" text="H" xml:id="S3.SS2.p2.m3">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">H</XMTok>
            </XMath>
          </Math> is the Shannon entropy of prediction <Math mode="inline" tex="\hat{y}" text="hat@(y)" xml:id="S3.SS2.p2.m4">
            <XMath>
              <XMApp>
                <XMTok name="hat" role="OVERACCENT" stretchy="false">^</XMTok>
                <XMTok font="italic" role="UNKNOWN">y</XMTok>
              </XMApp>
            </XMath>
          </Math> with N classes: <Math mode="inline" tex="H(\hat{y})=-\sum^{N}_{n=1}p(\hat{y}_{n})\log p(\hat{y}_{n})" text="H * hat@(y) = - ((sum ^ N) _ (n = 1))@(p * (hat@(y)) _ n * logarithm@(p) * (hat@(y)) _ n)" xml:id="S3.SS2.p2.m5">
            <XMath>
              <XMApp>
                <XMTok meaning="equals" role="RELOP">=</XMTok>
                <XMApp>
                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                  <XMTok font="italic" role="UNKNOWN">H</XMTok>
                  <XMDual>
                    <XMRef idref="S3.SS2.p2.m5.1"/>
                    <XMWrap>
                      <XMTok role="OPEN" stretchy="false">(</XMTok>
                      <XMApp xml:id="S3.SS2.p2.m5.1">
                        <XMTok name="hat" role="OVERACCENT" stretchy="false">^</XMTok>
                        <XMTok font="italic" role="UNKNOWN">y</XMTok>
                      </XMApp>
                      <XMTok role="CLOSE" stretchy="false">)</XMTok>
                    </XMWrap>
                  </XMDual>
                </XMApp>
                <XMApp>
                  <XMTok meaning="minus" role="ADDOP">-</XMTok>
                  <XMApp>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMApp>
                        <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                        <XMTok mathstyle="text" meaning="sum" role="SUMOP" scriptpos="post">∑</XMTok>
                        <XMTok font="italic" fontsize="70%" role="UNKNOWN">N</XMTok>
                      </XMApp>
                      <XMApp>
                        <XMTok fontsize="70%" meaning="equals" role="RELOP">=</XMTok>
                        <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                        <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                      </XMApp>
                    </XMApp>
                    <XMApp>
                      <XMTok meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">p</XMTok>
                      <XMDual>
                        <XMRef idref="S3.SS2.p2.m5.2"/>
                        <XMWrap>
                          <XMTok role="OPEN" stretchy="false">(</XMTok>
                          <XMApp xml:id="S3.SS2.p2.m5.2">
                            <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                            <XMApp>
                              <XMTok name="hat" role="OVERACCENT" stretchy="false">^</XMTok>
                              <XMTok font="italic" role="UNKNOWN">y</XMTok>
                            </XMApp>
                            <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                          </XMApp>
                          <XMTok role="CLOSE" stretchy="false">)</XMTok>
                        </XMWrap>
                      </XMDual>
                      <XMApp>
                        <XMTok meaning="logarithm" role="OPFUNCTION">log</XMTok>
                        <XMTok font="italic" role="UNKNOWN">p</XMTok>
                      </XMApp>
                      <XMDual>
                        <XMRef idref="S3.SS2.p2.m5.3"/>
                        <XMWrap>
                          <XMTok role="OPEN" stretchy="false">(</XMTok>
                          <XMApp xml:id="S3.SS2.p2.m5.3">
                            <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                            <XMApp>
                              <XMTok name="hat" role="OVERACCENT" stretchy="false">^</XMTok>
                              <XMTok font="italic" role="UNKNOWN">y</XMTok>
                            </XMApp>
                            <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                          </XMApp>
                          <XMTok role="CLOSE" stretchy="false">)</XMTok>
                        </XMWrap>
                      </XMDual>
                    </XMApp>
                  </XMApp>
                </XMApp>
              </XMApp>
            </XMath>
          </Math>.
The Shannon entropy serves as the optimization of the global pathway.</p>
      </para>
      <para xml:id="S3.SS2.p3">
        <p>The local pathway leverages dense sampling to take consecutive frames <Math mode="inline" tex="I_{d}" text="I _ d" xml:id="S3.SS2.p3.m1">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">I</XMTok>
                <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
              </XMApp>
            </XMath>
          </Math><!--  %**** iclr2023˙conference.tex Line 225 **** -->, at a random location of the input video, which randomly chooses a location as the center of the sampled sequence.
Compared to the large time gap in frames by uniform sampling, the time gap in the frames by dense sampling can be as low as one.
In this pathway, the temporal coherence between frames is much stronger.
Because the visual world is smoothly varying, the neighboring frames in video data will not change abruptly in the temporal dimension.
When common corruptions interrupt the natural structure of video data, we apply a temporal-coherent regularization to the output feature of the shallow layer. The regularization can be achieved by enforcing the feature similarity in consecutive feature maps of corrupted data. We explicitly represent it with the loss function: <Math mode="inline" tex="L_{coherence}=\sum_{t}||x^{t}-x^{t-i}||_{1}" text="L _ (c * o * h * e * r * e * n * c * e) = (sum _ t)@((norm@(x ^ t - x ^ (t - i))) _ 1)" xml:id="S3.SS2.p3.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="equals" role="RELOP">=</XMTok>
                <XMApp>
                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                  <XMTok font="italic" role="UNKNOWN">L</XMTok>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">c</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">h</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">r</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">c</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                  </XMApp>
                </XMApp>
                <XMApp>
                  <XMApp>
                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                    <XMTok mathstyle="text" meaning="sum" role="SUMOP" scriptpos="post">∑</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                  </XMApp>
                  <XMApp>
                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                    <XMDual>
                      <XMApp>
                        <XMTok meaning="norm"/>
                        <XMRef idref="S3.SS2.p3.m2.1"/>
                      </XMApp>
                      <XMWrap>
                        <XMTok role="OPEN">‖</XMTok>
                        <XMApp xml:id="S3.SS2.p3.m2.1">
                          <XMTok meaning="minus" role="ADDOP">-</XMTok>
                          <XMApp>
                            <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                            <XMTok font="italic" role="UNKNOWN">x</XMTok>
                            <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                          </XMApp>
                          <XMApp>
                            <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                            <XMTok font="italic" role="UNKNOWN">x</XMTok>
                            <XMApp>
                              <XMTok fontsize="70%" meaning="minus" role="ADDOP">-</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">i</XMTok>
                            </XMApp>
                          </XMApp>
                        </XMApp>
                        <XMTok role="CLOSE">‖</XMTok>
                      </XMWrap>
                    </XMDual>
                    <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                  </XMApp>
                </XMApp>
              </XMApp>
            </XMath>
          </Math>. We use <Math mode="inline" tex="l_{1}" text="l _ 1" xml:id="S3.SS2.p3.m3">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">l</XMTok>
                <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
              </XMApp>
            </XMath>
          </Math> distance to measure the similarity of feature tensors since it penalizes more on outliers and benefits the robustness <cite class="ltx_citemacro_citep">(<bibref bibrefs="alizadeh2020gradient,bektacs2010comparison" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. <Math mode="inline" tex="x^{t}" text="x ^ t" xml:id="S3.SS2.p3.m4">
            <XMath>
              <XMApp>
                <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">x</XMTok>
                <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
              </XMApp>
            </XMath>
          </Math> is the tensor at time <Math mode="inline" tex="t" text="t" xml:id="S3.SS2.p3.m5">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">t</XMTok>
            </XMath>
          </Math>, which is the output after <Math mode="inline" tex="f(I^{t}_{d})" text="f * (I ^ t) _ d" xml:id="S3.SS2.p3.m6">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">f</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS2.p3.m6.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMApp xml:id="S3.SS2.p3.m6.1">
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMApp>
                        <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" role="UNKNOWN">I</XMTok>
                        <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                      </XMApp>
                      <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
                    </XMApp>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math>. <Math mode="inline" tex="i" text="i" xml:id="S3.SS2.p3.m7">
            <XMath>
              <XMTok font="italic" role="UNKNOWN">i</XMTok>
            </XMath>
          </Math> is the time gap between two neighboring tensors.
<Math mode="inline" tex="L_{coherence}" text="L _ (c * o * h * e * r * e * n * c * e)" xml:id="S3.SS2.p3.m8">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">L</XMTok>
                <XMApp>
                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">c</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">h</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">r</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">c</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                </XMApp>
              </XMApp>
            </XMath>
          </Math> encourages the layers to generate smoother features. Correspondingly, the feature extractor will be more invariant to the perturbation, corruption, and disruption in the input data. It leads to more robust video classification models.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="S3.SS3">
      <tags>
        <tag>3.3</tag>
        <tag role="autoref">subsection 3.3</tag>
        <tag role="refnum">3.3</tag>
        <tag role="typerefnum">§3.3</tag>
      </tags>
      <title><tag close=" ">3.3</tag>Attentive Temporal Coherence Regularization</title>
      <para xml:id="S3.SS3.p1">
        <p>When we apply <Math mode="inline" tex="L_{coherence}" text="L _ (c * o * h * e * r * e * n * c * e)" xml:id="S3.SS3.p1.m1">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">L</XMTok>
                <XMApp>
                  <XMTok meaning="times" role="MULOP">⁢</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">c</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">h</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">r</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">c</XMTok>
                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                </XMApp>
              </XMApp>
            </XMath>
          </Math> on feature tenors, we simply assume the frames are varying in a uniform speed because of the same weights along the time dimensions.
However, the changes in neighboring frames have different scales in the real world. The example of bungee jumping in Figure <ref labelref="LABEL:framework-fig"/> shows that the change is small at the beginning of the video because the background is fixed.
It has only the blue sky and a jumping platform.
When the person is reaching the ground, the background changes rapidly, and there are houses and other people appearing.
Hence, we propose to assign different weights to different frames.
To encourage temporal consistency in the static background, the slower changes in temporal dimension correspond to higher weights in regularization.
The weights can be generated by an attention module <Math mode="inline" tex="h(\cdot)" text="h * cdot" xml:id="S3.SS3.p1.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">h</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS3.p1.m2.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMTok name="cdot" role="MULOP" xml:id="S3.SS3.p1.m2.1">⋅</XMTok>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math>.
In this work, we use the time dimension-only 1D non-local operation <cite class="ltx_citemacro_citep">(<bibref bibrefs="wang2018non" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite> to obtain the weights along the time dimension.
Hence, the attentive temporal coherent regularization can be written as:</p>
        <equation xml:id="S3.E2">
          <tags>
            <tag>(2)</tag>
            <tag role="autoref">Equation 2</tag>
            <tag role="refnum">2</tag>
          </tags>
          <Math mode="display" tex="L_{att-co}=\sum_{t}||h(f(I_{d}))^{t}-h(f(I_{d}))^{t-i}||_{1}," text="L _ (a * t * t - c * o) = (sum _ t)@((norm@(h * (f * I _ d) ^ t - h * (f * I _ d) ^ (t - i))) _ 1)" xml:id="S3.E2.m1">
            <XMath>
              <XMDual>
                <XMRef idref="S3.E2.m1.1"/>
                <XMWrap>
                  <XMApp xml:id="S3.E2.m1.1">
                    <XMTok meaning="equals" role="RELOP">=</XMTok>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" role="UNKNOWN">L</XMTok>
                      <XMApp>
                        <XMTok fontsize="70%" meaning="minus" role="ADDOP">-</XMTok>
                        <XMApp>
                          <XMTok meaning="times" role="MULOP">⁢</XMTok>
                          <XMTok font="italic" fontsize="70%" role="UNKNOWN">a</XMTok>
                          <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                          <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                        </XMApp>
                        <XMApp>
                          <XMTok meaning="times" role="MULOP">⁢</XMTok>
                          <XMTok font="italic" fontsize="70%" role="UNKNOWN">c</XMTok>
                          <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                        </XMApp>
                      </XMApp>
                    </XMApp>
                    <XMApp>
                      <XMApp>
                        <XMTok role="SUBSCRIPTOP" scriptpos="mid1"/>
                        <XMTok mathstyle="display" meaning="sum" role="SUMOP" scriptpos="mid">∑</XMTok>
                        <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                      </XMApp>
                      <XMApp>
                        <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                        <XMDual>
                          <XMApp>
                            <XMTok meaning="norm"/>
                            <XMRef idref="S3.E2.m1.1.1"/>
                          </XMApp>
                          <XMWrap>
                            <XMTok role="OPEN">‖</XMTok>
                            <XMApp xml:id="S3.E2.m1.1.1">
                              <XMTok meaning="minus" role="ADDOP">-</XMTok>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMTok font="italic" role="UNKNOWN">h</XMTok>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMDual>
                                    <XMRef idref="S3.E2.m1.1.1.1"/>
                                    <XMWrap>
                                      <XMTok role="OPEN" stretchy="false">(</XMTok>
                                      <XMApp xml:id="S3.E2.m1.1.1.1">
                                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                        <XMTok font="italic" role="UNKNOWN">f</XMTok>
                                        <XMDual>
                                          <XMRef idref="S3.E2.m1.1.1.1.1"/>
                                          <XMWrap>
                                            <XMTok role="OPEN" stretchy="false">(</XMTok>
                                            <XMApp xml:id="S3.E2.m1.1.1.1.1">
                                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                              <XMTok font="italic" role="UNKNOWN">I</XMTok>
                                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
                                            </XMApp>
                                            <XMTok role="CLOSE" stretchy="false">)</XMTok>
                                          </XMWrap>
                                        </XMDual>
                                      </XMApp>
                                      <XMTok role="CLOSE" stretchy="false">)</XMTok>
                                    </XMWrap>
                                  </XMDual>
                                  <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                                </XMApp>
                              </XMApp>
                              <XMApp>
                                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                <XMTok font="italic" role="UNKNOWN">h</XMTok>
                                <XMApp>
                                  <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                                  <XMDual>
                                    <XMRef idref="S3.E2.m1.1.1.2"/>
                                    <XMWrap>
                                      <XMTok role="OPEN" stretchy="false">(</XMTok>
                                      <XMApp xml:id="S3.E2.m1.1.1.2">
                                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                                        <XMTok font="italic" role="UNKNOWN">f</XMTok>
                                        <XMDual>
                                          <XMRef idref="S3.E2.m1.1.1.2.1"/>
                                          <XMWrap>
                                            <XMTok role="OPEN" stretchy="false">(</XMTok>
                                            <XMApp xml:id="S3.E2.m1.1.1.2.1">
                                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                                              <XMTok font="italic" role="UNKNOWN">I</XMTok>
                                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
                                            </XMApp>
                                            <XMTok role="CLOSE" stretchy="false">)</XMTok>
                                          </XMWrap>
                                        </XMDual>
                                      </XMApp>
                                      <XMTok role="CLOSE" stretchy="false">)</XMTok>
                                    </XMWrap>
                                  </XMDual>
                                  <XMApp>
                                    <XMTok fontsize="70%" meaning="minus" role="ADDOP">-</XMTok>
                                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                                    <XMTok font="italic" fontsize="70%" role="UNKNOWN">i</XMTok>
                                  </XMApp>
                                </XMApp>
                              </XMApp>
                            </XMApp>
                            <XMTok role="CLOSE">‖</XMTok>
                          </XMWrap>
                        </XMDual>
                        <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                      </XMApp>
                    </XMApp>
                  </XMApp>
                  <XMTok role="PUNCT">,</XMTok>
                </XMWrap>
              </XMDual>
            </XMath>
          </Math>
        </equation>
        <p>where <Math mode="inline" tex="h(\cdot)" text="h * cdot" xml:id="S3.SS3.p1.m3">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">h</XMTok>
                <XMDual>
                  <XMRef idref="S3.SS3.p1.m3.1"/>
                  <XMWrap>
                    <XMTok role="OPEN" stretchy="false">(</XMTok>
                    <XMTok name="cdot" role="MULOP" xml:id="S3.SS3.p1.m3.1">⋅</XMTok>
                    <XMTok role="CLOSE" stretchy="false">)</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMath>
          </Math> is the attention module.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="S3.SS4">
      <tags>
        <tag>3.4</tag>
        <tag role="autoref">subsection 3.4</tag>
        <tag role="refnum">3.4</tag>
        <tag role="typerefnum">§3.4</tag>
      </tags>
      <title><tag close=" ">3.4</tag>Overall Objective Function</title>
      <para xml:id="S3.SS4.p1">
        <p>To summarize, the global pathway uses <Math mode="inline" tex="I_{u}" text="I _ u" xml:id="S3.SS4.p1.m1">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">I</XMTok>
                <XMTok font="italic" fontsize="70%" role="UNKNOWN">u</XMTok>
              </XMApp>
            </XMath>
          </Math> as input and entropy minimization as objective,
and the local pathway leverages <Math mode="inline" tex="I_{d}" text="I _ d" xml:id="S3.SS4.p1.m2">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">I</XMTok>
                <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
              </XMApp>
            </XMath>
          </Math> as input and attentive temporal coherence regularization as objective.
The overall objective function for TeCo can be represented as:</p>
        <equation labels="LABEL:eq:overall_equation" xml:id="S3.E3">
          <tags>
            <tag>(3)</tag>
            <tag role="autoref">Equation 3</tag>
            <tag role="refnum">3</tag>
          </tags>
          <Math mode="display" tex="L=L_{ent}(I_{u},f,g)+\beta L_{att-co}(I_{d},f,h)," text="L = L _ (e * n * t) * vector@(I _ u, f, g) + beta * L _ (a * t * t - c * o) * vector@(I _ d, f, h)" xml:id="S3.E3.m1">
            <XMath>
              <XMDual>
                <XMRef idref="S3.E3.m1.5"/>
                <XMWrap>
                  <XMApp xml:id="S3.E3.m1.5">
                    <XMTok meaning="equals" role="RELOP">=</XMTok>
                    <XMTok font="italic" role="UNKNOWN">L</XMTok>
<!--  %**** iclr2023_conference.tex Line 250 **** -->                    <XMApp>
                      <XMTok meaning="plus" role="ADDOP">+</XMTok>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMApp>
                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                          <XMTok font="italic" role="UNKNOWN">L</XMTok>
                          <XMApp>
                            <XMTok meaning="times" role="MULOP">⁢</XMTok>
                            <XMTok font="italic" fontsize="70%" role="UNKNOWN">e</XMTok>
                            <XMTok font="italic" fontsize="70%" role="UNKNOWN">n</XMTok>
                            <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                          </XMApp>
                        </XMApp>
                        <XMDual>
                          <XMApp>
                            <XMTok meaning="vector"/>
                            <XMRef idref="S3.E3.m1.5.1"/>
                            <XMRef idref="S3.E3.m1.1"/>
                            <XMRef idref="S3.E3.m1.2"/>
                          </XMApp>
                          <XMWrap>
                            <XMTok role="OPEN" stretchy="false">(</XMTok>
                            <XMApp xml:id="S3.E3.m1.5.1">
                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                              <XMTok font="italic" role="UNKNOWN">I</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">u</XMTok>
                            </XMApp>
                            <XMTok role="PUNCT">,</XMTok>
                            <XMTok font="italic" role="UNKNOWN" xml:id="S3.E3.m1.1">f</XMTok>
                            <XMTok role="PUNCT">,</XMTok>
                            <XMTok font="italic" role="UNKNOWN" xml:id="S3.E3.m1.2">g</XMTok>
                            <XMTok role="CLOSE" stretchy="false">)</XMTok>
                          </XMWrap>
                        </XMDual>
                      </XMApp>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok font="italic" name="beta" role="UNKNOWN">β</XMTok>
                        <XMApp>
                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                          <XMTok font="italic" role="UNKNOWN">L</XMTok>
                          <XMApp>
                            <XMTok fontsize="70%" meaning="minus" role="ADDOP">-</XMTok>
                            <XMApp>
                              <XMTok meaning="times" role="MULOP">⁢</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">a</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">t</XMTok>
                            </XMApp>
                            <XMApp>
                              <XMTok meaning="times" role="MULOP">⁢</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">c</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">o</XMTok>
                            </XMApp>
                          </XMApp>
                        </XMApp>
                        <XMDual>
                          <XMApp>
                            <XMTok meaning="vector"/>
                            <XMRef idref="S3.E3.m1.5.2"/>
                            <XMRef idref="S3.E3.m1.3"/>
                            <XMRef idref="S3.E3.m1.4"/>
                          </XMApp>
                          <XMWrap>
                            <XMTok role="OPEN" stretchy="false">(</XMTok>
                            <XMApp xml:id="S3.E3.m1.5.2">
                              <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                              <XMTok font="italic" role="UNKNOWN">I</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN">d</XMTok>
                            </XMApp>
                            <XMTok role="PUNCT">,</XMTok>
                            <XMTok font="italic" role="UNKNOWN" xml:id="S3.E3.m1.3">f</XMTok>
                            <XMTok role="PUNCT">,</XMTok>
                            <XMTok font="italic" role="UNKNOWN" xml:id="S3.E3.m1.4">h</XMTok>
                            <XMTok role="CLOSE" stretchy="false">)</XMTok>
                          </XMWrap>
                        </XMDual>
                      </XMApp>
                    </XMApp>
                  </XMApp>
                  <XMTok role="PUNCT">,</XMTok>
                </XMWrap>
              </XMDual>
            </XMath>
          </Math>
        </equation>
        <p>where <Math mode="inline" tex="\beta&gt;0" text="beta &gt; 0" xml:id="S3.SS4.p1.m3">
            <XMath>
              <XMApp>
                <XMTok meaning="greater-than" role="RELOP">&gt;</XMTok>
                <XMTok font="italic" name="beta" role="UNKNOWN">β</XMTok>
                <XMTok meaning="0" role="NUMBER">0</XMTok>
              </XMApp>
            </XMath>
          </Math> is a balancing hyper-parameter.</p>
      </para>
    </subsection>
  </section>
  <section inlist="toc" xml:id="S4">
    <tags>
      <tag>4</tag>
      <tag role="autoref">section 4</tag>
      <tag role="refnum">4</tag>
      <tag role="typerefnum">§4</tag>
    </tags>
    <title><tag close=" ">4</tag>Experiments</title>
    <para xml:id="S4.p1">
      <p>We evaluate the corruption robustness of TeCo on Mini Kinetics-C and Mini SSV2-C. The mean performance on corruption is the main metric for robustness measurement. We also compare it with other baseline methods across architectures.</p>
    </para>
    <para xml:id="S4.p2">
      <p><text font="bold">Dataset.</text>
Kinetics <cite class="ltx_citemacro_citep">(<bibref bibrefs="carreira2017quo" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> and Something-Something-V2 (SSV2) <cite class="ltx_citemacro_citep">(<bibref bibrefs="goyal2017something" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> are two most popular large-scale datasets in video classification community.
Kinetics is extracted from the Youtube website and it relies on spatial information for classification.
As complementary, SSV2 is a first-view video dataset constructed systematically in the lab environment, which has more temporal changes.
We use their variants Mini Kinetic-C and Mini SSV2-C <cite class="ltx_citemacro_citep">(<bibref bibrefs="yi2021benchmarking" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> to evaluate the robustness of models against corruptions. The mini-version datasets randomly sample half of the classes from the original large-scale datasets. Hence, Mini Kinetics-C contains around 10K videos in the test dataset; Mini SSV2-C has a test dataset with a size of around 13K videos.
These two datasets apply 12 types of corruptions arising in nature on the original clean test datasets.
Each corruption has 5 levels of severities.
The samples of Mini Kinetics-C and Mini SSV2-s are shown in supplementary.</p>
    </para>
    <para xml:id="S4.p3">
      <p><text font="bold">Metrics.</text>
The classification accuracy on corrupted data is the most common metric to measure the corruption robustness of models.
In our experiments, the corrupted datasets have 12 types of corruptions and each has 5 levels of severities.
We use <Math mode="inline" tex="mPC" text="m * P * C" xml:id="S4.p3.m1">
          <XMath>
            <XMApp>
              <XMTok meaning="times" role="MULOP">⁢</XMTok>
              <XMTok font="italic" role="UNKNOWN">m</XMTok>
              <XMTok font="italic" role="UNKNOWN">P</XMTok>
              <XMTok font="italic" role="UNKNOWN">C</XMTok>
            </XMApp>
          </XMath>
        </Math> (stands for mean performance on corruptions) to evaluate the overall robustness.
We compute <Math mode="inline" tex="mPC" text="m * P * C" xml:id="S4.p3.m2">
          <XMath>
            <XMApp>
              <XMTok meaning="times" role="MULOP">⁢</XMTok>
              <XMTok font="italic" role="UNKNOWN">m</XMTok>
              <XMTok font="italic" role="UNKNOWN">P</XMTok>
              <XMTok font="italic" role="UNKNOWN">C</XMTok>
            </XMApp>
          </XMath>
        </Math> by averaging the classification accuracy as: <Math mode="inline" tex="mPC=\sum_{c=1}^{12}\sum_{s=1}^{5}CA_{c,s}," text="m * P * C = ((sum _ (c = 1)) ^ 12)@(((sum _ (s = 1)) ^ 5)@(C * A _ (list@(c, s))))" xml:id="S4.p3.m3">
          <XMath>
            <XMDual>
              <XMRef idref="S4.p3.m3.3"/>
              <XMWrap>
                <XMApp xml:id="S4.p3.m3.3">
                  <XMTok meaning="equals" role="RELOP">=</XMTok>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMTok font="italic" role="UNKNOWN">m</XMTok>
                    <XMTok font="italic" role="UNKNOWN">P</XMTok>
                    <XMTok font="italic" role="UNKNOWN">C</XMTok>
                  </XMApp>
                  <XMApp>
                    <XMApp>
                      <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                      <XMApp>
                        <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                        <XMTok mathstyle="text" meaning="sum" role="SUMOP" scriptpos="post">∑</XMTok>
                        <XMApp>
                          <XMTok fontsize="70%" meaning="equals" role="RELOP">=</XMTok>
                          <XMTok font="italic" fontsize="70%" role="UNKNOWN">c</XMTok>
                          <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                        </XMApp>
                      </XMApp>
                      <XMTok fontsize="70%" meaning="12" role="NUMBER">12</XMTok>
                    </XMApp>
                    <XMApp>
                      <XMApp>
                        <XMTok role="SUPERSCRIPTOP" scriptpos="post1"/>
                        <XMApp>
                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                          <XMTok mathstyle="text" meaning="sum" role="SUMOP" scriptpos="post">∑</XMTok>
                          <XMApp>
                            <XMTok fontsize="70%" meaning="equals" role="RELOP">=</XMTok>
                            <XMTok font="italic" fontsize="70%" role="UNKNOWN">s</XMTok>
                            <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                          </XMApp>
                        </XMApp>
                        <XMTok fontsize="70%" meaning="5" role="NUMBER">5</XMTok>
                      </XMApp>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok font="italic" role="UNKNOWN">C</XMTok>
                        <XMApp>
                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                          <XMTok font="italic" role="UNKNOWN">A</XMTok>
                          <XMDual>
                            <XMApp>
                              <XMTok meaning="list"/>
                              <XMRef idref="S4.p3.m3.1"/>
                              <XMRef idref="S4.p3.m3.2"/>
                            </XMApp>
                            <XMWrap>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN" xml:id="S4.p3.m3.1">c</XMTok>
                              <XMTok fontsize="70%" role="PUNCT">,</XMTok>
                              <XMTok font="italic" fontsize="70%" role="UNKNOWN" xml:id="S4.p3.m3.2">s</XMTok>
                            </XMWrap>
                          </XMDual>
                        </XMApp>
                      </XMApp>
                    </XMApp>
                  </XMApp>
                </XMApp>
                <XMTok role="PUNCT">,</XMTok>
              </XMWrap>
            </XMDual>
          </XMath>
        </Math><!--  %“begin–equation˝ -->
where <!--  %mPC=“sum˙–c=1˝^–12˝ “sum˙–s=1˝^–5˝ CA˙–c,s˝, 
     %“end–equation˝ 
     %**** iclr2023˙conference.tex Line 275 **** --><Math mode="inline" tex="CA_{c,s}" text="C * A _ (list@(c, s))" xml:id="S4.p3.m4">
          <XMath>
            <XMApp>
              <XMTok meaning="times" role="MULOP">⁢</XMTok>
              <XMTok font="italic" role="UNKNOWN">C</XMTok>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">A</XMTok>
                <XMDual>
                  <XMApp>
                    <XMTok meaning="list"/>
                    <XMRef idref="S4.p3.m4.1"/>
                    <XMRef idref="S4.p3.m4.2"/>
                  </XMApp>
                  <XMWrap>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN" xml:id="S4.p3.m4.1">c</XMTok>
                    <XMTok fontsize="70%" role="PUNCT">,</XMTok>
                    <XMTok font="italic" fontsize="70%" role="UNKNOWN" xml:id="S4.p3.m4.2">s</XMTok>
                  </XMWrap>
                </XMDual>
              </XMApp>
            </XMApp>
          </XMath>
        </Math> is the classification accuracy on corruption <Math mode="inline" tex="c" text="c" xml:id="S4.p3.m5">
          <XMath>
            <XMTok font="italic" role="UNKNOWN">c</XMTok>
          </XMath>
        </Math> at level <Math mode="inline" tex="s" text="s" xml:id="S4.p3.m6">
          <XMath>
            <XMTok font="italic" role="UNKNOWN">s</XMTok>
          </XMath>
        </Math>.</p>
    </para>
    <para xml:id="S4.p4">
      <p><text font="bold">Models.</text>
The test-time optimization techniques can usually be applied across architectures.
We use several classic 2D, 3D CNN-based architectures and transformers as the backbone to show the superiority of our method TeCo.
For 2D CNN, we choose vanilla ResNet18 and its variant TAM-ResNet18 <cite class="ltx_citemacro_citep">(<bibref bibrefs="fan2019more" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> as the model architecture. The vanilla ResNet18 treats single image frames in the video as the input data.
It has only an average fusion at the output stage for the overall video classification.
TAM-ResNet18 integrates a temporal module in the ResNet architecture. It helps the model store more temporal information for classification. Hence, it obtains improvements in clean accuracy, especially on the SSV2 dataset.
For 3D CNN, we use the 3D version ResNet18 <cite class="ltx_citemacro_citep">(<bibref bibrefs="hara2017learning" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> for training and evaluation. The 3D convolution layers enable it to capture sufficient spatio-temporal features in an end-to-end training manner. Since TeCo is model agnostic in principle, we use the state-of-the-art transformer MViTv2-S <cite class="ltx_citemacro_citep">(<bibref bibrefs="li2022mvitv2" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> as the backbone as well. Different from CNN-based architecture, the transformer uses Layer Normalization <cite class="ltx_citemacro_citep">(<bibref bibrefs="ba2016layer" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite>.</p>
    </para>
    <para xml:id="S4.p5">
      <p><text font="bold">Baseline.</text>
Though the video classification area lacks research in test-time optimization, we import the baseline methods in image-based test-time optimization. BN (test-time batch normalization) <cite class="ltx_citemacro_citep">(<bibref bibrefs="schneider2020improving" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> is a simple method that adapts the batch normalization statistics with test-time data. Tent <cite class="ltx_citemacro_citep">(<bibref bibrefs="wang2020tent" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> extends from BN by updating the transformation parameters in the normalization layers, with an objective of entropy minimization. SHOT <cite class="ltx_citemacro_citep">(<bibref bibrefs="pmlr-v119-liang20a" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> is well-known for combining entropy minimization and pseudo-labeling. TTT <cite class="ltx_citemacro_citep">(<bibref bibrefs="sun2020test" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite> optimizes the model with a self-supervised auxiliary task at both the training and test stages. Since we only conduct optimization at test stage and have no access to training data in our setting, we denote TTT as TTT*. We choose rotation prediction <cite class="ltx_citemacro_citep">(<bibref bibrefs="hendrycks2019using" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
            <bibrefphrase>, </bibrefphrase>
          </bibref>)</cite><!--  %“siyuan–We re-implement them on the video classification frameworks based on the public available codes.˝ --> as the self-supervised auxiliary task in TTT*. All the methods can improve the corruption robustness of image-based deep learning models.
We re-implement them on the video classification frameworks based on the public available codes.</p>
    </para>
<!--  %We fit them into video classification frameworks. -->    <para xml:id="S4.p6">
      <p><text font="bold">Trainig and Evaluation Setup.</text> We separate the setup of the experiment into three stages: pretraining, test-time optimization, and test-time evaluation.
In the pretraining stage, we use uniform sampling to create 16-frame-length input data.
Then we use multi-scale cropping on the input for data augmentation and resize the cropped frames to a size of 224.
We train the models with an initial learning rate of 0.01 and a cosine annealing learning rate schedule.
In test-time optimization, we use the pre-trained model for network weight initialization and update the model parameters for one epoch.
For standard (without test-time optimization), BN, Tent, and SHOT methods, we use uniform sampling to extract input from corrupted test data.
We apply both uniform and dense sampling to create input for TeCo.
In all the test-time optimization experiments, we follow the offline adaptation setting in <cite class="ltx_citemacro_cite"><bibref bibrefs="wang2020tent" separator=";" show="Authors Phrase1YearPhrase2" yyseparator=",">
            <bibrefphrase>(</bibrefphrase>
            <bibrefphrase>)</bibrefphrase>
          </bibref></cite>. Because the baseline methods have not been implemented in video classification previously, we follow their implementations in image classification and tune the hyper-parameter settings to obtain the best results<note mark="1" role="footnote" xml:id="footnote1"><tags>
            <tag>1</tag>
            <tag role="autoref">footnote 1</tag>
            <tag role="refnum">1</tag>
            <tag role="typerefnum">footnote 1</tag>
          </tags>The detailed implementation details can be found in the appendix.</note><!--  %**** iclr2023˙conference.tex Line 300 **** -->.
For the optimization, we use SGD with momentum. On Mini Kinetics-C, we use a batch size of 32 and a learning rate of 0.001; while for Mini SSV2-C, we use the same batch size but a learning rate of 0.00001.
After adapting the models with test data, we freeze the model weights and evaluate on the corrupted data.
The inference is only based on the global pathway.</p>
    </para>
    <figure inlist="lof" labels="LABEL:mean-robustness-fig" placement="t" xml:id="S4.F2">
      <tags>
        <tag><text fontsize="90%">Figure 2</text></tag>
        <tag role="autoref">Figure 2</tag>
        <tag role="refnum">2</tag>
        <tag role="typerefnum">Figure 2</tag>
      </tags>
      <graphics candidates="images/mean_mpc-v2.pdf" class="ltx_centering" graphic="./images/mean_mpc-v2.pdf" options="width=390.25534pt,keepaspectratio=true" xml:id="S4.F2.g1"/>
<!--  %˝ -->      <toccaption><tag close=" ">2</tag>Mean Robustness on Mini Kinetics-C and Mini SSV2-C of methods. TeCo reduces the gap between model accuracy on clean and corrupted data significantly.</toccaption>
      <caption><tag close=": "><text fontsize="90%">Figure 2</text></tag><text fontsize="90%">Mean Robustness on Mini Kinetics-C and Mini SSV2-C of methods. TeCo reduces the gap between model accuracy on clean and corrupted data significantly.</text></caption>
    </figure>
    <table inlist="lot" labels="LABEL:mPC-table" placement="htp" xml:id="S4.T1">
      <tags>
        <tag><text fontsize="90%">Table 1</text></tag>
        <tag role="autoref">Table 1</tag>
        <tag role="refnum">1</tag>
        <tag role="typerefnum">Table 1</tag>
      </tags>
      <toccaption><tag close=" ">1</tag>mPC across architectures on Mini Kinetics-C and Mini SSV2-C. TeCo outperforms other baseline methods on different architectures and datasets. <text font="bold">Clean Acc</text> is the accuracy of model tested on clean data.</toccaption>
      <caption><tag close=": "><text fontsize="90%">Table 1</text></tag><text fontsize="90%">mPC across architectures on Mini Kinetics-C and Mini SSV2-C. TeCo outperforms other baseline methods on different architectures and datasets. <text font="bold">Clean Acc</text> is the accuracy of model tested on clean data.</text></caption>
      <inline-block align="center" depth="0.9pt" height="153.2pt" width="424.9pt" xscale="0.945606876408984" xtranslate="-12.2pt" yscale="0.945606876408984" ytranslate="4.4pt">
        <tabular class="ltx_guessed_headers" vattach="middle">
          <thead>
            <tr>
              <td border="tt" thead="column row"/>
              <td align="center" border="r tt" thead="column row"><text font="bold">Backbone</text></td>
              <td align="center" border="r tt" thead="column row"><text font="bold">Clean Acc</text></td>
              <td align="center" border="tt" thead="column"><text font="bold">Standard</text></td>
              <td align="center" border="tt" thead="column"><text font="bold">BN</text></td>
              <td align="center" border="tt" thead="column"><text font="bold">Tent</text></td>
              <td align="center" border="tt" thead="column"><text font="bold">SHOT</text></td>
              <td align="center" border="tt" thead="column"><text font="bold">TTT*</text></td>
              <td align="center" border="tt" thead="column"><text font="bold">TeCo</text></td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" border="t" rowspan="4" thead="row"><text font="bold">Mini Kinetics-C</text></td>
              <td align="center" border="r t" thead="row">3D ResNet18</td>
              <td align="center" border="r t" thead="row">61.7</td>
              <td align="center" border="t">49.4</td>
              <td align="center" border="t">50.6</td>
              <td align="center" border="t">53.9</td>
              <td align="center" border="t">52.6</td>
              <td align="center" border="t">54.6</td>
              <td align="center" border="t"><text font="bold">56.9</text></td>
            </tr>
            <tr>
              <td align="center" border="r" thead="row">ResNet18</td>
              <td align="center" border="r" thead="row">66.0</td>
              <td align="center">51.6</td>
              <td align="center">53.0</td>
              <td align="center">55.6</td>
              <td align="center">53.2</td>
              <td align="center">59.5</td>
              <td align="center"><text font="bold">60.8</text></td>
            </tr>
            <tr>
              <td align="center" border="r" thead="row">TAM-ResNet18</td>
              <td align="center" border="r" thead="row">68.5</td>
              <td align="center">55.9</td>
              <td align="center">57.1</td>
              <td align="center">53.8</td>
              <td align="center">58.4</td>
              <td align="center">62.2</td>
              <td align="center"><text font="bold">63.4</text></td>
            </tr>
            <tr>
              <td align="center" border="r" thead="row">MViTv2-S</td>
              <td align="center" border="r" thead="row">84.4</td>
              <td align="center">77.9</td>
              <td align="center">78.0</td>
              <td align="center">79.2</td>
              <td align="center">78.2</td>
              <td align="center">78.0</td>
              <td align="center"><text font="bold">80.1</text></td>
            </tr>
            <tr>
              <td align="center" border="bb t" rowspan="4" thead="row"><text font="bold">Mini SSV2-C</text></td>
              <td align="center" border="r t" thead="row">3D ResNet18</td>
              <td align="center" border="r t" thead="row">52.2</td>
              <td align="center" border="t">39.3</td>
              <td align="center" border="t">40.0</td>
              <td align="center" border="t">39.9</td>
              <td align="center" border="t">42.4</td>
              <td align="center" border="t">41.5</td>
              <td align="center" border="t"><text font="bold">45.7</text></td>
            </tr>
            <tr>
              <td align="center" border="r" thead="row">ResNet18</td>
              <td align="center" border="r" thead="row">30.2</td>
              <td align="center">20.0</td>
              <td align="center">20.3</td>
              <td align="center">22.5</td>
              <td align="center">23.8</td>
              <td align="center">22.2</td>
              <td align="center"><text font="bold">24.5</text></td>
            </tr>
            <tr>
              <td align="center" border="r" thead="row">TAM-ResNet18</td>
              <td align="center" border="r" thead="row">55.5</td>
              <td align="center">44.2</td>
              <td align="center">45.0</td>
              <td align="center">44.5</td>
              <td align="center">45.2</td>
              <td align="center">46.7</td>
              <td align="center"><text font="bold">49.4</text></td>
            </tr>
            <tr>
              <td align="center" border="bb r" thead="row">MViTv2-S</td>
              <td align="center" border="bb r" thead="row">56.8</td>
              <td align="center" border="bb">48.1</td>
              <td align="center" border="bb">48.1</td>
              <td align="center" border="bb">48.4</td>
              <td align="center" border="bb">48.2</td>
              <td align="center" border="bb">48.1</td>
              <td align="center" border="bb"><text font="bold">48.5</text></td>
            </tr>
          </tbody>
        </tabular>
      </inline-block>
    </table>
    <figure inlist="lof" labels="LABEL:fig:full-kinetics-c" placement="t" xml:id="S4.F3">
      <tags>
        <tag><text fontsize="90%">Figure 3</text></tag>
        <tag role="autoref">Figure 3</tag>
        <tag role="refnum">3</tag>
        <tag role="typerefnum">Figure 3</tag>
      </tags>
      <graphics candidates="images/kinetics-I3D-RESNET18-full-plot-v2.png" class="ltx_centering" graphic="./images/kinetics-I3D-RESNET18-full-plot-v2.png" options="width=368.57964pt,keepaspectratio=true" xml:id="S4.F3.g1"/>
<!--  %˝ -->      <toccaption><tag close=" ">3</tag>Full results on Mini Kinetics-C, with a backbone of 3D ResNet18. The accuracy of standard and BN methods drops sharply when the models encounter severe corruptions like noise, fog, rain, and contrast change. TeCo remains robust against corruptions at even level-5 severity.</toccaption>
      <caption><tag close=": "><text fontsize="90%">Figure 3</text></tag><text fontsize="90%">Full results on Mini Kinetics-C, with a backbone of 3D ResNet18. The accuracy of standard and BN methods drops sharply when the models encounter severe corruptions like noise, fog, rain, and contrast change. TeCo remains robust against corruptions at even level-5 severity.</text></caption>
    </figure>
<!--  %**** iclr2023˙conference.tex Line 350 **** -->    <subsection inlist="toc" xml:id="S4.SS1">
      <tags>
        <tag>4.1</tag>
        <tag role="autoref">subsection 4.1</tag>
        <tag role="refnum">4.1</tag>
        <tag role="typerefnum">§4.1</tag>
      </tags>
      <title><tag close=" ">4.1</tag>Experimental Results</title>
      <para xml:id="S4.SS1.p1">
        <p><text font="bold">mPC across architectures and datasets.</text>
Benefiting from the temporal coherence, our method TeCo consistently outperforms image-based test-time optimization methods across architectures and datasets. Table <ref labelref="LABEL:mPC-table"/> shows the mPC on Mini Kinetics-C of various methods. Compared with the standard inference, BN improves the mPC by up to 2% because it only adapts BN statistics; Tent, SHOT and TTT* increase the robustness by a larger margin, due to their updates on more parameters. However, the improvements are not consistent across architectures. For TAM-ResNet18, the Tent degrades the performance. We hypothesize that Tent fully removes the training time BN statistics, but the test-time data is not sufficient to make the parameters reach the global optimum.
Though there is a smaller gap between clean and corruption accuracy on MViTv2-S, TeCo is still able to improve the mPC by 2.2%.
As a result, our proposed TeCo achieves the best performance, enhancing the mPC by 2.2<Math mode="inline" tex="\sim" text="similar-to" xml:id="S4.SS1.p1.m1">
            <XMath>
              <XMTok meaning="similar-to" name="sim" role="RELOP">∼</XMTok>
            </XMath>
          </Math>9.2% on various architectures.
In Figure <ref labelref="LABEL:mean-robustness-fig"/>, we show the mPC on various architectures, including 3D ResNet18, ResNet18, TAM-ResNet18 and MViTv2-S. There is a small gap of 5% between the accuracy testing on clean data and the mPC results achieved by TeCo.</p>
      </para>
      <para xml:id="S4.SS1.p2">
        <p>On Mini SSV2-C, our method TeCo surpasses other baseline methods significantly. Table <ref labelref="LABEL:mPC-table"/> demonstrates the overall mPC of methods on Mini SSV2-C. We find the ResNet18-based method has an obvious degradation in the robustness.
Because this method has no module in capturing temporal information except output fusion, their performance on Mini SSV2 is relatively poor when the Mini SSV2 replies more on temporal information for classification.
Similar to the phenomenon on Mini-Kinetics-C, BN increases the mPC by up to 1%; Tent, SHOT and TTT* show a diverging enhancement across architectures, up to 3.8%.
TeCo consistently improves the corruption robustness on various architectures from 0.4<Math mode="inline" tex="\sim" text="similar-to" xml:id="S4.SS1.p2.m1">
            <XMath>
              <XMTok meaning="similar-to" name="sim" role="RELOP">∼</XMTok>
            </XMath>
          </Math>6.4%.
Horizontally comparing the performance on Mini Kinetics-C and Mini SSV2-C, there is a larger gap between clean accuracy and mPC on Mini SSV2-C from absolute and relative aspects, as shown in Figure <ref labelref="LABEL:mean-robustness-fig"/>.
We hypothesize that the corrupted temporal information is hard to be compensated, but the spatial semantics can be extracted from the neighboring frames with the nature of temporal coherence.</p>
      </para>
      <para xml:id="S4.SS1.p3">
        <p><text font="bold">Accuracy w.r.t corruption and severity.</text>
When digging deeper into the robustness of methods, we find that TeCo achieves superior performance on corruptions at different levels of severities. Figure <ref labelref="LABEL:fig:full-kinetics-c"/><!--  %**** iclr2023˙conference.tex Line 375 **** --> shows the complete results on various corruptions from Mini Kinetics-C, with a backbone of 3D ResNet18.
The horizontal axis indicates the severity of corruption.
When the value is 0, it corresponds to the accuracy on clean test data.
We find the Standard and BN methods have much worse performance compared to Tent, SHOT, TTT*, and TeCo, when data is corrupted by shot noise, fog, contrast, saturate change, and rain.
We hypothesize that simply adapting the batch normalization statistics is not able to correct the shifts raised by these corruptions. Tent, SHOT, TTT*, and TeCo demonstrate a similar trend when they encounter corruptions at various severity levels.
However, TeCo always lies at the top of the trend lines, especially at a severity level of 5.
For instance, TeCo maintains the accuracy of 46.7% at level-5 contrast changes, while the accuracy of standard and BN methods drops to 12%.
It shows that TeCo benefits from the temporal coherence in video data, apart from an end-to-end test-time optimization scheme. In Appendix, we also show TeCo consistently outperforms other methods on various corruptions on Mini SSV2-C.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="S4.SS2">
      <tags>
        <tag>4.2</tag>
        <tag role="autoref">subsection 4.2</tag>
        <tag role="refnum">4.2</tag>
        <tag role="typerefnum">§4.2</tag>
      </tags>
      <title><tag close=" ">4.2</tag>Ablation Studies</title>
      <para xml:id="S4.SS2.p1">
        <p><text font="bold">Disentangle Modules in TeCo.</text>
Table <ref labelref="LABEL:ablation-table"/> shows the utility of three key components in TeCo: test-time optimization strategy, global and local pathways integration, and attentive temporal coherence regularization with local content.
The test-time optimization strategy balances training and test-time parameters and statistics, which is more effective than other baseline methods in improving video classification corruption robustness.
Global and local pathways provide complementary information for optimization. If we only use uniform sampling for entropy minimization and temporal coherence regularization, the improvement is as low as 0.1% on Mini SSV2-C.
When we apply the regularization on local content, the mPC increases by 1% and 1.3% respectively.</p>
      </para>
      <table inlist="lot" labels="LABEL:ablation-table" placement="t" xml:id="S4.T2">
        <tags>
          <tag><text fontsize="90%">Table 2</text></tag>
          <tag role="autoref">Table 2</tag>
          <tag role="refnum">2</tag>
          <tag role="typerefnum">Table 2</tag>
        </tags>
        <toccaption><tag close=" ">2</tag>Ablating components of TeCo on Mini Kinetics-C and Mini SSV2-C. The test-time optimization strategy, local pathway (l-path), and attentive temporal coherence regularization improve robustness.</toccaption>
        <caption><tag close=": "><text fontsize="90%">Table 2</text></tag><text fontsize="90%">Ablating components of TeCo on Mini Kinetics-C and Mini SSV2-C. The test-time optimization strategy, local pathway (l-path), and attentive temporal coherence regularization improve robustness.</text></caption>
        <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
          <thead>
            <tr>
              <td align="center" border="r tt" thead="column row">model, 3D R18</td>
              <td align="center" border="r tt" thead="column row">Standard</td>
              <td align="center" border="r tt" thead="column row">SHOT</td>
              <td align="center" border="tt" thead="column">TeCo (w/o l-path)</td>
              <td align="center" border="tt" thead="column">TeCo (uniform)</td>
              <td align="center" border="tt" thead="column">TeCo</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" border="r t" thead="row">Mini Kinetics-C</td>
              <td align="center" border="r t" thead="row">49.4</td>
              <td align="center" border="r t" thead="row">52.6</td>
              <td align="center" border="t">55.9</td>
              <td align="center" border="t">56.5</td>
              <td align="center" border="t"><text font="bold">56.9</text></td>
            </tr>
            <tr>
              <td align="center" border="bb r" thead="row">Mini SSV2-C</td>
              <td align="center" border="bb r" thead="row">39.3</td>
              <td align="center" border="bb r" thead="row">42.4</td>
              <td align="center" border="bb">44.4</td>
              <td align="center" border="bb">44.5</td>
              <td align="center" border="bb"><text font="bold">45.7</text></td>
            </tr>
          </tbody>
        </tabular>
      </table>
<!--  %“begin–table˝[t] 
     %“caption–Online test-time optimization setting on Mini Kinetics-C and Mini SSV2-C. Batch Size is 32. ˝ 
     %“label–ablation-table˝ 
     %“begin–center˝ 
     %“begin–tabular˝–c—c—c c c  ˝ 
     %“toprule 
     %model, 3D R18 &amp; Standard &amp; BN &amp; Tent&amp; TeCo ““ 
     %“midrule 
     %Mini Kinetics-C    &amp;49.4 &amp;51.6&amp; 52.2 &amp; 53.3 ““ 
     %Mini SSV2-C &amp; 39.3&amp; 41.8&amp; 39.4 &amp; 43.4““ 
     %“bottomrule 
     %“end–tabular˝ 
     %“end–center˝ 
     %“end–table˝ 
     %“begin–table˝[t] 
     %“caption–Online test-time optimization setting on Mini Kinetics-C and Mini SSV2-C. Batch Size is 64. ˝ 
     %“label–ablation-table˝ 
     %“begin–center˝ 
     %“begin–tabular˝–c—c—c c c  ˝ 
     %“toprule 
     %**** iclr2023˙conference.tex Line 425 **** 
     %model, 3D R18 &amp; Standard &amp; BN &amp; Tent&amp; TeCo ““ 
     %“midrule 
     %Mini Kinetics-C    &amp;49.4 &amp;ing &amp; &amp;  ““ 
     %Mini SSV2-C &amp; 39.3&amp; 38.4&amp; 39.0 &amp; ““ 
     %“bottomrule 
     %“end–tabular˝ 
     %“end–center˝ 
     %“end–table˝ -->      <para xml:id="S4.SS2.p2">
        <p><text font="bold">Study Hyper-parameter Sensitivity.</text> Table <ref labelref="LABEL:beta-table"/> demonstrates the impact of <Math mode="inline" tex="\beta" text="beta" xml:id="S4.SS2.p2.m1">
            <XMath>
              <XMTok font="italic" name="beta" role="UNKNOWN">β</XMTok>
            </XMath>
          </Math> in Equation <ref labelref="LABEL:eq:overall_equation"/>. We obtain the best result at <Math mode="inline" tex="\beta" text="beta" xml:id="S4.SS2.p2.m2">
            <XMath>
              <XMTok font="italic" name="beta" role="UNKNOWN">β</XMTok>
            </XMath>
          </Math>=1. Table <ref labelref="LABEL:stage-table"/> shows the comparison results of the attentive temporal coherence regularization module added after different blocks of 3D ResNet18. The performances after <Math mode="inline" tex="res_{1-3}" text="r * e * s _ (1 - 3)" xml:id="S4.SS2.p2.m3">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">r</XMTok>
                <XMTok font="italic" role="UNKNOWN">e</XMTok>
                <XMApp>
                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                  <XMTok font="italic" role="UNKNOWN">s</XMTok>
                  <XMApp>
                    <XMTok fontsize="70%" meaning="minus" role="ADDOP">-</XMTok>
                    <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                    <XMTok fontsize="70%" meaning="3" role="NUMBER">3</XMTok>
                  </XMApp>
                </XMApp>
              </XMApp>
            </XMath>
          </Math> are similar, while there is a slight drop after <Math mode="inline" tex="res_{4}" text="r * e * s _ 4" xml:id="S4.SS2.p2.m4">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">r</XMTok>
                <XMTok font="italic" role="UNKNOWN">e</XMTok>
                <XMApp>
                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                  <XMTok font="italic" role="UNKNOWN">s</XMTok>
                  <XMTok fontsize="70%" meaning="4" role="NUMBER">4</XMTok>
                </XMApp>
              </XMApp>
            </XMath>
          </Math>. We hypothesize the deeper layer will generate more distinguishable features. It benefits from variety instead of consistency for classification. Table <ref labelref="LABEL:batchsize-table"/> studies the impact of batch size on the performance of TeCo. The mPC keeps improving when we increase the batch size from 8 to 32, while it drops when we use the batch size of 64.
Because we optimize the model for one epoch for a fair comparison, a larger batch size will lead to fewer iterations of updates. The batch size of 32 balances the amount of data in one batch and the number of iterations in one epoch, which enables models to obtain the best performance. Hence, TeCo can improve robustness reliably without tuning the hyper-parameters carefully.</p>
      </para>
      <para xml:id="S4.SS2.p3">
        <p><text font="bold">Balance training and test statistics with <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.SS2.p3.m1">
              <XMath>
                <XMTok font="medium italic" name="alpha" role="UNKNOWN">α</XMTok>
              </XMath>
            </Math></text>. Figure <ref labelref="LABEL:data-fig"/> shows the mPC on Mini Kinetics-C w.r.t optimization iteration, with different <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.SS2.p3.m2">
            <XMath>
              <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
            </XMath>
          </Math> and the backbone of 3D ResNet18. The <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.SS2.p3.m3">
            <XMath>
              <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
            </XMath>
          </Math> in Section 3.1 controls the weights of training and test statistics in normalization layer initialization. When we set <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.SS2.p3.m4">
            <XMath>
              <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
            </XMath>
          </Math> to a smaller value (e.g., 0.2), the normalization layer relies more on test data initially. Hence, it converges faster in test-time optimization. After 40 iterations of test-time optimization, the mPC is enhanced from 49.6% to 55.3%. When the iterations increases, the performance improves slower and gets closer for different <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.SS2.p3.m5">
            <XMath>
              <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
            </XMath>
          </Math>. In summary, the model robustness is more sensitive to <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.SS2.p3.m6">
            <XMath>
              <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
            </XMath>
          </Math> when less data is used in test-time optimization.</p>
      </para>
      <para xml:id="S4.SS2.p4">
        <p><text font="bold">Visualize Space-Time Feature Map.</text>
Figure <ref labelref="LABEL:featuremap-fig"/> shows the feature maps of noise corrupted video after <Math mode="inline" tex="res_{1}" text="r * e * s _ 1" xml:id="S4.SS2.p4.m1">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">r</XMTok>
                <XMTok font="italic" role="UNKNOWN">e</XMTok>
                <XMApp>
                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                  <XMTok font="italic" role="UNKNOWN">s</XMTok>
                  <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                </XMApp>
              </XMApp>
            </XMath>
          </Math> stages. Horizontally, we visualize the features of different frames.
The time gap between neighboring frames is <Math mode="inline" tex="i=1" text="i = 1" xml:id="S4.SS2.p4.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="equals" role="RELOP">=</XMTok>
                <XMTok font="italic" role="UNKNOWN">i</XMTok>
                <XMTok meaning="1" role="NUMBER">1</XMTok>
              </XMApp>
            </XMath>
          </Math>.
Standard and BN methods fail to recover human-recognizable semantics from the noise-corrupted frames.
Tent obtains noisy outlines along the temporal dimension.
TeCo removes the noise and generates temporal coherent feature maps, indicating that TeCo helps the model obtain smoother and more consistent features.</p>
      </para>
      <table inlist="lot" placement="t" xml:id="S4.T3">
        <tags>
          <tag><text fontsize="90%">Table 3</text></tag>
          <tag role="autoref">Table 3</tag>
          <tag role="refnum">3</tag>
          <tag role="typerefnum">Table 3</tag>
        </tags>
        <toccaption><tag close=" ">3</tag>TeCo ablation experiments with 3D ResNet18 on Mini Kinetics-C. If not specified, the default setting is: the beta <Math mode="inline" tex="\beta" text="beta" xml:id="S4.T3.m1">
            <XMath>
              <XMTok font="italic" name="beta" role="UNKNOWN">β</XMTok>
            </XMath>
          </Math> that control the weight of temporal coherence regularization is 1, the regularization is applied after <Math mode="inline" tex="res_{2}" text="r * e * s _ 2" xml:id="S4.T3.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">r</XMTok>
                <XMTok font="italic" role="UNKNOWN">e</XMTok>
                <XMApp>
                  <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                  <XMTok font="italic" role="UNKNOWN">s</XMTok>
                  <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                </XMApp>
              </XMApp>
            </XMath>
          </Math> stage of model, the test time optimization uses a batch size of 32 and <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.T3.m3">
            <XMath>
              <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
            </XMath>
          </Math> is 0.4. The default setting has bold text.</toccaption>
        <caption><tag close=": "><text fontsize="90%">Table 3</text></tag><text fontsize="90%">TeCo ablation experiments with 3D ResNet18 on Mini Kinetics-C. If not specified, the default setting is: the beta <Math mode="inline" tex="\beta" text="beta" xml:id="S4.T3.m4">
              <XMath>
                <XMTok font="italic" name="beta" role="UNKNOWN">β</XMTok>
              </XMath>
            </Math> that control the weight of temporal coherence regularization is 1, the regularization is applied after <Math mode="inline" tex="res_{2}" text="r * e * s _ 2" xml:id="S4.T3.m5">
              <XMath>
                <XMApp>
                  <XMTok fontsize="111%" meaning="times" role="MULOP">⁢</XMTok>
                  <XMTok font="italic" role="UNKNOWN">r</XMTok>
                  <XMTok font="italic" role="UNKNOWN">e</XMTok>
                  <XMApp>
                    <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                    <XMTok font="italic" role="UNKNOWN">s</XMTok>
                    <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                  </XMApp>
                </XMApp>
              </XMath>
            </Math> stage of model, the test time optimization uses a batch size of 32 and <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.T3.m6">
              <XMath>
                <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
              </XMath>
            </Math> is 0.4. The default setting has bold text.</text></caption>
        <table class="ltx_figure_panel" inlist="lot" labels="LABEL:beta-table" placement="h" xml:id="S4.T3.st1">
          <tags>
            <tag><text fontsize="90%">(a)</text></tag>
            <tag role="autoref">3(a)</tag>
            <tag role="refnum">3(a)</tag>
          </tags>
<!--  %**** iclr2023˙conference.tex Line 450 **** -->          <toccaption><tag close=" ">(a)</tag><Math mode="inline" tex="\beta" text="beta" xml:id="S4.T3.st1.m1">
              <XMath>
                <XMTok font="italic" name="beta" role="UNKNOWN">β</XMTok>
              </XMath>
            </Math><text font="bold"> in Equation <ref labelref="LABEL:eq:overall_equation"/></text>. TeCo applies a balanced weight to temporal coherence regularization.</toccaption>
          <caption><tag close=" "><text fontsize="90%">(a)</text></tag><Math mode="inline" tex="\beta" text="beta" xml:id="S4.T3.st1.m2">
              <XMath>
                <XMTok font="italic" fontsize="90%" name="beta" role="UNKNOWN">β</XMTok>
              </XMath>
            </Math><text font="bold" fontsize="90%"> in Equation <ref labelref="LABEL:eq:overall_equation"/><text font="medium">. TeCo applies a balanced weight to temporal coherence regularization.</text></text></caption>
          <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
            <thead>
              <tr>
                <td align="center" border="r tt" thead="column row"><Math mode="inline" tex="\beta" text="beta" xml:id="S4.T3.st1.m3">
                    <XMath>
                      <XMTok font="italic" name="beta" role="UNKNOWN">β</XMTok>
                    </XMath>
                  </Math></td>
                <td align="center" border="tt" thead="column">mPC</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" border="r t" thead="row">0.1</td>
                <td align="center" border="t">56.4</td>
              </tr>
              <tr>
                <td align="center" border="r" thead="row">0.5</td>
                <td align="center">56.7</td>
              </tr>
              <tr>
                <td align="center" border="r" thead="row"><text font="bold">1</text></td>
                <td align="center"><text font="bold">56.9</text></td>
              </tr>
              <tr>
                <td align="center" border="bb r" thead="row">5</td>
                <td align="center" border="bb">56.6</td>
              </tr>
            </tbody>
          </tabular>
        </table>
        <table class="ltx_figure_panel" inlist="lot" labels="LABEL:stage-table" placement="h" xml:id="S4.T3.st2">
          <tags>
            <tag><text fontsize="90%">(b)</text></tag>
            <tag role="autoref">3(b)</tag>
            <tag role="refnum">3(b)</tag>
          </tags>
          <toccaption><tag close=" ">(b)</tag><text font="bold">Block stage</text>. The temporal coherence regularization module is added after certain block stage.</toccaption>
          <caption><tag close=" "><text fontsize="90%">(b)</text></tag><text font="bold" fontsize="90%">Block stage<text font="medium">. The temporal coherence regularization module is added after certain block stage.</text></text></caption>
          <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
            <thead>
              <tr>
                <td align="center" border="r tt" thead="column row">Stage</td>
                <td align="center" border="tt" thead="column">mPC</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" border="r t" thead="row"><Math mode="inline" tex="res_{1}" text="r * e * s _ 1" xml:id="S4.T3.st2.m1">
                    <XMath>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok font="italic" role="UNKNOWN">r</XMTok>
                        <XMTok font="italic" role="UNKNOWN">e</XMTok>
                        <XMApp>
                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                          <XMTok font="italic" role="UNKNOWN">s</XMTok>
                          <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                        </XMApp>
                      </XMApp>
                    </XMath>
                  </Math></td>
                <td align="center" border="t">56.6</td>
              </tr>
              <tr>
                <td align="center" border="r" thead="row"><Math mode="inline" tex="res_{2}" text="r * e * s _ 2" xml:id="S4.T3.st2.m2">
                    <XMath>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok font="italic" role="UNKNOWN">r</XMTok>
                        <XMTok font="italic" role="UNKNOWN">e</XMTok>
                        <XMApp>
                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                          <XMTok font="italic" role="UNKNOWN">s</XMTok>
                          <XMTok fontsize="70%" meaning="2" role="NUMBER">2</XMTok>
                        </XMApp>
                      </XMApp>
                    </XMath>
                  </Math></td>
                <td align="center"><text font="bold">56.9</text></td>
              </tr>
              <tr>
                <td align="center" border="r" thead="row"><Math mode="inline" tex="res_{3}" text="r * e * s _ 3" xml:id="S4.T3.st2.m3">
                    <XMath>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok font="italic" role="UNKNOWN">r</XMTok>
                        <XMTok font="italic" role="UNKNOWN">e</XMTok>
                        <XMApp>
                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                          <XMTok font="italic" role="UNKNOWN">s</XMTok>
                          <XMTok fontsize="70%" meaning="3" role="NUMBER">3</XMTok>
                        </XMApp>
                      </XMApp>
                    </XMath>
                  </Math></td>
                <td align="center">56.6</td>
              </tr>
              <tr>
                <td align="center" border="bb r" thead="row"><Math mode="inline" tex="res_{4}" text="r * e * s _ 4" xml:id="S4.T3.st2.m4">
                    <XMath>
                      <XMApp>
                        <XMTok meaning="times" role="MULOP">⁢</XMTok>
                        <XMTok font="italic" role="UNKNOWN">r</XMTok>
                        <XMTok font="italic" role="UNKNOWN">e</XMTok>
                        <XMApp>
                          <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                          <XMTok font="italic" role="UNKNOWN">s</XMTok>
                          <XMTok fontsize="70%" meaning="4" role="NUMBER">4</XMTok>
                        </XMApp>
                      </XMApp>
                    </XMath>
                  </Math></td>
                <td align="center" border="bb">56.2</td>
              </tr>
            </tbody>
          </tabular>
        </table>
        <table class="ltx_figure_panel" inlist="lot" labels="LABEL:batchsize-table" placement="h" xml:id="S4.T3.st3">
          <tags>
            <tag><text fontsize="90%">(c)</text></tag>
            <tag role="autoref">3(c)</tag>
            <tag role="refnum">3(c)</tag>
          </tags>
          <toccaption><tag close=" ">(c)</tag><text font="bold">Batch size</text>. TeCo uses batches of data in test time optimization.</toccaption>
          <caption><tag close=" "><text fontsize="90%">(c)</text></tag><text font="bold" fontsize="90%">Batch size<text font="medium">. TeCo uses batches of data in test time optimization.</text></text></caption>
          <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
            <thead>
              <tr>
                <td align="center" border="r tt" thead="column row">Batch Size</td>
                <td align="center" border="tt" thead="column">mPC</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" border="r t" thead="row">8</td>
                <td align="center" border="t">56.6</td>
              </tr>
              <tr>
                <td align="center" border="r" thead="row">16</td>
                <td align="center">56.7</td>
              </tr>
              <tr>
                <td align="center" border="r" thead="row"><text font="bold">32</text></td>
                <td align="center"><text font="bold">56.9</text></td>
              </tr>
              <tr>
                <td align="center" border="bb r" thead="row">64</td>
                <td align="center" border="bb">56.6</td>
              </tr>
            </tbody>
          </tabular>
        </table>
      </table>
      <figure placement="t" xml:id="S4.SS2.fig3">
        <block>
          <figure class="ltx_figure_panel" inlist="lof" labels="LABEL:data-fig" vattach="middle" width="212.5pt" xml:id="S4.F4">
            <graphics candidates="images/alpha-iteration-full_v3.png" class="ltx_centering ltx_figure_panel" graphic="./images/alpha-iteration-full_v3.png" options="width=359.90538pt,keepaspectratio=true" xml:id="S4.SS2.g1"/>
<!--  %˝ -->            <break class="ltx_break"/>
            <tags>
              <tag><text fontsize="90%">Figure 4</text></tag>
              <tag role="autoref">Figure 4</tag>
              <tag role="refnum">4</tag>
              <tag role="typerefnum">Figure 4</tag>
            </tags>
            <toccaption><tag close=" ">4</tag>Iteration in test-time optimization vs mPC, on various <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.F4.m1">
                <XMath>
                  <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
                </XMath>
              </Math>. The mPC improves rapidly in the beginning, then the improvements slow down when more data is used. Smaller <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.F4.m2">
                <XMath>
                  <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
                </XMath>
              </Math> uses more test data statistics, which enables the performance to converge faster.</toccaption>
            <caption><tag close=": "><text fontsize="90%">Figure 4</text></tag><text fontsize="90%">Iteration in test-time optimization vs mPC, on various <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.F4.m3">
                  <XMath>
                    <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
                  </XMath>
                </Math>. The mPC improves rapidly in the beginning, then the improvements slow down when more data is used. Smaller <Math mode="inline" tex="\alpha" text="alpha" xml:id="S4.F4.m4">
                  <XMath>
                    <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
                  </XMath>
                </Math> uses more test data statistics, which enables the performance to converge faster.</text></caption>
          </figure>
          <figure class="ltx_figure_panel" inlist="lof" labels="LABEL:featuremap-fig" vattach="middle" width="208.1pt" xml:id="S4.F5">
            <graphics candidates="images/feature_map.png" class="ltx_centering ltx_figure_panel" graphic="./images/feature_map.png" options="width=420.61192pt,keepaspectratio=true" xml:id="S4.SS2.g2"/>
<!--  %˝ -->            <break class="ltx_break"/>
            <tags>
              <tag><text fontsize="90%">Figure 5</text></tag>
              <tag role="autoref">Figure 5</tag>
              <tag role="refnum">5</tag>
              <tag role="typerefnum">Figure 5</tag>
            </tags>
            <toccaption><tag close=" ">5</tag>We visualize <Math mode="inline" tex="res_{1}" text="r * e * s _ 1" xml:id="S4.F5.m1">
                <XMath>
                  <XMApp>
                    <XMTok meaning="times" role="MULOP">⁢</XMTok>
                    <XMTok font="italic" role="UNKNOWN">r</XMTok>
                    <XMTok font="italic" role="UNKNOWN">e</XMTok>
                    <XMApp>
                      <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                      <XMTok font="italic" role="UNKNOWN">s</XMTok>
                      <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                    </XMApp>
                  </XMApp>
                </XMath>
              </Math> feature maps of consecutive frames. The time gap between frames is <Math mode="inline" tex="i=1" text="i = 1" xml:id="S4.F5.m2">
                <XMath>
                  <XMApp>
                    <XMTok meaning="equals" role="RELOP">=</XMTok>
                    <XMTok font="italic" role="UNKNOWN">i</XMTok>
                    <XMTok meaning="1" role="NUMBER">1</XMTok>
                  </XMApp>
                </XMath>
              </Math>. TeCo generates smoother feature maps in both spatial and temporal dimensions.</toccaption>
            <caption><tag close=": "><text fontsize="90%">Figure 5</text></tag><text fontsize="90%">We visualize <Math mode="inline" tex="res_{1}" text="r * e * s _ 1" xml:id="S4.F5.m3">
                  <XMath>
                    <XMApp>
                      <XMTok fontsize="111%" meaning="times" role="MULOP">⁢</XMTok>
                      <XMTok font="italic" role="UNKNOWN">r</XMTok>
                      <XMTok font="italic" role="UNKNOWN">e</XMTok>
                      <XMApp>
                        <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                        <XMTok font="italic" role="UNKNOWN">s</XMTok>
                        <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
                      </XMApp>
                    </XMApp>
                  </XMath>
                </Math> feature maps of consecutive frames. The time gap between frames is <Math mode="inline" tex="i=1" text="i = 1" xml:id="S4.F5.m4">
                  <XMath>
                    <XMApp>
                      <XMTok meaning="equals" role="RELOP">=</XMTok>
                      <XMTok font="italic" role="UNKNOWN">i</XMTok>
                      <XMTok meaning="1" role="NUMBER">1</XMTok>
                    </XMApp>
                  </XMath>
                </Math>. TeCo generates smoother feature maps in both spatial and temporal dimensions.</text></caption>
          </figure>
        </block>
      </figure>
    </subsection>
  </section>
  <section inlist="toc" xml:id="S5">
    <tags>
      <tag>5</tag>
      <tag role="autoref">section 5</tag>
      <tag role="refnum">5</tag>
      <tag role="typerefnum">§5</tag>
    </tags>
    <title><tag close=" ">5</tag>Conclusion</title>
    <para xml:id="S5.p1">
      <p>TeCo is a test-time optimization technique for improving the corruption robustness of video classification models, which updates model parameters during testing by two objectives with self-supervised learning:
TeCo uses global information for entropy minimization,
and it also applies attentive temporal coherence regularization on local information.
Benefiting from long-term and short-term spatio-temporal information, TeCo achieves significant corruption robustness improvement on Mini Kinetics-C and Mini SSV2-C.
We hope TeCo becomes a new baseline method that makes video classification models more reliable and robust in real-world deployment.</p>
    </para>
  </section>
  <section inlist="toc" xml:id="S6">
    <tags>
      <tag>6</tag>
      <tag role="autoref">section 6</tag>
      <tag role="refnum">6</tag>
      <tag role="typerefnum">§6</tag>
    </tags>
    <title><tag close=" ">6</tag>Acknowledgements</title>
    <para xml:id="S6.p1">
      <p>This work was done at Rapid-Rich Object Search (ROSE) Lab, Nanyang Technological University. This research is supported in part by the NTU-PKU Joint Research Institute (a collaboration between the Nanyang Technological University and Peking University that is sponsored by a donation from the Ng Teng Fong Charitable Foundation). This work is also supported by the Research Grant Council(RGC) of Hong Kong through Early Career Scheme (ECS) under the Grant21200522,CityU Applied Research Grant(ARG) 9667244,and Sichuan Science and Technology Program 2022NSFSC0551.</p>
    </para>
  </section>
  <bibliography xml:id="bib">
    <title>References</title>
    <biblist>
      <bibitem key="alizadeh2020gradient" xml:id="bib.bib1">
        <tags>
          <tag role="number">1</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Alizadeh et al.</tag>
          <tag role="fullauthors">Alizadeh, Behboodi, van Baalen, Louizos,
Blankevoort, and Welling</tag>
          <tag role="refnum">Alizadeh et al. (2020)</tag>
          <tag role="key">alizadeh2020gradient</tag>
        </tags>
        <bibblock>
Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen
Blankevoort, and Max Welling.
</bibblock>
        <bibblock>Gradient l1 regularization for quantization robustness.
</bibblock>
        <bibblock>In <emph font="italic">International Conference on Learning Representations</emph>, 2020.
</bibblock>
      </bibitem>
      <bibitem key="azimi2022self" xml:id="bib.bib2">
        <tags>
          <tag role="number">2</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Azimi et al.</tag>
          <tag role="fullauthors">Azimi, Palacio, Raue, Hees, Bertinetto, and
Dengel</tag>
          <tag role="refnum">Azimi et al. (2022)</tag>
          <tag role="key">azimi2022self</tag>
        </tags>
        <bibblock>
Fatemeh Azimi, Sebastian Palacio, Federico Raue, Jörn Hees, Luca
Bertinetto, and Andreas Dengel.
</bibblock>
        <bibblock>Self-supervised test-time adaptation on video data.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision</emph>, pp.  3439–3448, 2022.
</bibblock>
      </bibitem>
      <bibitem key="ba2016layer" xml:id="bib.bib3">
        <tags>
          <tag role="number">3</tag>
          <tag role="year">2016</tag>
          <tag role="authors">Ba et al.</tag>
          <tag role="fullauthors">Ba, Kiros, and Hinton</tag>
          <tag role="refnum">Ba et al. (2016)</tag>
          <tag role="key">ba2016layer</tag>
        </tags>
        <bibblock>
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
</bibblock>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 25 **** -->Layer normalization.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1607.06450</emph>, 2016.
</bibblock>
      </bibitem>
      <bibitem key="bektacs2010comparison" xml:id="bib.bib4">
        <tags>
          <tag role="number">4</tag>
          <tag role="year">2010</tag>
          <tag role="authors">Bektaş &amp; Şişman</tag>
          <tag role="fullauthors">Bektaş and
Şişman</tag>
          <tag role="refnum">Bektaş &amp; Şişman (2010)</tag>
          <tag role="key">bektacs2010comparison</tag>
        </tags>
        <bibblock>
Sebahattin Bektaş and Yasemin Şişman.
</bibblock>
        <bibblock>The comparison of l1 and l2-norm minimization methods.
</bibblock>
        <bibblock><emph font="italic">International Journal of the Physical Sciences</emph>, 5(11):1721–1727, 2010.
</bibblock>
      </bibitem>
      <bibitem key="bertasius2021space" xml:id="bib.bib5">
        <tags>
          <tag role="number">5</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Bertasius et al.</tag>
          <tag role="fullauthors">Bertasius, Wang, and
Torresani</tag>
          <tag role="refnum">Bertasius et al. (2021)</tag>
          <tag role="key">bertasius2021space</tag>
        </tags>
        <bibblock>
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
</bibblock>
        <bibblock>Is space-time attention all you need for video understanding?
</bibblock>
        <bibblock>In <emph font="italic">ICML</emph>, volume 2, pp.  4, 2021.
</bibblock>
      </bibitem>
      <bibitem key="carreira2017quo" xml:id="bib.bib6">
        <tags>
          <tag role="number">6</tag>
          <tag role="year">2017</tag>
          <tag role="authors">Carreira &amp; Zisserman</tag>
          <tag role="fullauthors">Carreira and Zisserman</tag>
          <tag role="refnum">Carreira &amp; Zisserman (2017)</tag>
          <tag role="key">carreira2017quo</tag>
        </tags>
        <bibblock>
Joao Carreira and Andrew Zisserman.
</bibblock>
        <bibblock>Quo vadis, action recognition? a new model and the kinetics dataset.
</bibblock>
        <bibblock>In <emph font="italic">proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</emph>, pp.  6299–6308, 2017.
</bibblock>
      </bibitem>
      <bibitem key="chen2020deep" xml:id="bib.bib7">
        <tags>
          <tag role="number">7</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Chen et al.</tag>
          <tag role="fullauthors">Chen, Panda, Ramakrishnan, Feris, Cohn, Oliva, and
Fan</tag>
          <tag role="refnum">Chen et al. (2021)</tag>
          <tag role="key">chen2020deep</tag>
        </tags>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 50 **** -->
Chun-Fu Chen, Rameswar Panda, Kandan Ramakrishnan, Rogerio Feris, John Cohn,
Aude Oliva, and Quanfu Fan.
</bibblock>
        <bibblock>Deep analysis of cnn-based spatio-temporal representations for action
recognition.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, 2021.
</bibblock>
      </bibitem>
      <bibitem key="chen2019temporal" xml:id="bib.bib8">
        <tags>
          <tag role="number">8</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Chen et al.</tag>
          <tag role="fullauthors">Chen, Kira, AlRegib, Yoo, Chen, and
Zheng</tag>
          <tag role="refnum">Chen et al. (2019)</tag>
          <tag role="key">chen2019temporal</tag>
        </tags>
        <bibblock>
Min-Hung Chen, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen, and Jian
Zheng.
</bibblock>
        <bibblock>Temporal attentive alignment for large-scale video domain adaptation.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</emph>, pp.  6321–6330, 2019.
</bibblock>
      </bibitem>
      <bibitem key="chen2022sparsity" xml:id="bib.bib9">
        <tags>
          <tag role="number">9</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Chen et al.</tag>
          <tag role="fullauthors">Chen, Zhang, Wang, Balachandra, Ma, Wang, and
Wang</tag>
          <tag role="refnum">Chen et al. (2022)</tag>
          <tag role="key">chen2022sparsity</tag>
        </tags>
        <bibblock>
Tianlong Chen, Zhenyu Zhang, Pengjun Wang, Santosh Balachandra, Haoyu Ma, Zehao
Wang, and Zhangyang Wang.
</bibblock>
        <bibblock>Sparsity winning twice: Better robust generaliztion from more
efficient training.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2202.09844</emph>, 2022.
</bibblock>
      </bibitem>
      <bibitem key="Dwibedi_2019_CVPR" xml:id="bib.bib10">
        <tags>
          <tag role="number">10</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Dwibedi et al.</tag>
          <tag role="fullauthors">Dwibedi, Aytar, Tompson, Sermanet, and
Zisserman</tag>
          <tag role="refnum">Dwibedi et al. (2019)</tag>
          <tag role="key">Dwibedi_2019_CVPR</tag>
        </tags>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 75 **** -->
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew
Zisserman.
</bibblock>
        <bibblock>Temporal cycle-consistency learning.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</emph>, June 2019.
</bibblock>
      </bibitem>
      <bibitem key="fan2019more" xml:id="bib.bib11">
        <tags>
          <tag role="number">11</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Fan et al.</tag>
          <tag role="fullauthors">Fan, Chen, Kuehne, Pistoia, and Cox</tag>
          <tag role="refnum">Fan et al. (2019)</tag>
          <tag role="key">fan2019more</tag>
        </tags>
        <bibblock>
Quanfu Fan, Chun-Fu Richard Chen, Hilde Kuehne, Marco Pistoia, and David Cox.
</bibblock>
        <bibblock>More is less: Learning efficient video representations by big-little
network and depthwise temporal aggregation.
</bibblock>
        <bibblock><emph font="italic">Advances in Neural Information Processing Systems</emph>, 32, 2019.
</bibblock>
      </bibitem>
      <bibitem key="feichtenhofer2020x3d" xml:id="bib.bib12">
        <tags>
          <tag role="number">12</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Feichtenhofer</tag>
          <tag role="refnum">Feichtenhofer (2020)</tag>
          <tag role="key">feichtenhofer2020x3d</tag>
        </tags>
        <bibblock>
Christoph Feichtenhofer.
</bibblock>
        <bibblock>X3d: Expanding architectures for efficient video recognition.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  203–213, 2020.
</bibblock>
      </bibitem>
      <bibitem key="feichtenhofer2019slowfast" xml:id="bib.bib13">
        <tags>
          <tag role="number">13</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Feichtenhofer et al.</tag>
          <tag role="fullauthors">Feichtenhofer, Fan, Malik, and
He</tag>
          <tag role="refnum">Feichtenhofer et al. (2019)</tag>
          <tag role="key">feichtenhofer2019slowfast</tag>
        </tags>
        <bibblock>
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.
</bibblock>
        <bibblock>Slowfast networks for video recognition.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF international conference on
computer vision</emph>, pp.  6202–6211, 2019.
</bibblock>
      </bibitem>
      <bibitem key="geirhos2017comparing" xml:id="bib.bib14">
        <tags>
          <tag role="number">14</tag>
          <tag role="year">2017</tag>
          <tag role="authors">Geirhos et al.</tag>
          <tag role="fullauthors">Geirhos, Janssen, Schütt, Rauber, Bethge, and
Wichmann</tag>
          <tag role="refnum">Geirhos et al. (2017)</tag>
          <tag role="key"><!--  %**** iclr2023_conference.bbl Line 100 **** -->geirhos2017comparing</tag>
        </tags>
        <bibblock>
Robert Geirhos, David HJ Janssen, Heiko H Schütt, Jonas Rauber, Matthias
Bethge, and Felix A Wichmann.
</bibblock>
        <bibblock>Comparing deep neural networks against humans: object recognition
when the signal gets weaker.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1706.06969</emph>, 2017.
</bibblock>
      </bibitem>
      <bibitem key="geirhos2018generalisation" xml:id="bib.bib15">
        <tags>
          <tag role="number">15</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Geirhos et al.</tag>
          <tag role="fullauthors">Geirhos, Temme, Rauber, Schütt, Bethge, and
Wichmann</tag>
          <tag role="refnum">Geirhos et al. (2018)</tag>
          <tag role="key">geirhos2018generalisation</tag>
        </tags>
        <bibblock>
Robert Geirhos, Carlos R Medina Temme, Jonas Rauber, Heiko H Schütt,
Matthias Bethge, and Felix A Wichmann.
</bibblock>
        <bibblock>Generalisation in humans and deep neural networks.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the Neural Information Processing Systems
(NIPS)</emph>, 2018.
</bibblock>
      </bibitem>
      <bibitem key="goroshin2015unsupervised" xml:id="bib.bib16">
        <tags>
          <tag role="number">16</tag>
          <tag role="year">2015</tag>
          <tag role="authors">Goroshin et al.</tag>
          <tag role="fullauthors">Goroshin, Bruna, Tompson, Eigen, and
LeCun</tag>
          <tag role="refnum">Goroshin et al. (2015)</tag>
          <tag role="key">goroshin2015unsupervised</tag>
        </tags>
        <bibblock>
Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun.
</bibblock>
        <bibblock>Unsupervised learning of spatiotemporally coherent metrics.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE international conference on computer
vision</emph>, pp.  4086–4093, 2015.
</bibblock>
      </bibitem>
      <bibitem key="goyal2017something" xml:id="bib.bib17">
        <tags>
          <tag role="number">17</tag>
          <tag role="year">2017</tag>
          <tag role="authors">Goyal et al.</tag>
          <tag role="fullauthors">Goyal, Ebrahimi Kahou, Michalski, Materzynska,
Westphal, Kim, Haenel, Fruend, Yianilos, Mueller-Freitag,
et al.</tag>
          <tag role="refnum">Goyal et al. (2017)</tag>
          <tag role="key">goyal2017something</tag>
        </tags>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 125 **** -->
Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska,
Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos,
Moritz Mueller-Freitag, et al.
</bibblock>
        <bibblock>The” something something” video database for learning and evaluating
visual common sense.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE International Conference on Computer
Vision</emph>, pp.  5842–5850, 2017.
</bibblock>
      </bibitem>
      <bibitem key="guo2018sparse" xml:id="bib.bib18">
        <tags>
          <tag role="number">18</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Guo et al.</tag>
          <tag role="fullauthors">Guo, Zhang, Zhang, and Chen</tag>
          <tag role="refnum">Guo et al. (2018)</tag>
          <tag role="key">guo2018sparse</tag>
        </tags>
        <bibblock>
Yiwen Guo, Chao Zhang, Changshui Zhang, and Yurong Chen.
</bibblock>
        <bibblock>Sparse dnns with improved adversarial robustness.
</bibblock>
        <bibblock><emph font="italic">Advances in neural information processing systems</emph>, 31, 2018.
</bibblock>
      </bibitem>
      <bibitem key="hara2017learning" xml:id="bib.bib19">
        <tags>
          <tag role="number">19</tag>
          <tag role="year">2017</tag>
          <tag role="authors">Hara et al.</tag>
          <tag role="fullauthors">Hara, Kataoka, and Satoh</tag>
          <tag role="refnum">Hara et al. (2017)</tag>
          <tag role="key">hara2017learning</tag>
        </tags>
        <bibblock>
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.
</bibblock>
        <bibblock>Learning spatio-temporal features with 3d residual networks for
action recognition.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE international conference on computer
vision workshops</emph>, pp.  3154–3160, 2017.
</bibblock>
      </bibitem>
      <bibitem key="hendrycks2019benchmarking" xml:id="bib.bib20">
        <tags>
          <tag role="number">20</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Hendrycks &amp; Dietterich</tag>
          <tag role="fullauthors">Hendrycks and
Dietterich</tag>
          <tag role="refnum">Hendrycks &amp; Dietterich (2019)</tag>
          <tag role="key">hendrycks2019benchmarking</tag>
        </tags>
        <bibblock>
Dan Hendrycks and Thomas Dietterich.
</bibblock>
        <bibblock>Benchmarking neural network robustness to common corruptions and
perturbations.
</bibblock>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 150 **** -->In <emph font="italic">International Conference on Learning Representations</emph>, 2019.
</bibblock>
      </bibitem>
      <bibitem key="hendrycks2019using" xml:id="bib.bib21">
        <tags>
          <tag role="number">21</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Hendrycks et al.</tag>
          <tag role="fullauthors">Hendrycks, Mazeika, Kadavath, and
Song</tag>
          <tag role="refnum">Hendrycks et al. (2019)</tag>
          <tag role="key">hendrycks2019using</tag>
        </tags>
        <bibblock>
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song.
</bibblock>
        <bibblock>Using self-supervised learning can improve model robustness and
uncertainty.
</bibblock>
        <bibblock><emph font="italic">Advances in neural information processing systems</emph>, 32, 2019.
</bibblock>
      </bibitem>
      <bibitem key="hendrycks2019augmix" xml:id="bib.bib22">
        <tags>
          <tag role="number">22</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Hendrycks et al.</tag>
          <tag role="fullauthors">Hendrycks, Mu, Cubuk, Zoph, Gilmer, and
Lakshminarayanan</tag>
          <tag role="refnum">Hendrycks et al. (2020)</tag>
          <tag role="key">hendrycks2019augmix</tag>
        </tags>
        <bibblock>
Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and
Balaji Lakshminarayanan.
</bibblock>
        <bibblock>Augmix: A simple data processing method to improve robustness and
uncertainty.
</bibblock>
        <bibblock>In <emph font="italic">International Conference on Learning Representations</emph>, 2020.
</bibblock>
      </bibitem>
      <bibitem key="huang2020neural" xml:id="bib.bib23">
        <tags>
          <tag role="number">23</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Huang et al.</tag>
          <tag role="fullauthors">Huang, Gornet, Dai, Yu, Nguyen, Tsao, and
Anandkumar</tag>
          <tag role="refnum">Huang et al. (2020)</tag>
          <tag role="key">huang2020neural</tag>
        </tags>
        <bibblock>
Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Tsao, and
Anima Anandkumar.
</bibblock>
        <bibblock>Neural networks with recurrent generative feedback.
</bibblock>
        <bibblock><emph font="italic">Advances in Neural Information Processing Systems</emph><!--  %**** iclr2023˙conference.bbl Line 175 **** -->,
33:535–545, 2020.
</bibblock>
      </bibitem>
      <bibitem key="jayaraman2015learning" xml:id="bib.bib24">
        <tags>
          <tag role="number">24</tag>
          <tag role="year">2015</tag>
          <tag role="authors">Jayaraman &amp; Grauman</tag>
          <tag role="fullauthors">Jayaraman and
Grauman</tag>
          <tag role="refnum">Jayaraman &amp; Grauman (2015)</tag>
          <tag role="key">jayaraman2015learning</tag>
        </tags>
        <bibblock>
Dinesh Jayaraman and Kristen Grauman.
</bibblock>
        <bibblock>Learning image representations tied to ego-motion.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE International Conference on Computer
Vision</emph>, pp.  1413–1421, 2015.
</bibblock>
      </bibitem>
      <bibitem key="kamann2020benchmarking" xml:id="bib.bib25">
        <tags>
          <tag role="number">25</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Kamann &amp; Rother</tag>
          <tag role="fullauthors">Kamann and Rother</tag>
          <tag role="refnum">Kamann &amp; Rother (2020)</tag>
          <tag role="key">kamann2020benchmarking</tag>
        </tags>
        <bibblock>
Christoph Kamann and Carsten Rother.
</bibblock>
        <bibblock>Benchmarking the robustness of semantic segmentation models.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  8828–8838, 2020.
</bibblock>
      </bibitem>
      <bibitem key="kar20223d" xml:id="bib.bib26">
        <tags>
          <tag role="number">26</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Kar et al.</tag>
          <tag role="fullauthors">Kar, Yeo, Atanov, and Zamir</tag>
          <tag role="refnum">Kar et al. (2022)</tag>
          <tag role="key">kar20223d</tag>
        </tags>
        <bibblock>
Oğuzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir.
</bibblock>
        <bibblock>3d common corruptions and data augmentation.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  18963–18974, 2022.
</bibblock>
      </bibitem>
      <bibitem key="karpathy2014large" xml:id="bib.bib27">
        <tags>
          <tag role="number">27</tag>
          <tag role="year">2014</tag>
          <tag role="authors">Karpathy et al.</tag>
          <tag role="fullauthors">Karpathy, Toderici, Shetty, Leung, Sukthankar,
and Fei-Fei</tag>
          <tag role="refnum">Karpathy et al. (2014)</tag>
          <tag role="key">karpathy2014large</tag>
        </tags>
        <bibblock>
Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul
Sukthankar, and Li Fei-Fei.
</bibblock>
        <bibblock>Large-scale video classification with convolutional neural networks.
</bibblock>
        <bibblock>In <emph font="italic"><!--  %**** iclr2023˙conference.bbl Line 200 **** -->Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</emph>, pp.  1725–1732, 2014.
</bibblock>
      </bibitem>
      <bibitem key="kondratyuk2021movinets" xml:id="bib.bib28">
        <tags>
          <tag role="number">28</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Kondratyuk et al.</tag>
          <tag role="fullauthors">Kondratyuk, Yuan, Li, Zhang, Tan, Brown, and
Gong</tag>
          <tag role="refnum">Kondratyuk et al. (2021)</tag>
          <tag role="key">kondratyuk2021movinets</tag>
        </tags>
        <bibblock>
Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew
Brown, and Boqing Gong.
</bibblock>
        <bibblock>Movinets: Mobile video networks for efficient video recognition.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  16020–16030, 2021.
</bibblock>
      </bibitem>
      <bibitem key="li2020domain" xml:id="bib.bib29">
        <tags>
          <tag role="number">29</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Li et al.</tag>
          <tag role="fullauthors">Li, Wang, Wan, Wang, Li, and Kot</tag>
          <tag role="refnum">Li et al. (2020)</tag>
          <tag role="key">li2020domain</tag>
        </tags>
        <bibblock>
Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, and Alex Kot.
</bibblock>
        <bibblock>Domain generalization for medical imaging classification with
linear-dependency regularization.
</bibblock>
        <bibblock><emph font="italic">Advances in Neural Information Processing Systems</emph>,
33:3118–3129, 2020.
</bibblock>
      </bibitem>
      <bibitem key="li2008unsupervised" xml:id="bib.bib30">
        <tags>
          <tag role="number">30</tag>
          <tag role="year">2008</tag>
          <tag role="authors">Li &amp; DiCarlo</tag>
          <tag role="fullauthors">Li and DiCarlo</tag>
          <tag role="refnum">Li &amp; DiCarlo (2008)</tag>
          <tag role="key">li2008unsupervised</tag>
        </tags>
        <bibblock>
Nuo Li and James J DiCarlo.
</bibblock>
        <bibblock>Unsupervised natural experience rapidly alters invariant object
representation in visual cortex.
</bibblock>
        <bibblock><emph font="italic">science</emph>, 321(5895):1502–1507, 2008.
</bibblock>
      </bibitem>
      <bibitem key="li2022mvitv2" xml:id="bib.bib31">
        <tags>
          <tag role="number">31</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Li et al.</tag>
          <tag role="fullauthors">Li, Wu, Fan, Mangalam, Xiong, Malik, and
Feichtenhofer</tag>
          <tag role="refnum">Li et al. (2022)</tag>
          <tag role="key">li2022mvitv2</tag>
        </tags>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 225 **** -->
Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra
Malik, and Christoph Feichtenhofer.
</bibblock>
        <bibblock>Mvitv2: Improved multiscale vision transformers for classification
and detection.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  4804–4814, 2022.
</bibblock>
      </bibitem>
      <bibitem key="pmlr-v119-liang20a" xml:id="bib.bib32">
        <tags>
          <tag role="number">32</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Liang et al.</tag>
          <tag role="fullauthors">Liang, Hu, and Feng</tag>
          <tag role="refnum">Liang et al. (2020)</tag>
          <tag role="key">pmlr-v119-liang20a</tag>
        </tags>
        <bibblock>
Jian Liang, Dapeng Hu, and Jiashi Feng.
</bibblock>
        <bibblock>Do we really need to access the source data? Source hypothesis
transfer for unsupervised domain adaptation.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the 37th International Conference on Machine
Learning</emph>, pp.  6028–6039, 2020.
</bibblock>
      </bibitem>
      <bibitem key="lin2019tsm" xml:id="bib.bib33">
        <tags>
          <tag role="number">33</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Lin et al.</tag>
          <tag role="fullauthors">Lin, Gan, and Han</tag>
          <tag role="refnum">Lin et al. (2019)</tag>
          <tag role="key">lin2019tsm</tag>
        </tags>
        <bibblock>
Ji Lin, Chuang Gan, and Song Han.
</bibblock>
        <bibblock>Tsm: Temporal shift module for efficient video understanding.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</emph>, pp.  7083–7093, 2019.
</bibblock>
      </bibitem>
      <bibitem key="NEURIPS2021_b618c321" xml:id="bib.bib34">
        <tags>
          <tag role="number">34</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Liu et al.</tag>
          <tag role="fullauthors">Liu, Kothari, van Delft, Bellot-Gurlet, Mordan, and
Alahi</tag>
          <tag role="refnum">Liu et al. (2021)</tag>
          <tag role="key">NEURIPS2021_b618c321</tag>
        </tags>
        <bibblock>
Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor
Mordan, and Alexandre Alahi.
</bibblock>
        <bibblock>Ttt++: When does self-supervised test-time training fail or thrive?
</bibblock>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 250 **** -->In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan (eds.), <emph font="italic">Advances in Neural Information Processing Systems</emph>,
volume 34, pp.  21808–21820. Curran Associates, Inc., 2021.
</bibblock>
        <bibblock>URL
<ref class="ltx_url" font="typewriter" href="https://proceedings.neurips.cc/paper/2021/file/b618c3210e934362ac261db280128c22-Paper.pdf">https://proceedings.neurips.cc/paper/2021/file/b618c3210e934362ac261db280128c22-Paper.pdf</ref>.
</bibblock>
      </bibitem>
      <bibitem key="michaelis2019benchmarking" xml:id="bib.bib35">
        <tags>
          <tag role="number">35</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Michaelis et al.</tag>
          <tag role="fullauthors">Michaelis, Mitzkus, Geirhos, Rusak, Bringmann,
Ecker, Bethge, and Brendel</tag>
          <tag role="refnum">Michaelis et al. (2019)</tag>
          <tag role="key">michaelis2019benchmarking</tag>
        </tags>
        <bibblock>
Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver
Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel.
</bibblock>
        <bibblock>Benchmarking robustness in object detection: Autonomous driving when
winter is coming.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1907.07484</emph>, 2019.
</bibblock>
      </bibitem>
      <bibitem key="niu2022efficient" xml:id="bib.bib36">
        <tags>
          <tag role="number">36</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Niu et al.</tag>
          <tag role="fullauthors">Niu, Wu, Zhang, Chen, Zheng, Zhao, and
Tan</tag>
          <tag role="refnum">Niu et al. (2022)</tag>
          <tag role="key">niu2022efficient</tag>
        </tags>
        <bibblock>
Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin
Zhao, and Mingkui Tan.
</bibblock>
        <bibblock>Efficient test-time model adaptation without forgetting.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2204.02610</emph>, 2022.
</bibblock>
      </bibitem>
      <bibitem key="rusak2020increasing" xml:id="bib.bib37">
        <tags>
          <tag role="number">37</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Rusak et al.</tag>
          <tag role="fullauthors">Rusak, Schott, Zimmermann, Bitterwolf, Bringmann,
Bethge, and Brendel</tag>
          <tag role="refnum">Rusak et al. (2020)</tag>
          <tag role="key">rusak2020increasing</tag>
        </tags>
        <bibblock>
Evgenia Rusak, Lukas Schott, Roland Zimmermann, Julian Bitterwolf, Oliver
Bringmann, Matthias Bethge, and Wieland Brendel.
</bibblock>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 275 **** -->Increasing the robustness of dnns against image corruptions by
playing the game of noise.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:2001.06057</emph>, 2020.
</bibblock>
      </bibitem>
      <bibitem key="schneider2020improving" xml:id="bib.bib38">
        <tags>
          <tag role="number">38</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Schneider et al.</tag>
          <tag role="fullauthors">Schneider, Rusak, Eck, Bringmann, Brendel, and
Bethge</tag>
          <tag role="refnum">Schneider et al. (2020)</tag>
          <tag role="key">schneider2020improving</tag>
        </tags>
        <bibblock>
Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel,
and Matthias Bethge.
</bibblock>
        <bibblock>Improving robustness against common corruptions by covariate shift
adaptation.
</bibblock>
        <bibblock><emph font="italic">Advances in Neural Information Processing Systems</emph>,
33:11539–11551, 2020.
</bibblock>
      </bibitem>
      <bibitem key="shocher2018zero" xml:id="bib.bib39">
        <tags>
          <tag role="number">39</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Shocher et al.</tag>
          <tag role="fullauthors">Shocher, Cohen, and Irani</tag>
          <tag role="refnum">Shocher et al. (2018)</tag>
          <tag role="key">shocher2018zero</tag>
        </tags>
        <bibblock>
Assaf Shocher, Nadav Cohen, and Michal Irani.
</bibblock>
        <bibblock>“zero-shot” super-resolution using deep internal learning.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</emph>, pp.  3118–3126, 2018.
</bibblock>
      </bibitem>
      <bibitem key="shu2022" xml:id="bib.bib40">
        <tags>
          <tag role="number">40</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Shu et al.</tag>
          <tag role="fullauthors">Shu, Nie, Huang, Yu, Goldstein, Anandkumar, and
Xiao</tag>
          <tag role="refnum">Shu et al. (2022)</tag>
          <tag role="key">shu2022</tag>
        </tags>
        <bibblock>
Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar,
and Chaowei Xiao.
</bibblock>
        <bibblock>Test-time prompt tuning for zero-shot generalization in
vision-language models.
</bibblock>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 300 **** --><emph font="italic">arXiv preprint arXiv:2209.07511</emph>, 2022.
</bibblock>
      </bibitem>
      <bibitem key="simonyan2014two" xml:id="bib.bib41">
        <tags>
          <tag role="number">41</tag>
          <tag role="year">2014</tag>
          <tag role="authors">Simonyan &amp; Zisserman</tag>
          <tag role="fullauthors">Simonyan and Zisserman</tag>
          <tag role="refnum">Simonyan &amp; Zisserman (2014)</tag>
          <tag role="key">simonyan2014two</tag>
        </tags>
        <bibblock>
Karen Simonyan and Andrew Zisserman.
</bibblock>
        <bibblock>Two-stream convolutional networks for action recognition in videos.
</bibblock>
        <bibblock><emph font="italic">Advances in neural information processing systems</emph>, 27, 2014.
</bibblock>
      </bibitem>
      <bibitem key="sun2020test" xml:id="bib.bib42">
        <tags>
          <tag role="number">42</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Sun et al.</tag>
          <tag role="fullauthors">Sun, Wang, Liu, Miller, Efros, and Hardt</tag>
          <tag role="refnum">Sun et al. (2020)</tag>
          <tag role="key">sun2020test</tag>
        </tags>
        <bibblock>
Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt.
</bibblock>
        <bibblock>Test-time training with self-supervision for generalization under
distribution shifts.
</bibblock>
        <bibblock>In <emph font="italic">International conference on machine learning</emph>, pp. 9229–9248. PMLR, 2020.
</bibblock>
      </bibitem>
      <bibitem key="tsipras2018robustness" xml:id="bib.bib43">
        <tags>
          <tag role="number">43</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Tsipras et al.</tag>
          <tag role="fullauthors">Tsipras, Santurkar, Engstrom, Turner, and
Madry</tag>
          <tag role="refnum">Tsipras et al. (2018)</tag>
          <tag role="key">tsipras2018robustness</tag>
        </tags>
        <bibblock>
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
Aleksander Madry.
</bibblock>
        <bibblock>Robustness may be at odds with accuracy.
</bibblock>
        <bibblock><emph font="italic">arXiv preprint arXiv:1805.12152</emph>, 2018.
</bibblock>
      </bibitem>
      <bibitem key="wang2020tent" xml:id="bib.bib44">
        <tags>
          <tag role="number">44</tag>
          <tag role="year">2021a</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Shelhamer, Liu, Olshausen, and
Darrell</tag>
          <tag role="refnum">Wang et al. (2021a)</tag>
          <tag role="key">wang2020tent</tag>
        </tags>
        <bibblock>
Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell.
</bibblock>
        <bibblock>Tent: Fully test-time adaptation by entropy minimization.
</bibblock>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 325 **** -->In <emph font="italic">International Conference on Learning Representations</emph>,
2021a.
</bibblock>
      </bibitem>
      <bibitem key="wang2021augmax" xml:id="bib.bib45">
        <tags>
          <tag role="number">45</tag>
          <tag role="year">2021b</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Xiao, Kossaifi, Yu, Anandkumar,
and Wang</tag>
          <tag role="refnum">Wang et al. (2021b)</tag>
          <tag role="key">wang2021augmax</tag>
        </tags>
        <bibblock>
Haotao Wang, Chaowei Xiao, Jean Kossaifi, Zhiding Yu, Anima Anandkumar, and
Zhangyang Wang.
</bibblock>
        <bibblock>Augmax: Adversarial composition of random augmentations for robust
training.
</bibblock>
        <bibblock><emph font="italic">Advances in neural information processing systems</emph>,
34:237–250, 2021b.
</bibblock>
      </bibitem>
      <bibitem key="wang2016temporal" xml:id="bib.bib46">
        <tags>
          <tag role="number">46</tag>
          <tag role="year">2016</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Xiong, Wang, Qiao, Lin, Tang, and
Gool</tag>
          <tag role="refnum">Wang et al. (2016)</tag>
          <tag role="key">wang2016temporal</tag>
        </tags>
        <bibblock>
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and
Luc Van Gool.
</bibblock>
        <bibblock>Temporal segment networks: Towards good practices for deep action
recognition.
</bibblock>
        <bibblock>In <emph font="italic">European conference on computer vision</emph>, pp.  20–36.
Springer, 2016.
</bibblock>
      </bibitem>
      <bibitem key="wang2021tdn" xml:id="bib.bib47">
        <tags>
          <tag role="number">47</tag>
          <tag role="year">2021c</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Tong, Ji, and Wu</tag>
          <tag role="refnum">Wang et al. (2021c)</tag>
          <tag role="key">wang2021tdn</tag>
        </tags>
        <bibblock>
Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu.
</bibblock>
        <bibblock>Tdn: Temporal difference networks for efficient action recognition.
</bibblock>
        <bibblock>In <emph font="italic"><!--  %**** iclr2023˙conference.bbl Line 350 **** -->Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  1895–1904, 2021c.
</bibblock>
      </bibitem>
      <bibitem key="wang2022continual" xml:id="bib.bib48">
        <tags>
          <tag role="number">48</tag>
          <tag role="year">2022a</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Fink, Van Gool, and
Dai</tag>
          <tag role="refnum">Wang et al. (2022a)</tag>
          <tag role="key">wang2022continual</tag>
        </tags>
        <bibblock>
Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai.
</bibblock>
        <bibblock>Continual test-time domain adaptation.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  7201–7211, 2022a.
</bibblock>
      </bibitem>
      <bibitem key="wang2018non" xml:id="bib.bib49">
        <tags>
          <tag role="number">49</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Girshick, Gupta, and He</tag>
          <tag role="refnum">Wang et al. (2018)</tag>
          <tag role="key">wang2018non</tag>
        </tags>
        <bibblock>
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
</bibblock>
        <bibblock>Non-local neural networks.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</emph>, pp.  7794–7803, 2018.
</bibblock>
      </bibitem>
      <bibitem key="wang2019learning" xml:id="bib.bib50">
        <tags>
          <tag role="number">50</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Jabri, and Efros</tag>
          <tag role="refnum">Wang et al. (2019)</tag>
          <tag role="key">wang2019learning</tag>
        </tags>
        <bibblock>
Xiaolong Wang, Allan Jabri, and Alexei A Efros.
</bibblock>
        <bibblock>Learning correspondence from the cycle-consistency of time.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  2566–2576, 2019.
</bibblock>
      </bibitem>
      <bibitem key="wang2020heterogeneous" xml:id="bib.bib51">
        <tags>
          <tag role="number">51</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Li, and Kot</tag>
          <tag role="refnum">Wang et al. (2020)</tag>
          <tag role="key">wang2020heterogeneous</tag>
        </tags>
        <bibblock>
Yufei Wang, Haoliang Li, and Alex C Kot.
</bibblock>
        <bibblock>Heterogeneous domain generalization via domain mixup.
</bibblock>
        <bibblock>In <emph font="italic"><!--  %**** iclr2023˙conference.bbl Line 375 **** -->ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</emph>, pp.  3622–3626. IEEE, 2020.
</bibblock>
      </bibitem>
      <bibitem key="wang2022variational" xml:id="bib.bib52">
        <tags>
          <tag role="number">52</tag>
          <tag role="year">2022b</tag>
          <tag role="authors">Wang et al.</tag>
          <tag role="fullauthors">Wang, Li, Cheng, Wen, Chau, and
Kot</tag>
          <tag role="refnum">Wang et al. (2022b)</tag>
          <tag role="key">wang2022variational</tag>
        </tags>
        <bibblock>
Yufei Wang, Haoliang Li, Hao Cheng, Bihan Wen, Lap-Pui Chau, and Alex Kot.
</bibblock>
        <bibblock>Variational disentanglement for domain generalization.
</bibblock>
        <bibblock><emph font="italic">Transactions on Machine Learning Research</emph>, 2022b.
</bibblock>
        <bibblock>ISSN 2835-8856.
</bibblock>
        <bibblock>URL <ref class="ltx_url" font="typewriter" href="https://openreview.net/forum?id=fudOtITMIZ">https://openreview.net/forum?id=fudOtITMIZ</ref>.
</bibblock>
      </bibitem>
      <bibitem key="wiskott2002slow" xml:id="bib.bib53">
        <tags>
          <tag role="number">53</tag>
          <tag role="year">2002</tag>
          <tag role="authors">Wiskott &amp; Sejnowski</tag>
          <tag role="fullauthors">Wiskott and Sejnowski</tag>
          <tag role="refnum">Wiskott &amp; Sejnowski (2002)</tag>
          <tag role="key">wiskott2002slow</tag>
        </tags>
        <bibblock>
Laurenz Wiskott and Terrence J Sejnowski.
</bibblock>
        <bibblock>Slow feature analysis: Unsupervised learning of invariances.
</bibblock>
        <bibblock><emph font="italic">Neural computation</emph>, 14(4):715–770, 2002.
</bibblock>
      </bibitem>
      <bibitem key="wood2016smoothness" xml:id="bib.bib54">
        <tags>
          <tag role="number">54</tag>
          <tag role="year">2016</tag>
          <tag role="authors">Wood</tag>
          <tag role="refnum">Wood (2016)</tag>
          <tag role="key">wood2016smoothness</tag>
        </tags>
        <bibblock>
Justin N Wood.
</bibblock>
        <bibblock>A smoothness constraint on the development of object recognition.
</bibblock>
        <bibblock><emph font="italic">Cognition</emph>, 153:140–145, 2016.
</bibblock>
      </bibitem>
      <bibitem key="wood2016development" xml:id="bib.bib55">
        <tags>
          <tag role="number">55</tag>
          <tag role="year">2016</tag>
          <tag role="authors">Wood &amp; Wood</tag>
          <tag role="fullauthors">Wood and Wood</tag>
          <tag role="refnum">Wood &amp; Wood (2016)</tag>
          <tag role="key">wood2016development</tag>
        </tags>
        <bibblock>
Justin N Wood and Samantha MW Wood.
</bibblock>
        <bibblock>The development of newborn object recognition in fast and slow visual
worlds.
</bibblock>
        <bibblock><emph font="italic">Proceedings of the Royal Society B: Biological Sciences</emph><!--  %**** iclr2023˙conference.bbl Line 400 **** -->,
283(1829):20160166, 2016.
</bibblock>
      </bibitem>
      <bibitem key="Wu_2020_CVPR" xml:id="bib.bib56">
        <tags>
          <tag role="number">56</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Wu &amp; Kwiatkowska</tag>
          <tag role="fullauthors">Wu and Kwiatkowska</tag>
          <tag role="refnum">Wu &amp; Kwiatkowska (2020)</tag>
          <tag role="key">Wu_2020_CVPR</tag>
        </tags>
        <bibblock>
Min Wu and Marta Kwiatkowska.
</bibblock>
        <bibblock>Robustness guarantees for deep neural networks on videos.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</emph>, June 2020.
</bibblock>
      </bibitem>
      <bibitem key="xie2019feature" xml:id="bib.bib57">
        <tags>
          <tag role="number">57</tag>
          <tag role="year">2019</tag>
          <tag role="authors">Xie et al.</tag>
          <tag role="fullauthors">Xie, Wu, Maaten, Yuille, and He</tag>
          <tag role="refnum">Xie et al. (2019)</tag>
          <tag role="key">xie2019feature</tag>
        </tags>
        <bibblock>
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, and Kaiming He.
</bibblock>
        <bibblock>Feature denoising for improving adversarial robustness.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</emph>, pp.  501–509, 2019.
</bibblock>
      </bibitem>
      <bibitem key="xie2018rethinking" xml:id="bib.bib58">
        <tags>
          <tag role="number">58</tag>
          <tag role="year">2018</tag>
          <tag role="authors">Xie et al.</tag>
          <tag role="fullauthors">Xie, Sun, Huang, Tu, and Murphy</tag>
          <tag role="refnum">Xie et al. (2018)</tag>
          <tag role="key">xie2018rethinking</tag>
        </tags>
        <bibblock>
Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy.
</bibblock>
        <bibblock>Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs
in video classification.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the European conference on computer vision
(ECCV)</emph>, pp.  305–321, 2018.
</bibblock>
      </bibitem>
      <bibitem key="ECCV2020ysy" xml:id="bib.bib59">
        <tags>
          <tag role="number">59</tag>
          <tag role="year">2020</tag>
          <tag role="authors">Yang et al.</tag>
          <tag role="fullauthors">Yang, Liu, Lu, Er, and Kot</tag>
          <tag role="refnum">Yang et al. (2020)</tag>
          <tag role="key">ECCV2020ysy</tag>
        </tags>
        <bibblock>
Siyuan Yang, Jun Liu, Shijian Lu, Meng Hwa Er, and Alex C. Kot.
</bibblock>
        <bibblock>Collaborative learning of gesture recognition and 3d hand pose
estimation with multi-order feature analysis.
</bibblock>
        <bibblock><!--  %**** iclr2023˙conference.bbl Line 425 **** -->In <emph font="italic">Proceedings of the European conference on computer vision
(ECCV)</emph>, pp.  769–786, 2020.
</bibblock>
      </bibitem>
      <bibitem key="yi2021benchmarking" xml:id="bib.bib60">
        <tags>
          <tag role="number">60</tag>
          <tag role="year">2021</tag>
          <tag role="authors">Yi et al.</tag>
          <tag role="fullauthors">Yi, Yang, Li, Tan, and Kot</tag>
          <tag role="refnum">Yi et al. (2021)</tag>
          <tag role="key">yi2021benchmarking</tag>
        </tags>
        <bibblock>
Chenyu Yi, Siyuan Yang, Haoliang Li, Yap-peng Tan, and Alex Kot.
</bibblock>
        <bibblock>Benchmarking the robustness of spatial-temporal models against
corruptions.
</bibblock>
        <bibblock>In <emph font="italic">Advance in Neural Information Processing Systems Track on
Datasets and Benchmarks</emph>, 2021.
</bibblock>
      </bibitem>
      <bibitem key="yu2022towards" xml:id="bib.bib61">
        <tags>
          <tag role="number">61</tag>
          <tag role="year">2022</tag>
          <tag role="authors">Yu et al.</tag>
          <tag role="fullauthors">Yu, Yang, Tan, and Kot</tag>
          <tag role="refnum">Yu et al. (2022)</tag>
          <tag role="key">yu2022towards</tag>
        </tags>
        <bibblock>
Yi Yu, Wenhan Yang, Yap-Peng Tan, and Alex C Kot.
</bibblock>
        <bibblock>Towards robust rain removal against adversarial attacks: A
comprehensive benchmark analysis and beyond.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</emph>, pp.  6013–6022, 2022.
</bibblock>
      </bibitem>
      <bibitem key="zheng2016improving" xml:id="bib.bib62">
        <tags>
          <tag role="number">62</tag>
          <tag role="year">2016</tag>
          <tag role="authors">Zheng et al.</tag>
          <tag role="fullauthors">Zheng, Song, Leung, and
Goodfellow</tag>
          <tag role="refnum">Zheng et al. (2016)</tag>
          <tag role="key">zheng2016improving</tag>
        </tags>
        <bibblock>
Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow.
</bibblock>
        <bibblock>Improving the robustness of deep neural networks via stability
training.
</bibblock>
        <bibblock>In <emph font="italic">Proceedings of the ieee conference on computer vision and
pattern recognition</emph>, pp.  4480–4488, 2016.
</bibblock>
      </bibitem>
    </biblist>
  </bibliography>
<!--  %**** iclr2023˙conference.bbl Line 450 **** -->  <pagination role="newpage"/>
  <appendix inlist="toc" xml:id="A1">
    <tags>
      <tag>Appendix A</tag>
      <tag role="autoref">Appendix A</tag>
      <tag role="refnum">A</tag>
      <tag role="typerefnum">Appendix A</tag>
    </tags>
    <title><tag close=" ">Appendix A</tag>Appendix</title>
    <toctitle><tag close=" ">A</tag>Appendix</toctitle>
    <subsection inlist="toc" xml:id="A1.SS1">
      <tags>
        <tag>A.1</tag>
        <tag role="autoref">subsection A.1</tag>
        <tag role="refnum">A.1</tag>
        <tag role="typerefnum">§A.1</tag>
      </tags>
      <title><tag close=" ">A.1</tag>Source-Free Domain Adaptation</title>
      <para xml:id="A1.SS1.p1">
        <p>We benchmark the video domain adaptation for shift from UCF to HMDB and vice versa. In our setting, we have only access to test data and pre-trained model. In the domain adaptation problem, 12 common classes of 3209 video data in UCF and HMDB are used, following the official split provided by <cite class="ltx_citemacro_citep">(<bibref bibrefs="chen2019temporal" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. For the pre-trained models, we use Kinetics-pretrained 3D ResNet50. Then we fine tune the model with training data in UCF and HMDB.</p>
      </para>
      <para xml:id="A1.SS1.p2">
        <p>Table <ref labelref="LABEL:domain-table"/> reports the target accuracy of standard and test-time optimization methods. We denote Tent as Tent* because we remain the training statistics in normalization layer, otherwise the performance will drop significantly. All the methods can help model adapt to video data with domain shift. TTT* obtains competitive improvement of 2.7% on UCF-HMDB. Tent* also increases accuracy on HMDB-UCF by 2.4%. We find that TeCo consistently outperforms other baseline methods. When we train model with UCF and optimize on HMDB at test time, it improves the accuracy by 3.3%. Similarly, on HMDB-UCF, the accuracy is enhanced by 2.8%.</p>
      </para>
      <table inlist="lot" labels="LABEL:domain-table" placement="h" xml:id="A1.T4">
        <tags>
          <tag><text fontsize="90%">Table 4</text></tag>
          <tag role="autoref">Table 4</tag>
          <tag role="refnum">4</tag>
          <tag role="typerefnum">Table 4</tag>
        </tags>
        <toccaption><tag close=" ">4</tag>Adapting models to target domain without access to source data. We use Kinetics for model pre-training, and fine tune the model with UCF and HMDB training data respectively. UCF-HMDB means training on UCF and testing on HMDB data.</toccaption>
        <caption><tag close=": "><text fontsize="90%">Table 4</text></tag><text fontsize="90%">Adapting models to target domain without access to source data. We use Kinetics for model pre-training, and fine tune the model with UCF and HMDB training data respectively. UCF-HMDB means training on UCF and testing on HMDB data.</text></caption>
        <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
          <thead>
            <tr>
              <td align="center" border="r tt" thead="column row">model, 3D R50</td>
              <td align="center" border="r tt" thead="column row">Standard</td>
              <td align="center" border="tt" thead="column">BN</td>
              <td align="center" border="tt" thead="column">Tent*</td>
              <td align="center" border="tt" thead="column">SHOT</td>
              <td align="center" border="tt" thead="column">TTT*</td>
              <td align="center" border="tt" thead="column">TeCo</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" border="r t" thead="row">UCF-HMDB</td>
              <td align="center" border="r t" thead="row">74.2</td>
              <td align="center" border="t">75.6</td>
              <td align="center" border="t">76.4</td>
              <td align="center" border="t">76.7</td>
              <td align="center" border="t">76.9</td>
              <td align="center" border="t"><text font="bold">77.5</text></td>
            </tr>
            <tr>
              <td align="center" border="bb r" thead="row">HMDB-UCF</td>
              <td align="center" border="bb r" thead="row">80.7</td>
              <td align="center" border="bb">82.6</td>
              <td align="center" border="bb">83.1</td>
              <td align="center" border="bb">82.7</td>
              <td align="center" border="bb">81.9</td>
              <td align="center" border="bb"><text font="bold">83.5</text></td>
            </tr>
          </tbody>
        </tabular>
      </table>
    </subsection>
    <subsection inlist="toc" xml:id="A1.SS2">
      <tags>
        <tag>A.2</tag>
        <tag role="autoref">subsection A.2</tag>
        <tag role="refnum">A.2</tag>
        <tag role="typerefnum">§A.2</tag>
      </tags>
      <title><tag close=" ">A.2</tag>Full results on Mini SSV2-C</title>
      <figure inlist="lof" labels="LABEL:fig:full-ssv2-c" placement="h" xml:id="A1.F6">
        <tags>
          <tag><text fontsize="90%">Figure 6</text></tag>
          <tag role="autoref">Figure 6</tag>
          <tag role="refnum">6</tag>
          <tag role="typerefnum">Figure 6</tag>
        </tags>
        <graphics candidates="images/ssv2-i3d-resnet18-full-plot-v2.png" class="ltx_centering" graphic="./images/ssv2-i3d-resnet18-full-plot-v2.png" options="width=398.9296pt,keepaspectratio=true" xml:id="A1.F6.g1"/>
<!--  %˝ 
     %**** iclr2023˙conference.tex Line 575 **** -->        <toccaption><tag close=" ">6</tag>Full results on Mini SSV2-C, with a backbone of 3D ResNet18. Tent becomes vulnerable to motion blur and bit error, while TeCo lies above other methods on most types of corruptions. </toccaption>
        <caption><tag close=": "><text fontsize="90%">Figure 6</text></tag><text fontsize="90%">Full results on Mini SSV2-C, with a backbone of 3D ResNet18. Tent becomes vulnerable to motion blur and bit error, while TeCo lies above other methods on most types of corruptions. </text></caption>
      </figure>
      <para xml:id="A1.SS2.p1">
        <p>Figure <ref labelref="LABEL:fig:full-ssv2-c"/> shows the complete results on Mini SSV2-C.
Apart from the corruptions which cause severe accuracy degradation in Mini Kinetics-C, the corruptions like motion blur, bit error, and frame rate conversion also lead to significant drops in accuracy in Mini SSV2-C.
Intuitively, these three types of corruption have more impact on temporal information.
For example, bit error propagates the perturbation in the consecutive frames, and frame rate conversion changes the speed of motions in the video clips.
The performance of Tent is devastating when facing motion blur and bit error.
In contrast, TeCo, which uses a balanced test-time optimization framework, guarantees performance even though the test-time data has a large shift from the clean data.
As a result, TeCo achieves consistent robustness improvements on different corruptions across datasets.</p>
      </para>
    </subsection>
    <subsection inlist="toc" xml:id="A1.SS3">
      <tags>
        <tag>A.3</tag>
        <tag role="autoref">subsection A.3</tag>
        <tag role="refnum">A.3</tag>
        <tag role="typerefnum">§A.3</tag>
      </tags>
      <title><tag close=" ">A.3</tag>Baseline Implementation and Hyper-parameters</title>
      <para xml:id="A1.SS3.p1">
        <p>Because all the baseline methods are model-agnostic, we follow the implementation of the original papers and code. BN simply controls the weights of training and test statistics in the normalization layers. We denote the hyper-parameter as <Math mode="inline" tex="\alpha" text="alpha" xml:id="A1.SS3.p1.m1">
            <XMath>
              <XMTok font="italic" name="alpha" role="UNKNOWN">α</XMTok>
            </XMath>
          </Math> in Section 3.1 as well. Tent and TTT* have no specific hyper-parameter, but they are sensitive to the learning rate. SHOT contains two objectives. One is pseudo-label classification loss and the other is entropy loss. We use <Math mode="inline" tex="\lambda" text="lambda" xml:id="A1.SS3.p1.m2">
            <XMath>
              <XMTok font="italic" name="lambda" role="UNKNOWN">λ</XMTok>
            </XMath>
          </Math> to indicate the weight of the pseudo-label classification loss. For a fair comparison, we use a batch size of 32 in all the implementations.</p>
      </para>
      <table inlist="lot" labels="LABEL:Tab:_corruption_implement" placement="h" xml:id="A1.T5">
        <tags>
          <tag><text fontsize="90%">Table 5</text></tag>
          <tag role="autoref">Table 5</tag>
          <tag role="refnum">5</tag>
          <tag role="typerefnum">Table 5</tag>
        </tags>
<!--  %[htb] -->        <toccaption class="ltx_centering"><tag close=" "><text fontsize="90%">5</text></tag><text fontsize="90%">Implementation detail for baseline methods</text></toccaption>
        <caption class="ltx_centering" fontsize="90%"><tag close=": ">Table 5</tag>Implementation detail for baseline methods</caption>
<!--  %“footnotesize -->        <tabular class="ltx_centering ltx_guessed_headers" colsep="3.0pt" vattach="middle">
          <thead>
            <tr>
              <td align="left" border="l r t" thead="column row"><!--  %horizontal line 
     %**** iclr2023˙conference.tex Line 600 **** 
     %“multirow–2˝–*˝–Dataset˝&amp;“multirow–2˝–*˝–Corruption˝&amp; “multirow–2˝–*˝–Parameter˝ &amp; “multicolumn–5˝–c—˝–Value˝““ --><text fontsize="90%">Dataset</text></td>
              <td align="center" border="r t" thead="column"><text fontsize="90%">Baseline</text></td>
              <td align="center" border="r t" thead="column"><text fontsize="90%">Hyper-Parameter</text></td>
              <td align="center" border="r t" thead="column"><text fontsize="90%">Learning Rate</text></td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" border="l r t" rowspan="4" thead="row"><text fontsize="90%">Mini Kinetics-C</text></td>
              <td align="center" border="r t"><text fontsize="90%">BN</text></td>
              <td align="center" border="r t"><text fontsize="90%">weight of training statistics </text><Math mode="inline" tex="\alpha" text="alpha" xml:id="A1.T5.m1">
                  <XMath>
                    <XMTok font="italic" fontsize="90%" name="alpha" role="UNKNOWN">α</XMTok>
                  </XMath>
                </Math><text fontsize="90%">=0.5</text></td>
              <td align="center" border="r t"><text fontsize="90%">-</text></td>
            </tr>
            <tr>
              <td align="center" border="r"><text fontsize="90%">Tent</text></td>
              <td align="center" border="r"><text fontsize="90%">-</text></td>
              <td align="center" border="r"><text fontsize="90%">1e-3</text></td>
            </tr>
            <tr>
              <td align="center" border="r"><text fontsize="90%">SHOT</text></td>
              <td align="center" border="r"><text fontsize="90%">weight of pseudo-label classification loss </text><Math mode="inline" tex="\lambda" text="lambda" xml:id="A1.T5.m2">
                  <XMath>
                    <XMTok font="italic" fontsize="90%" name="lambda" role="UNKNOWN">λ</XMTok>
                  </XMath>
                </Math><text fontsize="90%">=0.3</text></td>
              <td align="center" border="r"><text fontsize="90%">1e-3</text></td>
            </tr>
            <tr>
              <td align="center" border="r"><text fontsize="90%">TTT*</text></td>
              <td align="center" border="r"><text fontsize="90%">-</text></td>
              <td align="center" border="r"><text fontsize="90%">1e-4</text></td>
            </tr>
            <tr>
              <td align="left" border="b l r t" rowspan="4" thead="row"><text fontsize="90%">Mini SSV2-C</text></td>
              <td align="center" border="r t"><text fontsize="90%">BN</text></td>
              <td align="center" border="r t"><text fontsize="90%">weight of training statistics </text><Math mode="inline" tex="\alpha" text="alpha" xml:id="A1.T5.m3">
                  <XMath>
                    <XMTok font="italic" fontsize="90%" name="alpha" role="UNKNOWN">α</XMTok>
                  </XMath>
                </Math><text fontsize="90%">=0.6</text></td>
              <td align="center" border="r t"><text fontsize="90%">-</text></td>
            </tr>
            <tr>
              <td align="center" border="r"><text fontsize="90%">Tent</text></td>
              <td align="center" border="r"><text fontsize="90%">-</text></td>
              <td align="center" border="r"><text fontsize="90%">1e-5</text></td>
            </tr>
            <tr>
              <td align="center" border="r"><text fontsize="90%">SHOT</text></td>
              <td align="center" border="r"><text fontsize="90%">weight of pseudo-label classification loss </text><Math mode="inline" tex="\lambda" text="lambda" xml:id="A1.T5.m4">
                  <XMath>
                    <XMTok font="italic" fontsize="90%" name="lambda" role="UNKNOWN">λ</XMTok>
                  </XMath>
                </Math><text fontsize="90%"> = 0.3</text></td>
              <td align="center" border="r"><text fontsize="90%">1e-5</text></td>
            </tr>
            <tr>
              <td align="center" border="b r"><text fontsize="90%">TTT*</text></td>
              <td align="center" border="b r"><text fontsize="90%">-</text></td>
              <td align="center" border="b r"><text fontsize="90%">1e-5</text></td>
            </tr>
          </tbody>
        </tabular>
      </table>
    </subsection>
    <subsection inlist="toc" xml:id="A1.SS4">
      <tags>
        <tag>A.4</tag>
        <tag role="autoref">subsection A.4</tag>
        <tag role="refnum">A.4</tag>
        <tag role="typerefnum">§A.4</tag>
      </tags>
      <title><tag close=" ">A.4</tag>Ablating l1/l2 Distance and Partial/Full Parameter Update</title>
      <para xml:id="A1.SS4.p1">
        <p>In the default setting of TeCo, we use <Math mode="inline" tex="l_{1}" text="l _ 1" xml:id="A1.SS4.p1.m1">
            <XMath>
              <XMApp>
                <XMTok role="SUBSCRIPTOP" scriptpos="post1"/>
                <XMTok font="italic" role="UNKNOWN">l</XMTok>
                <XMTok fontsize="70%" meaning="1" role="NUMBER">1</XMTok>
              </XMApp>
            </XMath>
          </Math> distance to measure the similarity of features along the time dimension. The <Math mode="inline" tex="l1" text="l * 1" xml:id="A1.SS4.p1.m2">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">l</XMTok>
                <XMTok meaning="1" role="NUMBER">1</XMTok>
              </XMApp>
            </XMath>
          </Math> loss will penalize small perturbations and encourage sparsity in the feature maps, which enhances model robustness <cite class="ltx_citemacro_citep">(<bibref bibrefs="guo2018sparse,chen2022sparsity" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>. <Math mode="inline" tex="l2" text="l * 2" xml:id="A1.SS4.p1.m3">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">l</XMTok>
                <XMTok meaning="2" role="NUMBER">2</XMTok>
              </XMApp>
            </XMath>
          </Math> loss is also widely used in de-noising. It usually leads to smoother change. We find that <Math mode="inline" tex="l2" text="l * 2" xml:id="A1.SS4.p1.m4">
            <XMath>
              <XMApp>
                <XMTok meaning="times" role="MULOP">⁢</XMTok>
                <XMTok font="italic" role="UNKNOWN">l</XMTok>
                <XMTok meaning="2" role="NUMBER">2</XMTok>
              </XMApp>
            </XMath>
          </Math> distance also achieves promising performance on corruption robustness. We obtain a mPC of 56.8% when TeCo follows the default settings. It remains an open question to explore the optimal metric for measuring similarity.</p>
      </para>
      <para xml:id="A1.SS4.p2">
        <p>We also conduct an ablation study on Mini Kinetics-C and UCF-HMDB datasets to understand the impact of network parameter updates. TeCo updates part of network parameters as mentioned in Section 3.1. Table <ref labelref="LABEL:param-table"/> shows that fully updating network parameters may hurt the performance. Especially for small datasets like UCF-HMDB, the accuracy drops by 1.1% on UCF-HMDB and by 1.3% HMDB-UCF. Limiting the parameters which can be updated can improve both optimization stability and efficiency <cite class="ltx_citemacro_citep">(<bibref bibrefs="wang2020tent" separator=";" show="AuthorsPhrase1Year" yyseparator=",">
              <bibrefphrase>, </bibrefphrase>
            </bibref>)</cite>.</p>
      </para>
      <table inlist="lot" labels="LABEL:param-table" placement="h" xml:id="A1.T6">
        <tags>
          <tag><text fontsize="90%">Table 6</text></tag>
          <tag role="autoref">Table 6</tag>
          <tag role="refnum">6</tag>
          <tag role="typerefnum">Table 6</tag>
        </tags>
        <toccaption><tag close=" ">6</tag><text font="bold">Partially/Fully update network parameters</text>.
Comparison of experiments with updating all/partial network parameters. All the experiments in this table followed the default hyper-parameters.
</toccaption>
        <caption><tag close=": "><text fontsize="90%">Table 6</text></tag><text font="bold" fontsize="90%">Partially/Fully update network parameters<text font="medium">.
Comparison of experiments with updating all/partial network parameters. All the experiments in this table followed the default hyper-parameters.
</text></text></caption>
        <tabular class="ltx_centering ltx_guessed_headers" vattach="middle">
          <thead>
            <tr>
              <td align="center" border="r tt" thead="column row">Network Parameters</td>
              <td align="center" border="tt" thead="column">Mini Kinetics-C</td>
              <td align="center" border="tt" thead="column">UCF-HMDB</td>
              <td align="center" border="tt" thead="column">HMDB-UCF</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" border="r t" thead="row">Partially</td>
              <td align="center" border="t"><text font="bold">56.9</text></td>
              <td align="center" border="t"><text font="bold">77.5</text></td>
              <td align="center" border="t"><text font="bold">83.5</text></td>
            </tr>
            <tr>
              <td align="center" border="bb r" thead="row">Fully</td>
              <td align="center" border="bb">56.2(-0.7)</td>
              <td align="center" border="bb">76.4(-1.1)</td>
              <td align="center" border="bb">82.2(-1.3)</td>
            </tr>
          </tbody>
        </tabular>
      </table>
    </subsection>
  </appendix>
</document>
