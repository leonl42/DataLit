%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
% \usepackage{algorithmicx}
% \usepackage{algpseudocode}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{RNAFlow: RNA Structure \& Sequence Design via Inverse Folding-Based Flow Matching}

\begin{document}

\twocolumn[
\icmltitle{RNAFlow: RNA Structure \& Sequence Design via \\ Inverse Folding-Based Flow Matching}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Divya Nori}{yyy}
\icmlauthor{Wengong Jin}{xxx}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA}
\icmlaffiliation{xxx}{Broad Institute of MIT and Harvard, Cambridge, MA, USA}

\icmlcorrespondingauthor{Divya Nori}{divnor80@mit.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
The growing significance of RNA engineering in diverse biological applications has spurred interest in developing AI methods for structure-based RNA design. While diffusion models have excelled in protein design, adapting them for RNA presents new challenges due to RNA's conformational flexibility and the computational cost of fine-tuning large structure prediction models. To this end, we propose RNAFlow, a flow matching model for protein-conditioned RNA sequence-structure design. Its denoising network integrates an RNA inverse folding model and a pre-trained RosettaFold2NA network for generation of RNA sequences and structures. The integration of inverse folding in the structure denoising process allows us to simplify training by fixing the structure prediction network. We further enhance the inverse folding model by conditioning it on inferred conformational ensembles to model dynamic RNA conformations. Evaluation on protein-conditioned RNA structure and sequence generation tasks demonstrates RNAFlow's advantage over existing RNA design methods.
\end{abstract}


\input{intro}
\input{related_work}
\input{method}
\input{experiments}



\section{Conclusion}

In this paper, we present RNAFlow, the first protein-conditioned generative model for RNA structure and sequence design. We show that an inverse folding model is an effective score prediction network within the flow matching framework, enabling the design of RNAs that outperform the baselines in terms of structure and sequence accuracy. Additionally, we show that by inverse folding over an inferred conformational ensemble, we can design plausible aptamers for GRK2 binding.

While RNAFlow shows an empirical advantage over existing methods, there is still a large gap to reach the level of accuracy achieved in \textit{de novo} protein design. We observe that currently, RNAFlow tends to achieve lower RMSD and higher recovery when the ground-truth RNA is more structurally stable (i.e. more base pairing). This suggests that RNAFlow is practically useful for aptamer design. On the other hand, when the RNA is very conformationally diverse (i.e. short single-stranded RNAs), RNAFlow often does not outperform a random sequence. In this setting, a pure language approach may be desired as structural supervision is less relevant.

To improve performance on this task, one major bottleneck is accuracy and efficiency of protein-RNA folding and docking models. This would allow us to supervise docked full-atom complexes and better model protein-RNA structural interaction, which is essential for designing RNAs that rely heavily on side chains for their function (i.e. ribozymes).

Code is available at \url{https://github.com/divnori/rnaflow}.

\section*{Impact Statement}

This paper presents work whose goal is to advance machine learning applications for structural biology and drug discovery. There are many potential societal consequences of our work, including application of our model for discovery of an RNA drug candidate. We highlight that molecules designed by RNAFlow have not been tested experimentally which is critical for any biological application. 

\bibliography{main}
\bibliographystyle{icml2024}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Flow Matching on RNA Structures}

We justify the main RNAFlow objective given in Equation \hyperref[loss]{3}, following much of the theory given by \citet{jing2023alphafold}. By the standard formulation of flow matching in Euclidean space $\mathbb{R}^{3L_r}$ ($L_r$ is RNA length), we expect to supervise with the reparameterized denoising objective (MSE).

$$ \mathcal{L} = \mathbb{E}_{R_1 \sim p_1, R_t \sim p_t} [{||\hat R_1 - \vec R_1||}^2] $$

$\hat R_1$ is predicted by neural network $\hat R_1 (\vec R_t; \theta)$ which accepts noised input $\vec R_t$ and a timestep embedding. While $\hat R_1$ is composed of an inverse folding model and pre-trained folding model, the denoised sequence can simply be thought of as an intermediate representation from which we predict the clean structure.

However, $\hat R_1 (\vec R_t; \theta)$ is $SE(3)$-invariant given that RF2NA is trained with $SE(3)$-invariant FAPE loss. We are thus flow matching over the Riemannian manifold given by quotient space $\mathbb{R}^{3L_r} / SE(3)$. To flow match in this new space, we must define a conditional probability path $p_t(\vec R_t | \vec R_1)$. We can generalize linear interpolation in the Euclidean case to interpolant $\vec R_t | \vec R_1 = \psi(R_0 | R_1)$ defined as the geodesic path from $R_0$ to $R_1$, where the geodesic path is Kabsch alignment followed by linear interpolation in $\mathbb{R}^{3L_r}$.

$$ R_0 = \text{Kabsch}(R_0, R_1) $$

$$ \psi(R_0 | R_1) = (1-t) *R_0 + t * R_1 $$

We define a distance metric $d$ on the manifold correspondingly, where $d$ is a valid metric because it is defined for all points on the manifold.

$$ d = ||\hat R_1 - \text{Kabsch}(\vec R_1, \hat R_1)|| $$

We can thus plug this expression into our original flow matching objective,

$$ \mathcal{L} = \mathbb{E}_{\vec R_1 | \vec R_t} [{||\hat R_1 - \text{Kabsch}(\vec R_1, \hat R_1)||}^2] = \text{MSE}(\hat R_1, \text{Kabsch}(\vec R_1, \hat R_1)) $$

which is equivalent to the structure loss expression in Equation \hyperref[loss]{3}. The cross entropy term simply exerts auxiliary sequence supervision.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.6\columnwidth]{rna_sequence_lengths_histogram.png}
    \label{fig:lengths}
    \caption{Distribution of RNA lengths in processed PDBBind dataset.}
    \centering
    \includegraphics[width=0.6\columnwidth]{rnasolo_histogram.png}
    \label{fig:lengths2}
    \caption{Distribution of RNA lengths in processed RNASolo dataset.}
\end{figure*}

\section{Dataset Details}

\subsection{PDBBind}

We filter PDBBind to complexes where at least one protein $C_\alpha$ atom and RNA $C_{4}'$ atom were  within $7$ \AA, a threshold that has been used to perform alanine scans for protein-RNA interaction site analysis \cite{kruger2018protein}. For complexes containing many protein-RNA interaction sites, we use the interaction with least distance between the protein $C_\alpha$ atom and RNA $C_{4}'$ atom. We filter to RNA chains of length $\geq 6$ and $\leq 96$, and protein chains are contiguously cropped to length $50$. As shown in Figure \hyperref[fig:lengths]{5}, while there are many examples of short RNAs, the model is also exposed to several longer RNA examples. 

In the sequence similarity split, there are $1015$ complexes in train, $105$ in validation, and $72$ in test. The RF2NA split has $1059$ complexes in train, $117$ in validation, and $16$ in test. 



\subsection{RNASolo}

As mentioned, we use the RNASolo dataset for Traj-to-Seq training. The dataset consists of RNAs from apo RNA structures, protein-RNA complexes, and RNA-DNA complexes. As shown in Figure \hyperref[fig:lengths2]{6}, the distribution of RNA lengths in the dataset is somewhat bimodal, with many sequences around length $10$ and around length $75$. In the sequence similarity split, there are $2314$ distinct RNA sequences in train, $106$ in validation, and $78$ in test. In the RF2NA split, there are $2370$ distinct RNA sequences in train, $110$ in validation, and $16$ in test. 

\section{Training and Architecture Details}

\subsection{Noise-to-Seq}

For both splits, the Noise-to-Seq encoder and decoder GVP layers use a node scalar feature dimension of $128$, node vector feature dimension of $16$, edge scalar feature dimension of $32$, and edge vector feature dimension of $1$. On both splits, the model was pre-trained for $100$ epochs using an Adam optimizer with a learning rate of $0.001$, which takes a few hours on an NVIDIA A5000-24GB GPU.

\subsection{Traj-to-Seq}

For both splits, the Traj-to-Seq encoder and decoder GVP layers use a node scalar feature dimension of $128$, node vector feature dimension of $16$, edge scalar feature dimension of $64$, and edge vector feature dimension of $4$. The model was trained for $100$ epochs by an Adam optimizer with a learning rate of $0.001$.

\subsection{RNAFlow}

We fine-tune RNAFlow for $100$ epochs which takes one day on an NVIDIA A5000-24GB GPU, and we use the Adam optimizer with a learning rate of $0.001$.

\subsection{LSTM}

For both splits, the encoder's hidden dimension is $128$ which was selected by a hyperparameter sweep, and the model was trained by an Adam optimizer with a learning rate of $0.001$.


\subsection{Output Rescoring Model}

Following Noise-to-Seq, the encoder and decoder GVP layers use a node scalar feature dimension of $128$, node vector feature dimension of $16$, edge scalar feature dimension of $32$, and edge vector feature dimension of $1$. The node output feature dimension is $256$, and the final feedforward network consists of three fully connected layers with ReLU activation. The model was trained for $50$ epochs using an Adam optimizer with a learning rate of $0.01$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
