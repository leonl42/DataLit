
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}
\usepackage{tikz}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{graphicx}
\usepackage{wrapfig}

\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{url}
\usepackage{float}
\usepackage{algorithm}
\usepackage{amssymb}
\usepackage{comment}

\usepackage[noend]{algpseudocode}
\newcommand{\cupdot}{\mathbin{\mathaccent\cdot\cup}}

\newcommand{\ofir}[1]{{\color{blue} Ofir: #1}}
\newcommand{\sherry}[1]{{\color{purple} Sherry: #1}}

\title{Multi-Environment Pretraining Enables Transfer to Action Limited Datasets}

% diverse offline pretraining improves transfer to action limited datasets
% diverse pretraining improves transfer to action limited offline RL datasets
% multi-environment pretraining improves generalization in action limited offline datasets
% multi-environment pretraining leads to generalizeable modelling in action limited offline datasets

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{%
  David Venuto\thanks{Work done as a Student Researcher at Google Brain}  \textsuperscript{$1,2$}, Mengjiao Yang\textsuperscript{$3,4$}, Pieter Abbeel\textsuperscript{$4$}, \\ \textbf{Doina Precup\textsuperscript{$1,2,5$}, Igor Mordatch\textsuperscript{$3$}, Ofir Nachum\textsuperscript{$3$}} \\
  \textsuperscript{$1$}McGill University, \textsuperscript{$2$}Mila, \textsuperscript{$3$}Google Brain, \textsuperscript{$4$}University of California, Berkeley, \textsuperscript{$5$}DeepMind\\
  \texttt{david.venuto@mail.mcgill.ca}
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Using massive datasets to train large-scale models has emerged as a dominant approach  for  broad  generalization  in  natural  language  and  vision  applications. In reinforcement learning, however, a key challenge is that available data of sequential decision making is often not annotated with actions - for example, videos of game-play are much more available than sequences of frames paired with their logged game controls. We propose to circumvent this challenge by combining large but sparsely-annotated datasets from a \emph{target} environment of interest with fully-annotated datasets from various other \emph{source} environments. Our method, Action Limited PreTraining (ALPT), leverages the generalization capabilities of inverse dynamics modelling (IDM) to label missing action data in the target environment. We show that utilizing even one additional environment dataset of labelled data during IDM pretraining gives rise to substantial improvements in generating action labels for unannotated sequences. We evaluate our method on benchmark game-playing environments and show that we can significantly improve game performance and generalization capability compared to other approaches, using annotated datasets equivalent to only $12$ minutes of gameplay.
Highlighting the power of IDM, we show that these benefits remain even when target and source environments share no common actions.
\end{abstract}

\section{Introduction}

The training of large-scale models on large and diverse data  has become a standard approach  in natural language and computer vision applications \citep{devlin-etal-2019-bert, NEURIPS2020_1457c0d6, DBLP:conf/eccv/MahajanGRHPLBM18, https://doi.org/10.48550/arxiv.2106.04560}. 
Recently, a number of works have shown that a similar approach can be applied to tasks more often tackled by reinforcement learning (RL), such as robotics and game-playing.
For example,~\citet{https://doi.org/10.48550/arxiv.2205.06175} suggest combining large datasets of expert behavior from a variety of RL domains in order to train a single generalist agent, while~\citet{https://doi.org/10.48550/arxiv.2205.15241} demonstrate a similar result but using non-expert (offline RL) data from a suite of Atari game-playing environments and using a decision transformer (DT) sequence modeling objective \citep{NEURIPS2021_7f489f64}.

Applying large-scale training necessarily relies on the ability to gather sufficiently large and diverse  datasets. For RL domains, this can be a challenge, as
the most easily available data -- for example, videos of a human playing a video game or a human completing a predefined task -- often does not contain \emph{labelled} actions, i.e., game controls or robot joint controls.
We call such datasets \emph{action limited}, because little or none of the dataset is annotated with action information.
Transferring the success of approaches like DT to such tasks is therefore bottlenecked by the ability to acquire action labels, which can be expensive and time-consuming~\citep{https://doi.org/10.48550/arxiv.2011.13885}. 
%Whereas much of internet-sized natural language and computer vision data is generally well annotated, large datasets of interest in the reinforcement learning (RL) paradigm require knowledge of the agent action as well as state at each frame.  Easily available datasets such as videos of a human playing a video game or a robot completing a predefined task do not generally contain \emph{labelled} actions, making it difficult to learn a behavior policy.  The lack of \emph{massive} labelled datasets for training RL policies, generally requires sample inefficient strategies such as DQN \citep{mnih-atari-2013}.  This problem is exacerbated when exploration is a challenging component of solving the environment successfully \citep{Baker2020Emergent, https://doi.org/10.48550/arxiv.1912.06680, sciencepub, Baker2020Emergent}. \ofir{This seems a bit vague. What are these sample-inefficient strategies?}  

%As opposed to relatively smaller models used in RL \citep{10.5555/3045390.3045684}, training large models \citep{NIPS2017_3f5ee243} using massive task agnostic datasets has lead to improved \emph{generalist} task adaptation in a variety of challenging domains \citep{JMLR:v21:20-074, 10.1145/3462244.3479913}.  These models have shown the capability of learning from unrelated data and modality extension \citep{https://doi.org/10.48550/arxiv.2103.05247}. Most RL methods have only examined comparatively small models \citep{ijcai2020p651, 10.5555/3327144.3327175} which solve only related tasks in highly similar environments and do not take advantage of highly scale-able model architectures.  Training across a large suite of environments with radically different dynamics, but similar state spaces has shown promise using the Arcade learning environment (ALE) \citep{10.5555/2566972.2566979} using a decision transformer (DT) based sequence modelling architecture \citep{NEURIPS2021_7f489f64, https://doi.org/10.48550/arxiv.2205.15241, https://doi.org/10.48550/arxiv.2205.06175}.  These methods still require a large amount of labelled data from each task, which can be difficult to generate in domains outside of synthetic highly contained tasks.

Some recent works have explored approaches to mitigate the issue of action limited datasets. For example, Video PreTraining (VPT) \citep{https://doi.org/10.48550/arxiv.2206.11795} proposes gathering a small amount ($2$k hours of video) of labeled data manually which is used to  train an inverse dynamics model (IDM) \citep{4936}; the  IDM is then used to provide action labels on a much larger video-only dataset ($70$k hours). %Inverse dynamics being a well positioned choice as opposed to behavior cloning, since it is non-causal, allowing it to see past and future information.  
This method is shown to achieve human level performance in Minecraft.  It has also been demonstrated that some agents can learn directly from videos without any action labels \citep{pmlr-v162-seo22a}.

While VPT shows promising results, it still requires over $2$k hours of manually-labelled data; thus, a similar amount of expensive labelling is potentially necessary to extend VPT to other environments. 
In this paper, we propose an orthogonal but related approach to  VPT: leveraging a large set of labeled data from various \emph{source} domains to learn an agent policy on a limited action dataset of a \emph{target} evaluation environment. % Can the availability of such multi-environment datasets help reduce the amount of action labels needed to solve the target environment?
%we first examine the performance of these method given a \emph{much} smaller amount of labeled data, approximately only $12$ minutes. We then propose an orthogonal problem to VPT: can we leverage a larger set of labeled data from similar tasks to better learn an agent policy from limited labeled action data in our evaluation environment? We investigate if IDM based architectures exploit shared structure between environments and generalize under limited labeled data?
To tackle this setting, we propose Action Limited Pretraining (ALPT), which relies on the hypothesis  that shared structures between environments can be exploited by non-causal (i.e., bidirectional) transformer IDMs. This allows us to look at both past and future frames to infer actions.  In many experimental settings, the dynamics are far simpler than multi-faceted human behavior in the same setting. It has been suggested that IDMs are therefore more data efficient and this has been empirically shown \citep{https://doi.org/10.48550/arxiv.2206.11795}.  ALPT thus uses the multi-environment source datasets as pretraining for an IDM, which is then finetuned on the action-limited data of the target environment in order to provide labels for the  unlabelled target data, which is then used for training a 
%causal transformer 
DT agent. 
%We propose training on a diverse set of environments as a practical method to accelerating labeling observation data.  A summary of our method, which we name Action Limited Pretraining (ALPT), is detailed in Figure \ref{fig:idm}.  The key hypothesis of this work is that shared structures between environments can be exploited by these architectures and be made practical for dynamics modeling.  We aim to provide a bootstrap-able method for labeling of frames in large scale RL datasets, as more labelled datasets become available.

Through various experiments and ablations, we demonstrate that leveraging the generalization capabilities of IDMs is critical to the success of ALPT, as opposed to, for example, pretraining the DT model alone on the multi-environment datasets or training the IDM only on the target environment.
%To highlight the difficulty of learning under limited labelled data, we first show that sequence modeling fails to learn under limited action data.  We then show that performance cannot be recovered using dynamics modeling under the same quantities of limited actions.  We then present an alternative approach, training under a highly diverse set of labeled data.  Using numerous ablations of ALPT we show that this can result in substantial performance improvements in modelling and improved downstream offline RL performance follows. 
On a benchmark game-playing environment, we show that ALPT yields as much as $5$x improvement in performance, with as little as $10k$ labelled samples required (i.e., $0.01\%$ of the original labels), derived from only $12$ minutes of labelled game play \citep{ye2021mastering}.
We show that these benefits even hold when the source and target environments use distinct action spaces; i.e., the environments share similar states but \emph{no} common actions, further demonstrating the power of IDM pretraining.

While ALPT is, algorithmically, a straightforward application of existing offline RL approaches, our results provide a new perspective on  large-scale training for RL. Namely, our results suggest that the most efficient path to large-scale RL methods may be via generalist inverse dynamics modelling paired with specialized agent finetuning, instead of generalist agent training alone.
%In these results, we show successful pretraining given a diverse dataset and limited data from our evaluation environment, can be used for alignment.  We further show that a pretraining the dynamics model under diverse environments is required for improved downstream performance and that pretraining only a sequence model with this diverse dataset does not result in better performance across action spaces.

\begin{figure}
    \centering
    \includegraphics[width=11cm]{idm_model.png}
    \caption{The dynamics model pretraining procedure of ALPT using the source set of environments along with the limited action target environment dataset.}
    \label{fig:idm}
    \vspace{-0.4cm}
\end{figure}

\section{Related Work}
In this section, we briefly review relevant works in multi-task RL, meta-learning for RL, semi-supervised learning, and transfer learning.

\textbf{Multi-Task RL.} It is commonly assumed that  similar tasks share similar structure and properties \citep{10.1023/A:1007379606734, https://doi.org/10.48550/arxiv.1706.05098, 10.1007/978-3-319-10599-4_7, Radford2019LanguageMA}.  Many multi-task RL works leverage this assumption by learning a shared low-dimensional representation across all tasks \citep{NIPS2014_94c7bb58, https://doi.org/10.48550/arxiv.1603.02041, DEramo2020Sharing}.  These methods have also been extended to tasks where the action space does not align completely \citep{10.1007/978-3-030-46133-1_9}.  Other methods assume a  universal dynamics model when the reward structure is shared but dynamics are not \citep{zhang2021learning}.  Multi-task RL has generally relied on a task identifier (ID) to provide contextual information, but recent methods have explored using additional side information available in the task meta-data to establish a richer context \citep{pmlr-v139-sodhani21a}.  ALPT can be seen as multi-task RL, given that we train both the sequence model and IDM using multiple different environments, but we do not explicitly model context information or have access to task IDs.

\textbf{Meta RL.} Meta-learning is a set of approaches for \emph{learning to learn} which leverages a set of meta-training tasks \citep{metarl, 155621}, from which an agent can learn either parts of the learning algorithms (eg how to tune the learning rate) or the entire algorithm \citep{9533842, kalousis2002algorithm}.  %Meta RL makes use of these principles to learn structure from experience.  %The introduce using these selected meta-training tasks and then aiming to learn new behaviors quickly at evaluation.  
In this setting, meta-learning can be used to learn policies \citep{duan2017rl, 10.5555/3305381.3305498} or dynamics models \citep{clavera2018learning}.  A distribution of tasks is assumed to be available for sampling, in order to provide additional contextual information to the policy. One such method models contextual information as probabilistic context variables which condition the policy \citep{pmlr-v97-rakelly19a}. This method has been shown to learn from only a handful of trajectories. Meta-training can be used to learn policies offline, while using online interaction to correct for distribution shift, without requiring any rewards in the online data \citep{pmlr-v162-pong22a}. These methods are commonly used to train on a source set of tasks, like  ALPT, but usually require task labels. % and an explicitly defined set of meta-training tasks.  
Meta-training tasks need to be   hand-selected, and the results are highly dependent on the quality of that process.

\textbf{Semi-supervised learning.} \emph{Semi-supervised} learning uses both labelled and unlabelled data to improve supervised learning performance \citep{10.5555/1717872}. It is especially useful when a limited amount of labelles data is given and additional labels are difficult to acquire,  unlabelled data is plentiful.  Early methods of this type infer unknown labels using a classifier trained on the labeled data~\citep{Zhu02learningfrom}.  Other methods rely on additional structural side information to regularize supervised objectives \citep{NIPS2001_a82d922b}, such as the time scale of a Markov random walk over a representation of the data.  Many methods, especially those using deep learning, combine supervised and unsupervised learning objectives \citep{10.5555/2969442.2969635}.  More recent methods use generative models and approximate Bayesian inference to fill in missing labels \citep{10.5555/2969033.2969226}.  The problem of semi-supervised learning is especially relevant in RL, where large datasets of experience containing action descriptions or rewards may  hard to acquire, eg. through manual annotation of videos or running robotic experiments. 
%Despite their relevance to practice, semi-supervised methods have not commonly been applied to actions and control.  
By using an inverse dynamics model, ALPT applies semi-supervised learning to label actions in a large dataset of experience frames, given only limited labeled action data.

\textbf{Transfer Learning and Zero-shot RL.} Policies learned by RL in one domain can have limited capability to generalize to new settings \citep{10.5555/3045390.3045684}. The most difficult problem is zero-shot RL, where the agent must generalize  at evaluation time to a new environment that was not seen in training, without acquiring any new data.  Transfer learning \citep{JMLR:v10:taylor09a}  tackles a subset of generalization problems where the agent can access interactions from a related environment or task during training.  This prior experience in other environments is leveraged to improve learning in novel environments.  Transfer learning has been applied across both environments \citep{7487140, Tzeng2020} and tasks \citep{policydist,ParisottoBS15}.  It has also been examined in hard exploration games, using imitation learning from human-generated video data~\citep{10.5555/3327144.3327216}.  ALPT can be seen as tackling the transfer learning problem, with limited action data from the target environment and providing pseudo-labels for actions. Notably, we consider the under-explored scenario where the action space is not completely shared between the training and test environments.

\textbf{Offline RL.} Offline RL describes a setting that during learning, the agent has access to only a fixed dataset of experience. When function approximation becomes necessary in environments with large, complex state spaces, online RL algorithms are extremely sensitive to the dataset distribution.  It has been shown that even when the logged data is collected with a single behavior policy, standard online algorithms fail \citep{fujimoto2019off}.  This has been hypothesized to be due erroneous generalization of the state action function.  One such class of methods applies random ensembles of Q-value function targets \citep{10.5555/3524938.3524949}.  Other works suggest regularizing the agent policy to the behavior policy \citep{zhang2021brac}. In offline RL, errors arise when needed to bootstrap the value function and algorithms must apply strong regularizations on both learned policy and value function to achieve stable performance \citep{wu2020behavior,NEURIPS2019_c2073ffa,zhang2021brac,nachum2019algaedice}.  We use decision transformers (DT)~\citep{NEURIPS2021_7f489f64} as the backbone for ALPT, and these have been shown to be successful in learning generalizable agents from logged data in a diverse set of environments \citep{https://doi.org/10.48550/arxiv.2205.15241}.
\vspace{-0.4cm}
\section{Background}
In this section, we review the standard offline RL setting and the use of decision transformers (DT) as a sequence modelling objective for offline RL. We then define the setting of multi-environment offline RL with action-limited data, which is our focus.

\subsection{Offline Reinforcement Learning}
\label{offline-rl}
We consider an agent acting within a Markov decision process (MDP) defined by $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R} \rangle$, where $\mathcal{S}$ is the set of states, $\mathcal{A}$ is the set of actions, $\mathcal{P}: \mathcal{S} \times\mathcal{A} \rightarrow \mathrm{Dist}(\mathcal{S}) $ is the transition probability kernel and $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$ is the scalar reward function.

In offline RL, the agent is given a dataset of episodes, i.e.,  sequences of states, actions, and rewards collected by unknown policies interacting with the environment:
\begin{equation}
    \langle \dots, \mathbf{s_t}, a_t, r_t, \dots \rangle.
\end{equation}
The objective is typically to use this dataset in order to learn a conditional action distribution, $P_{\theta}(a_t | \mathbf{s}_{\leq t}, a_{< t}, r_{< t})$, that maximizes the expectation of the total return, $G_t = \sum_{k\geq 0} r_{t+k}$ when used to interact with the environment from which the training episodes were generated.

\subsection{Offline RL as Sequence Modeling}
\label{seqmodel}
Decision transformer (DT) \citep{https://doi.org/10.48550/arxiv.2106.01345} is an  approach to  offline RL which formulates this problem as sequence modeling, and then uses transformer-based architectures to solve it. For this purpose, the episodes in the offline dataset are augmented with the returns associated with each step:
\begin{equation}
    \tau = \langle \dots, \mathbf{s_t}, a_t, r_t, G_t,  \dots \rangle.
\end{equation}
This sequence is tokenized  and passed to a causal transformer $P_\theta$, which predicts both returns and actions using a cross-entropy loss.
%In practice, we divide the state $\mathbf{s}_t$ into $M$ image patches, $\mathbf{s}^1_t , \dots, \mathbf{s}^M_t$. The state patches are combined additive with a trainable position encoding. The returns are quantized into a discrete range uniformly, and actions are discrete in the ALE. The model is trained to predict the next return and action in a sequence using a cross-entropy loss, defined for a single trajectory $\tau$ as, 
Thus, the learning objective for $\theta$ is:
\begin{equation}
\label{dtloss}
    J(\theta) = \mathbb{E}_{\tau} \left[\sum_{t} -\log P_\theta(G_t | \mathbf{s}_{\le t},a_{<t},r_{<t}) - \log P_\theta(a_t | \mathbf{s}_{\le t},a_{<t},r_{<t},G_{t}) \right].    
\end{equation}
During inference, at each timestep $t$, after observing $\mathbf{s}_t$, DT uses the predicted return distribution $P_\theta(G_t | \mathbf{s}_{\le t},a_{<t},r_{<t})$ to choose an optimistic estimate $\hat{G}_{t}$ of return, before using $P_\theta(a_t | \mathbf{s}_{\le t},a_{<t},r_{<t},\hat{G}_{t})$ to select an action $\hat{a}_t$ (see \citet{https://doi.org/10.48550/arxiv.2205.15241} for details).
%, we implement an inference time method to predict expert actions.  This method utilizes a binary classifier to predict if an action is optimal $P(a_t \sim \mathrm{expert} | \hat{R}_t)$.  The binary classifier is defined as $\mathrm{exp}(\kappa \hat{R_t})$ as demonstrated in \citep{NIPS2006_d806ca13,10.2307/171139}.  During evaluation time, returns are sampled with $P(\hat{R_t} | \dots) + \kappa \hat{R_t}$ and then actions are sampled according to $P(a_t | \hat{R}_t)$.

\subsection{Multi-Environment and Action Limited Datasets}
\label{mtl}
Our goal is to use pretraining on a set of environments where labelled data is plentiful, in order to do well on a target environment where only limited action-labelled data is available. Therefore, the offline RL setting we consider includes multiple environments and action-limited datasets, as we detail below. 

%We introduce notation describing both a \emph{diverse} and \emph{target} environment training procedure that ALPT will rely on. 
We consider a set of $n$ \emph{source} environments, defined by a set of MDPs: $E =\{\mathcal{M}_1,\dots,\mathcal{M}_{n_{}}\}$, and a single \emph{target} environment $\mathcal{M}_\star$. 
For each source environment $\mathcal{M}_d$, we have an offline dataset of episodes generated from $\mathcal{M}_d$, denoted by $\mathcal{D}_d = \left\{\tau:=\langle \dots,\mathbf{s}_t, a_t, r_t, \dots \rangle\right\}$, fully labelled with actions. %The agent is trained using the offline dataset of episodes $\bigcup_{d=1}^n \mathcal{D}_d\right$.  
For the target environment, the agent has access to a small labelled dataset from $\mathcal{M}_\star$, denoted as $\mathcal{D}^+_\star = \left\{\tau:=\langle \dots,\mathbf{s}_t, a_t, r_t, \dots \rangle \right\}$, and a large  dataset without action labels, $\mathcal{D}^-_\star = \left\{\tau:=\langle \dots,\mathbf{s}_t, r_t, \dots \rangle \right\}$.  %We denote the combined action limited dataset of the target environment as $\mathcal{D}_\star = \mathcal{D}^-_\star \cup \mathcal{D}^+_\star $.

%One approach to the offline RL problem using these datasets is to first label the target environment, making use of the diverse environment during training of a dynamics model for labelling.  We now introduce preliminary information on a method for dynamics modelling that can be used to label missing actions in our target environment, given the observation frames. 

%\subsection{Inverse Dynamics Modeling}
%\label{idm}
%\ofir{I would move this to methods}
%Inverse dynamics models (IDM) aim to minimize the negative log-likelihood of an action $a_t$ at timestep $t$, trained with a trajectory of $T$ frames $\tau = \{\mathbf{s}_0,\dots, \mathbf{s}_T\}$. As opposed to imitation learning, inverse dynamics modeling is not causally constrained and allows conditioning on past and future frames.  It has been shown that an IDMs are much easier to learn and can be trained accurately under limited data in complex environments \citep{https://doi.org/10.48550/arxiv.2206.11795}.

\section{Action Limited Pretraining (ALPT)}
%\ofir{TODO: determine if we also label returns, or just actions}  Just actions

%\ofir{TODO: determine if we need pretraining for DT} Done with pretraining

We now describe our proposed approach to offline RL in multi-environment and action limited settings. ALPT relies upon an inverse dynamics model (IDM) which uses the combined labelled data in order to learn a representation that generalizes well to the limited action data from the target environment. The predicted labels of the IDM on the unlabelled portion of the target environment dataset are then used for training a sequence model parameterized as a decision transformer (DT). %, trained with limited labeled data.  This dynamics model is used to label either actions or returns in trajectories only containing frames.  
We elaborate on this procedure below and summarize the full algorithm in Table~\ref{tab:data}.
%We detail our dynamics models and training procedure in the following: Algorithm \ref{pretrain}, Table \ref{tab:data}, Subsection \ref{idm} and \ref{multigametrain}.

\subsection{Inverse Dynamics Modeling}
\label{idm}
Our inverse dynamics model (IDM) is a bidirectional transformer trained to predict actions from an action-unlabelled sub-trajectory of an episode. The training objective for learning an IDM $P_{\beta}$ is
\begin{equation}
\label{eq:idm}
    J(\beta) = \E_{\tau}\left[\sum_t \sum_{i=0}^{k-1} - \log P_{\beta} (a_{t+i} | \mathbf{s}_t, \dots, \mathbf{s}_{t+k})\right],
\end{equation}
where $k$ is the length of training sub-trajectories.
In our experiments, we use $k=5$ and parameterize $P_\beta$ using the GPT-2 transformer architecture \citep{radford2019language}, modified to be bidirectional by changing the attention mask.  %We obtained $k$ through experimental tuning.

\subsection{Multi-Environment Pretraining and Finetuning}
\label{multigametrain}
ALPT is composed of a two-stage \textbf{pretraining} and \textbf{finetuning} process.  
%In this section, we utilize notation described in Subsection \ref{mtl}.

During \textbf{pretraining}, we use the combined labelled datasets for all source environments combined with the labelled portion of the target environment dataset: $\left(\bigcup_{d=1}^n \mathcal{D}_d\right) \cup \mathcal{D}^+_\star$, to train the IDM $P_\beta$. Concurrently, we also train the DT $P_\theta$ on the combined labelled and unlabelled datasets for all source environments combined with the target environment datasets, by using the IDM to provide action labels on the unlabelled portion $\mathcal{D}^-_\star$.  The DT training dataset is therefore: $\left(\bigcup_{d=1}^n \mathcal{D}_d\right) \cup \mathcal{D}^+_\star \cup \mathcal{D}^-_\star$.
%We therefore train our sequence model on the labeled and unlabelled data from our target environment. The IDM (objective from Equation \ref{eq:idm}) is trained using both labeled data from the diverse set of environments and a small portion of labeled data from the target evaluation environment, using dataset: $\{\mathcal{D}_d \cup \mathcal{D}_t^+\}$.

During \textbf{finetuning}, we simultaneously train both the IDM and DT exclusively on the target environment dataset. We train the IDM on the labelled portion $\mathcal{D}^+_\star$. We train DT on the full action limited dataset $\mathcal{D}^+_\star\cup\mathcal{D}^-_\star$ by using the IDM to provide action labels on the unlabelled portion $\mathcal{D}^-_\star$.

Finally, during evaluation we use the trained DT agent to select actions in the target environment $\mathcal{M}_\star$, following the same protocol described in Section~\ref{seqmodel}.

%\begin{algorithm}
%\caption{Pretraining}\label{pretrain}
%\begin{algorithmic}[1]
%\Require{$\mathcal{D}_t^-, \mathcal{D}_t^+, \mathcal{D}_d, \mathcal{M}_t, E_D, \text{ Init IDM parameters: } \beta, \text{ Init DT parameters: } \theta, \alpha_1, \alpha_2$ } 
%\While{\text{not done}}
%\State $\tau_{\mathrm{IDM}} \sim  \{{\mathcal{D}}_d \cup \mathcal{D}_t^+\}$ \text{ Sample from diverse and limited action target dataset to train IDM. }
%\State $J_{\mathrm{IDM}} = \sum_{t=0}^T - \log \pi_{\theta} (a_t | \mathbf{s}_0, \dots, \mathbf{s}_t), a_t \sim p_{\beta}(a_t | \mathbf{s}_1, \dots \mathbf{s}_T) \forall \{\mathbf{s}_t, a_t\} \in \tau_{\mathrm{IDM}}$
%\State $\beta \leftarrow \beta - \alpha_1 \nabla_{\beta}  J_{\mathrm{IDM}}$ Compute IDM loss and update IDM parameters.
%\State $\forall \mathbf{s}_t \in \mathcal{D}_t^-, \hat{\mathcal{D}_t} = \{\mathbf{s}_0, \hat{a}_0 \dots \mathbf{s}_t, \hat{a}_t \}, \text{ where }  \hat{a}_t \sim p_{\beta}(\hat{a}_t | \mathbf{s}_0, \dots \mathbf{s}_T)$ Label missing data.
%\State $\tau_{\mathrm{DT}} \sim  \{\mathcal{D}_d \cup \mathcal{D}_t^+ \cup \hat{\mathcal{D}_t}\}$ Sample from diverse dataset, and labeled actions to train DT.
%\State $\mathbf{\hat{a}} = f_{\mathrm{DT}, \theta}(\tau_{\mathrm{DT}})$ Obtain predicted actions from DT.
%\State Obtain loss $J_{\mathrm{DT}}$ using Equation \ref{dtloss}.
%\State $\theta \leftarrow \theta - \alpha_2 \nabla_{\theta}  J_{\mathrm{DT}}$ Update DT parameters.
%\EndWhile
%\end{algorithmic}
%\end{algorithm}


\section{Experiments}

\begin{table}[t]
\caption{A summary of ALPT.}
\label{tab:data}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf Step} & 
\multicolumn{1}{c}{\bf Procedure} 
\\ \hline \\
\textbf{Pretraining} & Train IDM on all labelled data: $\left(\bigcup_{d=1}^n \mathcal{D}_d\right)\cup\mathcal{D}^+_\star$. \\
& Train DT on all data: $\left(\bigcup_{d=1}^n \mathcal{D}_d\right)\cup\mathcal{D}^+_\star\cup\mathcal{D}^-_\star$, with IDM \\
& providing action labels on $\mathcal{D}^-_\star$ 
\\ \hline \\
\textbf{Finetuning}             & 
Train IDM on labelled data in target environment dataset: $\mathcal{D}^+_\star$. \\ 
& Train DT on all data in target environment dataset: $\mathcal{D}^+_\star\cup\mathcal{D}^-_\star$, \\
& with IDM providing action labels on $\mathcal{D}^-_\star$ 
\\ \hline \\
\textbf{Evaluation}             & 
Use trained DT agent to interact with target environment $\mathcal{M}_\star$. \\
\end{tabular}
%\begin{tabular}{lll}
%\multicolumn{1}{c}{} & 
%\multicolumn{1}{c}{\bf Pretraining}  &\multicolumn{1}{c}{\bf Finetuning}
%\\ \hline \\
%\textbf{IDM Training Dataset} & \{Diverse, Act. Lim. Target\} & Act. Lim. Target \\  &$\{{\mathcal{D}}_d \cup \mathcal{D}_t^+\}$          & $\mathcal{D}_t^+$ \\ \\ \hline %\\
%\textbf{DT Training Dataset}             & 
%\{Diverse, Act. Lim. Target, IDM Labelled Target\} & Act. Lim. Target\\ &$\{\mathcal{D}_d \cup \mathcal{D}_t^+ \cup \hat{\mathcal{D}_t}\}, \text{ where } \mathcal{D}_t^- \rightarrow^{\mathrm{IDM}} \hat{\mathcal{D}_t}$       & $\mathcal{D}_t^+$ \\
%\end{tabular}
\end{center}
\end{table}

We evaluate ALPT on a multi-game Atari setup similar to \citet{https://doi.org/10.48550/arxiv.2205.15241}. Our findings are three-fold: (1) ALPT, when pretrained on multiple source games, demonstrates significant benefits on the target game with limited action labels; (2) ALPT maintains its significant benefits even when pretrained on just a single source game with a disjoint action space, (3) we demonstrate similar benefits on maze navigation tasks.
%We formulate our experiments to evaluate the following aspects of ALPT:
%\begin{itemize}
%    \item How does the DT sequence modelling architecture perform under limited action data, but a large dataset of observations?
%    \item How well do IDM methods recover performance with action limited training data?
%    \item Is dynamics modelling improved under more or less source datasets?
%    \item Can we apply ALPT to pretraining on environments where the action space is disjoint with the target environment?
%    \item Can training an IDM under a source data regime generalize in a practical navigation task given action limited data?
%\end{itemize}

\subsection{Experimental Procedure}
\paragraph{Architecture and Training.} Our architecture and training protocol follow the multi-game Atari setting outlined in~\citet{https://doi.org/10.48550/arxiv.2205.15241}. Specifically, we use a transformer with 6 layers of 8 heads each and hidden size 512. The rest of the architecture and training hyperparameters remain unchanged for experiments on Atari. For the Maze navigation experiments, we modify the original hyperparameters to use a batch size of $256$ and a weight decay of $5 \times 10^{-5}$.  During pre-training, we train the DT and IDM for $1M$ frames.  %We choose a smaller batch size given the complexity of the environment and adjust the weight decay accordingly.  

%\ofir{Can we just cite the MGDT paper? Are there any differences?}
%A few differences in batch size and weight decay. 
%\textbf{DT Architecture.} Our DT architecture is derived from GPT-2.  All results are utilizing an architecture with 200M parameters. The sequence length contains 4 transitions.
%\textbf{Training Procedure.} We train our DT architecture using the JaxLine \citep{deepmind2020jax} framework.  We use a LAMB optimizer \citep{https://doi.org/10.48550/arxiv.1904.00962} with a learning rate of $3 \times 10^{-4}$, $4000$ warm up steps, a gradient clipping of $0.25$, weight decay $5 \times 10^{-5}$ and a batch size of $1024$.

\paragraph{Datasets}  
As in \citet{https://doi.org/10.48550/arxiv.2205.15241}, we use the standard offline RL Atari datasets from RL Unplugged~\citep{gulcehre2020rl}. Each game's dataset consists of 100M environment steps of training a DQN agent~\citep{agarwal2020optimistic}.
For the source games, we use this dataset in its entirety. For the target game, we derive an action-limited dataset by keeping the action labels for randomly sampled sequences consisting of a total 10k transitions (0.01\% of the original dataset, equivalent to 12 minutes of gameplay) and removing action labels in the remainder of the dataset. For the maze navigation experiments, we generate the data ourselves.  The offline datasets for each maze configuration contain $500$ trajectories with a length of $500$ steps or until the goal state is reached.  They are generated using an optimal policy for each maze, with an $\epsilon$-greedy exploration rate of $0.5$ to increase data diversity.
%For multi-game pretraining, we consider 
%\ofir{Can we cite the MGDT paper and the RLUnplugged paper instead of this paragraph?}
%Let see how much space we have in the end?
%For each Atari environment we generate 2 training runs, with every run having $1$ million environment steps.  Since we include the entire training run, the dataset contains expert and non-expert trajectories.  We do this since suboptimal trajectories are both highly diverse and can improve representation learning of environmental effects of poor agent behavior.
%For the action limited dataset associated with the target environment, we take a small subset of the data ($10K$ transitions, equivalent to $12$ minutes of gameplay), and leave it unchanged while replacing the action labels in the remainder of the dataset with a null token. % This is a token that does not correspond to any true action in the game, which in practice is $-1$.
%\textbf{Arcade Learning Environments.} 


%\textbf{Minatar Environments.} Next, we evaluate our method on MinAtar \citep{young19minatar}, a simplified and miniaturized set of Arcade learning environment tasks where the ALE games are compressed into a $10 \times 10$ state representation.

%\textbf{Procgen. }In addition, we evaluate our proposed approach on the offline version of the Procgen set of environments \citep{pmlr-v119-cobbe20a}.  This set of environments is commonly used to evaluate zero-shot generalization given a wide set of visual perturbation. The Procgen benchmark samples procedurally generated \emph{levels} or perturbed configurations for various different games.  The approach for our method is similar to ALE with each selected game of our diverse set from Procgen being labeled and one game having limited labeled frames.


\subsection{Baseline Methods}
We detail the methods that we compare ALPT to below.

\textbf{Single-game variants.}  To evaluate the benefit of multi- versus single-environment training, we assess the performance of training either DT alone or DT and IDM simultaneously on the target game. When training DT alone (\textbf{DT1}), we train it only on the $10$k subset of data that is labelled, while when training DT and IDM simultaneously (\textbf{DT1-IDM}) we use the IDM to provide action labels on the unlabelled portion of the data. 
%The IDM is trained to label the missing actions on only the single game given a dataset of limited actions ($10K$ true actions).  We pretrain a DT \textbf{with IDM labeling} and \textbf{without IDM labeling} (containing the null token labels), denoted as \textbf{DT$1$-IDM} and \textbf{DT$1$} respectively.  For the DT$1$ game, we exclude null action data from the dataset.

\textbf{Multi-game DT variants.} To assess the need for IDM versus training on DT alone, we evaluate a multi-game baseline (\textbf{DT5}) composed of DT alone. For this baseline, we pretrain DT on all labelled datasets combined from both the source and target environments before finetuning the DT model on the $10$k labelled portion of the target game. %We pretrain a DT on all games from the limited set of Atari games and evaluate it on a single selected game.  The IDM is used to label the missing actions on the target game.  This training dataset contains all true action labels from the source set of non-evaluation games and only a limited amount of labeled actions ($10K$) on the evaluation game.  We pretrain a DT \textbf{with} and \textbf{without} IDM labelling, denoted as \textbf{ALPT} (our method) and \textbf{DT5} respectively.

\textbf{Return prediction DT variants.} As an alternative way for DT to leverage the unlabelled portion of the target environment dataset, we evaluate a baseline (\textbf{DT5-RET}) that uses the unlabelled portion for training its return prediction. 
The model still undergoes a pretraining and finetuning stage, first pretraining on all available data and then finetuning only on data from the target game.
%The agent is trained to predict tokenized returns using a cross entropy loss.  The DT is not trained using a cross-entropy loss to predict actions. This method is titled \textbf{DT$5$-RET}.

We give a summary of the baseline methods as well as ALPT in Table \ref{tab:baselines}. 
We also present results of an additional variant of ALPT in which only the IDM is pretrained (rather than both the IDM and DT) in Appendix~\ref{app:alpt-no-dt}.
\begin{table}[t]
\caption{A summary of the baseline methods.}
\label{tab:baselines}
\begin{center}
\begin{tabular}{lcc}
\multicolumn{1}{c}{\bf Method} & 
\multicolumn{1}{c}{\bf Training Games} & \multicolumn{1}{c}{\bf IDM} 
\\
\hline 
DT-1 & $1$ &  $\times$ 
\\
\hline 
DT-5 & $5$ &  $\times$ 
\\
\hline 
DT-1-IDM & $1$ & \checkmark
\\
\hline 
DT-5-RET & $1$ & $\times$ 
\\
\hline 
\textbf{ALPT-$X$} & $X$ if specified, otherwise $5$ & \checkmark
\end{tabular}
%\begin{tabular}{lll}
%\multicolumn{1}{c}{} & 
%\multicolumn{1}{c}{\bf Pretraining}  &\multicolumn{1}{c}{\bf Finetuning}
%\\ \hline \\
%\textbf{IDM Training Dataset} & \{Diverse, Act. Lim. Target\} & Act. Lim. Target \\  &$\{{\mathcal{D}}_d \cup \mathcal{D}_t^+\}$          & $\mathcal{D}_t^+$ \\ \\ \hline %\\
%\textbf{DT Training Dataset}             & 
%\{Diverse, Act. Lim. Target, IDM Labelled Target\} & Act. Lim. Target\\ &$\{\mathcal{D}_d \cup \mathcal{D}_t^+ \cup \hat{\mathcal{D}_t}\}, \text{ where } \mathcal{D}_t^- \rightarrow^{\mathrm{IDM}} \hat{\mathcal{D}_t}$       & $\mathcal{D}_t^+$ \\
%\end{tabular}
\end{center}
\end{table}

\subsection{How does ALPT perform compared to the baselines?}
%the DT sequence modelling architecture perform under limited action data, but a large dataset of observations?}
%In these experiments, we investigate the ability of the DT architecture to solve ALE tasks given a action limited target environment dataset. We pretrain our model using the Multi-game DT variants and Single-game DT variants, where the target environment dataset is action limited. We finetune all models on the action limited target environment dataset only ($\mathcal{D}_*^-$).

%We first examine this experimental result with the ALE in Figure \ref{aleres}.  In this figure, we show the evaluation game performance during finetuning with the action limited target environment. We first show that the performance of the DT under both the Single-game (DT$1$) and Multi-game (DT$5$) regimes is poor, achieving minimal game performance during finetuning evaluation.   We note that performance is especially poor when actions are removed as opposed to when we train our DT with only returns (DT$5$-RET). Overall, there is a failure to achieve even marginal game performance using a standard DT.  %Additionally, pretraining an IDM (DT$1$-IDM) with the limited dataset from our target game does not recover performance well.  This indicates that $10k$ actions is not sufficient to train our IDM to label actions correctly.  Pretraining the DT and IDM (ALPT) with a diverse dataset of environments recovers performance up to $~\approx500\%$ higher than pretraining only using the action limited target dataset.  The performance difference is particularly stark while finetuning on \emph{Seaquest}.  


%\subsection{How well do IDM methods recover performance with action limited training data?}

We focus our first set of multi-game pretraining experiments on 5 Atari games: $\{$\emph{Asterix, Breakout, SpaceInvaders, Freeway, Seaquest}$\}$. 
This subset of games is selected due to having a similar shared game structure and access to high-quality and diverse pretraining data.  
We evaluate each choice of target game in this setting, i.e., for each game we evaluate using it as the target game while the remaining $4$ games comprise the source environments.
%In subsequent results, involving generalization to disjoint action spaces, we expand the set of Atari games to include additional games that have action spaces disjoint with \emph{Freeway}.

%We evaluate the performance of the DT architecture while using a pretrained IDM to label the missing actions. 
We compare our pretraining regime (ALPT) with the single-game variant and standard DT baselines in Figure \ref{aleres}. We see that pretraining ALPT on the source games results in substantial downstream performance improvements. We show that there are relatively minimal performance improvements when pretraining on datasets that do not include any non-target environments (DT$1$-IDM). Utilizing ALPT results in improvements up to $\approx 500\%$ higher than the single-game training regime. The performance difference is especially stark in \emph{Breakout} and \emph{Seaquest}. Additionally, we show that performance is not recovered under pretraining only the sequence model (DT$5$) on the action rich environments, indicating that most generalization benefits are occurring due to the IDM pretraining. We also show that performance using DT1 and DT5-RET is poor, highlighting the need for explicit re-labelling to achieve good performance during sequence model finetuning.

We also encourage the reader to look to Appendix~\ref{app:alpt-no-dt} for a comparison to a variant of ALPT for which only the IDM is pretrained, while the DT is initialized from scratch during finetuning. We find that this variant maintains strong performance compared to DT1-IDM, suggesting that the main benefit of pretraining is the IDM.

\begin{figure}[]
\centering
\subfloat{\includegraphics[width = 1.6in]{asterix.png}} 
\subfloat{\includegraphics[width = 1.6in]{breakout.png}}
\subfloat{\includegraphics[width = 1.6in]{freeway.png}} \\
\centering
\subfloat{\includegraphics[width = 1.6in]{seaquest.png}} 
\subfloat{\includegraphics[width = 1.75in]{spaceinvaders.png}} 
\caption{Game performance across the ALE environments for the baseline and ALPT.  The figure shows the evaluation game performance (Episodic Return) of our DT policies during finetuning on the limited action target dataset.  Higher score is better.  The shaded area represents the standard deviation over $3$ random seeds.  The $x$-axis shows the number of finetuning steps. We evaluate ALPT on 16 episodes of length 2500 each following \citep{https://doi.org/10.48550/arxiv.2205.15241}.}
\label{aleres}
\end{figure}

\subsection{Is dynamics modelling improved under more or less source datasets?}

In this set of experiments, we expand the set of source and target games.  As in the previous experiments, each source game provides a fully-labelled dataset of size 100M, while each target game has a dataset of size 100M with only 10k action labels. We evaluate performance using 5 target games (\{\textit{`Pong', `SpaceInvaders', `StarGunner', `MsPacman', `Alien'}\}) and using either 36 source games (ALPT-36, trained on all 41 game datasets available in~\citet{agarwal2020optimistic} minus the 5 target games) and using 9 source games (ALPT-9, trained on \{\textit{`Asterix', `Breakout', `Freeway', `Seaquest', `Atlantis', `DemonAttack', `Frostbite', `Gopher', `TimePilot'}\}). 
Note that for each of these ALPT variants, we perform a \emph{single} pretraining phase and then \emph{multiple} finetuning phases (one for each target game), thus showing that a single pretrained model can be transferred to various target games.

We compare pretraining with ALPT to training DT1-IDM on each target game alone. Results are presented in Figure \ref{alptall}, and show that target game performance improves with more source games.  
%We selected $5$ as a practical baseline in our initial experiments, but as more data becomes available, it is possible to achieve better performance.
\begin{figure}

\centering
\subfloat{\includegraphics[width = 1.6in]{Pong.pdf}} 
\subfloat{\includegraphics[width = 1.6in]{SpaceInvaders.pdf}}
\subfloat{\includegraphics[width = 1.6in]{StarGunner.pdf}} \\
\centering
\subfloat{\includegraphics[width = 1.6in]{MsPacman.pdf}} 
\subfloat{\includegraphics[width = 1.6in]{Alien.pdf}} 
\caption{We evaluate performance of ALPT with a higher number of source games. We show performance of ALPT trained on 36 Atari source games (ALPT-36) and ALPT trained on 9 Atari source games (ALPT-9). We find that performance generally improves with more source games.}
\label{alptall}
\end{figure}
\subsection{Can ALPT help when source and target have disjoint action spaces?}
The previous experiments have included at least one source game during pretraining whose the action space overlaps with that of the target environment.  The next set of experiments aims to explore pretraining on source environments where the action space ($\mathcal{A}_d$) is disjoint with the target environment action space ($\mathcal{A}_*$), that is, $\mathcal{A}_d \cap \mathcal{A}_* = \emptyset$.  To do so, we use \emph{Freeway} as our single source environment dataset. In \emph{Freeway}, the action space consists of \{Up, Down\}. In contrast, a game such as \emph{Breakout} has an action space  consisting of \{Left, Right\}. 
Surprisingly, using \emph{Freeway} as a source environment and \emph{Breakout} as a target environment still yields significant benefits for ALPT. 
We present this result in Figure~\ref{disres}, as well as a variety of other choices for the target environment, none of which share any actions with the source environment \emph{Freeway}.

We hypothesize that performance improvements are not unexpected due to the already known broad generalization capabilities of transformer architectures. It may also be the case that, despite the action spaces being disjoint, they still exhibit similar structure. For example, a top-down action space and a left-right action space are structurally opposite to each other in terms of movement, differing only in orientation.  This similar structure is potentially learned and leveraged during finetuning.


%during finetuning, generalizing with similar state spaces but disjoint actions. Again, we fine performance improvements by using a multi-environment dataset to pretrain our method (ALPT) when compared to training on only the action limited target dataset.

%We evaluate this phenomenon by training on multiple different games with disjoint actions to our target environment, with the results being consistent across these disjoint pretraining environments.

%todo: add breakout
%todo: standardize legend 
\begin{figure}
\centering
\subfloat{\includegraphics[width = 1.3in]{atlantis_disjoint.png}} 
\subfloat{\includegraphics[width = 1.25in]{Breakout.pdf}}
\subfloat{\includegraphics[width = 1.25in]{demon_attack_disjoint.png}} 
\centering
\subfloat{\includegraphics[width = 1.25in]{pong_disjoint.png}}\\ 
\subfloat{\includegraphics[width = 1.3in]{phoenix_disjoint.png}} 
\subfloat{\includegraphics[width = 1.3in]{name_this_game_disjoint.png}}  
\subfloat{\includegraphics[width = 1.25in]{enduro_disjoint.png}}
\subfloat{\includegraphics[width = 1.25in]{space_invaders_disjoint.png}} 

\caption{We evaluate performance of ALPT when source and target games have disjoint action spaces. In each of these plots we pretrain using a single source game Freeway. Despite the disjoint action space, we still see benefits of pretraining.}
\label{disres}
\end{figure}
\vspace{-0.5cm}
\subsection{Do ALPT's benefits persist in other domains ?}
We now demonstrate ALPT's benefits on an action-limited navigation task. This corresponds to a scenario where we have densely annotated navigation maps for a set of source regions but only a sparsely annotated navigation map for a target region. We would like to evaluate whether ALPT, pretrained on source regions with abundant action labels, can generalize to navigating in a target region (of a different layout) with limited action labels.

\textbf{Maze Navigation Environments.} %To showcase the practicality of ALPT, we evaluate it on a discrete action maze navigation task.  
To answer the above question, we consider a gridworld navigation task where an agent seeks to navigate to a goal location in a $20 \times 20$ 2D maze from a random starting location to a random goal location using 4 discrete actions: \{Up, Down, Left, Right\}. The agent receives a reward of $1$ at the goal state and $r=0$ otherwise. To collect the offline training datasets, we follow \citet{yang2022chain,zhang2018study} to algorithmically generate maze layouts with random internal walls that form blocked or tunneled obstacles as shown in Figure~\ref{fig:maze}. We start with blocked obstacles, and generate one \emph{source} maze from which we collect $500$ trajectories with full action labels. We then use a different random seed to generate the \emph{target} maze, from which we collect $500$ trajectories with only $250$ action labels ($0.5\%$ of the full action labels). We then train the IDM of ALPT-Blocked on both the source and target datasets, labeling the missing actions from the target game, and train DT all at the same time (no separate finetuning stage). ALPT-Blocked and other baselines are evaluated in the target maze only.

%Specifically, we consider $2$ configurations of the Maze, where $10$ different blocks are randomly placed in each configuration as shown in Figure~\ref{fig:maze} (a). One of these configurations is used to generate an action rich source environment ($500$ trajectories) dataset.  The other is used to generate an action limited \emph{target} environment dataset (with $250$ labelled actions).  Training on these environments is referred to as the ALPT-Blocked method.  

\textbf{Results.} Performance in the target maze environment with limited action labels is presented in Figure~\ref{fig:maze} (b). ALPT-Blocked trained on both source and target mazes allows us to solve the target task twice as fast compared to only training on the target maze without access to another source maze. To further illustrate the benefit of multi-environment training on more diverse data, we introduce the tunnelled maze, and train ALPT-Blocked+Tunnelled on $500$ trajectories with full actions from a source blocked maze and a source tunneled maze, respectively, as well as $250$ action samples of the target blocked maze. Training on both tunnelled and blocked mazes enables greater dataset diversity, which further improves  generalization, leading to even faster convergence on the target task. %Training ALPT, with source environments consisting of both Tunneled and Blocked datasets is referred to as ALPT-Tunneled. We also consider an additional maze where the walls are tunnels as part of our action rich source environment dataset, which we refer to as Tunneled. 
These preliminary results on navigation suggest that multi-environment pretraining can benefit a broad set of tasks.
%We present this result to complement our dramatic performance improvements in the ALE, showing that our method is capable outside of that domain.

%We note that this maze configuration is rather simple, with the single-game DT1-IDM baseline being able to solve the task in slightly less than $5000$ steps.  We show that using ALPT to train with multiple source configurations allows us to solve the task approximately twice as fast. 


\begin{figure}
    \centering
    \subfloat[]{\includegraphics[width = 0.9in]{maze_fig.png}} \hspace{2cm}
    \subfloat[]{\includegraphics[width = 1.9in]{20x20maze.png}}
    \caption{(a) An example diagram of the Blocked (above) and Tunneled (below) mazes.  The green cell is the goal state. (b) Evaluation performance while training on the 20x20 Maze dataset. Higher score is better.  The shaded area represents the standard deviation over $3$ random seeds.  The action limited target dataset contains $250$ labelled actions in these experiments.}
    \label{fig:maze}
\end{figure}

\section{Conclusion}
We explored the problem of learning agent policies from action limited datasets. 
Inspired by the paradigm of large-scale, multi-environment training, we proposed ALPT, which pretrains an inverse dynamics model (IDM) on multiple environments to provide accurate action labels for a decision transformer (DT) agent on an action limited target environment dataset.
Our experiments and ablations highlight the importance of pretraining the IDM, as opposed to pretraining the DT agent alone.  Our results  support the importance of generalist inverse dynamics models as an efficient way to implement large-scale RL.
As more labelled data becomes available for training offline RL agents, ALPT provides an efficient way of bootstrapping performance on new tasks with action limited data.
%This is arguably distinct from more common paradigms for large-scale training of RL agents, and
%We hypothesize that the inverse dynamics model may be a better mechanism to exploit shared structure between tasks.
%We hope that our results encourage researchers to continue investigating these and other alternative approaches for large-scale RL agent training.
%As more labelled decision making data becomes available for training offline RL policies, we present a fast way of bootstrapping for learning effectively with the vast amount of unannotated sequential decision making data already available.
%problem of utilizing vast quantities of unlabelled data, even from environments where the action space is not shared for training sequence models.  Given an action limited target environment dataset, we highlight improvements by pretraining with labelled data from environments that differ from our target evaluation task. We propose a novel dynamics model training regime, applicable in scenarios where we have access to a large amount of labelled data from similar problems but limited labelled data in a target environment.  Our results indicate the difficulty of traditional sequence models to learn under limited data, and provide an effective solution which does not require additional labels in the target environment.  We hypothesize that this dynamics model better exploits shared structure between tasks for downstream generalization.  ALPT is both scale-able and is highly applicable for bootstrapping.  As more labelled decision making data becomes available for training offline RL policies, we present a fast way of bootstrapping for learning effectively with the vast amount of unannotated sequential decision making data already available.

\subsection{Limitations}
The largest limitation of ALPT is the assumption that we would have plentiful labelled data from related environments.  One interesting fact we uncover is that this data does not have to be based on the same action space as the desired target environment, indicating the versatility of ALPT and its ability to ingest diverse source environment training data.  We also caution that we have only evaluated ALPT so far on limited, self-contained video game tasks and simple navigation environments. %Our method may not generalize as widely as comparable large language or computer vision models do using internet-sized datasets during pretraining.  
We hope that as more labelled data becomes available in RL domains, ALPT will have wider applicability, allowing RL agents to scale and bootstrap to new environments.  It would also be useful to investigate further how much labelled data from a limited set of source environments is required to be able to handle a much larger set of unlabelled datasets. 
\newpage

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix
\section{Experiments with no DT pretraining}
\label{app:alpt-no-dt}
In the following set of experiments, we pretrain only the IDM component of ALPT and not the DT.  We show the finetuning performance results for the Narrow set of Atari games in Figure~\ref{fig:nodt}. Note that the axis here is up to $100$k steps as opposed to 1M for the figures in the main text.
\begin{figure}[H]
    \centering
            \subfloat{\includegraphics[width = 1.75in]{asterix_nodt.png}} 
        \subfloat{\includegraphics[width = 1.65in]{breakout_nodt.png}} \\
    \subfloat{\includegraphics[width = 1.65in]{freeway_nodt.png}} 
    \subfloat{\includegraphics[width = 1.75in]{seaquest_nodt.png}}
        \subfloat{\includegraphics[width = 1.75in]{space_invaders_nodt.png}}

    \caption{Evaluation game performance during finetuning of ALPT and DT$1$-IDM.  In these experiments we do not pretrain the DT. $100k$ steps are shown.}
    \label{fig:nodt}
\end{figure}


\section{Implementation Details}
In Table \ref{tab:params} we give the implementation details of our IDM and DT transformer architectures.

The IDM model is the same as the DT model, except that it is non-causal.  This is enforced by changing the attention mask to a matrix of all $1$ values in the IDM.

\begin{table}[H]
\caption{A summary of the transformer model parameters.}
\label{tab:params}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf Parameter} & 
\multicolumn{1}{c}{\bf Value} 
\\ \hline
{Layers} & $6$
\\
{Hidden Size} & $512$
\\
{Heads} & $8$ 
\\
{Batch Size} & $256$
\\
{Weight Decay} & $5 \times 10^{-5}$ 
\\
{Learning Rate} & $3 \times 10^{-4}$ 
\\
{Gradient Clipping} & $1.0$ 
\\
{$\beta_1, \beta_2$} & $0.9, 0.999$ 
\\
{Warm-up Steps} & $4000$ 
\\
{Optimizer} & LAMB 
\end{tabular}
\end{center}
\end{table}

\section{Experiments with Conservative Q-Learning (CQL)}
In this set of experiments, we examine the performance of Conservative Q-Learning (CQL) \citep{10.5555/3495724.3495824} trained on a dataset of $10,000$ frames, as opposed to $500,000$ in the original work (Table 3 of CQL, $1\%$ dataset size), from various Atari games utilized in our experiments.  In Table \ref{cql} we report the final evaluation performance on the game after training for $100$ iterations.  All implementation details are consistent with the original implementation in the cited work.  We utilize the CQL$(\mathcal{H}$) method.
\begin{table}[H]
\caption{The final evaluation game performance after training CQL for 100 iterations on a dataset of 10,000 labelled frames from each Atari game.}
\label{cql}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf Game Name} & 
\multicolumn{1}{c}{\bf Final Performance} 
\\ \hline
{Asterix} & $227.5$
\\
{Breakout} & $12.3$
\\
{Freeway} & $10.2$ 
\\
{Seaquest} & $236.0$
\\
{SpaceInvaders} & $250.9$ 
\end{tabular}
\end{center}
\end{table}

\end{document}
