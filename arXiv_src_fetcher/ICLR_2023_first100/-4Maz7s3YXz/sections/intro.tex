\vspace{-0.3cm}
\section{Introduction}
It is evident from recent studies that the memorization effect (or benign overfitting)~\cite{feldman2020does, feldman2020neural, bartlett2020benign, chatterji2020finite, muthukumar2020harmless} can be one necessary factor for overparametrized deep neural networks (DNNs) to achieve the close-to-optimal generalization error. From the empirical perspective, the works~\cite{feldman2020does, feldman2020neural} suggest that modern benchmark datasets, such as CIFAR10, CIFAR100~\cite{krizhevsky2009learning} and  ImageNet~\cite{krizhevsky2012imagenet}, always have very diverse data distributions, especially containing a large fraction of ``atypical'' samples. These atypical samples are both visually and statistically very different from the other samples in their labeled class. For example, the images in the class ``bird'' may have a variety of sub-populations or species, with many samples in the main sub-population and other (atypical) samples in less-frequent and distinct sub-populations. Since these atypical samples are deviated from the main sub-population, DNNs can only fit these atypical samples by ``memorizing'' their labels. While, memorizing or fitting these atypical samples will not hurt the DNNs' performance, but can help DNNs to correctly classify the atypical samples appearing in the test set and hence improve the test accuracy.

%During test, DNN models can utilize their ``memorization'' to correctly classify the atypical samples appearing in the test set, and consequently benefit the classification accuracy.

%\han{2. Similar to standard ERM, adv training also memorizes the labels of all training samples, and their adversarial counterpart. However, adv training still suffers from bad generalization. Moreover, when we fit or ``memorize'' more samples, the models performance will even decrease. }

%\han{3. So its natural to ask the question: in adversarial training, can the memorization effect still benefit model's performance or generalization? Or have different behavior in natural training?}


Similar to the classification models which are trained via empirical risk minimization (ERM) algorithms, adversarial training methods
~\cite{madry2017towards, kurakin2016adversarial} are also devised to fit the whole training dataset. Specifically, adversarial training minimizes the model's error against adversarial perturbations~\cite{goodfellow2014explaining, szegedy2013intriguing} by fitting the model on manually generated adversarial examples of all training data. Although adversarial training can fit and memorize all training data as well as their adversarially perturbed counterparts, they always suffer from both poor clean accuracy and adversarial accuracy (or robustness)\footnotemark on the test set~\cite{tsipras2018robustness, schmidt2018adversarially}. Recent study~\cite{rice2020overfitting} indicates that, during the adversarial training process, one model's test adversarial accuracy will even keep decreasing as it hits higher training adversarial accuracy (during the fine-tuning epochs). Thus, it is natural to ask a question: \textit{What is the effect of memorization in adversarial training? In particular, can memorizing (the labels of) the atypical samples and their adversarial counterparts benefit the model's accuracy and robustness?}

\footnotetext{Clean Accuracy: Model's accuracy on unperturbed samples. Adversarial accuracy: Model's accuracy on adversarially perturbed samples. Without loss of generality, this paper discusses the adversarial accuracy under $l_\infty$-8/255 PGD attack~\cite{madry2017towards}.} 

To answer this question, we first conduct preliminary studies to check whether memorizing atypical samples in adversarial training can benefit the DNNs' test performance, especially on those test atypical samples. In Section~\ref{sec:pre1}, we implement PGD adversarial training~\cite{madry2017towards} on CIFAR100 under ResNet18 and WideResNet models~\cite{he2016deep} and fine-tune them until achieving the optimal training performance.
%evaluate their clean \& adversarial accuracy on test atypical set.
%and show the DNNs like ResNet18 and WideResNet~\cite{he2016deep} have sufficient capacity to achieve close-to-perfect training performance. 
%They can fit almost all the clean and adversarial atypical samples during training. 
From the results in Section~\ref{sec:pre1}, we observe that {\it the memorization in adversarial training can only benefit the clean accuracy of test atypical samples.} With the DNNs gradually fit/memorize more atypical samples, they can finally achieve fair clean accuracy close to $\sim40\%$ on test atypical set.
However, the adversarial accuracy on test atypical set is constantly low ($\sim10\%$) during the whole training process, even though the models can fit almost all atypical (adversarial) training samples. Based on the theoretical study~\cite{schmidt2018adversarially}, the adversarial robustness is hard to generalize especially when the training data size is limited. Since each single atypical sample is distinct from the main sub-population and has rare frequency to appear in the training set, the data complexity for each specific atypical sample is also very low. Thus, its adversarial robustness can be extremely hard to generalize. Remarkably, the entire atypical set covers a non-ignorable fraction in complex datasets such as CIFAR100. Completely failing on atypical samples could be one important reason that contributes to the overall poor robustness generalization of DNNs. 


Furthermore, we find that {\it in adversarial training, fitting atypical samples will even hurt DNNs' performance of those ``typical'' samples (the samples in the main sub-population)}. In the Section~\ref{sec:pre2}, we again implement PGD adversarial training~\cite{madry2017towards} on CIFAR100 for several trails, which are trained with different amount of atypical samples existed. 
Based on the results from Section~\ref{sec:pre2}, an adversarially trained model on the training set without any atypical samples has 92\% clean accuracy and 52\% adversarial accuracy on the test typical set. While, the model trained with 100\% atypical samples included only has 85\%/44\% clean/adversarial accuracy respectively. In other words, atypical samples act more like \textit{``poisoning data''}~\cite{biggio2012poisoning} to deteriorate model performance on typical samples. In this paper, we demonstrate that the atypical samples which ``poison'' the models are the ones which deviated from the distribution of their own classes, but close to the distribution of other classes. Some examples are shown in Fig.~\ref{fig:atypical_samples}. For instance, an atypical ``plate'' in CIFAR100 has strong visual similarity to ``apples''. If DNNs memorize the features of this ``plate'' and predict any other images with similar features to be ``plate'', the DNNs cannot distinguish ``apples'' and ``plates''.

Motivated by our findings, we propose a novel algorithm called \textit{Benign Adversarial Training (BAT)}, which can prevent the negative influence brought by memorizing ``poisoning'' atypical samples, meanwhile preserve the model's ability to memorize those ``benign / useful'' atypical samples. It is worth mentioning that, by fitting those ``benign'' atypical samples, the BAT method can achieve good clean accuracy on the atypical set. Compared with PGD adversarial training~\cite{madry2017towards} when it achieves the highest adversarial robustness, BAT has higher clean accuracy and comparable adversarial accuracy. Compared to other popular variants of adversarial training such as~\cite{zhang2019theoretically, wang2019improving, zhang2020geometry}, BAT is the only one obtaining both better (or comparable) clean and adversarial accuracy than~\cite{madry2017towards}, on complex datasets such as CIFAR100 and Tiny~Imagenet~\cite{le2015tiny}. 







