\vspace{-0.4cm}
\section{Related Works}
\vspace{-0.3cm}
\textbf{Memorization and atypical samples.} The memorization effect of overparameterized DNNs have been extensively studied both empirically~\cite{zhang2016understanding, nakkiran2019deep} and theoretically~\cite{bartlett2002rademacher}. From traditional views, the memorization can be harmful to the model generalization, because it makes DNN models easily fit those outliers and noisy labels.  However, recent studies point out the concept of ``benign overfitting''~\cite{bartlett2020benign, feldman2020does, feldman2020neural}, which suggests the memorization effect necessary for DNNs to have extraordinary performance on modern machine learning tasks. 
Especially, the recent work~\cite{feldman2020neural} empirically figures out those atypical/rare samples in benchmark datasets and show the contribution from memorizing atypical samples to the DNN's performance. Besides the work~\cite{feldman2020neural}, there are also other strategies~\cite{carlini2019distribution} to find atypical samples in training dataset.  Notably, our work is not the first effort to study the influence of memorization on DNN's adversarial robustness. A previous study~\cite{sanyal2020benign} illustrates that memorizing the mis-labeled samples might be a reason to cause the DNNs' adversarial vulnerability. In our paper, we focus on atypical samples, which appear much more frequently in common datasets, and we study their impacts especially on adversarial training algorithms~\cite{madry2017towards, zhang2019theoretically,chatterji2020finite, muthukumar2020harmless}.\\
\textbf{Adversarial robustness. } 
Adversarial training methods~\cite{madry2017towards, zhang2019theoretically, wang2019improving, zhang2016understanding, rice2020overfitting} are considered as one of the most reliable and effective methods to protect DNN models against adversarial attacks~\cite{goodfellow2014explaining, xu2019adversarial}. However, there are several intrinsic properties of adversarial training which requires deeper understandings. 
For example, they always suffer from poor robustness generalization~\cite{ schmidt2018adversarially, rice2020overfitting}, and they always present strong trade-off relation between clean accuracy vs. robustness~\cite{tsipras2018robustness, zhang2019theoretically}. Our work aims to study these properties from the data perspective and demonstrate the significant connection of the memorization effect with these properties.

\vspace{-0.4cm}
\section{Conclusion}
\vspace{-0.2cm}
In this paper, we draw significant connections of the memorization effect of deep neural networks with the behaviors of adversarial training algorithms. Based on the findings, we devise a novel algorithm BAT to enhance the performance of adversarial training. The findings of the paper can motivate the futures studies in building robust DNNs with more attention on the data perspective.