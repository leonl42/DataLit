\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[preprint]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2021}
\input{math_commands.tex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{mathtools}% Loads amsmath
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}
%\usepackage{subfig}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{comment}
\usepackage{csquotes}
\usepackage{subcaption}

\newcommand{\han}[1]{\textcolor{red}{#1}}
\newcommand{\xr}[1]{\textcolor{red}{#1}}
\newcommand{\yx}[1]{\textcolor{purple}{#1}}
\newcommand{\jt}[1]{\textcolor{brown}{#1}}


\renewcommand{\eqref}[1]{(\ref{#1})}  %%%  Use  Eq.~\eqref{
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}
\newtheorem{corollary}{Corollary}


\title{Towards the Memorization Effect of Neural Networks in Adversarial Training}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Han Xu, Xiaorui Liu, Wentao Wang, Anil K. Jain. Jiliang Tang\\ 
  Department of Computer Science and Engineering\\
  Michigan State University\\
  \And Wenbiao Ding, Zhongqin Wu, Zitao Liu\\
  TAL Education Group\\
  
}



\begin{document}

\maketitle

\begin{abstract}
Recent studies suggest that ``memorization'' is one important factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: \textbf{(a)} Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and \textbf{(b)} Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose \textit{Benign Adversarial Training (BAT)} which can facilitate adversarial training to avoid fitting ``harmful'' atypical samples and fit as more ``benign'' atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets such as CIFAR100 and Tiny~ImageNet.
%\jt{we need to discuss some experimental conclusions and findings}


\end{abstract}

\input{sections/intro}
\input{sections/def}
\input{sections/preliminary1}
\input{sections/preliminary2}
\input{sections/method}
\input{sections/experiment}
\input{sections/rel}
\bibliographystyle{unsrt}
\bibliography{sample}
%\input{sections/checklist}


\input{sections/append0}
\input{sections/append1}
\input{sections/append2}
\input{sections/append3}


\end{document}
