\section{Related Works}
\label{sec:related_works}

\iffalse
To show the difference and improvement of our work to other 3DR works, we measure 3DR into three major standards based on their training strategies(supervised/unsupervised), generalization abilities, and representations(explicit/implicit), as shown in Table \ref{table:3dr class}.
\fi

In this section, we discuss details of each method based on the three standards in Table \ref{table:3dr class}. 

\noindent
\textbf{Generalizable Explicit depth Reconstruction}.
Initially, 3D mesh reconstruction relies on depth estimation, the typical way is to perform TSDF fusion from depth and use marching cube to recover mesh from TSDF. Fully supervised training is the first choice when available depth map ground truth is used. These models take as input single image, and explore different model designs \cite{deepv2d,CNMNet,neuralrgb,mvdepthnet,dpsnet}, but all regress to 2D depth map and directly supervise with depth ground truth. However, since  annotations are missing and unreliable in many cases, self-supervision has to be used without depth annotations. With camera poses, the depth can be estimated by binocular stereo matching, leveraging epipolar-geometric consistent matching between image pixels \cite{mvdepthnet}, or replacing pixels with extracted deep features to produce a matching cost volume \cite{ESTDepth, deepvideomvs}. Without camera poses, it becomes a structure-from-motion (SFM) problem, the network can jointly estimate depth and pose with reconstruction error \cite{monoindoor,p2net,movingindoor,structdepth,monodepth2}. However, self-supervised depth has scale drifting and ambiguity problems, which need other priors to recover real scale. Alternatively, we directly regress voxel-SDF from network in real scale.

\noindent
\textbf{Generalizable Supervised Explicit 3D mesh Reconstruction}.
Directly regressing voxel-SDF is generalizable and straightforward, the advantages are real-scale, time and memory efficient. However the huge volume of 3D convolution \cite{atlas} requires strong supervise signals - voxel-SDF ground truth fused from depth ground truth, although later works \cite{neucon, vortx} shrink the volume by using 3D sparse convolution, SDF ground truth is still inevitable, which is not easy to obtain in many cases. In this work, we explore pure self-supervision to train voxel-SDF free from any SDF or depth annotations.

\noindent
\textbf{Unsupervised implicit 3D Reconstruction}. NeRF \cite{nerf} is the representative of implicit 3DR. Many works \cite{mvsnerf, pointnerf, pixelnerf, nsvf, plenoxels, instant, blocknerf, nerfusion} improve the baseline NeRF from different aspects. MVSNeRF \cite{mvsnerf} speeds up NeRF in a cost volume built within a multi-view-system, while PointNeRF \cite{pointnerf} speeds up by introducing point cloud from estimated depth maps to sample the ray-level key points only with the nearby point cloud. However, all these works require per-scene optimization and are limited to implicit RGB synthesis. PixelNeRF \cite{pixelnerf} improves volume rendering function by allowing rays from one view to refer to key points on rays from other views, which endows NeRF a limited generalization by using fewer views. Later works \cite{mpi-nerf, ibrnerf} explored increasing generalization by adding an explicit image encoder on top of positional encoding that enables NeRF to generalize to similar but unseen scenes. However, these works are still implict NeRF where the outputs are synthesized RGB image, unlike our desired explicit 3D mesh.

\noindent
\textbf{Non-Generalizable Unsupervised Explicit 3D mesh Reconstruction}.
The standard NeRF for implicit 2D view synthesis has been successfully extended to explicit 3D mesh representation \cite{hnerf, isdf, manhattansdf, monosdf, volsdf, neuris, neus, neuralangelo, neuralwarp, unisurf}. With specified 3D coordinates and pose as input, these models estimate SDF values wrt. ray-level key points. The core design is a density function converting per-point SDF estimation to opacity values for volumetric rendering \cite{volsdf}, where the ray-level SDF values are queried to obtain rendered pixel intensity, which brings SDF to the reconstruction loss. Later works add more priors to supervise together with SDF-NeRF loss. \cite{manhattansdf} uses supervised pre-trained depth and semantic segmentation estimator, while \cite{monosdf} and \cite{neuris} use supervised pre-trained depth estimator and surface normal detector. However, the generalization ability shown in implicit NeRF has not been successfully extended to explict SDF-NeRF, which means once trained, the SDF-NeRF can only estimate 3D mesh of a specific scene. Our work aims to keep the explicit representation, self-supervised training, while enables generalization.

\noindent
\textbf{Motivation.} The question is that If these per-scene optimization methods are fast enough, then there's no disadvantage to them. However, non-generalizable methods take a long time to converge per scene. Neuralangelo\cite{neuralangelo} takes over 4 hours(single 3090Ti) to converge on a simple lego toy and almost a day for a big room, which is comparable to the generalized methods (ours 36 hours training on single 3090Ti but once trained can be used on different indoor scenes). Non-generalizable methods can be used where a scene is accessed repeatedly and unchanged, such as a city block, long training is acceptable as once trained it can be used for a while unless the scene changes. However, generalizable methods are better when different scenes need to be reconstructed ASAP and no time for per-scene optimization for hours, such as a real estate agent visits 20 different buildings in a day to scan 3D mesh models for unseen rooms and send it to clients right away. One compromise is part optimization but it is only suitable for unchanged scenes, whenever the scene is changed, they need to retrain the NeRF of corresponding part, while ours donâ€™t need retrain and can infer the new building in real time. We adapted NeuralRecon as backbone, our inference speed is 30 ms per frame (single 2080Ti).




