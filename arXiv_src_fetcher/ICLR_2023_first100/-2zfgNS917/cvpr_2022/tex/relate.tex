

\section{Related Works}
\paragraph{Camera-based 3D Object Detection.} Camera-based 3D object detection has been widely used for applications like autonomous driving since its low cost compared with LiDAR-based detectors. FCOS3D \cite{b12} first predicts the 3D attributes of objects through the features around 2D centers and PGD \cite{b17} utilizes the relational graphs to improve the depth estimation for 3D monocular object detection. Further, MonoDETR~\cite{b47} introduces DETR-like~\cite{b18} architectures without complex post-processing.
Recently, Birdâ€™s-Eye-View~(BEV), as a unified representation of surrounding views same as LiDAR-based detector, has attracted much attention. DETR3D \cite{b13} follows the DETR \cite{b18} to adopt the 3D reference points in BEV space by using object queries. BEVDet \cite{b19} utilizes the Lift-Splat-Shoot~(LSS) operation \cite{b20} to transform 2D image features into 3D Ego-car coordinate to generate 3D BEV feature. PETR \cite{b21} obtain the 3D position-aware ability by 3D positional embedding. Inspired by the recently developed attention mechanism, BEVFormer \cite{b11} and PolarFormer \cite{b22} automates the camera-to-BEV process with learnable attention modules and queries a BEV feature according to its position in 3D space. To further improve the detection performance, the temporal information has been introduced in BEVDet4D \cite{b23} and PETRv2 \cite{b24}, which achieve significant performance enhancement. Moreover, BEVDepth \cite{b7} observes that accurate depth estimation is essential for BEV 3D object detection supervised by projected LiDAR points. MonoDETR-MV~\cite{b48} proposes a depth-guided transformer for multi-view geometric cues, but predicts only foreground depth map without dense depth supervision. As a LiDAR-to-camera learning scehme, our TiG-BEV leverages the pre-trained LiDAR-based detector to improve the performance of camera-based detectors for multi-view BEV 3D object detection.

\paragraph{Depth Estimation.} Depth estimation is a classical problem in computer vision. These method can be divided into single-view depth estimation and multi-view depth estimation. Single-view depth estimation is either regarded as a regression problem of a dense depth map or a classification problem of the depth distribution. \cite{b26, b27, b28, b29, b30} generally build an encoder-decoder architecture to regress the depth map from contextual features. Multi-view depth estimation methods usually construct a cost volume to regress disparities based on photometric consistency \cite{b31, b32, b33, b34, b35, b62}. For 3D object detection, previous methods~\cite{b61,b51,b52} also introduce additional networks for depth estimation to improve the localization accuracy in 3D space.
Notably, MonoDETR \cite{b47,b48} proposes to only predict the foreground depth maps instead of the dense depth values, but cannot leverage the advanced geometries provided by LiDAR modality. Different from them, our TiG-BEV conducts inner-depth supervision that captures local sptial structures of different foreground targets.

\paragraph{Knowledge Distillation.} Knowledge Distillation has shown very promising ability in transferring learned representation from the larger model (teacher) to the smaller one (student). Prior works \cite{b37, b38, b39, b40} have been proposed to help the student network learn the structural representation for better generalization ability. These methods generally utilize the correlation of the instances to describe the geometry, similarity, or dissimilarity in the feature space. The following methods extend the teacher-student paradigm to many vision task, demonstrating its effectiveness including action recognition \cite{b41}, video caption \cite{b42}, 3D representation learning~\cite{b59,b49,b50,b60}, object detection \cite{b43, b44} and semantic segmentation \cite{b45, b46}. However, only a few of works consider the multi-modal setting between different sensor sources. For 3D representation learning, I2P-MAE~\cite{b49} leverages masked autoencoders to distill 2D pre-trained knowledge into 3D transformers. UVTR \cite{b25} presents a simple approach by directly regularizing the voxel representations between the student and teacher models. BEVDistill \cite{b9} transfer knowledge from LiDAR feature to the cam feature by dense feature distillation and sparse instance distillation. Our TiG-BEV also follows such teacher-student paradigm and effectively distills knowledge from the LiDAR modality into the camera modality,


%However, they overestimated the prior of spatial order while neglected the issues of semantic mismatch, \ie, the pixels of teacher feature map often contains richer semantic compared to that of student on the same spatial location. We found that some works~\cite{Park2019RelationalKD, Passalis2018LearningDR,Peng2019CorrelationCF,Tung2019SimilarityPreservingKD,Yim2017AGF,huang2017like,Liu2021ICKD}, though unintended, have been proposed to relax the spatial constrain during feature transfer. Typically, they defined the relational graph, and similarity matrix in the feature space of teacher network and transferred it to the student network. For instances, Tung and Mori~\cite{Tung2019SimilarityPreservingKD} calculated the similarity matrix where each entry encoded the similarity between two instances. Liu \etal~\cite{Liu2021ICKD} measured the correlation between channels by inner-product. They condensed and compressed the entire feature to some properties (often scalar) and thus collapsed the spatial information. On the other hand, such process damaged the original teacher feature and may lead to sub-optimal solution. 

%The spread of KD has also driven some methods designed for specific vision tasks including video captioning~\cite{pan2020spatio}, action recognition~\cite{wang2019progressive,cui2020knowledge}, object detection~\cite{chen2017learning,zhang2020improve,dai2021general} and semantic segmentation~\cite{liu2019structured,he2019knowledge,Wang2020IntraclassFV}. Regarding the semantic segmentation, these methods are indeed related to relation knowledge distillation which computes similarity matrix~\cite{Tung2019SimilarityPreservingKD}. To investigate the potential of our method, we also adapt the method to semantic segmentation with hierarchical distillation.


