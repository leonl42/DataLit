% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{threeparttable}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{color, colortbl}
\newcommand{\lsh}[1]{\textcolor{magenta}{ (lsh: #1)}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
% \newcommand{\myplus}[1]{\color{green}{\tiny{$+$#1}}}
% \newcommand{\myminus}[1]{\color{red}{\tiny{$-$#1}}}
\newcommand{\myplus}[1]{\color{green}{\tiny{}}}
\newcommand{\myminus}[1]{\color{red}{\tiny{}}}
\newcommand{\xd}[1]{\color{orange}{\tiny{$-$}#1}}

\newcommand\mypara[1]{\vspace{1mm}\noindent\textbf{#1}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{7432} % *** Enter the CVPR Paper ID here
\def\confName{ICCV}
\def\confYear{2022}


\newcommand{\ky}[1]{{\color{blue}{#1}}}
\newcommand{\KY}[1]{{\color{blue}{\bf #1}}}

% \newcommand{\sf}[1]{{\color{red}{#1}}}
\newcommand{\SF}[1]{{\color{red}{\bf #1}}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
%\title{Self-assembling Knowledge Distillation via Semantic Position Encoding}

\title{TiG-BEV: Multi-view BEV 3D Object Detection via\\Target Inner-Geometry Learning}

%\title{Self-assembling Knowledge Distillation with Semantic Alignment}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

\input{cvpr_2022/tex/abstract}
\input{cvpr_2022/tex/intro}
\input{cvpr_2022/tex/relate}
\input{cvpr_2022/tex/method}
\input{cvpr_2022/tex/exp}

\section{Conclusion}
In this paper, we propose a novel target inner-geometry learning framework that enables the camera-based detector to inherit the effective foreground geometric semantics from the LiDAR modality. We first introduce an inner-depth supervision with target-adaptive depth reference to help the student learn better local geometric structures. Then, we conduct inner-feature distillation in BEV space for both channel-wise and keypoint-wise, which contributes to high-level inner-geometry semantics learning from the LiDAR modality. Extensive experiments are implemented to illustrate the significance of TiG-BEV for multi-view BEV 3D object detection. For future works, we will focus on exploring multi-modal learning strategy that can boost both camera and LiDAR modalities for superior real-world perception.

% This work develops a framework for knowledge distillation through a target-aware transformation that enables the student to aggregate the useful semantic over itself to enhance the expressivity of each pixel, which allows the student to act as a whole to mimic the teacher rather than minimize each partial divergence in parallel.
% Our method is successfully extended to semantic segmentation by the proposed hierarchical distillation consisting of patch-group and anchor-point distillation, designed to focus on local feature and long-range dependency. We conduct thorough experiments to validate the effectiveness of the method and advance the state-of-the-art.

%\section{Discussion}
%\textbf{Potential negative societal impact.} Our method has no ethical risk on dataset usage and privacy violation as all the benchmarks are public and transparent.

%\textbf{Limitations.} There are some issues of interest that we would like to explore in the future: (1) Currently, we only select the last layer of the backbone network for distillation. It would be interesting to see the efficacy when multiple layers are get involved with distillation which has been explored by some works \cite{Zagoruyko2017PayingMA,chen2021distilling}. (2) Also, we didn't investigate the effectiveness on other applications like object detection, which may need to design the new objective to fit the nature of specific application. 

%%%%%%%%% REFERENCES
\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
