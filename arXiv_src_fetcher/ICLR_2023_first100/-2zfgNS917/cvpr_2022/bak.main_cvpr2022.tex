% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{color, colortbl}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\myplus}[1]{\color{green}{\tiny{$+$#1}}}
\newcommand{\myminus}[1]{\color{red}{\tiny{$-$#1}}}
\newcommand{\xd}[1]{\color{orange}{\tiny{$-$}#1}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Self-Assessment Knowledge Distillation via Cross-attention Feature Matching: A Theoretical Analysis}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
  Knowledge Distillation is a technique to transfer learned representation from teacher network to student network.
  Yet previous works attempt to transfer the inner-relation from teacher to student which is instance-specific and may dramatically vary from instances like Euclidean distance. Thus it will impede the compact network to reconstruct the geometry distribution of large network. 
  This work explores the relation that is informative and each instances holds by studying the similarity between pixels of the teacher feature. We empirically and theoretically demonstrate that in the inner-product space, the similarity, normalized by softmax function, of a pixel with itself should be 1 while 0 with other pixels, resulting in an identity matrix when formulated by matrix multiplication.
  Based on this finding, we propose the self-assessment that discriminatively measures the similarity against difference between pixels of student and teacher. The measurement result, named cross-attention, is expected to be the identical matrix, which is the necessary condition if the student feature is equivalent to teacher feature. 
  Therefore, the self-assessment can help the student adapt itself to approach the teacher.
  By means of the nature of identity matrix, the cross-attention will be applied on the student feature for further feature matching.
  In addition to image-level classification, our self-assessment KD can be extended to dense pixel-level classification with the proposed sequence-level and anchor-point distillation, which can facilitate the calculation of cross-attention by reducing the noise interference and computation complexity. 
  Extensive experiments over large scale benchmarks including ImageNet and COCOStuff10k are conducted to validate the effectiveness of our model, which advances the state-of-the art.
  We will release our code upon acceptance.
%   Knowledge Distillation is a technique to transfer learned representation from teacher network to student network.
%   Yet previous works attempt to transfer the instance-specific\xd{unclear about instance-specific, more details} relation or attention from teacher to student, which may dramatically vary from instances and impede the learning process\xd{what drawbacks that may lead to}.\xd{better give more explicit/visible drawbacks of previous KD}\xd{shall claim our motivation first, shall mention ``self-assessment"}
%   Rather, this work explicitly models the cross-attention\xd{what here cross-attention means as there are many cross-attentions} between student and teacher in a definitive\xd{replace with a better name} fashion, which is theoretically demonstrated to be the identity matrix\xd{what are the benefits using identity matrix}.
%   To achieve this goal, the student feature is required to perform self-assessment by measuring the similarity against difference between itself and the teacher feature. \xd{why need self-assessment and how self-assessment relates with ``cross-attention feature matching"} \xd{what are the merits of this KD}
% %   We theoretically demonstrate that the cross-attention should be the identity matrix if the student is able to mimic the teacher completely.
%   In addition to image-level classification, our self-assessment KD can be extended to dense pixel-level classification with the proposed sequence-level and anchor-point distillation, which are complementary and able to capture local and global attention respectively. \xd{explicit benefits of using our KD, more efficient? smaller model with better performance?}
%   Extensive experiments over large scale benchmarks including ImageNet and COCOStuff10k are conducted to validate the effectiveness of our model, which advances the state-of-the art.
%   We will release our code upon acceptance.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Recent advances \cite{He2016DeepRL} in the field of Convolution Neural Networks (CNNs) has boosted a great amount of vision tasks. In spit of the success, the application of these CNNs has been confronted with several challenges including energy consumption and computation cost. There has recently shown research interest to reduce the model size while maintaining a comparable performance, unlocking the hardware limitation and thus making it feasible to deploy on edge devices such as mobile phone. Knowledge Distillation (KD) promises a way to increase both accuracy and availability which transfers the knowledge from a superior network to an inferior network. 
% One of the research interest is to reduce the model size while maintaining a comparable performance, making it feasible to deploy on edge devices such as mobile phone. Knowledge Distillation (KD) is a way to transfer the knowledge from a superior network to an inferior network whose performance generally would be improved.

The concept of KD is first brought by Bucilua \etal \cite{bucilua2006model} and Hinton \etal \cite{Hinton2015DistillingTK} popularised this idea, which required the student to \textit{mimic} the teacher by minimizing the Kullback-Leibler Divergence between the output logits. The philosophy behind this idea is that superior network can learn the knowledge that inferior one is unable to discover, which is referred to \textit{dark} knowledge. Nevertheless, \cite{Hinton2015DistillingTK} has not explicitly considered the transfer of structural information. It has been shown that teacher possesses richer and more meaningful representation due to depth and width. Romero \etal \cite{Romero2015FitNetsHF} introduced the transfer between intermediate layers, which minimized the $L_2$ loss between features of student and teacher.

\begin{figure}
  \centering
  \begin{subfigure}{0.45\linewidth}
    \fbox{\rule{0pt}{1in} \rule{.9\linewidth}{0pt}}
    \caption{Covariance between feature points.}
    \label{fig:short-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \fbox{\rule{0pt}{1in} \rule{.9\linewidth}{0pt}}
    \caption{Self-attention of teacher.}
    \label{fig:short-b}
  \end{subfigure}
  \vfill
  \begin{subfigure}{0.45\linewidth}
    \fbox{\rule{0pt}{1in} \rule{.9\linewidth}{0pt}}
    \caption{Cross-attention before distillation.}
    \label{fig:short-c}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \fbox{\rule{0pt}{1in} \rule{.9\linewidth}{0pt}}
    \caption{Cross-attention after distillation.}
    \label{fig:short-d}
  \end{subfigure}
  \caption{(a) shows the statistics of covariance and correlation coefficient between feature points of the teacher feature cross spatial dimensions. (b) is an example of self-attention of the teacher feature approaching the identity matrix. We further theoretically demonstrate the self-attention of the teacher feature is to be an identity matrix. Base on this finding, this work designs the novel self-assessment loss for knowledge distillation which requires the cross-attention between student and teacher to be identity matrix. }
  \label{fig:short}
\end{figure}

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%   \caption{}
%   \label{fig:motivation}
% \end{figure}

Still, directly transferring the learned representation from teacher network to student network remains inefficient and difficult due to the gap of learning capacities. As pointed out by Cho and Hariharan \cite{Cho2019OnTE}, student, if lagging behind the teacher by a significant margin, is often unable to catch up with the teacher. Hence, researchers have devoted many effort to develop the soft transfer between student and teacher by mining the relations within the feature space. Existing works \cite{Park2019RelationalKD, Passalis2018LearningDR,Peng2019CorrelationCF,Tung2019SimilarityPreservingKD} have proposed the inner-relation transfer. Generally, the relation between or among the instances is measured in the feature space of teacher network, and then is transferred to the student network, where the same instances are require to have the same behaviour. However, the established relation is instance-specific, which means it will varies according to certain instances. For example, \cite{Park2019RelationalKD} measures the Euclidean distance between instances in the teacher's feature space and asks the student to maintain the same property. It's obvious that the Euclidean distance will change dramatically according to different instances, thus making the training process unstable. To alleviate the issue, the authors introduce the normalization by dividing the average distance of all the instances.

Another route is to explore the interactive relation, or say attention. Prior works \cite{Ji2021ShowAA,Chen2020CrossLayerDW} have proposed the attention mechanism to regulate the feature matching. The measured attention between student and teacher is used as guidance to loss balancing and neural layer reallocation. Such attention mechanism is primitive since it aggregates the attention between two layers into a single scalar and indeed does not change the nature of feature matching.

%Besides, AT \cite{Zagoruyko2017PayingMA} proposed to transfer the attention learned by the teacher network. 

% Another influential route is to let student share the same feature space with teacher. Romero \etal \cite{Romero2015FitNetsHF} introduced the transfer of intermediate layers between student and teacher. However, directly matching two layers of student and teacher would be inefficient due to the gap of learning capacity. As pointed out by Cho and Hariharan \cite{Cho2019OnTE}, student, if lagging behind the teacher by a significant margin, is often unable to catch up with the teacher, which deteriorates the performance of student. Hence, prior works \cite{Ji2021ShowAA,Chen2020CrossLayerDW} have proposed the attention mechanism to regulate the feature matching. Such attention mechanism, acting as global manner, could alleviate the issue of mismatching but fail to capture the local representation. For example, \cite{Ji2021ShowAA} produced a series of attention values bounded by the softmax function to weight the feature matching loss. 

% Some existing works \cite{Zagoruyko2017PayingMA} implicitly encourage the student to have the similar feature semantic with the teacher. 
% For example, \cite{Ji2021ShowAA} produced a series of attention values bounded by the softmax function to weight the feature matching loss. 

Starting from the perspective of interactive relation, this paper proposes the novel framework for knowledge distillation based on cross-attention mechanism, referred to self-assessment. The cross-attention provides the context that reveals the similarity against the difference between two feature space. Specifically, the student is required to perform self-assessment by measuring the inner-product of each pair of pixel-wise features between itself and teacher on where a softmax function is applied for normalization, resulting in the cross-attention matrix. An intuition to self-assessment is that since the student is enforced to approach the feature space of teacher, the pixel-wise feature in the associated position between student and teacher should be similar, otherwise distinct, which is reflected by inner-product. Different from the previous works based on inner-relation transfer, the proposed cross-attention has a definitive objective. We theoretically demonstrate that cross-attention matrix ideally should be an identity matrix, where diagonal (element 1) represents similarity and non-diagonal (element 0) means distinction. Note that the cross-attention module would not be used in the testing, thus would not increase model size and computation overhead.
 
The most important advantage of knowledge distillation is to transfer the knowledge regardless of network architectures and specific tasks, with dense classification has no exception, which, unfortunately, attracts less attention in the existing literature. Due to the large size of feature map in segmentation network, the calculation of cross-attention mainly suffers from two challenges, including computation complexity and noise interference. 
% Assuming the height and width of the feature map are $H$ and $W$, this means the computation complexity will reach $\mathcal{O}(H^2\cdot W^2)$. In practice, the order of magnitude of the feature map can be 2, which indicates the size of the cross-attention map, bounded by softmax function, will reach at $10^4\times10^4$. Such large attention map contains redundant information and will be interfered by noise inevitably. 
Motivated by the success of Transformer and ViT, we therefore propose the sequence-level distillation, which reshapes the original feature into a sequence of 2D patches and performs distillation in patch level. This procedure can significantly reduce computation complexity while retaining the local semantic. To compensate the lack of global attention, we further propose the anchor-point distillation, where the anchor point is extracted from the full-size feature map and then used for distillation. The proposed sequence-level and anchor-point distillation are able to capture local and global attention respectively and thus complementary.

To summarize, our contributions are:
\begin{itemize}
    \item We propose the self-assessment knowledge distillation, where the student is required to measure the cross-attention between itself and the teacher. The cross-attention provides the context allowing the student be aware of the discrepancy with the teacher.
    \item Unlike the previous relation-based methods that have instance-specific relation, we theoretically demonstrate the objective of the proposed cross-attention should be an identity matrix, which is unified and stable for training.
    \item Our method can be extended to semantic segmentation. We propose the sequence-level and anchor-point distillation to address the issues of computation complexity and the lack of global attention.
\end{itemize}

\section{Related Works}
The pioneer work of \cite{bucilua2006model} and \cite{Hinton2015DistillingTK} introduced the idea of knowledge distillation, which is a technique of model compressing. A practical application is to boost the compact network by means of the learned representation from the large network. To this end, Bucilua \etal \cite{bucilua2006model} asked the compact network (i.e. student) to match the output logits of the large network (i.e. teacher). Lately, Hinton \etal \cite{Hinton2015DistillingTK} additionally applied a temperature factor to the logits before softmax function. 

In order to capture the structural information, prior works \cite{Park2019RelationalKD, Passalis2018LearningDR,Peng2019CorrelationCF,Tung2019SimilarityPreservingKD} of inner-relation transfer have defined the geometric relation in the feature space of teacher network and transferred it to the student network. Peng \etal \cite{Peng2019CorrelationCF} measured the correlation between instances in the embedded feature space of teacher network and asked the student to mimic the correlation. Similarly, Tung and Mori proposed the similarity-preserving by measure the similarity between instances. Park \etal \cite{Park2019RelationalKD} computed the Euclidean distance and angle of the given instances in teacher feature space and required the student to hold the same property under identical instances. To capture the geometry, Passalis and Tefas \cite{Passalis2018LearningDR} used kernel density to estimate the conditional probability and delivered to student. Unfortunately, the above methods of inner-relation transfer have defined the instance-specific relation, which varies according to different instances and impedes the smaller network to mimic the teacher. In contrast, our proposed cross-attention set a definitive objective that is unified and stable for training.  

Since feature map contains rich semantic, Romero \etal \cite{Romero2015FitNetsHF} proposed the intermediate transfer by feature matching. However, it's unviable for the compact model to reconstruct all details of feature space of the large model. To facilitate the feature matching, Zagoruyko and Komodakis \cite{Zagoruyko2017PayingMA} proposed to transfer only the high response area (\ie attention) of the feature map. The rationale is that the high response area is dominating and meaningful, while the low response is discarded as noise. Chen \etal \cite{Chen2020CrossLayerDW} proposed the semantic calibration for neural layers reallocation by using attention weight factors between features of student and teacher, which allowed the student to learn from the most semantic-related teacher layer. In \cite{Ji2021ShowAA}, the similarities of the teacher feature with multiple student features were calculated and then were used as weight to balance the feature matching. Nevertheless, the attention mechanism used by above methods is primitive. AT \cite{Zagoruyko2017PayingMA} did not enable the student to perceive the interactive context with teacher. On the other hand, \cite{Chen2020CrossLayerDW,Ji2021ShowAA} did not take good advantage of the potential of attention mechanism since they integrated the attention context into the scalars. Rather, the cross-attention proposed by this work measures the similarity against difference between student and teacher across spatial dimensions, which allows the student to be aware of the discrepancy with teacher and adapt itself. 

Yet the aforementioned discussion is mainly limited in the circumstance of image classification. Semantic segmentation aims for pixel-level classification that is more difficult than image classification. Some works have been proposed for semantic segmentation in the fields of knowledge distillation regardless of great challenge. Liu \etal \cite{liu2019structured} introduced the pair-wise distillation that transferred the similarities among pixels and holistic distillation for high-order relations which was formulated by adversarial learning \cite{goodfellow2014generative}. To promote the learning process, He \etal \cite{he2019knowledge} pre-trained an autoencoder that compressed the teacher feature into more informative one and proposed the affinity distillation by computing the interactions between different positions across feature map. Moreover, Wang \etal \cite{Wang2020IntraclassFV} characterized the transfer of intra-class feature variation between feature map and its prototype obtained by averaging the pixels sharing the same prediction. These methods are indeed related to the concept of inner-relation transfer, especially self-similarity. Our proposed self-assessment computes the cross-attention between student feature and teacher feature across the spatial dimensions, which is different from previous works since they only compute the relation within the feature space of student and teacher individually. More importantly, our cross-attention is endowed with the definitive objective. To address the issues of computation complexity brought by large-size feature map in segmentation network, we further propose the sequence-level distillation and anchor-point distillation.

The rest of this paper is arranged as follows. We introduce the proposed method in Sec. \ref{sec:method}. We will empirically analyse the insight behind the formulation and theoretically demonstrate that the cross-attention matrix is to be the identity matrix. In Sec. \ref{sec:experiment}, extensive experiments upon large scale benchmarks including ImageNet and COCOStuff10k are presented to evaluate the effectiveness of the proposed method. Through the ablation and sensitivity analysis, we disclose the details of each proposed component.


\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{framwork_2.pdf}
  \caption{Illustration of our framework.(a) \textbf{Self-assessment Distillation}. Given a pair of features of student and teacher, the cross-attention map $\rm{attn}$ is first computed and then applied on the student feature to generate the output $\rm{out}$, which is then asked to minimize the L$_2$ loss with the corresponding teacher feature. (b) \textbf{Anchor-point Distillation}. Each color indicates a region. We use average pooling to extract the \textit{anchor} within a local area of the given feature map, forming the new feature of smaller size. The generated anchor-point features will participate the self-assessment distillation. (c) \textbf{Sequence-level Distillation}. Both teacher and student features are to be sliced and rearranged as sequences. We use MHA heads to calculate the cross-attention for the later self-assessment distillation.}
  \label{fig:framework}
\end{figure*}

%-------------------------------------------------------------------------
\section{Method}
\label{sec:method}
There is a temptation, though, to let the student perfectly mimic the teacher, which does not appear to be viable in reality because student is inferior to teacher in learning capacities. Rather, we introduce the cross-attention to assist the feature matching between student and teacher. The cross-attention is conditioned by a pair of teacher feature and student feature that reflects the context of two given features. In practice \cite{Cho2019OnTE,Ji2021ShowAA,Chen2020CrossLayerDW}, it's hard for the student to directly match (\ie reconstruct) a teacher layer. Prior methods used an attention \textit{weight} to regulate the loss function or decide the allocation of the teacher layers to student, which is a global manner. In contrast, our method allows the student to perform self-assessment through the cross-attention and be aware of the difference and similarity compared to teacher feature, which can facilitate the knowledge transfer. (See Figure~\ref{fig:framework} (a)).

This section first presents the general formulation of the proposed method in the scope of image classification. We then explain the insights of the model with the theoretical demonstration. Furthermore, we introduce the sequence-level and anchor-point distillation designed specifically for semantic segmentation.  

\subsection{Formulation}
\label{sec:formulation}
Suppose that teacher and student are differential functions, which are parameterised by CNNs in this work and denoted by $T$ and $S$. Let $f_t\in {\mathbb{R}^{H\times W\times C}}$ and ${f_s}\in \mathbb{R}^{H\times W\times C^{'}}$ denote the teacher feature and student feature, where $H$ and $W$ are the height and width of feature map, and $C$ represents the channel numbers. Without loss of generality, we presume that $C^{'}$ aligns with $C$. For convenient discussion, we first reshape both $f_s$ and $f_t$ into 2D matrices:
\begin{equation}
\begin{aligned}
    X={\rm{F}}(f_s)\in \mathbb{R}^{H\cdot W \times C},\\ Y={\rm{F}}(f_t)\in \mathbb{R}^{H\cdot W \times C},
\end{aligned}
\end{equation}
\noindent where $\rm{F}(\cdot)$ is a function that flattens the 3D feature tensor into 2D matrix and each raw of the matrix is associated with a pixel in the feature tensor.

To compute the cross-attention map $\rm{attn}$, the linear transformations will be applied on $Y$ and $X$ to generate the associated key and query. We then use Multi-Head Attention (MAH) to produce the cross-attention map based on the key and query:
\begin{equation}
    {\rm{attn}}=softmax(\rm{MAH}({\rm{query}},{\rm{key}}^\top)),
    \label{eq:attn}
\end{equation}

\begin{equation}
\begin{aligned}
    {\rm{key}}=\theta(Y),\;{\rm{query}}=\gamma(X),
\end{aligned}
\end{equation}

\noindent where $\theta(\cdot)$ and $\gamma(\cdot)$ are the linear transformations. In case that the channel numbers of $f_s$ do not match with that of $f_t$, $\gamma(\cdot)$ can help with alignment. In the setting of image classification, $\theta(\cdot)$ is set to identical mapping, while $\gamma(\cdot)$ consists of a $1\times1$ convolution layer followed a BN layer. MAH will partition the whole matrix into multiple sub-matrices and compute the cross-attention individually, which is formulated by matrix multiplication and normalized by the softmax function. Each raw of the $\rm{attn}$ represents the similarity along with the difference of a pixel of the student feature with respect to all the pixels of teacher feature. 

Hence, the resulting cross-attention map $\rm{attn}$ is of size $\mathbb{R}^{H\cdot W \times H\cdot W}$, which is acceptable considering that most classification networks have small feature map size on the top layers. On ImageNet, the spatial size of the 4-th feature layer of the ResNet18 is, for example, $7\times7$. 
Conditioned by a pair of key and query, the student is able to adapt itself to fit the feature space of the teacher through self-assessment.

The cross-attention map $\rm{attn}$ is used to apply to the student feature $X$ in order to generate the output $\rm{out}$, which is lately asked to minimize the L$_2$ loss with the teacher feature $Y$. Our feature matching loss with guidance of cross-attention can be described as :
\begin{equation}
    {\rm{out}}= {\rm{attn}} \odot X \in \mathbb{R}^{H\cdot W \times C},
    \label{eq:out}
\end{equation}
\begin{equation}
    \mathcal{L}_{\rm{FM}}= ||{\rm{out}}-Y||_2.
    \label{eq:fm}
\end{equation}
Here $\odot$ denotes matrix multiplication.

Finally, the total loss of our proposed method can be defined by:
\begin{equation}
    \mathcal{L}_{\rm{KL}}={\rm{KLD}}(\sigma(\frac{T(x)}{\tau}),\sigma(\frac{S(x)}{\tau})),
\end{equation}
\begin{equation}
\label{eq:objective}
    \mathcal{L}_{\rm{CA}}=\alpha\mathcal{L}_{\rm{CE}}+\beta\mathcal{L}_{\rm{KL}}+\epsilon\mathcal{L}_{\rm{FM}},
\end{equation}
where $\mathcal{L}_{\rm{KL}}$ is the loss proposed in \cite{Hinton2015DistillingTK}. $T(x)$ and $S(x)$ are the output logits given specific input $x$, $\tau$ is the temperature factor, and $\sigma(\cdot)$ is the softmax function. Here $\mathcal{L}_{\rm{CE}}$ is the standard Cross Entropy loss function.
$\alpha$, $\beta$ and $\epsilon$ are the weight factors to balance the loss. We find that our model benefits from $\mathcal{L}_{\rm{KL}}$. However, the model can achieve state-of-the-art without the help of $\mathcal{L}_{\rm{KL}}$, i.e., $\beta$ is set to 0.

\subsection{Empirical \& Theoretical Analysis}
This section provides some intuition to the formulation discussed above. Without loss of generality, let's remove the linear functions $\theta(\cdot)$ and $\gamma(\cdot)$ and consider only one attention head. The Eq. \ref{eq:fm} can be expressed in another way:
\begin{equation}
    softmax(X\cdot Y^{\top})\cdot X=Y.
\end{equation}
\noindent Here $softmax(X\cdot Y)$ is the cross-attention matrix which is applied to the student feature $X$. The objective for the student is to reconstruct the teahcer feature. Denote the optimum solution to $X$ as $\hat{X}$. The non-trivial solution indeed requires that $softmax(\hat{X}\cdot Y^{\top})=I$ and $\hat{X}=Y$.

Recall that each raw of $X$ and $Y$ corresponds to a pixel in the original feature tensor. By means of matrix multiplication, it calculates the inner-product of each paired pixels between student and teacher, resulting the cross-attention matrix. The inner-product between two pixels measures the similarity against difference, and it's normalized by the softmax function. Since the cross-attention matrix is required to be the identity matrix, this can be interpreted that the distance, reflected by the inner-product, of the associated positions between $f_s$ and $f_t$ should be as close as possible, otherwise distant. This is the necessary condition if $\hat{X}=Y$ holds.

We now begin to give a theoretical analyse to the existence of the solution $\hat{X}$. Because student feature is expected to match the teacher feature, we have $\hat{X}=Y$. Thus, we need to prove that $softmax(Y,Y^{\top})=I$. We presume that the elements of $Y$ is Gaussian distribution and each raw is not linearly dependent from each other. Here $Y$ can be represented as:
\begin{equation}
\begin{aligned}
   Y=[{y_1}^{\top},{y_2}^{\top},{y_3}^{\top},\dots,{y_N}^{\top}]^{\top},\\
\end{aligned}
\end{equation}
\noindent where $Y$ has $N=H\cdot W$ raw vectors. Suppose that $y_i$ and $y_j$ are two distinct vectors, they can be described as:
\begin{equation}
\begin{aligned}
   y_i=[y_{i,1},y_{i,2},y_{i,3},\dots,y_{i,C}],\\
   y_j=[y_{j,1},y_{j,2},y_{j,3},\dots,y_{j,C}],\\
\end{aligned}
\end{equation}
where $(1\leq i,j\leq N)$ and each vector is of length $C$. The expected value of the inner-product of two vectors can be given by:
\begin{equation}
\label{eq:in_prd_same}
    \begin{aligned}
       \mathbb{E}\langle y_i,y_i\rangle &= \mathbb{E}(y_{i,1}^2+y_{i,2}^2+y_{i,3}^2+\dots+y_{i,C}^2) \\
       &=\sum_{k=1}^{C}\mathbb{E}y_{i,k}^2= \sum_{k=1}^{C}(\mu_{i,k}^2+\sigma_{i,k}^2)=C ,
    \end{aligned}
\end{equation}

\begin{equation}
\label{eq:in_prd_diff}
    \begin{aligned}
       \mathbb{E}\langle y_i,y_j\rangle 
       &= \mathbb{E}(y_{i,1}\cdot y_{j,1}+y_{i,2}\cdot y_{j,2}+\dots+y_{i,C}\cdot y_{j,C}) \\
       &=\sum_{k=1}^{C}\mathbb{E}(y_{i,k}\cdot y_{j,k})\\
       &=\sum_{k=1}^C \left[ \mathbb{E}y_{i,k}\cdot \mathbb{E}y_{j,k}+Cov(y_{i,k},y_{j,k}) \right]  \\
       &\leq \sum_{k=1}^{C}(\mu_{i,k}\cdot \mu_{j,k}+|\sigma_{i,k}\cdot \sigma_{j,k}|)\\
       &=\rho \cdot C .
    \end{aligned}
\end{equation}
Here $\mu=0$ and $\sigma=1$ is the mean and standard deviation of Gaussian distribution. The Eq. \ref{eq:in_prd_same} indicates the expected value of the inner-product between a vector and itself, while Eq. \ref{eq:in_prd_diff} illustrates the inner-product of two distinct vectors, which is derived by Cauchy–Schwarz inequality a.k.a covariance inequality. Here $0\leq \rho \leq 1$ and it equals to 1 if and only if two vectors are linearly dependent. Since we presume that vectors are not linearly dependent in matrix $Y$, we have $\rho<1$.

Given Eq. \ref{eq:in_prd_same} and Eq. \ref{eq:in_prd_diff}, we consider the diagonal of the cross-attention matrix. Normalized with softmax function, the limiting condition of the $i$-th position of the $i$-th raw can be described by:
\begin{equation}
\label{eq:lim}
    \lim_{C\to \infty} \frac{e^C}{(N-1)\cdot e^{\rho\cdot C}+e^C}=1.
\end{equation}

The limiting condition presented in Eq. \ref{eq:lim} means the $i$-th raw of the cross-attention map is the one-hot vector where $i$-th position is 1 as long as the feature channel is deep enough. Thus the resulting cross-attention matrix is an identity matrix. In our experiment setting, the feature tensor of 4-th layer in ResNet18 is of size $7\times 7 \times 512$, \ie $N=49$ and $C=512$. Even though $\rho$ reaches 0.98, Eq. \ref{eq:lim} can return 0.998 that is very close to 1.  
\subsection{Sequence-level Distillation}
\label{sec:seq}
In practice, the channel number $C$ is limited and we need to jointly consider the effects brought by other parameters in Eq. \ref{eq:lim} including the pixel numbers $N$ and $\rho$ that reflects the correlation of two vectors. In this section we introduce the adaption of the model discussed previously and show its application on semantic segmentation.

The proposed self-assessment mechanism lends itself to connecting teacher and student. Unfortunately, the computation complexity of $\rm{attn}$ will become unmanageable when it comes to large feature map. Assuming the spatial dimensions of the feature map are $H$ and $W$, this means the computation complexity will reach $\mathcal{O}(H^2\cdot W^2)$. Recent benchmarks of semantic segmentation includes high-resolution scenes. As a result, the feature map size in the segmentation network commonly exceeds $100 \times 100$, which implies the cross-attention matrix (Eq. \ref{eq:attn}) would be larger than $10,000 \times 10,000$. Such large attention map contains redundant information and will be interfered by noise inevitably. 

Since each column of $\rm{attn}$ is to be normalized by softmax function, which turns the $N$-dimensional vector into probability distribution. The probability reflects the similarity against difference. Due to the non-negative nature of the exponential function that is used in softmax, this procedure will lower the sensitivity of the large cross-attention matrix. Suppose that we have a vector of length $k$, when adding an additional value, in spite of positive or negative, the associated probability of the original element will decrease. It is intuitive that a large $\rm{attn}$ becomes redundant and some elements should be excluded. For example, the cross-attention between the up-left and down-right corner of the large features is unnecessary, which thus contributes to noise. Note that the problems are related with the parameter $N$. 

We propose the sequence-level distillation to address the issues of computation complexity and noise interference brought by the large cross-attention map. Given the original student feature $f_s$ and teacher feature $f_t$, they are partitioned and reorganized as sequences, denoted as $f_s^{'}\in\mathbb{R}^{(h \cdot w \cdot C) \times D}$ and $f_t^{'}\in\mathbb{R}^{(h \cdot w \cdot C)\times D}$, where $(h,w)$ is the spatial size of the patches, $D$ is the length of the sequence, and $D=(H/h)\times(W/w)$ (See Fig. \ref{fig:framework}). Similar to the formulation presented in Sec. \ref{sec:formulation}, the sequence-level distillation can be given by simply replacing the original input with the sequence-level one and the variant is denoted as $\mathcal{L}_{\rm{FM}}^\mathcal{S}$.

\subsection{Anchor-point Distillation}
\label{sec:anchor}
Although sequence-level distillation is able to reduce the computation complexity dramatically and alleviate the noise interference, it mainly focus on the distillation on the local patches, which is not capable of perceiving the global information. We resolve the conundrum of efficiency versus effectiveness by the proposed anchor-point distillation. Specifically, we extract the key point, referred to \textit{anchor}, within a local area that is representative and informative to describe the semantic of the given area of the feature.  

The rationale of anchor-point distillation is two-folds. First, it will obviously reduce the feature map along with the computation complexity. Second, although the pixels within a local area are not strictly linear dependent with each other, they may correlated with each other to some extent. The parameter $\rho$ will be relatively high in a local area, which will lower the result of Eq. \ref{eq:lim}. Therefore, there is no need for all the pixels to participate the distillation. 

We simply use average pooling with the predefined kernel (i.e. kernel size and stride) to extract the anchor points. Then all the anchors are scattered back to the associated position to form a new feature map, which is of smaller size and remains the global information to some extent. The anchor-point feature is used for distillation as described in Sec. \ref{sec:formulation} and the objective is denoted as $\mathcal{L}_{\rm{FM}}^\mathcal{A}$. The sequence-level distillation enables the student to mimic the fine-grained feature in a local view while the anchor-point distillation allows the student to learn the global representation over the coarse anchor-point feature, which are complementary to each other. Therefore, the combination of these two objective can bring the best of two worlds. Finally, our objective designed for semantic segmentation can be written by:
\begin{equation}
    \mathcal{L}_{\rm{Seg}}=\alpha\mathcal{L}_{\rm{CE}}+
    \delta\mathcal{L}_{\rm{FM}}^\mathcal{S}+
    \zeta\mathcal{L}_{\rm{FM}}^\mathcal{A}
\end{equation}

%-------------------------------------------------------------------------

\section{Experiment}
\label{sec:experiment}
In this section, we empirically evaluate the effectiveness of the proposed method via a series of experiments. We conduct the experiment of classification task on Cifar-100 and ImageNet (\cite{deng2009imagenet}). We show that our model, though simple, is effective to boost the small model, pushing forward the state-of-the-art. We also conduct some ablation study to see the impact of some components in the cross-attention module and the coefficient in Eq.~\ref{eq:objective}. For semantic segmentation, we evaluate the model on two benchmark Pascal VOC and Cityscapes. The effectiveness of the proposed sequence-level and anchor-point distillation will be elaborated in ablation study.

\subsection{Datasets}
\textbf{Cifar-100}. This benchmark contains one hundreds categories including 600 samples each. For each category, there are 500 images for training while 100 images for testing. The image size is of $32\times 32$. We report top-1 accuracy as evaluation metric.   

\textbf{ImageNet}. This is a challenging benchmark for image classification including more than 1\textbf{M} training samples. The image size is irregular and usually is cropped and resized to $224\times224$ for both training and testing. Conventionally, both top-1 and top-5 accuracy are used to measure the model performance.

\textbf{Pascal VOC}. This benchmark is developed for a variety of vision tasks including detection, classification and segmentation. In terms of semantic segmentation, it contains 20 foreground classes with an background class. It provides with 1,464 training, 1,499 validation, and 1,456 testing samples. Apart from the fine annotated samples, we also use additional coarse annotated images from [] for training, resulting in 10,582 training samples. We report the mean Intersection over Union (mIoU) on validation set to measure the proposed method.

\textbf{Cityscapes}. This benchmark records the urban scene from different street views and has 19 classes in total. It consists of 2,975 training, 500 validation, and 1,525 testing samples with fine annotations. We do not use extra coarse annotations. We evaluate our model on the validation set and report the mIoU as evaluation metric for comparison.

\subsection{Implementation Details}
For the experiments on Cifar-100, we use SGD optimizer \cite{sutskever2013importance} and the total running epoch is set to 240. The initial learning rate is 0.05 with a decay rate 0.1 at epoch 150, 180 and 210. In terms of data augmentation, the input images will be randomly cropped and flipped horizontally. We use Bayesian optimization for hyperparameter (i.e. $\alpha$ and $\epsilon$ in Eq.~\ref{eq:objective}) searching.
For the experiments on ImageNet, we use 8 GPUs with the batch size of 256 per GPU. The AdamW optimizer \cite{loshchilov2017decoupled} is used for training. The learning rate is 1.6e-4 and will decay by 0.1 at epoch 30, 60 and 90. We apply random crop and horizontal flip for data augmentation. We use grid searching to obtain the coefficients $\alpha$, $\beta$ and $\epsilon$ in Eq.~\ref{eq:objective}.

Regarding semantic segmentation, we select DeepLabV3[], which uses dilated convolution, as the network architecture where ResNet101 is adopted as the teacher backbone. We employ both similar and distinct architecture as student backbone including ResNet18 and MoibleNet[]. We use random flip and Gaussian blur for data augmentation. On Pascal VOC, the samples are randomly cropped and rescaled to $513\times 513$ for training and are resized to the same resolution during testing. This data pre-processing is also applied to Cityscapes except that image resolution becomes $769\times769$. The student backbone is to be trained 100 epochs with initial learning rate 7e-3 for Pascal VOC and xx for Cityscapes, to which cosine scheduler is applied.   

\begin{table*}[!t]
\centering
\caption{Top-1 accuracy(\%) in Cifar-100 testing set. For fair comparison, the loss term $\mathcal{L}_{\rm{KL}}$ in Eq.~\ref{eq:objective} is removed in this experiment. We report the performance gain of each method against traditional KD \cite{Hinton2015DistillingTK}. Our method outperforms other comparison methods by a significant margin. We have at least 1 absolute point of performance gain against KD \cite{Hinton2015DistillingTK} on 5 out of 7 experimental settings.}
\resizebox{1.9\columnwidth}{!}{
\begin{tabular}{l|ccccccc} 
\toprule
\multirow{3}{*}{Method} & \multicolumn{7}{c}{Network Architecture}  \\ %\cline{2-8}
\cmidrule{2-8}
& WRN-40-2 & WRN-40-2  & ResNet56 & ResNet110 & ResNet110 & ResNet32$\times$4 & VGG13 \\
& WRN-16-2 & WRN-40-1  & ResNet20 & ResNet20  & ResNet32  & ResNet8$\times$4  & VGG8 \\ \midrule
Teacher &75.61\hspace{5mm} &75.61\hspace{5mm} &72.34\hspace{5mm} &74.31\hspace{5mm} &74.31\hspace{5mm} &79.42\hspace{5mm} &74.64\hspace{5mm} \\ 
Vanilla &73.26\hspace{5mm} &71.98\hspace{5mm} &69.06\hspace{5mm} &69.06\hspace{5mm}	&71.14\hspace{5mm} &72.50\hspace{5mm} &70.36\hspace{5mm} \\  %\hline
KD &74.92\hspace{5mm} &73.54\hspace{5mm} &70.66\hspace{5mm} &70.67\hspace{5mm} &73.08\hspace{5mm} &73.33\hspace{5mm} &72.98\hspace{5mm} \\ 
FitNet &73.58\myminus{1.34}&72.24\myminus{1.30}&69.21\myminus{1.45}&68.99\myminus{1.68}&71.06\myminus{2.02}&73.50\myplus{0.17}&71.02\myminus{1.96} \\ 
AT &74.08\myminus{0.84}&72.77\myminus{0.77}&70.55\myminus{0.11}&70.22\myminus{0.45}&72.31\myminus{0.77}&73.44\myplus{0.11}&71.43\myminus{1.55} \\ 
SP &73.83\myminus{1.09}&72.43\myminus{1.11}&69.67\myminus{0.99}&70.04\myminus{0.63}&72.69\myminus{0.39}&72.94\myminus{0.39}&72.68\myminus{0.20} \\ 
CC &73.56\myminus{1.36}&72.21\myminus{1.33}&69.63\myminus{1.03}&69.48\myminus{1.19}&71.48\myminus{1.60}&72.97\myminus{0.36}&70.71\myminus{2.27} \\ 
RKD &73.35\myminus{1.57}&72.22\myminus{1.32}&69.61\myminus{1.05}&69.25\myminus{1.42}&71.82\myminus{1.26}&71.90\myminus{1.43}&71.48\myminus{1.50} \\
PKT &74.54\myminus{0.38}&73.45\myminus{0.09}&70.34\myminus{0.32}&70.25\myminus{0.42}&72.61\myminus{0.47}&73.64\myplus{0.31}&72.88\myminus{0.10} \\ %\hline
FSP &72.91\myminus{2.01}&NA&69.95\myminus{0.71}&70.11\myminus{0.56}&71.89\myminus{1.19}&72.62\myminus{0.71}&70.20\myminus{2.78}\\
NST &73.68\myminus{1.24}&72.24\myminus{1.30}&69.60\myminus{1.06}&69.53\myminus{1.14}&71.96\myminus{1.12}&73.30\myminus{0.03}&71.53\myminus{1.45}\\ 
CRD&75.48\myplus{0.56}&74.14\myplus{0.60}&71.16\myplus{0.50}&71.46\myplus{0.79}&73.48\myplus{0.40}&\textbf{75.51}\myplus{2.18}&73.94\myplus{0.96}\\ \midrule
Ours w/o $\mathcal{L}_{\rm{KL}}$&\textbf{76.02}\myplus{1.10}&\textbf{74.97}\myplus{1.43}&\textbf{71.39}\myplus{0.73}&\textbf{71.70}\myplus{1.03}&\textbf{73.96}\myplus{0.88}&75.50\myplus{2.17}&\textbf{74.39}\myplus{1.41}\\
\bottomrule
\end{tabular}}
\label{tab:cifar_sota}
\end{table*}

%--------------------------------------------------------------------------------
\begin{table*}[thb]
\centering
\caption{Top-1 and Top-5 Accuracy(\%) on ImageNet validation set. The ResNet34 is employed as teacher network and the ResNet18 is selected as student network. Methods denoted by * do not release Top-5 accuracy. With the assistance of $\mathcal{L}_{\rm{KL}}$, our method can boost the performance of ResNet18 beyond 72\%. Our method still outperforms other method even without $\mathcal{L}_{\rm{KL}}$.}
% \setlength{\tabcolsep}{5pt}
% \renewcommand\arraystretch{1.1}
\resizebox{1.9\columnwidth}{!}{%\tablestyle{10pt}{1.05}
\begin{tabular}{@{}l|ccccccccccc|c@{}}  \toprule
%\multirow{3}{*}{Method} & \multicolumn{7}{c}{Network Architecture}  \\ \cline{2-8}
%&\multicolumn{11}{c|}{ResNet34 (73.31/91.42) $\rightarrow$ ResNet18}&\\  \shline
&Vanilla&KD &AT &RKD & SCKD$^*$ &CRD &CRD+KD& SAD$^*$ &CC$^*$ & Ours w/o $\mathcal{L}_{\rm{KL}}$&Ours& Teacher\\ \midrule
w/ $\mathcal{L}_{\rm KL}$  &      &$\checkmark$      &      &$\checkmark$       &$\checkmark$       & &$\checkmark$  &     &$\checkmark$  &    & $\checkmark$  &  \\ %\hline
Top-1&70.04  &70.68  &70.59  &71.34  &70.87  &71.17  &71.38 &71.38  &70.74 &72.07 & \textbf{72.41} & 73.31\\
Top-5&89.48  &90.16  &89.73  &90.37  &NA     &90.13  &90.49 &NA     &NA &90.48&\textbf{90.75} & 91.42\\ 
\bottomrule
\end{tabular}
}\label{tab:imagenet}
\end{table*}

%--------------------------------------------------------------------------------

\subsection{Results on Cifar-100}
The experimental results is displayed in Table~\ref{tab:cifar_sota}. To fully explore the potential of the proposed method, we demonstrate the effectiveness of the model on a variety of network architectures, including ResNet \cite{He2016DeepRL}, VGG \cite{simonyan2014very} and WideResNet \cite{Zagoruyko2016WideRN}. As can be observed, our method surpasses all the comparison methods, often exceeding the other methods by a significant margin. For example, in the setting of distillation from WRN-40-2 to WRN-40-1 (2nd column in Table~\ref{tab:cifar_sota}), our method exceeds CRD by 0.89\% and other methods by at least 1\%. Compared to FitNet, which directly links teacher layers to student layers, our method exhibits great improvements, which demonstrated that learning a in-between state is more effective than trying to entirely mimic the teacher network. In the experiment, we found that setting $\theta(\cdot)$ as identical function would not deteriorate the performance while saving more training cost. We also report the result of setting $\theta(\cdot)$ as $1\times1$ convolution in the ablation study. When it comes to VGG architecture, the BN layer of $\gamma(\cdot)$ is removed since no BN layer is deployed in the previous blocks.

\subsection{Result on ImageNet}
We further evaluate the model on another challenging dataset ImageNet. We choose ResNet34 as teacher and ResNet18 as student. The result is exhibited on Table~\ref{tab:imagenet}. Our methods outperforms the other state-of-the-art. Without the help of $\mathcal{L}_{\rm{KL}}$, our model can reach at stat-of-the-art (Top-1 71.97\%). Especially, when combined with $\mathcal{L}_{\rm{KL}}$, the proposed method can boost the Top-1 accuracy of ResNet18 to 72.41\%. This demonstrates the potential of our method that can bridge the gap between small model and large model. Compared to SCKD which uses attention to re-allocate the teacher layers to student, our method have a significant improvement. That means even matching two layers of teacher and student in similar semantic, student may not be able to catch up with teacher. In contrast, our method, by means of the cross-attention, provides a soft way with student to mimic the teacher.
Here $\alpha$, $\beta$ and $\epsilon$ are set to 0.5, 0.5 and 0.1, respectively. In the experiment that $\mathcal{L}_{\rm{KL}}$ is not employed, $\alpha$ and $\epsilon$ are set to 0.1 and 0.2. We study the impact of $\epsilon$ in ablation study.

\subsection{Result of Semantic Segmentation}
This section presents another application of the proposed framework. As discussed, our method can be extended to semantic segmentation with assistance by the proposed sequence-level and anchor-point distillation. We conduct experiments on two popular benchmarks Pascal VOC and Cityscapes. The result is presented on Table \ref{tab:seg_voc} and Table \ref{tab:seg_cityscapes} respectively. Our model is able to boost the tiny network by a large margin. For instance, on Pascal VOC, the proposed model can improve the MobileNetV3 by more than 5\%, which shows great potential to unlock the hardware limitation. Compared to existing works, our model achieves state-of-the-art and surpasses other methods. The contribution of two proposed components will be detailed in ablation study.  

\begin{table}[h]
    \caption{The performance of our model under different $\epsilon$ on ImageNet. Here the loss $\mathcal{L}_{\rm{KL}}$ is removed and $\alpha$ is set to 0.1.}
    \centering
    \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{c|cccccccccc}
        \toprule
        $\epsilon$  &0.05 &0.08 &0.09 &0.1 &0.11 &0.12 &0.15&0.20&0.21&0.25  \\
        \midrule
        Top-1 Acc.&71.43 &71.58&71.71&71.91&71.82&71.82&71.942&71.972&71.95&71.95\\ 
        \bottomrule
    \end{tabular}}
    \label{tab:imagenet_gamma}
\end{table}

\begin{table}[h]
    \caption{The performance of our model under different attention heads on ImageNet. Here the loss $\mathcal{L}_{\rm{KL}}$ is removed. $\alpha$ and $\epsilon$ is set to 0.1 and 0.2.}
    \centering
    \begin{tabular}{c|ccccc}
        \toprule
        Heads  &1 &4 &8 &16 &32 \\
        \midrule
        Top-1 Acc.&71.97 &71.94 &\textbf{72.07} &71.83 &71.89 \\
        \bottomrule
    \end{tabular}
    \label{tab:imagenet_heads}
\end{table}

\section{Ablation \& Sensitivity Study}
\label{sec:ablation}
We first study the impact of function $\theta(\cdot)$. In the original work Transformer[], a linear transform function, consisting of a fully-connected layer plus a LN layer, is applied to generate the key. In contrast, we found that the model performance would not benefit apparently from linear transform. We compared different setting of $\theta(\cdot)$ including identical mapping against Conv$+$BN (See Table \ref{tab:cifar_1x1}). Neither one of them can consistently outperforms the other. An interesting finding is that for the distillation VGG13$\rightarrow$VGG8, identical mapping exceeds Conv$+$BN by $\sim$1\%. The objective of the proposed cross-attention is to enable the student to perform self-assessment by measuring the discrepancy with teacher feature. Since VGG does not use BN for feature normalization, it will change the distribution of teacher feature, thus impeding the self-assessment. 

To investigate the efficacy brought by the proposed Eq. \ref{eq:fm}, we then further explore the different settings of the coefficient $\epsilon$ used in Eq. \ref{eq:objective} (See Table~\ref{tab:imagenet_gamma}). When increased from 0.05 to 0.25, the objective $\mathcal{L}_{\rm{FM}}$ can bring positive and stable effect. Another hyperparameter of the interest is the number of Multi-head attention, which plays a critical role in the calculation of cross-attention. Basically, both $f_t$ and $f_s$ are to be divided into several subgroups channel-wisely according to the MAH. Each subgroup is associated with a certain attention head. The result is presented on Table \ref{tab:imagenet_heads}. The performance gain is minor when increasing the MHA and it will deteriorate the model with excessive attention head. 

We also conduct the thorough experiments to understand the contribution brought by the proposed sequence-level distillation $\mathcal{L}_{\rm{FM}}^{\mathcal{S}}$ and anchor-point distillation $\mathcal{L}_{\rm{FM}}^{\mathcal{A}}$. As discussed previously, $\mathcal{L}_{\rm{FM}}^{\mathcal{A}}$ is proposed to learn the global representation to capture long-range dependency while $\mathcal{L}_{\rm{FM}}^{\mathcal{S}}$ is designed to learn local information and fine-grained feature. 
By covering each one of them, the individual effectiveness of the two components can be examined.  
As shown in Table. \ref{tab:seg_ablation}, both objectives can improve the vanilla student significantly while $\mathcal{L}_{\rm{FM}}^{\mathcal{S}}$ presents more efficacy. 
The combination of both components achieves best performance, demonstrating that the two proposed objective are complementary.  

We give more insight with respect to the proposed objectives' functionality through sensitivity analysis. Specifically, we investigate the hyperparameters that would influence the behaviour of the training process. In terms of the anchor-point distillation, this work utilizes average pooling to extract the anchor in a local area from the original feature, forming the associated anchor-point feature. It is a trade-off between noise removal and representation preservation since bigger kernel would filter more noise along with more informative representation. Thus we study the sampling area (i.e. kernel size) that directly yields different feature resolution. Specially, when a $1\times 1$ kernel is used to generate the anchor-point feature, it degrades to the circumstance discussed in Sec. \ref{sec:formulation} and returns the original feature.
The result exhibited in Table \ref{tab:anchor} supports our hypothesis that the full-size feature deteriorate the performance due to noise and presents poor performance compared to the other anchor-point feature of smaller size. Though saving computation cost of the cross-attention map, enlarging the sampling area would omit useful and informative representation and damage the performance.

Next we analyse the two key factors of sequence-level distillation, i.e. patch size and Multi-head Attention. The former decides the sequence resolution and length while the later conditions the range to that cross-attention can apply. In Table \ref{tab:seq}, we found that generally, larger patch size is advantageous to sequence-level distillation and overlarge patch size, however, may be unfavourable since it approaches the original feature.  Regarding the Multi-head attention, it divides the sequence into corresponding sub-sequences so that each sub-sequence can perform distillation individually. In the experiment that shown in Table \ref{tab:head}, the patch size is set to $8\times 8$ and the sequence length is $128/8*128/8=256$. There are two extreme situations. When only one MAH head is used, it indicates that the whole sequence will join the distillation. On the contrary, using 256 MAH heads means each patch will be distilled individually. In this example, we found that two patches as an unit can reach the best performance.



\begin{table*}[th]
\centering
\caption{Impact of function $\theta(\cdot)$ on a variety of network architecture. The Top-1 accuracy of Cifar-100 is reported. We found that $1\times1$ convolution layer is not apparently superior to identical mapping.}
\resizebox{1.9\columnwidth}{!}
{\tablestyle{20pt}{1.2}
\begin{tabular}{@{}l|ccccccc@{}} 
\toprule
Teacher& WRN-40-2 & WRN-40-2  & ResNet56 & ResNet110 & ResNet110 & ResNet32$\times$4 & VGG13 \\
Student& WRN-16-2 & WRN-40-1  & ResNet20 & ResNet20  & ResNet32  & ResNet8$\times$4  & VGG8 \\ 
\midrule
Conv+BN&76.08&74.99&71.45&71.68&73.75&75.30&73.48\\
Identity Mapping &76.02&74.97&71.39&71.70&73.96&75.50&74.39\\
\bottomrule
\end{tabular}}
\label{tab:cifar_1x1}
\end{table*}

\begin{table}[]
    \centering
    \caption{Performance (mIoU\%) comparison of different methods on Pascal VOC. $^{*}$ denotes reimplementation with 100 training epochs. }
    \begin{tabular}{l|cc}
    \toprule
         &ResNet18 &MobilenetV3   \\
         \hline
        Student &72.07 &68.46     \\
        KD      &73.74 &71.73     \\
        AT      &73.01 &71.39     \\
        FitNet  &73.31 &69.23     \\
        Overhaul$^{*}$&73.98 &72.3     \\
        ICKD    &75.01 &72.79     \\
        Ours    &\textbf{75.76} &\textbf{73.85} \\
    \bottomrule
    \end{tabular}
    \label{tab:seg_voc}
\end{table}

\begin{table}[]
    \centering
    \caption{Performance (mIoU\%) comparison of different methods on Cityscapes. }
    \begin{tabular}{l|cc}
    \toprule
         &ResNet18 &MobilenetV3   \\
         \hline
        Student &70.26 &     \\
        KD      & &     \\
        AT      & &     \\
        FitNet  & &     \\
        Overhaul&71.15 &     \\
        Ours    & & \\
    \bottomrule
    \end{tabular}
    \label{tab:seg_cityscapes}
\end{table}

\begin{table}[]
    \centering
    \caption{Contribution of sequence-level and anchor-point distillation. }
    \begin{tabular}{cc|c}
    \toprule
          Sequence-level &Anchor-point & mIoU  \\
            \checkmark & \checkmark&75.76 \\
            \checkmark & &75.63 \\
            &\checkmark&75.52\\
    \bottomrule
    \end{tabular}
    \label{tab:seg_ablation}
\end{table}

\begin{table}[]
    \centering
    \caption{Performance (\%) of anchor-point distillation on Pascal VOC under different down-sampling settings. }
    \begin{tabular}{c|ccccc}
    \toprule
         Down Samp. &Full&$1/2$ &$1/4$ &$1/8$ &$1/16$  \\
         mIoU       &74.51&75.52 &75.27 &74.79 &74.56 \\
    \bottomrule
    \end{tabular}
    \label{tab:anchor}
\end{table}

\begin{table}[]
    \centering
    \caption{Performance (\%) of sequence-level distillation on Pascal VOC under different settings of patch size. }
    \begin{tabular}{c|cccc}
    \toprule
         Patch &4$\times$4 &8$\times$8 &16$\times$16 &32$\times$32  \\
         mIoU &75.33 &75.45 & 75.59 & 75.47\\
    \bottomrule
    \end{tabular}
    \label{tab:seq}
\end{table}

\begin{table}[]
    \centering
    \caption{Performance (\%) od sequenc-level distillation on Pascal VOC under different settings of Multi-head attention where patch size is $8\times 8$. }
    \begin{tabular}{c|cccccc} 
    \toprule
         MAHs &1 &32 &64 &128 &256 &512  \\
         mIoU &75.26&75.57&75.63&75.62&75.50&75.03\\
    \bottomrule
    \end{tabular}
    \label{tab:head}
\end{table}

\section{Conclusion}
This work develops a framework for knowledge distillation through self-assessment that allows student to perceive the context between itself and the teacher, which is accomplished by the proposed cross-attention mechanism. The obtained cross-attention map can facilitate the feature matching, thus helping the student better mimic the teacher.
Our method can be applied on image classification and is successfully extended to semantic segmentation by the proposed sequence-level and anchor-point distillation, designed to remove noise and reduce computation complexity. We conduct thorough experiment to validate the effectiveness of the method and advance the state-of-the-art.

%%%%%%%%% REFERENCES
\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
