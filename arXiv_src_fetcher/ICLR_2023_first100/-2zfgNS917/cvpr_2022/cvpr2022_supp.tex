% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

% \documentclass[10pt,twocolumn,letterpaper]{article}
\documentclass[10pt,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{7432} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Supplementary Material:\\ Knowledge Distillation via the Target-aware Transformer}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% BODY TEXT
\section{Asset Usage }
\label{sec:asset}
This work is built upon some public dataset and code assets. We appreciate their efforts. We will release our code upon acceptance. The benchmark dataset has been introduced in main paper. Here we list the URL, version, and license of the code assets that we used:

\begin{table*}[h]
    \centering
    \caption{Usage of Code assets.}
    \begin{tabular}{l|l|c|l}
        \toprule
         Exp.&\multicolumn{1}{c|}{URL} &Ver. &Licence \\\midrule
         ImageNet&{\tt \small https://github.com/yoshitomo-matsubara/torchdistill} &{\tt \small 7b883ec} &MIT\\ \midrule
         Cifar100&{\tt \small https://github.com/HobbitLong/RepDistiller}&{\tt \small 9b56e97} &BSD 2-Clause \\ \midrule
         \multirow{2}{*}{Pascal VOC}&{\tt \small https://github.com/jfzhang95/pytorch-deeplab-xception}&{\tt \small 9135e10} & MIT\\
         &{\tt \small https://github.com/clovaai/overhaul-distillation} &{\tt \small 76344a8} &MIT\\ \midrule
         \multirow{2}{*}{COCOStuff10k} &{\tt \small https://github.com/kazuto1011/deeplab-pytorch} &{\tt \small 4219467} &MIT\\
         &{\tt \small https://github.com/dvlab-research/ReviewKD} &{\tt \small cede6ea} &N/A\\
        %  COCOStuff10k&{\tt \small }&{\tt \small} & \\
        \bottomrule
    \end{tabular}
    \label{tab:my_label}
\end{table*}
% \noindent \textbf{ImageNet.} We conduct the ImageNet experiments on this code asset which is under MIT licence. The URL is {\tt \small https://github.c om/yoshitomo-matsubara/torchdistill} while the version is {\tt \small 7b883ec}.


%-------------------------------------------------------------------------



\section{Additional Experiments}
\subsection{Comparison on COCOStuff10k}
For the experiments of semantic segmentation, we have compared our method to a variety of stat-of-the-art methods in the Section 4 of the main paper. In terms of COCOStuff10k, since some methods do not support this dataset, we re-implement them and the result is presented on Table~\ref{tab:supp_seg}. We found that our method is competitive and it outperforms the comparison methods.

\begin{table*}[h]
    \centering
    \caption{Comparison (mIoU\%) on COCOStuff10k.}
    \begin{tabular}{l|ccc}
    \toprule
         &ICKD~\cite{Liu2021ICKD} &Overhaul~\cite{Heo2019ACO} &Ours  \\ \midrule
        ResNet18 &27.22 &27.86 &28.75 \\   \midrule
        MobileNetV2 &26.64 &26.96 &28.05 \\
    \bottomrule
    \end{tabular}
    \label{tab:supp_seg}
\end{table*}

% \subsection{More powerful teacher on ImageNet}


\subsection{Hyperparameters on Cifar-100}
We used Bayesian optimization to obtain the weight factors $\alpha$ and $\epsilon$ in Eq. 9. Here we show the searching result on different backbones (See Table~\ref{tab:supp_cifar100_p}). We found that in most cases (4 out of 6), $\epsilon$ is greater than $\alpha$, which indicates that our proposed objective is more important than the standard Cross-entropy during distillation. For instance, in the distillation VGG13$\rightarrow$VGG8, $\epsilon$ is 8 and $\alpha$ is only 0.1. We also found that for the similar architectures, the searching result is similar, \eg, when WRN-40-2 and ResNet110 are selected as teacher.





\begin{figure*}[t]
    \centering
    \includegraphics[scale=1]{cvpr_2022/supp-2.pdf}
    \caption{
    \textbf{Visualization of feature map and TaT map.} The input is selected from ImageNet validation set. The teacher backbone is ResNet34 and student backbone is ResNet18. The feature map of the distillation layer (4-th block) has been visualized. While there are 512 feature channels in total, we visualize 64 channels for better visualization. Through the Target-aware transformer, we found that the reconfigured student feature (3rd column) has a similar pattern with teacher feature (4th column). 
    The associated TaT map has also been visualized, which indicates the student would aggregate the semantic mostly from neighbor to enhance its pixels. 
    }
    \label{fig:supp_vis}
\end{figure*}

\begin{table*}[]
    \centering
    \caption{Coefficients $\alpha$ and $\epsilon$ on different backbones on Cifar-100.}
    \begin{tabular}{c|ccccccc}
    \toprule
        Teacher & WRN-40-2 & WRN-40-2  & ResNet56 & ResNet110 & ResNet110 & ResNet32$\times$4 & VGG13 \\
        Student & WRN-16-2 & WRN-40-1  & ResNet20 & ResNet20  & ResNet32  & ResNet8$\times$4  & VGG8 \\ \midrule 
        $\alpha$   &0.8 &0.7 &0.8 &1 &1 &6 &0.1\\
        $\epsilon$ &4 &3.6 &0.4 &0.75 &1 &39 &8\\
        \bottomrule
         
    \end{tabular}
    \label{tab:supp_cifar100_p}
\end{table*}

\subsection{Feature Visualization}
We further visualize the feature map and the associated TaT map to intuitively understand the functionality behind the proposed Target-aware Transformer. As exhibited in Figure~\ref{fig:supp_vis},  we visualize the feature maps of student before and after distillation, which are compared to the feature map of teacher. The teacher backbone is ResNet34 and student backbone is ResNet18. The input images are randomly selected from ImageNet validation set. While the 4-th block (\ie distillation layer) of ResNet34 and ResNet18 has 512 channels, we visualize 64 channels for better visualization. 

Obviously, the reconfigured student feature (3rd column) has a more similar pattern with teacher feature (4th column), which demonstrates that TaT can effectively adapt the student to mimic the teacher. In terms of the TaT map, which controls the intensity of semantic aggregation, it is close to the identity matrix. Recall that we apply the linear function $\phi(\cdot)$ on student feature $f^s$.
And the TaT map will be further applied on $\phi(f^{s})$ to reconfigure the student feature, which is lately asked to minimize the L$_2$ distance with teacher feature. When the TaT map is an identity matrix, it means that $\phi(f^{s})$ can reconstruct the teacher feature on its own. However, since TaT map is not strictly the identity matrix, it indicates that each pixel of $\phi(f^{s})$ still needs to \textit{borrow} the semantic from other position (mostly neighborhood) to enhance itself. Indeed, by aggregating the semantic from neighbors, each pixel increases the receptive field and thus semantic capacity. This demonstrates the semantic mismatch between student and teacher due to the variation on network depth and width.


%%%%%%%%% REFERENCES
% \clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
