% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{threeparttable}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{color, colortbl}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[accsupp]{axessibility}
\newcommand{\lsh}[1]{\textcolor{magenta}{ (#1)}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
% \newcommand{\myplus}[1]{\color{green}{\tiny{$+$#1}}}
% \newcommand{\myminus}[1]{\color{red}{\tiny{$-$#1}}}
\newcommand{\myplus}[1]{\color{green}{\tiny{}}}
\newcommand{\myminus}[1]{\color{red}{\tiny{}}}
\newcommand{\xd}[1]{\color{orange}{\tiny{$-$}#1}}

\newcommand\mypara[1]{\vspace{1mm}\noindent\textbf{#1}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{7432} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\newcommand{\ky}[1]{{\color{blue}{#1}}}
\newcommand{\KY}[1]{{\color{blue}{\bf #1}}}

% \newcommand{\sf}[1]{{\color{red}{#1}}}
\newcommand{\SF}[1]{{\color{red}{\bf #1}}}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
%\title{Self-assembling Knowledge Distillation via Semantic Position Encoding}

\title{TiG-BEV: Multi-view BEV 3D Object Detection via\\Target Inner-Geometry Learning}

%\title{Self-assembling Knowledge Distillation with Semantic Alignment}



\author{Peixiang Huang$^{1}$\footnotemark[2]\;,\;
Li Liu$^{2}$\footnotemark[2]\, \footnotemark[4]\;,\;
Renrui Zhang$^{3}$\footnotemark[2]\;,\;
Song Zhang$^{4}$\;,\;
Xinli Xu$^{2}$\;\;\\
Baichao Wang$^{2}$\;\;
Guoyi Liu$^{2}$\vspace{0.2cm}\\
%Baichao Wang$^{2}$,\;
%Guoyi Liu$^{2}$\\
$^1$Peking University\;
$^2$NIO\;
$^3$The Chinese University of Hong Kong\;\\
$^4$University of Chinese Academy of Sciences\\%\;
%$^5$Beijing Institute of Technology\\
%{\tt\small \{linsihao6, hongwei.xie.90, Kaicheng.yu.yt, xdliang328\}@gmail.com}\\
{\tt\small liuli.ll9412@gmail.com,}\;
{\tt\small huangpx@stu.pku.edu.cn,}\;
{\tt\small 1155186671@link.cuhk.edu.hk,}\;\\
{\tt\small zhangsong20@mails.ucas.ac.cn,}\;
{\tt\small xxlbigbrother@gmail.com,}\;
{\tt\small \{baichao.wang,gary.liu\}@nio.com}\\
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle

\input{cvpr_2022/tex/abstract}
\footnotetext[2]{Equal Contribution.}
\footnotetext[4]{Corresponding Author.}
%\footnotetext[3]{Part of the work done when as an intern in NIO.}


\input{cvpr_2022/tex/intro}
\input{cvpr_2022/tex/relate}
\input{cvpr_2022/tex/method}
\input{cvpr_2022/tex/exp}
% \vspace{-2mm}
\section{Conclusion}
% \vspace{-2mm}
In this paper, we propose a novel target inner-geometry learning framework that enables the camera-based detector to inherit the effective foreground geometric semantics from the LiDAR modality. We first introduce an inner-depth supervision with target-adaptive depth reference to help the student learn better local geometric structures. Then, we conduct inner-feature distillation in BEV space for both channel-wise and keypoint-wise, which contributes to high-level inner-geometry semantics learning from the LiDAR modality. Extensive experiments are implemented to illustrate the significance of TiG-BEV for multi-view BEV 3D object detection. For future works, we will focus on exploring multi-modal learning strategy that can boost both camera and LiDAR modalities for unified real-world perception.
%In this paper, we propose a novel cross-modality geometry-aware distillation framework that enables the camera-based detector to aggregate the useful geometry component information transferred from lidar modality to predict more accurate 3D bboxes for multi-view 3d object detection. Our inner-geometry relative attention depth supervision transfer the structured spatial position knowledge of target to effectively improve the Network’s perception of the inner-geometry of each object. Our structured attention feature supervision can transfer more reliable relative spatial feature relationships from LiDAR-based detectors to improve the structured awareness ability of camera-based detectors. Our method is successfully involve the target inner-geometry aware to cross-modality distillation. We conduct thorough experiments to validate the effectiveness of the method and advance the state-of-the-art.

%\vspace{-1mm}
%\section{Discussion}
%\vspace{-2mm}
%\textbf{Potential negative societal impact.} Our method has no ethical risk on dataset usage and privacy violation as all the benchmarks are public and transparent.

%\textbf{Limitations.} There are some issues of interest that we would like to explore in the future: (1) Currently, we only select the last layer of the backbone network for distillation. It would be interesting to see the efficacy when multiple layers are get involved with distillation which has been explored by some works \cite{Zagoruyko2017PayingMA,chen2021distilling}. (2) Also, we didn't investigate the effectiveness on other applications like object detection, which may need to design the new objective to fit the nature of specific application. 

%\vspace{-3mm}
%\section*{Acknowledgement}
%\vspace{-2mm}
% This work was supported in part by Australian Research Council (ARC) Discovery Early Career Researcher Award (DECRA) under DE190100626, National Natural Science Foundation of China (NSFC) under Grant No.61976233, and ”Leading Innovation
% Team of the Zhejiang Province” (2018R01017).
% National Key Research and Development Program of China (Grant NO. 2020AAA0108104) and Alibaba Innovative Research (AIR) Program.
%This work was supported in part by National Natural Science Foundation of China (NSFC) under Grant No.61976233, National Key Research and Development Program of China (Grant NO. 2020AAA0108104), Australian Research Council (ARC) Discovery Early Career Researcher Award (DECRA) under DE190100626, and Alibaba Innovative Research (AIR) Program.
%%%%%%%%% REFERENCES
% \clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
