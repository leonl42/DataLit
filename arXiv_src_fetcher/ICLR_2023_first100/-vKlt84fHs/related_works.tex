\section{Related works}
\label{rw}
Isolation Forest \cite{liu2008isolation, liu2012isolation} represents an innovative and original anomaly detection algorithm. The primary benefits of the algorithm, distinguishing itself with respect to other existing approaches, depend on its core structure. In the majority of AD models, normal points have a central role: data points that do not fit with the computed model, and which therefore break away from computed normal instance profile, are classified as anomalies. Instead of creating a profile for normal points, Isolation Forest focuses its structure on anomalous points, relying on the hypothesis that anomalies are fewer in number and different in attribute-values and therefore easier to be separated from the rest of the data  with the use of a small number of partitions. Isolation Forest is a non-parametric cost-effective ensemble method that revealed to be successful, often exceeding the results of more elaborate state of the art methods \cite{susto2017anomaly}. 

Similarly to Random Forest \cite{ho1995random}, Isolation Forest training phase constructs an ensemble of decision trees, also known as isolation trees (iTrees), relying on the fact that anomalies are easier to be isolated, i.e., partitioned from the rest of the data, due to their distinctive features. First, Isolation Forest randomly sub-samples the dataset so that each iTree is obtained with a different set of data. Then, the structure of an iTree is generated in a completely random way: each partition is produced with a random selection of an attribute value from the subset at disposal and by the choice of the split value, selected randomly in the range of the picked attribute. This recursive procedure is repeated until all points are isolated or a predefined limit height is reached. After that, a novel random sub-sample is selected and the isolation procedure is repeated, in order to create a new random iTree. Once the training phase is completed and every iTree is fully grown, data traverse the different iTrees and its depths, i.e., the number of edges traversed from the root node to the external node containing it, are collected. Based on these depths $h^u$, also known as path lengths, the anomaly score is computed, i.e., an indicator of the likelihood that a point is an anomaly. 

Let $X$ be a generic set of data. Every iTree is generated from a sub-sample $X' \subset X$, $|X'|= \psi$: the recursive branching process starts by selecting a random feature and a random value between the minimum and the maximum value in that feature. At each branch, points are split into two daughter nodes based on the corresponding feature attribute. 
%To determine whether a point is more likely to be an anomaly or not, an anomaly score is defined \tommi{non sono sicuro, ma non si ripete quello che Ã¨ stato appena descritto?}, directly depending from the average path length of the point with respect to the entire forest. 
Let $x \in X$, then the corresponding anomaly score is defined as
\begin{equation}
    a(x) = 2^{-\frac{E(h^\mathcal{u}(x))}{c(\psi)}} \label{as}
\end{equation}
where $E(h^\mathcal{u}(x))$ is the average path length of $x$ with respect to all trees and $c(\psi)$ is a normalization factor with the sub-sample set size as input. Specifically, \[c(\psi)=2H(\psi - 1)-2(\psi-1)/\psi\] and defines the average path length of an unsuccessful search in a binary search tree computed with a set of cardinality $\psi$. Based on Eq. (\ref{as}): when $E(h^\mathcal{u}) \rightarrow \psi -1$, $a(x)\rightarrow 0$ and it is quite safe to consider $x$ a normal point; on the contrary when $E(h^\mathcal{u}) \rightarrow 0$, $a(x)\rightarrow 1$ and $x$ is most likely an anomaly; when $E(h^\mathcal{u}) \rightarrow c(\psi)$, namely when the average path length of $x$ is close to the normalization factor, then $a(x) \approx 0.5$ and the sample has no recognizable anomalous factor. Note that, in order to classify whether or not a point is anomalous, it is necessary to define a score based border value, establishing a division between anomalous points and normal ones. However, the choice of such border value is strongly data dependent, relying upon its target subject, making it not a trivial task to be managed \cite{hofmockel2018isolation}.

%We present a model that addresses this limitation \tommi{dici che affrontiamo questa limitazione?} with minimal additional computation cost and at the same time provides promising results. The proposed algorithm starts by growing a standard isolation forest
%In the proposed algorithm, for a start the standard unsupervised Isolation Forest is performed with no modification, leading to a fully grown forest.
The proposed algorithm starts by growing a standard Isolation Forest.
After that, an active learning based approach is carried on: in an iterative way, the model is allowed to ask for a point to be labeled, obtaining the true information about its nature as a direct consequence. Based on it, the inner structure of each tree is modified to exploit the incoming information and to improve/calibrate the model if necessary.
% to obtain  a consensus with respect to the true label received. 

%Before we define the complete proposed strategy in detail, we discuss some of the current existing weakly supervised Isolation Forest approaches. To the best of our knowledge, very few recent papers deal with the idea of using an active learning approach in tree-based models.
A similar active learning anomaly detection algorithm using an optimization based approach is presented in \cite{das2016incorporating}. The paper describes an Active Anomaly Discovery (AAD) method where the points having the highest anomaly score are presented to an expert analyst in an iterative way, requesting the corresponding true label. Based on the information received, the algorithm tries to place the points labeled as anomalies as close as possible to the higher part of the model. Specifically, the model considers the structure defined by the Isolation Forest and associates to each leaf a weight value, starting from a uniform fixed value and, based on the feedback received, such weights are iteratively updated. Such updating approach is performed with the use of an iterative optimization problem, where the optimal weights are computed so that every labeled anomaly has a score which is higher with respect to the labeled normal points ones. Note that, in this approach the partitions computed by the Isolation Forest are not modified, only the corresponding weights are. The proposed method is applied into a tree-based detector \cite{das2017incorporating}, where the described updating approach is used into the Isolation Forest algorithm. Such algorithm is called IF-AAD. 
%Very recently, \cite{li2021tree} applies feedback into a tree-based detector aiming at modifying both the anomaly scores and the partitions. Specifically, the paper proposes a streaming approach where, together with the current information, also the past feedback is considered, in order to maintain a storage factor in the process. When a new feedback is received, the model fits to the achieved information by considering the instance itself together with the region where it belongs. In this way, the paper proposes an approach which allows the pruning of the original tree-based structure according to the feedback received. 
%An online convex optimization (OCO) based approach is presented in \cite{siddiqui2018feedback}. The paper proposes to assign a weight vector to each edge defining a cost of isolation function, i.e., the cost of traversing the edge, such that, the average cost of isolation indicates the average anomaly score of a point. The more anomalous a point is, the smaller the corresponding cost of isolation is. Such weights are updated within an OCO framework based on the feedback received. A self-adapting tree-based approach is presented in \cite{li2021privacy}. The article uses a multi-agent forest model where, when a new true label is presented, each agent collaborates, updating the tree-based model with the use of a reinforcement learning approach. 

Here we present a novel approach, called Active Learning-based Isolation Forest (\approach), which differs from the aforementioned works because of its easy yet efficient formulation: independently from the size of the input set, the algorithm will execute in constant time, modifying the average path length of the input points but leaving unchanged the Isolation Forest partitions, with the perk of using both current and past information. Differently from IF-AAD and Random Forest, the proposed approach does not need to pass through an optimization procedure, leading to a much lighter update.  In Section \ref{exp} we report the performance benchmarking of our approach, IF-AAD and Random Forests.
