\section{Related work}
\label{Rel_wk}

\subsection{Combination of Transformers and convolution}
Convolution is relatively efficient but not suitable to capture long-range dependencies and vision Transformer has the powerful capability with a global receptive field but lacks efficiency due to the computation of self-attention. 
In order to make full use of both of their advantages, MobileViT~\cite{mehta2021mobilevit}, TopFormer~\cite{zhang2022topformer}, LVT~\cite{yang2022lite}, Mobile-Former~\cite{chen2022mobile}, EdgeViTs~\cite{pan2022edgevits}, MobileViTv2~\cite{mehta2022separable}, EdgeFormer~\cite{zhang2022edgeformer} and EfficientFormer~\cite{li2022efficientformer} are constructed as efficient ViTs by combining convolution with Transformers. MobileViT, Mobile-Former, TopFormer and EfficientFormer are restricted by Transformer blocks and have to trade off between efficiency and performance in model design. LVT, MobileViTv2 and EdgeViTs keep the model size small at the cost of relatively high computation, which also means high latency.

\subsection{Axial attention and variants}
Axial attention~\cite{huang2019ccnet, ho2019axial, wang2020axial} is designed to reduce the computational complexity of original global self-attention~\cite{vaswani2017attention}. 
It computes self-attention over a single axis at a time and stacks a horizontal and a vertical axial attention module to obtain the global receptive field. 
Strip pooling~\cite{hou2020strip} and Coordinate attention~\cite{hou2021coordinate} uses a band shape pooling window to pool along either the horizontal or the vertical dimension to gather long-range context. Kronecker Attention Networks~\cite{gao2020kronecker} uses the juxtaposition of horizontal and lateral average matrices to average the input matrices and performs attention operation. These methods and other similar implementations provide performance gains partly at considerably low computational cost compared with Axial attention. However, they ignore the lack of local details brought by the pooling/average operation. 

\subsection{Mobile semantic segmentation}
The mainstream of efficient segmentation methods are based on lightweight CNNs. 
DFANet~\cite{li2019dfanet} adopts a lightweight backbone to reduce computation cost and adds a feature aggregation module to refine high-level and low-level features. 
ICNet~\cite{zhao2018icnet} designs an image cascade network to speed up the algorithm, while BiSeNet~\cite{yu2018bisenet, yu2021bisenet} proposes two-stream paths for low-level details and high-level context information, separately. 
Fast-SCNN~\cite{poudel2019fast} shares the computational cost of the multi-branch network to yield a run-time fast segmentation CNN. 
TopFormer~\cite{zhang2022topformer} presents a new architecture with a combination of CNNs and ViT and achieves a good trade-off between accuracy and computational cost for mobile semantic segmentation. 
However, it is still restricted by the heavy computation load of global self-attention. 
