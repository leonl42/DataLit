\section{Conclusion}
\label{con}
In this paper, we propose {\em squeeze-enhanced Axial Transformer} ({\bf{\em SeaFormer}}) for mobile semantic segmentation, filling the vacancy of mobile-friendly efficient Transformer.
Moreover, we create a family of backbone architectures of SeaFormer and achieve cost-effectiveness.
The superior performance on the ADE20K, Cityscapes Pascal Context and COCO-Stuff datasets, and the lowest latency demonstrate its effectiveness on the ARM-based mobile device.
Moreover, we employ a feature upsampling-based multi-resolution distillation technique, significantly reducing the inference latency of our framework. This approach enhances model performance at various resolutions, enabling a high-resolution trained teacher model to instruct a low-resolution student model, thus facilitating efficient semantic understanding and prediction on mobile devices with reduced computational demands.
Beyond semantic segmentation, we further apply the proposed SeaFormer architecture to image classification and object detection problems, demonstrating the potential of serving as a versatile mobile-friendly backbone. 

\section{Data Availability Statement}
The datasets generated during and/or analysed during the current study are available in the Imagenet~\cite{deng2009imagenet} (\url{https://www.image-net.org/}), COCO~\cite{caesar2018coco} (\url{https://cocodataset.org}), ADE20K~\cite{zhou2017scene} (\url{https://groups.csail.mit.edu/vision/datasets/ADE20K/}), Cityscapes~\cite{cordts2016cityscapes} (\url{https://www.cityscapes-dataset.com}), Pascal Context~\cite{mottaghi2014role} (\url{https://cs.stanford.edu/~roozbeh/pascal-context/}) and COCO-Stuff~\cite{caesar2018coco} (\url{https://github.com/nightrome/cocostuff?tab=readme-ov-file}) repositories.