\vspace{-.8em}
\section{Related Work}
\vspace{-.8em}
Recent works on the E2E ToD systems \citep[\eg,][]{wu2019alternating,mintl2020,simpletod2020,ham2020end,soloist2021,ubar2021} have significantly improved the overall system's performance
and simplified the algorithmic designs in earlier works, which require solving several pipeline based sub-tasks \citep[\eg,][]{young2013pomdp,gao2018neural,damd2020}.
%Our approach may provide further enhancement to the original frameworks of E2E modeling.
The reward function trained by our methods can be leveraged as guidance to train existing E2E models, without changing the underlying structures. 
We demonstrate the effectiveness of our proposed reward learning methods under the structure of MinTL \citep{mintl2020} and GALAXY (\citet{he2022galaxy}; in Appendix~\ref{sec:exp_galaxy})
where we only add an additional reward-function-guided objective for the response-generation model, while keeping other components of the respective structure unchanged. 

%\sz{I am a little confused with this sentence: I feel another line of related research should be replaced to One line of. becasue I did not see the contrast between this paragraph and the above paragraph.}
One line of related research is applying RL to train ToD agents.
It is often unsuccessful to directly apply RL algorithms such as the DDPG \citep{lillicrap2015continuous} or PPO \citep{schulman2017proximal} since the agent training could potentially diverge \citep{zhao2019rethinking,gptcritic2022,kwan2022survey}.
Recently, a number of works consider offline RL \citep{offlinetutorial2020} as a promising solution to stabilize the agent training on a static dataset \citep[\eg,][]{jaques2020human,caspi2021,gptcritic2022,verma2022chai,snell2022offline,snell2022context}.
Following the offline RL principle, we use a reward-weighted regularization to stabilize the dialogue-agent training. 
%However, instead of learning an additional Q-function network in previous works \citep[\eg,][]{gptcritic2022,snell2022context},
%we only use the reward function to guide the dialogue-agent training.
Together with the incorporation of the Gumbel-softmax trick to estimate the policy gradient, 
our work retains algorithmic simplicity while improving the training stability and overall performance. 

Finally, 
our paper closely relates to works on reward learning for the ToD systems \citep[\eg,][]{takanobu2019guided,caspi2021}.
This research thread differs from works that directly use a manually designed reward function, 
which only gives sparse signals to indicate whether the agent achieves the goal or not \citep[\eg,][]{weisz2018sample,wu2019switch,gptcritic2022,snell2022context}. 
One line of this research direction is utilizing inverse reinforcement learning (IRL) \citep{russell1998learning} to learn a dense reward function, 
by assuming the collected data be expert demonstrations \citep{takanobu2019guided}. 
%Though using the reward function estimated by IRL allows the dialogue agents to learn faster, especially during the early stage, 
%theoretically the learned dialogue agents can only achieve similar performance with the collected data. 
However, modern IRL techniques such as GAIL-style algorithms \citep{ho2016generative,fu2017learning} often require iterating between reward learning and policy training \citep{finn2016connection}, which is computationally expensive and less scalable to dialogue-generation models.
Besides, the IRL methods aim at justifying the data, while the reward-learning framework in our work seeks to explain the preference among multiple trajectories, potentially leading to \textit{better-than-demo} agents \citep{trex2019,drex2020}.
Our paper is more closely related to the research on reward learning from preferences,
which has recently been adopted in NLP tasks, including training language models \citep{fan2020bayesian, ouyang2022training} and fine-tuning \citep{ziegler2019fine, zhang2021learning, zhang2022allsh},
question-answering with verification \citep{nakano2021webgpt,zhang2021knowing, menick2022teaching, zhang2022passage}, 
and ToD systems \citep{caspi2021}.
These works use the pairwise preference-learning objective in \citet{christiano2017deep}, which can be viewed as a special case of the \texttt{RewardNet} loss discussed in Section~\ref{sec:main_rew_obj}.  
Our work mainly focuses on the ToD task, where we study reward-function learning and reward utilization for training E2E dialogue agents. 
Appendix~\ref{sec:add_related_work} continues the discussion on related reward-learning methods.
%To the best of our knowledge, our in ToD tasks, u.

%in ToD tasks to utilize multiple trajectories to train reward function,
%which could potentially improve the 
%Compared with the pairwise based approaches,
%, \yy{our work can improve the learned reward-function (rewording, I will also discuss this in the main)} and thus the performance of the ToD system.







% \myparagraph{}
