\section{Examples of the generated dialogues} \label{sec:appendix_example}

Tables~\ref{table:dia_pmul4610} and \ref{table:dia_sng1012} show two case study comparing the generated responses from our method and from the baselines GPT-Critic and CASPI.
Our method outperforms the baselines in terms of both task completion and the quality of the generated expressions.

\begin{table}[H]
\captionsetup{font=small}
\caption{
Generated and original dialogue for ID PMUL4610.
``User" denotes User Utterance. ``System" denotes true System Response.
\textcolor{Maroon}{Red} annotations explain unsatisfactory responses and \textcolor{SeaGreen}{green} for satisfactory ones.
} 
\label{table:dia_pmul4610}
\centering 
\resizebox{1.\textwidth}{!}{
\begin{tabular}{@{}cll@{}}
\toprule
\multicolumn{3}{c}{PMUL4610}                                                                                                                                                                                                                      \\ \midrule
Turn Num.          & Method          & Original / Generated Dialogue                                                                                                                                                                              \\ \midrule
\multirow{5}{*}{1} & User  & i am looking for a museum in the east to visit.                                                                                                                                                             \\ \cmidrule(l){2-3} 
                   & GPT-Critic      & \makecell[l]{there are {[}value\_choice{]} {[}value\_type{]} in the {[}value\_area{]}.  {[}value\_name{]} is {[}value\_price{]}. \\ {[}value\_name{]} is {[}value\_price{]}. \bad{Do not make recommendation.}}                                                \\ \cmidrule(l){2-3} 
                   & CASPI           & \makecell[l]{there are {[}value\_choice{]} {[}value\_type{]} in the {[}value\_area{]}.  i would recommend \\ {[}value\_name{]}.  \bad{Do not explain recommendation.}
                   }                                                                                             \\ \cmidrule(l){2-3} 
                   & Ours            & \makecell[l]{there are {[}value\_choice{]} {[}value\_type{]} in the {[}value\_area{]}.  i recommend {[}value\_name{]} \\ because it is {[}value\_price{]} to get in . \good{Make recommendation and explain.}}                                                       \\ \cmidrule(l){2-3} 
                   & System & \makecell[l]{we have {[}value\_choice{]} {[}value\_type{]} in the {[}value\_area{]} area , i just went to the {[}value\_name{]} \\ and they have new exhibits . would you like more information on them ?}                  \\ \midrule
\multirow{5}{*}{2} & User Utterance  & sounds wonderful! can you give me their postcode?                                                                                                                                                          \\ \cmidrule(l){2-3} 
                   & GPT-Critic      & their postcode is {[}value\_postcode{]}. is there anything else i can help you with?                                                                                                                       \\ \cmidrule(l){2-3} 
                   & CASPI           & sure thing ! the postcode is {[}value\_postcode{]} \bad{Do not elicit conversation.}.                                                                                                                                                        \\ \cmidrule(l){2-3} 
                   & Ours            & sure , the postcode is {[}value\_postcode{]}. is there anything else i can help you with ?                                                                                                                 \\ \cmidrule(l){2-3} 
                   & System Response & their postcode is {[}value\_postcode{]}. can i help you with anything else today ?                                                                                                                         \\ \midrule
\multirow{5}{*}{3} & User Utterance  & i am also looking for a restaurant in the east part of town in the moderate price range.                                                                                                                   \\ \cmidrule(l){2-3} 
                   & GPT-Critic      & \makecell[l]{there are {[}value\_choice{]} restaurant -s in that area. {[}value\_name{]} and {[}value\_name{]}.\\ \bad{Do not elicit conversation}}                                                                                                         \\ \cmidrule(l){2-3} 
                   & CASPI           & {[}value\_name{]} is a {[}value\_price{]} restaurant in the {[}value\_area{]}.             \bad{Do not elicit conversation.}                                                                                                                \\ \cmidrule(l){2-3} 
                   & Ours            & \makecell[l]{there are {[}value\_choice{]} restaurant -s that fit your criteria . \\ {[}value\_name{]} serves {[}value\_food{]} food and is located in the {[}value\_area{]} part of town . \\ would you like their address ?} \\ \cmidrule(l){2-3} 
                   & System Response & \makecell[l]{there are {[}value\_choice{]} {[}value\_food{]} restaurant -s and {[}value\_choice{]} {[}value\_food{]}.\\ which would you prefer ?}                                                                          \\ \midrule
\multirow{5}{*}{4} & User Utterance  & give me the address of the 1 you recommend                                                                                                                                                                 \\ \cmidrule(l){2-3} 
                   & GPT-Critic      & \makecell[l]{{[}value\_name{]} is located at {[}value\_address{]}. is there anything else i can help you with? \\ \bad{Do not make recommendation.}}                                                                                                          \\ \cmidrule(l){2-3} 
                   & CASPI           & \makecell[l]{{[}value\_name{]} is a favorite of mine . they are located at {[}value\_address{]} \\ \bad{Do not elicit conversation.}}.                                                                                                                        \\ \cmidrule(l){2-3} 
                   & Ours            & \makecell[l]{{[}value\_name{]} is a great {[}value\_food{]} restaurant in the {[}value\_area{]}. \good{More plural expression.} \\ the address is {[}value\_address{]}. \bad{Do not elicit conversation.}}                                                                                   \\ \cmidrule(l){2-3} 
                   & System Response & \makecell[l]{i recommend {[}value\_name{]}, they are t {[}value\_address{]}, postal is {[}value\_postcode{]}, \\ can i assist with anything else ?}                                                                         \\ \midrule
\multirow{5}{*}{5} & User Utterance  & not at this time, thank you.                                                                                                                                                                               \\ \cmidrule(l){2-3} 
                   & GPT-Critic      & thank you for using our system. goodbye                                                                                                                                                                  \\ \cmidrule(l){2-3} 
                   & CASPI           & you are welcome . have a great day !                                                                                                                                                                       \\ \cmidrule(l){2-3} 
                   & Ours            & you are welcome . have a great day !                                                                                                                                                                       \\ \cmidrule(l){2-3} 
                   & System Response & thank you for calling. please let us know if we can assist you in the future. goodbye.                                                                                                                     \\ \bottomrule
\end{tabular}
}
\end{table}

\clearpage


\begin{table}[H]
\captionsetup{font=small}
\caption{
Generated and original dialogue for ID SNG1012.
``User" denotes User Utterance. ``System" denotes true System Response.
\textcolor{Maroon}{Red} annotations explain unsatisfactory responses and \textcolor{SeaGreen}{green} for satisfactory ones.
} 
\label{table:dia_sng1012}
\centering 
\resizebox{1.\textwidth}{!}{
\begin{tabular}{@{}cll@{}}
\toprule
\multicolumn{3}{c}{SNG1012}                                                                                                                                                                                                                                         \\ \midrule
Turn Num.          & Method     & Original / Generated Dialogue                                                                                                                                                                                                     \\ \midrule
\multirow{5}{*}{1} & User       & i need to find a guest house with a 3 star rating                                                                                                                                                                                 \\ \cmidrule(l){2-3} 
                   & GPT-Critic & \makecell[l]{i have {[}value\_choice{]} options for you. \\ do you have a price range or area of town you would like to stay in? }                                                                                                                 \\ \cmidrule(l){2-3} 
                   & CASPI      & \makecell[l]{i have {[}value\_choice{]} {[}value\_type{]} that meet your criteria . \\ do you have a preference of area or price range ? }                                                                                                         \\ \cmidrule(l){2-3} 
                   & Ours       & \makecell[l]{i have {[}value\_choice{]} {[}value\_type{]} with {[}value\_stars{]} stars .\\  would you prefer a {[}value\_price{]} or {[}value\_price{]} price range? \bad{Do not ask location.}}                                                                           \\ \cmidrule(l){2-3} 
                   & System     & we have {[}value\_choice{]} such {[}value\_type{]}. do you have a preferred location?                                                                                                                                             \\ \midrule
\multirow{5}{*}{2} & User       & no, i don't. i want 1 that include -s free wifi.                                                                                                                                                                                  \\ \cmidrule(l){2-3} 
                   & GPT-Critic & \makecell[l]{{[}value\_name{]} is a {[}value\_price{]} {[}value\_stars{]} star {[}value\_type{]} in the {[}value\_area{]}\\ with free wifi and parking.  would you like me to book it for you? }                                                   \\ \cmidrule(l){2-3} 
                   & CASPI      & \makecell[l]{i have {[}value\_choice{]} {[}value\_type{]} that meet your criteria . {[}value\_name{]} and {[}value\_name{]}. \\ both are {[}value\_price{]} and offer free wifi and parking . do you have a preference ?} \\ \cmidrule(l){2-3} 
                   & Ours       & \makecell[l]{i have {[}value\_choice{]} {[}value\_type{]} that have free wifi .  \\ i would recommend the {[}value\_name{]}. would you like me to book a room ?}                                                                                    \\ \cmidrule(l){2-3} 
                   & System     & {[}value\_name{]}, {[}value\_name{]}, and {[}value\_name{]} are all available if you would like 1 of those?                                                                                                                       \\ \midrule
\multirow{5}{*}{3} & User       & please book the hamilton lodge, for 6 people.                                                                                                                                                                                     \\ \cmidrule(l){2-3} 
                   & GPT-Critic & \makecell[l]{booking was successful. reference number is : {[}value\_reference{]}.   is there anything else \\ i can help you with? \bad{Finish booking before acquire sufficient information.}}                                                                                                                 \\ \cmidrule(l){2-3} 
                   & CASPI      & i can book the {[}value\_name{]} for you . how many nights will you be staying ?                                                                                                                                                  \\ \cmidrule(l){2-3} 
                   & Ours       & i would be happy to book the {[}value\_name{]} for you . what day would you like to arrive ?                                                                                                                                      \\ \cmidrule(l){2-3} 
                   & System     & \makecell[l]{i would be happy to. how many nights would you like to stay? \\ and i 'll need to know the day you would like to start your stay.}                                                                                                    \\ \midrule
\multirow{5}{*}{4} & User       & i 'll be staying 2 nights starting on wednesday. oh, and there are 6 of us.                                                                                                                                                       \\ \cmidrule(l){2-3} 
                   & GPT-Critic & \makecell[l]{i have booked you a room at {[}value\_name{]} for {[}value\_people{]} people for {[}value\_stay{]} \\ nights starting {[}value\_day{]}. your reference number is {[}value\_reference{]}. \\ is there anything else i can help you with? \bad{Dialogue diverges from ground truth.}} \\ \cmidrule(l){2-3} 
                   & CASPI      & \makecell[l]{i am sorry , but there are no rooms available for those dates .\\  would you like to try a different day or a shorter stay ?}                                                                                                         \\ \cmidrule(l){2-3} 
                   & Ours       & i am sorry , but i was unable to book that {[}value\_type{]}. would you like to try another 1 ?                                                                                                                                   \\ \cmidrule(l){2-3} 
                   & System     & i am sorry, my reservation attempt was unsuccessful. will another day or length of stay work?                                                                                                                                     \\ \midrule
\multirow{5}{*}{5} & User       & can you try the same parameters at another hotel?                                                                                                                                                                                 \\ \cmidrule(l){2-3} 
                   & GPT-Critic & \makecell[l]{i have booked you at {[}value\_name{]} for {[}value\_stay{]} nights starting {[}value\_day{]} \\ for {[}value\_people{]} people. your reference number is {[}value\_reference{]}.  \\ is there anything else i can help you with? \bad{Repeating diverged dialogue.}}        \\ \cmidrule(l){2-3} 
                   & CASPI      & i am sorry , i am not able to book at another {[}value\_type{] \bad{Fail to continue conversation.}}.                                                                                                                                                                  \\ \cmidrule(l){2-3} 
                   & Ours       & \makecell[l]{i have {[}value\_name{]} in the {[}value\_area{]}. it is a {[}value\_price{]} {[}value\_stars{]} star {[}value\_type{]}. \\ would you like me to book that for you ?}                                                                 \\ \cmidrule(l){2-3} 
                   & System     & of course, could you tell me the area you would like to stay in so i can narrow it for you?                                                                                                                                       \\ \midrule
\multirow{5}{*}{6} & User       & the are does not matter, just 3 stars and wifi at a guest house please.                                                                                                                                                           \\ \cmidrule(l){2-3} 
                   & GPT-Critic & would you like me to book it for you?      \bad{Fail to complete booking.}                                                                                                                                                                                       \\ \cmidrule(l){2-3} 
                   & CASPI      & i was able to book you at {[}value\_name{]}. reference number {[}value\_reference{]}.                                                                                                                                             \\ \cmidrule(l){2-3} 
                   & Ours       & \makecell[l]{i was able to book you at {[}value\_name{]}. your reference number is {[}value\_reference{]}.\\ is there anything else i can help you with ? \good{More plural expression and elicit conversation.}}                                                                                        \\ \cmidrule(l){2-3} 
                   & System     & \makecell[l]{i have got you a reservation for {[}value\_people{]} at hobson's house for {[}value\_stay{]} nights. \\ your reference number is {[}value\_reference{]}.}                                                                             \\ \midrule
\multirow{5}{*}{7} & User       & thank you so much! that should be all i need.                                                                                                                                                                                 \\ \cmidrule(l){2-3} 
                   & GPT-Critic & you are welcome. have a great day.                                                                                                                                                                                                \\ \cmidrule(l){2-3} 
                   & CASPI      & you are welcome. have a great day!                                                                                                                                                                                              \\ \cmidrule(l){2-3} 
                   & Ours       & you are welcome. have a great day!                                                                                                                                                                                              \\ \cmidrule(l){2-3} 
                   & System     & you are welcome. enjoy your day.                                                                                                                                                                                                  \\ \bottomrule
\end{tabular}
}
\end{table}


\clearpage
\section{Algorithmic details}\label{sec:algo_box}

\myparagraph{Prepossessing.} 
The raw corpus is prepossessed by common practice in the ToD literature.
Specifically, we represent the database (DB) query results as one-hot vectors following \citet{multiwoz2018}, use domain-adaptive delexicalization proposed by \citet{wen2016network}, and generate delexicalized responses with placeholders for specific DST/DB information as in
\citet{damd2020}.

\myparagraph{Implementation of the response model.} 
Our model in \Secref{sec:exp} is based on the MinTL ToD model \citep{mintl2020}, which uses the pre-trained BART-large model \citep{bart2019}.
MinTL directly works on the system response and does not explicitly output the dialogue act.
Our proposed method in \Secref{sec:main_method} is applied to the response training, and we retain the DST-training loss in MinTL.
Our model is trained by fine-tuning BART on the training set and early-stopping by the validation set.


\myparagraph{Implementation of the reward model.} 
Our reward model is implemented by the encoder part of the BART-base model, followed by a simple two-layer MLP.
The output of the reward model is scaled to $[0,1]$ via the sigmoid function.
The input to the reward model is the concatenation of the belief state, system response, and dialogue goal, at each turn of the sampled dialogue rollout.
The model outputs the reward of each turn in the dialogue rollout, which is summed and fed into the losses proposed in \Secref{sec:main_rew_obj}.
We use the HuggingFace library \citep{huggingface2019} to implement our reward model. 

Algorithm~\ref{algo:main} illustrates the pipeline of our methods.

\begin{algorithm}[H]
\captionsetup{font=small}
\caption{Pipeline of the proposed reward learning and utilization methods for training E2E ToD agents.}
\label{algo:main}
\begin{algorithmic}
\STATE \textbf{Input:} Reward function $\gR_{\theta}(o, a, g)$, ToD agent $\pi_{\phi}$, dataset $\hat{\mathsf{D}} := \left\{\br{g^{(k)}, (o_t^{(k)}, a_{t}^{(k)})_{t=0}^{T}} \right\}_{k=1}^{K}$, number of iterations $M_{1}$ and $M_{2}$, probabilistic transform function $\Phi$, hyperparameters $N$, $\alpha$.
% \STATE Scale the rewards in the offline dataset.
\STATE
\FOR{iteration $\in \{1, \ldots,M_1\}$}
%\IF{iteration $\%$ {\tt model\_retrain\_period} == 0}
%\STATE Estimate $\omega(s,a)$ via Eq.~\eqref{eq:dr_target_final}; train \rebuttal{$\widehat P$ and $\hat r$} by weighted MLE (Eq.~\eqref{eq:model_weighted_mle}) with $\omega(s,a)$.
%\STATE Train MIW network $\omega$  using Eq.~\eqref{eq:dr_target_final}.
%\STATE Train $\widehat P$ by weighted MLE (Eq.~\eqref{eq:model_weighted_mle}) with $\omega(s,a)$.
%\ENDIF
\STATE Sample $N$ dialogue trajectories from the dataset $\hat{\mathsf{D}}$.
\STATE Optimize $\gR_{\theta}$ via \rewardnet (Eq.~\eqref{eq:listnet}) or \rewardmle (Eq.~\eqref{eq:listmle}).
%\STATE XX
%Get critic target by Eq.~\eqref{eq:q_tilde} and train critics by Eq.~\eqref{eq:critic_target_main}.
%\STATE Construct $\gB_{\mathrm{fake}}$ by Eq.~\eqref{eq:fake_sample} and sample $\gB_{\mathrm{true}}\sim \denv$.
%\STATE Calculate generator loss $\gL_g(\vphi)$ using Eq.~\eqref{eq:generator_loss}.
%\STATE XX
%\STATE XX
\ENDFOR
\STATE Fix the reward function $\gR_{\theta}$.
\FOR{iteration $\in \{1, \ldots,M_2\}$}
\STATE Sample a batch of transition tuples $\br{g^{(k)}, (o_t^{(k)}, a_{t}^{(k)})}$ from the dataset $\hat{\mathsf{D}}$.
\STATE Optimize the ToD agent $\pi_{\phi}$ via Eq.~\eqref{eq:final_loss}.
\ENDFOR
\STATE
\STATE \textbf{Output:} Trained ToD agent $\pi_{\phi}$. 

\end{algorithmic}
\end{algorithm}


\clearpage


\section{Comparison with some other reward-learning methods in RL-based dialogue agents} \label{sec:add_related_work}
As an additional discussion on related work, in this section we briefly compare our work with three other reward-learning methods in RL-based dialogue agents, \ie, \citet{saito2018curriculum}, \citet{hu2018playing}, and \citet{li2020guided}.



These three papers all focus on the dialogue-management module of the pipeline design, wherein the action spaces of the agents are some abstract dialogue acts rather than the human-language-like system response as in our paper.
The possible system responses are fixed in these three papers.
For example, \citet{hu2018playing} train the agent to select from ``the set of the indices to all available questions in the Q20 game;'' and \citet{li2020guided} have an action space of size $300$.
As discussed in \Secref{sec:intro}, such a pipeline approach requires intensive structural designs, such as determining the possible questions in the Q20 games; and may not enjoy the language plurality and conversation elicitation that our E2E model could offer, \eg, as shown in
Table~\ref{table:dia_sng1012}.
Due to the complexity and the much higher dimension of our action space as the system responses, the methods proposed in these three papers are not directly applicable to our setting, which will be discussed in detail below.



The  method proposed in \citet{saito2018curriculum} require specially-designed curriculum data and hand-crafted decomposition of the entire task into sub-tasks, which are not readily available and are non-trival for large-scale multi-domain dialogue corpus such as our tested MultiWOZ 2.0 dataset.
The use of progressive neural networks to provide reward information in \citet{saito2018curriculum} require additional computation and memory complexity and thus may not scale to transformers. 
Meanwhile, our method scales well to transformers, as shown in our experiments (\Secref{sec:exp}).
Further, the method in \citet{saito2018curriculum} may  only be feasible on the task of constrained information-retrieval, but not on some more general tasks such as the booking requirement in the tested MultiWOZ 2.0 dataset.
Our experimental results show that our method is capable of such tasks.



Similar to our work, \citet{hu2018playing} propose a neural network to approximate the reward function to deliver immediate rewards at each timestep.
Apart from the aforementioned simple action space, \citet{hu2018playing} use the long-term return $G_t$ as a surrogate indicator of $r_{t+1}$ to train the reward function (Eq.~(6) of \citet{hu2018playing}), which is lack of justification.
By contrast, as discussed in \Secref{sec:background} and \Secref{sec:main_method}, our method is based on the classical
approaches in the learning-to-rank (LTR) literature and extends the classical reward-learning-from-preferences into utilizing multiple dialogue trajectories simultaneously to optimize the reward function.


As discussed before, \citet{li2020guided} consider a relatively small action space of size $300$ and learns the reward model via the GAN structure, which may not stably scale up to high-dimensional action space such as the system response in our E2E ToD system.
The learned reward function in \citet{li2020guided} only measure the probability that the input is from the real-data distribution, \ie, only considers \emph{a pair of} dialogue state $s_t$ and the corresponding system action $a_t$.
This reward function does not consider the success of the entire dialogue, which is intuitively less favorable to the E2E ToD systems.
By contrast, our method trains a reward function that aligns with some evaluations on the \emph{entire} dialogue trajectories, which is more directly related to the usage of the ToD systems.




We further note that apart from the reward-learning method, our paper also discusses using the Gumbel-softmax trick as a more stable method to train the E2E ToD systems, and conducts a toy experiment in \Secref{sec:exp_abla} to illustrate the advantage of the Gumbel-softmax trick over the classical REINFORCE method.
This is not covered in the three prior works \citet{saito2018curriculum}, \citet{hu2018playing}, and \citet{li2020guided}.


Finally, these three prior works use online RL methods such as DQN, REINFORCE, and PPO, which require environmental interactions and are thus less practical, as discussed in \Secref{sec:intro}.
In contrast, our method allows training E2E response-generation models from static datasets by utilizing offline RL techniques \citep[\eg,][]{offlinetutorial2020,td3bc2021,sdmgan2022,jointmatching2022,wmbrl2022}.

% -----------------------
% \rebuttal{
% We also note that 
% \rewardmle objective potentially can cope with some ``preference'' ordering since it only requires ranking order, instead of the metric scores
% }

\clearpage
\section{Detailed comparison with CASPI} \label{sec:detailed_comp_caspi}

Table~\ref{table:detailed_caspi} further compares CASPI and our two methods: \rewardnet+GS, $N=3, \Phi=(\cdot)^1$ and  \rewardmle+GS, $N=5, \Phi=\exp(\cdot)$, showing a detailed breakdown of the scores onto each of the five tested  random seeds.

The performances of both our methods and CASPI are relatively stable across random seeds.
In particular, both of our methods have higher Combined Score than CASPI \emph{on each of the five tested random seeds}.
This stable improvement of our methods over CASPI aligns with our intuition discussed in \Secref{sec:main_method} and our main experimental results in \Secref{sec:exp_main}.

We note that both the Average score and the Median score across the tested random seeds are valid metrics for performance comparison.
Nevertheless, there is some ambiguity in calculating the ``Median'' of the Combined Score, namely, should it be the median of the Combined Scores on each random seed, or should it be calculated as $\mathrm{Median}(\mathrm{Combined~Score}) \triangleq (\mathrm{Median}(\mathrm{Inform}) + \mathrm{Median}(\mathrm{Success})) \times 0.5 + \mathrm{Median}(\mathrm{BLEU})$?
The first way aligns better with the definition of ``Median'' while the second way aligns better with the definition of Combined Score.
Such an ambiguity is cleared out when using the Average as the evaluation metric.
Besides, the metric Average score is widely used in prior work \citep[\eg,][]{damd2020,mintl2020,gptcritic2022}.
With these considerations, we choose to report the Average over five random seeds in our experimental section (\Secref{sec:exp}).

\begin{table}[H] 
\captionsetup{font=small}
\caption{
\footnotesize{
Per random-seed results of the E2E response generation task on the MultiWOZ 2.0 dataset, comparing CASPI and our two models: \rewardnet+GS, $N=3, \Phi=(\cdot)^1$ and  \rewardmle+GS, $N=5, \Phi=\exp(\cdot)$.
Here, $(\cdot)^1$ denotes the power function with power $1$.
``Comb.'' is the Combined Score.
The row ``Median'' shows the median score of the corresponding column over the five random seeds.
}
} 
\label{table:detailed_caspi} 
% \vspace{-.7em}
\centering 
% \def\arraystretch{1.2}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
\multirow{2}{*}{Seed}        & \multicolumn{4}{c}{CASPI}                                & \multicolumn{4}{c}{\rewardnet+GS, $N=3, \Phi=(\cdot)^1$} & \multicolumn{4}{c}{\rewardmle+GS, $N=5, \Phi=\exp(\cdot)$} \\
                             & Inform & Success & BLEU   & Comb.                        & Inform & Success & BLEU  & Comb.                        & Inform       & Success       & BLEU        & Comb.        \\ \midrule
\multicolumn{1}{c|}{111}     & 88.19  & 80.88   & 18.88  & \multicolumn{1}{c|}{103.42} & 90.49  & 82.68   & 18.42 & \multicolumn{1}{c|}{105.01} & 93.99        & 84.18         & 16.82       & 105.91      \\
\multicolumn{1}{c|}{333}     & 91.69  & 83.58   & 18.17  & \multicolumn{1}{c|}{105.81} & 94.39  & 85.09   & 18.69 & \multicolumn{1}{c|}{108.43}  & 95.10         & 85.09         & 17.51       & 107.61      \\
\multicolumn{1}{c|}{555}     & 91.99  & 81.78   & 17.21  & \multicolumn{1}{c|}{104.10} & 91.29  & 82.18   & 18.54 & \multicolumn{1}{c|}{105.28} & 91.99        & 83.68         & 18.67       & 106.51      \\
\multicolumn{1}{c|}{777}     & 93.39  & 83.48   & 17.13  & \multicolumn{1}{c|}{105.57} & 91.89  & 84.18   & 18.36 & \multicolumn{1}{c|}{106.40} & 92.79        & 83.18         & 17.73       & 105.72      \\
\multicolumn{1}{c|}{999}     & 91.59  & 84.28   & 17.09  & \multicolumn{1}{c|}{105.03} & 95.10   & 87.49   & 17.74 & \multicolumn{1}{c|}{109.04} & 91.59        & 83.38         & 19.48       & 106.97      \\ \midrule
\multicolumn{1}{c|}{Average} & 91.37  & 82.80    & 17.70 & \multicolumn{1}{c|}{104.78} & 92.63 & 84.32  & 18.35 & \multicolumn{1}{c|}{106.83} & 93.09       & 83.90        & 18.04      & 106.54      \\
\multicolumn{1}{c|}{Median}  & 91.69  & 83.48   & 17.21  & \multicolumn{1}{c|}{105.03} & 91.89  & 84.18   & 18.42 & \multicolumn{1}{c|}{106.40} & 92.79        & 83.68         & 17.73       & 106.51      \\ \bottomrule
\end{tabular}
}
\end{table}


\section{Experiments with the GALAXY} \label{sec:exp_galaxy}

To further demonstrate the efficacy and applicability of our approach, we apply our reward learning and utilization methods to the recently proposed GALAXY backbone \citep{he2022galaxy}, which achieves competitive performance on the E2E response-generation task on the MultiWOZ 2.0 dataset.
We note that the GALAXY paper does not disclose \emph{how many} and \emph{which} random seeds were used to obtain its main results; and the official codebase fixes the random seed as $10$.
This makes us unsure if its reported results are only from this single seed of $10$.
To mitigate the randomness in the optimization process, we re-run the vanilla GALAXY on the five random seeds used to generate our main results and compare it on these seeds with the variants equipped with our methods \rewardnet+GS, $N=3, \Phi=(\cdot)^1$ and \rewardmle+GS, $N=5, \Phi=\exp(\cdot)$.
Table~\ref{table:detailed_galaxy} shows a detailed breakdown of the scores on each of the five random seeds.
Note that since we use different random seeds than the original GALAXY paper, we are unable to get its reported scores.

We see from Table~\ref{table:detailed_galaxy} that adding our reward learning and utilization methods improves the performance of the vanilla GALAXY, in almost all evaluation metrics, in both the Average and the Median scores.
In particular, adding our \rewardmle+GS method improves the average Combined Score of the vanilla GALAXY by $3.27$, and adding our \rewardnet +GS improves the vanilla GALAXY by $2.11$.
These relatively significant improvements may further demonstrate the effectiveness and general applicability of our proposed methods.


\begin{table}[H] 
\captionsetup{font=small}
\caption{
\footnotesize{
Per random-seed results of the E2E response-generation task on the MultiWOZ 2.0 dataset, comparing the vanilla GALAXY and the variants with our proposed methods: \rewardnet+GS, $N=3, \Phi=(\cdot)^1$ and  \rewardmle+GS, $N=5, \Phi=\exp(\cdot)$.
Here, $(\cdot)^1$ denotes the power function with power $1$.
``Comb.'' is the Combined Score.
The row ``Median'' shows the median score of the corresponding column over the five tested random seeds.
}
} 
\label{table:detailed_galaxy} 
% \vspace{-.7em}
\centering 
% \def\arraystretch{1.2}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
\multirow{2}{*}{Seed}        & \multicolumn{4}{c}{GALAXY}                             & \multicolumn{4}{c}{\rewardnet+GS, $N=3, \Phi=(\cdot)^1$} & \multicolumn{4}{c}{\rewardmle+GS, $N=5, \Phi=\exp(\cdot)$} \\
                             & Inform & Success & BLEU  & Comb.                       & Inform  & Success & BLEU  & Comb.                       & Inform       & Success       & BLEU        & Comb.        \\ \midrule
\multicolumn{1}{c|}{111}     & 93.60  & 85.40   & 19.97 & \multicolumn{1}{c|}{109.47} & 91.30   & 83.00   & 19.03 & \multicolumn{1}{c|}{106.18} & 92.50        & 84.10         & 18.89       & 107.19       \\
\multicolumn{1}{c|}{333}     & 86.50  & 77.90   & 17.87 & \multicolumn{1}{c|}{100.07} & 90.00   & 82.00   & 17.95 & \multicolumn{1}{c|}{103.95} & 90.90        & 82.70         & 18.27       & 105.07       \\
\multicolumn{1}{c|}{555}     & 89.20  & 81.60   & 19.96 & \multicolumn{1}{c|}{105.36} & 92.00   & 83.40   & 19.44 & \multicolumn{1}{c|}{107.14} & 93.90        & 85.50         & 19.47       & 109.17       \\
\multicolumn{1}{c|}{777}     & 90.40  & 81.90   & 18.81 & \multicolumn{1}{c|}{104.96} & 93.70   & 85.10   & 18.90 & \multicolumn{1}{c|}{108.30} & 95.80        & 86.10         & 19.36       & 110.31       \\
\multicolumn{1}{c|}{999}     & 88.30  & 79.40   & 18.59 & \multicolumn{1}{c|}{102.44} & 92.60   & 83.60   & 19.19 & \multicolumn{1}{c|}{107.29} & 91.90        & 82.50         & 19.70       & 106.90       \\ \midrule
\multicolumn{1}{c|}{Average} & 89.60  & 81.24   & 19.04 & \multicolumn{1}{c|}{104.46} & 91.92   & 83.42   & 18.90 & \multicolumn{1}{c|}{106.57} & 93.00        & 84.18         & 19.14       & 107.73       \\
\multicolumn{1}{c|}{Median}  & 89.20  & 81.60   & 18.81 & \multicolumn{1}{c|}{104.96} & 92.00   & 83.40   & 19.03 & \multicolumn{1}{c|}{107.14} & 92.50        & 84.10         & 19.36       & 107.19       \\ \bottomrule
\end{tabular}
}
\end{table}

\section{Experiments on the MultiWOZ 2.1 dataset} \label{sec:exp_multiwoz21}

To test the efficacy of our proposed methods on additional datasets, we run our two methods \rewardnet + GS, $N=3$, $\Phi=(\cdot)^{1}$  and \rewardmle + GS, $N=5$, $\Phi=\exp(\cdot)$ on the MultiWOZ 2.1 dataset.
Table~\ref{table:main21} compares our two methods with the baselines SimpleTOD and UBAR in the main evaluation (Table~\ref{table:main}), which also report results on the MultiWOZ 2.1 dataset.
Additionally, we also present our rerun of CASPI on this dataset.
As in Table~\ref{table:main}, the Combined Scores of our methods are generally better than the baselines.
In fact, our methods achieve both good task completion (Inform and Success rates) and fluent generated responses (BLEU score).

Table~\ref{table:detailed_m21} shows a detailed breakdown of the scores of CASPI and our two methods on each of the five tested random seeds.
We see that the scores of our methods are generally robust across random seeds.
In fact, on four out of those five seeds, at least one of our methods performs better than CASPI.
Further, our methods have higher Average and Median scores than CASPI on each of the four evaluation metrics.
This set of experiments may further demonstrate the efficacy of our proposed methods.


\begin{table}[H]
\captionsetup{font=small}
\caption{
\footnotesize{Results of the E2E response-generation task on the MultiWOZ 2.1 dataset.
The best result on each metric is bold.
The results of SimpleTOD and  UBAR are from the original paper.
The results of CASPI are from our reproduction.
All our provided results are the average over five random seeds.
``GS" denotes the Gumbel-softmax trick.
$(\cdot)^{1}$ denotes the power function with power $1$.}
} 
\label{table:main21} 
\centering 
% \def\arraystretch{1.2}
\resizebox{.85\textwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
Algorithms                                                & Inform & Success & BLEU  & Combined Score \\ \midrule
SimpleTOD  \citep{simpletod2020}                                                & 85.00  & 70.50   & 15.23 & 92.98          \\
UBAR    \citep{ubar2021}                                                   & \textbf{95.70}  & 81.80   & 16.50 & 105.25         \\
CASPI       \citep{caspi2021}                                               & 91.43  & 83.50   & 17.93 & 105.40         \\ \midrule
\rewardnet + GS, $N=3$, $\Phi=(\cdot)^{1}$ & 92.79  & \textbf{84.48}   & 17.99 & 106.62         \\
\rewardmle + GS, $N=5$, $\Phi=\exp(\cdot)$ & 92.87  & 83.90   & \textbf{18.73} & \textbf{107.11}         \\ \bottomrule
\end{tabular}
}
\end{table}




\begin{table}[H] 
\captionsetup{font=small}
\caption{
\footnotesize{
Per random-seed results of the E2E response-generation task on the MultiWOZ 2.1 dataset, comparing the CASPI and the variants with our proposed methods: \rewardnet+GS, $N=3, \Phi=(\cdot)^1$ and  \rewardmle+GS, $N=5, \Phi=\exp(\cdot)$.
Here, $(\cdot)^1$ denotes the power function with power $1$.
``Comb.'' is the Combined Score.
The row ``Median'' shows the median score of the corresponding column over the five tested random seeds.
}
} 
\label{table:detailed_m21} 
% \vspace{-.7em}
\centering 
% \def\arraystretch{1.2}
\resizebox{1.\textwidth}{!}{
\begin{tabular}{@{}ccccccccccccc@{}}
\toprule
\multirow{2}{*}{Seed}        & \multicolumn{4}{c}{CASPI}                              & \multicolumn{4}{c}{\rewardnet+GS, $N=3, \Phi=(\cdot)^1$} & \multicolumn{4}{c}{\rewardmle+GS, $N=5, \Phi=\exp(\cdot)$} \\
                             & Inform & Success & BLEU  & Comb.                       & Inform  & Success & BLEU  & Comb.                       & Inform       & Success       & BLEU        & Comb.        \\ \midrule
\multicolumn{1}{c|}{111}     & 89.39  & 82.08   & 18.14 & \multicolumn{1}{c|}{103.88} & 96.6    & 87.19   & 17.23 & \multicolumn{1}{c|}{109.13} & 93.39        & 84.58         & 19.39       & 108.38       \\
\multicolumn{1}{c|}{333}     & 90.99  & 82.08   & 18.25 & \multicolumn{1}{c|}{104.79} & 91.69   & 83.78   & 18.43 & \multicolumn{1}{c|}{106.17} & 92.19        & 81.78         & 17.36       & 104.35       \\
\multicolumn{1}{c|}{555}     & 91.59  & 82.68   & 17.9  & \multicolumn{1}{c|}{105.04} & 91.79   & 85.29   & 17.63 & \multicolumn{1}{c|}{106.17} & 93.19        & 85.49         & 19.08       & 108.42       \\
\multicolumn{1}{c|}{777}     & 91.89  & 84.28   & 17.78 & \multicolumn{1}{c|}{105.87} & 91.49   & 82.68   & 18.23 & \multicolumn{1}{c|}{105.32} & 93.29        & 83.88         & 19.15       & 107.74       \\
\multicolumn{1}{c|}{999}     & 93.29  & 86.39   & 17.6  & \multicolumn{1}{c|}{107.44} & 92.39   & 83.48   & 18.41 & \multicolumn{1}{c|}{106.35} & 92.29        & 83.78         & 18.65       & 106.69       \\ \midrule
\multicolumn{1}{c|}{Average} & 91.43  & 83.50   & 17.93 & \multicolumn{1}{c|}{105.40} & 92.79   & 84.48   & 17.99 & \multicolumn{1}{c|}{106.62} & 92.87        & 83.90         & 18.73       & 107.11       \\
\multicolumn{1}{c|}{Median}  & 91.59  & 82.68   & 17.90 & \multicolumn{1}{c|}{105.04} & 91.79   & 83.78   & 18.23 & \multicolumn{1}{c|}{106.17} & 93.19        & 83.88         & 19.08       & 107.74       \\ \bottomrule
\end{tabular}
}
\end{table}


\section{Implementation details} \label{sec:impl_details}

Our implementation is based on the official codebase of MinTL and CASPI.
Apart from the hyperparameters discussed in \Secref{sec:exp_abla}, most other hyperparameters and the training procedure of our models follow MinTL and CASPI.
In addition to the discussion of our method in \Secref{sec:main_method}, we list the important hyperparameters for training our reward model in Table~\ref{table:param_rew} and the important hyperparameters for training our response-generation model in Table~\ref{table:param_response}.
Both BART models use the default token length of 142. 

We note that unlike CASPI which uses the dialogue acts as the actions, the action space of our reward model is the system response, which is a combinatorial space of the vocabulary.
We use this action space because we want to pass gradients from the reward model to the E2E response-generation model during the training process, where the output of the E2E model is the human-language-like system response.

For the reward-function learning, we do not change the length of the trajectories in the dataset. 
The reward model is updated by the preference scores/orderings among multiple trajectories of the same length.
Our intuition is that trajectories of the same length may roughly correspond to tasks of similar complexity, making the preference among them more comparable and meaningful.
This approach is also taken by the prior work CASPI.

Our tested MultiWOZ2.0 dataset is publicly available at \href{https://github.com/budzianowski/multiwoz}{https://github.com/budzianowski/multiwoz}, and the MultiWOZ2.1 dataset is available at \href{https://github.com/thu-coai/ConvLab-2/tree/master/data/multiwoz}{https://github.com/thu-coai/ConvLab-2/tree/master/data/multiwoz}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{tabular}{cc}
    \begin{minipage}{.5\linewidth}
\begin{table}[H]
\captionsetup{font=small}
\caption{
\small Hyperparameters for training our reward model.
} 
\label{table:param_rew} 
\centering 
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}ll@{}}
\toprule
Hyperparameter              & Value                                       \\ \midrule
Action space                & System response                             \\
Gradient clipping norm      & 1.0                                         \\
Learning rate               & 3e-5                                        \\
Gradient accumulation steps & 16                                          \\
Batch size                  & 4                                           \\
Context window              & 2                                           \\
Learning-rate decay         & 0.8                                         \\
Learning-rate scheduler     & \texttt{ReduceLROnPlateau} \\
Scheduler patience          & 3                                           \\
Early-stop count            & 7                                           \\
Backbone                    & BART-base                                   \\ \bottomrule
\end{tabular}
}
\end{table}
\end{minipage} &
\begin{minipage}{.5\linewidth}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[H]
\captionsetup{font=small}
\caption{
\small Hyperparameters for training our response generation model.
} 
\label{table:param_response} 
\centering 
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}ll@{}}
\toprule
Hyperparameter              & Value                       \\ \midrule
Action space                & System response             \\
Gradient clipping norm      & 1.0                         \\
Learning rate               & 3e-5                        \\
Gradient accumulation steps & 8                           \\
Batch size                  & 8                           \\
Context window              & 2                           \\
Learning-rate decay         & 0.8                         \\
Learning-rate scheduler     & \texttt{LambdaLR}                    \\
Early-stop count            & 7                           \\
Seed                        & \{111, 333, 555, 777, 999\} \\
Backbone                    & BART-large-cnn              \\ \bottomrule
\end{tabular}
}
\end{table}
\end{minipage} 
\end{tabular}




