\begin{thebibliography}{93}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Pieter~Abbeel, and
  Zaremba]{andrychowicz2017hindsight}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter~Abbeel, and Wojciech
  Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Bradley \& Terry(1952)Bradley and Terry]{bradley1952rank}
Ralph~Allan Bradley and Milton~E Terry.
\newblock Rank analysis of incomplete block designs: I. the method of paired
  comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brown et~al.(2019)Brown, Goo, Nagarajan, and Niekum]{trex2019}
Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum.
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock In \emph{International conference on machine learning}, pp.\
  783--792. PMLR, 2019.

\bibitem[Brown et~al.(2020)Brown, Goo, and Niekum]{drex2020}
Daniel~S Brown, Wonjoon Goo, and Scott Niekum.
\newblock Better-than-demonstrator imitation learning via automatically-ranked
  demonstrations.
\newblock In \emph{Conference on robot learning}, pp.\  330--359. PMLR, 2020.

\bibitem[Buckman et~al.(2020)Buckman, Gelada, and
  Bellemare]{buckman2020importance}
Jacob Buckman, Carles Gelada, and Marc~G Bellemare.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock \emph{arXiv preprint arXiv:2009.06799}, 2020.

\bibitem[Budzianowski et~al.(2018)Budzianowski, Wen, Tseng, Casanueva, Ultes,
  Ramadan, and Ga{\v{s}}i{\'c}]{multiwoz2018}
Pawe{\l} Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva,
  Stefan Ultes, Osman Ramadan, and Milica Ga{\v{s}}i{\'c}.
\newblock Multiwoz--a large-scale multi-domain wizard-of-oz dataset for
  task-oriented dialogue modelling.
\newblock \emph{arXiv preprint arXiv:1810.00278}, 2018.

\bibitem[Burges et~al.(2005)Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton,
  and Hullender]{ranknet2005}
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole
  Hamilton, and Greg Hullender.
\newblock Learning to rank using gradient descent.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pp.\  89--96, 2005.

\bibitem[Cao et~al.(2007)Cao, Qin, Liu, Tsai, and Li]{listnet2007}
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li.
\newblock Learning to rank: from pairwise approach to listwise approach.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  129--136, 2007.

\bibitem[Casanueva et~al.(2020)Casanueva, Tem{\v{c}}inas, Gerz, Henderson, and
  Vuli{\'c}]{casanueva2020efficient}
I{\~n}igo Casanueva, Tadas Tem{\v{c}}inas, Daniela Gerz, Matthew Henderson, and
  Ivan Vuli{\'c}.
\newblock Efficient intent detection with dual sentence encoders.
\newblock In \emph{Proceedings of the 2nd Workshop on Natural Language
  Processing for Conversational AI}, pp.\  38--45, 2020.

\bibitem[Chen et~al.(2017)Chen, Liu, Yin, and Tang]{chen2017survey}
Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang.
\newblock A survey on dialogue systems: Recent advances and new frontiers.
\newblock \emph{Acm Sigkdd Explorations Newsletter}, 19\penalty0 (2):\penalty0
  25--35, 2017.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Durugkar et~al.(2021)Durugkar, Tec, Niekum, and Stone]{aim2021}
Ishan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone.
\newblock Adversarial intrinsic motivation for reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 8622--8636, 2021.

\bibitem[Fan et~al.(2020)Fan, Zhang, Chen, and Zhou]{fan2020bayesian}
Xinjie Fan, Shujian Zhang, Bo~Chen, and Mingyuan Zhou.
\newblock Bayesian attention modules.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16362--16376, 2020.

\bibitem[Fan et~al.(2021)Fan, Zhang, Tanwisuth, Qian, and
  Zhou]{fan2021contextual}
Xinjie Fan, Shujian Zhang, Korawat Tanwisuth, Xiaoning Qian, and Mingyuan Zhou.
\newblock Contextual dropout: An efficient sample-dependent dropout module.
\newblock \emph{arXiv preprint arXiv:2103.04181}, 2021.

\bibitem[Finn et~al.(2016)Finn, Christiano, Abbeel, and
  Levine]{finn2016connection}
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine.
\newblock A connection between generative adversarial networks, inverse
  reinforcement learning, and energy-based models.
\newblock \emph{arXiv preprint arXiv:1611.03852}, 2016.

\bibitem[Freund et~al.(2003)Freund, Iyer, Schapire, and
  Singer]{rankingboosting2003}
Yoav Freund, Raj Iyer, Robert~E Schapire, and Yoram Singer.
\newblock An efficient boosting algorithm for combining preferences.
\newblock \emph{Journal of machine learning research}, 4\penalty0
  (Nov):\penalty0 933--969, 2003.

\bibitem[Fu et~al.(2017)Fu, Luo, and Levine]{fu2017learning}
Justin Fu, Katie Luo, and Sergey Levine.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1710.11248}, 2017.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{td3bc2021}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock {A Minimalist Approach to Offline Reinforcement Learning}.
\newblock \emph{{ArXiv}}, abs/2106.06860, 2021.

\bibitem[Gao et~al.(2018)Gao, Galley, and Li]{gao2018neural}
Jianfeng Gao, Michel Galley, and Lihong Li.
\newblock Neural approaches to conversational ai.
\newblock In \emph{The 41st International ACM SIGIR Conference on Research \&
  Development in Information Retrieval}, pp.\  1371--1374, 2018.

\bibitem[Georgila \& Traum(2011)Georgila and Traum]{georgila2011reinforcement}
Kallirroi Georgila and David Traum.
\newblock Reinforcement learning of argumentation dialogue policies in
  negotiation.
\newblock In \emph{Twelfth Annual Conference of the International Speech
  Communication Association}, 2011.

\bibitem[Ham et~al.(2020)Ham, Lee, Jang, and Kim]{ham2020end}
Donghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, and Kee-Eung Kim.
\newblock End-to-end neural pipeline for goal-oriented dialogue systems using
  gpt-2.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  583--592, 2020.

\bibitem[He et~al.(2022)He, Dai, Zheng, Wu, Cao, Liu, Jiang, Yang, Huang, Si,
  et~al.]{he2022galaxy}
Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng
  Jiang, Min Yang, Fei Huang, Luo Si, et~al.
\newblock Galaxy: A generative pre-trained model for task-oriented dialog with
  semi-supervised learning and explicit policy injection.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  10749--10757, 2022.

\bibitem[Herbrich et~al.(1999)Herbrich, Graepel, and Obermayer]{ranksvm1999}
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
\newblock Support vector learning for ordinal regression.
\newblock \emph{IET}, 1999.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Hosseini-Asl et~al.(2020)Hosseini-Asl, McCann, Wu, Yavuz, and
  Socher]{simpletod2020}
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard
  Socher.
\newblock A simple language model for task-oriented dialogue.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20179--20191, 2020.

\bibitem[Hu et~al.(2018)Hu, Wu, Luo, Tao, Xu, Wu, and Chen]{hu2018playing}
Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu, and Zhan
  Chen.
\newblock Playing 20 question game with policy-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1808.07645}, 2018.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Jang et~al.(2022)Jang, Lee, and Kim]{gptcritic2022}
Youngsoo Jang, Jongmin Lee, and Kee-Eung Kim.
\newblock {GPT}-critic: Offline reinforcement learning for end-to-end
  task-oriented dialogue systems.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{offlinerldialog2019}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson,
  {\`A}.~Lapedriza, Noah~J. Jones, S.~Gu, and Rosalind~W. Picard.
\newblock {Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human
  Preferences in Dialog}.
\newblock \emph{ArXiv}, abs/1907.00456, 2019.

\bibitem[Jaques et~al.(2020)Jaques, Shen, Ghandeharioun, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2020human}
Natasha Jaques, Judy~Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata
  Lapedriza, Noah Jones, Shixiang~Shane Gu, and Rosalind Picard.
\newblock Human-centric dialog training via offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.05848}, 2020.

\bibitem[Kaelbling et~al.(1998)Kaelbling, Littman, and
  Cassandra]{kaelbling1998planning}
Leslie~Pack Kaelbling, Michael~L Littman, and Anthony~R Cassandra.
\newblock Planning and acting in partially observable stochastic domains.
\newblock \emph{Artificial intelligence}, 101\penalty0 (1-2):\penalty0 99--134,
  1998.

\bibitem[Kwan et~al.(2022)Kwan, Wang, Wang, and Wong]{kwan2022survey}
Wai-Chung Kwan, Hongru Wang, Huimin Wang, and Kam-Fai Wong.
\newblock A survey on recent advances and challenges in reinforcement
  learningmethods for task-oriented dialogue policy learning.
\newblock \emph{arXiv preprint arXiv:2202.13675}, 2022.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{batchrl2012}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock \emph{{Batch Reinforcement Learning}}, pp.\  45--73.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.
\newblock ISBN 978-3-642-27645-3.
\newblock \doi{10.1007/978-3-642-27645-3_2}.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{offlinetutorial2020}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{bart2019}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Li et~al.(2020)Li, Lee, Peng, Li, Kiseleva, de~Rijke, Shayandeh, and
  Gao]{li2020guided}
Ziming Li, Sungjin Lee, Baolin Peng, Jinchao Li, Julia Kiseleva, Maarten
  de~Rijke, Shahin Shayandeh, and Jianfeng Gao.
\newblock Guided dialog policy learning without adversarial learning in the
  loop.
\newblock \emph{arXiv preprint arXiv:2004.03267}, 2020.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lin et~al.(2020)Lin, Madotto, Winata, and Fung]{mintl2020}
Zhaojiang Lin, Andrea Madotto, Genta~Indra Winata, and Pascale Fung.
\newblock {M}in{TL}: Minimalist transfer learning for task-oriented dialogue
  systems.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  3391--3405, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.273}.

\bibitem[Liu et~al.(2019)Liu, Trott, Socher, and Xiong]{liu2018competitive}
Hao Liu, Alexander Trott, Richard Socher, and Caiming Xiong.
\newblock Competitive experience replay.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Liu(2009)]{liu2009learning}
Tie-Yan Liu.
\newblock Learning to rank for information retrieval.
\newblock \emph{Foundations and Trends{\textregistered} in Information
  Retrieval}, 3\penalty0 (3):\penalty0 225--331, 2009.

\bibitem[Luce(2012)]{luce2012individual}
R~Duncan Luce.
\newblock \emph{Individual choice behavior: A theoretical analysis}.
\newblock Courier Corporation, 2012.

\bibitem[Maddison et~al.(2016)Maddison, Mnih, and Teh]{maddison2016concrete}
Chris~J Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock \emph{arXiv preprint arXiv:1611.00712}, 2016.

\bibitem[Mehri et~al.(2019)Mehri, Srinivasan, and Eskenazi]{sfnrl2019}
Shikib Mehri, Tejas Srinivasan, and Maxine Eskenazi.
\newblock Structured fusion networks for dialog.
\newblock \emph{arXiv preprint arXiv:1907.10016}, 2019.

\bibitem[Mei et~al.(2020)Mei, Xiao, Dai, Li, Szepesv{\'a}ri, and
  Schuurmans]{escort2020}
Jincheng Mei, Chenjun Xiao, Bo~Dai, Lihong Li, Csaba Szepesv{\'a}ri, and Dale
  Schuurmans.
\newblock Escaping the gravitational pull of softmax.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 21130--21140, 2020.

\bibitem[Menick et~al.(2022)Menick, Trebacz, Mikulik, Aslanides, Song,
  Chadwick, Glaese, Young, Campbell-Gillingham, Irving,
  et~al.]{menick2022teaching}
Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song,
  Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
  Geoffrey Irving, et~al.
\newblock Teaching language models to support answers with verified quotes.
\newblock \emph{arXiv preprint arXiv:2203.11147}, 2022.

\bibitem[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse,
  Jain, Kosaraju, Saunders, et~al.]{nakano2021webgpt}
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
  Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,
  et~al.
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock \emph{arXiv preprint arXiv:2112.09332}, 2021.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{bleu2002}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association
  for Computational Linguistics}, pp.\  311--318, 2002.

\bibitem[Peng et~al.(2017)Peng, Li, Li, Gao, Celikyilmaz, Lee, and
  Wong]{peng2017composite}
Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz, Sungjin Lee,
  and Kam-Fai Wong.
\newblock Composite task-completion dialogue policy learning via hierarchical
  deep reinforcement learning.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2231--2240, 2017.

\bibitem[Peng et~al.(2021)Peng, Li, Li, Shayandeh, Liden, and Gao]{soloist2021}
Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, and
  Jianfeng Gao.
\newblock Soloist: Buildingtask bots at scale with transfer learning and
  machine teaching.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 807--824, 2021.

\bibitem[Plackett(1975)]{plackett1975analysis}
Robin~L Plackett.
\newblock The analysis of permutations.
\newblock \emph{Journal of the Royal Statistical Society: Series C (Applied
  Statistics)}, 24\penalty0 (2):\penalty0 193--202, 1975.

\bibitem[Qian et~al.(2021)Qian, Beirami, Lin, De, Geramifard, Yu, and
  Sankar]{qian2021annotation}
Kun Qian, Ahmad Beirami, Zhouhan Lin, Ankita De, Alborz Geramifard, Zhou Yu,
  and Chinnadhurai Sankar.
\newblock Annotation inconsistency and entity bias in multiwoz.
\newblock In \emph{Proceedings of the 22nd Annual Meeting of the Special
  Interest Group on Discourse and Dialogue}, pp.\  326--337, 2021.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Ramachandran et~al.(2021)Ramachandran, Hashimoto, and
  Xiong]{caspi2021}
Govardana~Sachithanandam Ramachandran, Kazuma Hashimoto, and Caiming Xiong.
\newblock Causal-aware safe policy improvement for task-oriented dialogue.
\newblock \emph{arXiv preprint arXiv:2103.06370}, 2021.

\bibitem[Russell(1998)]{russell1998learning}
Stuart Russell.
\newblock Learning agents for uncertain environments.
\newblock In \emph{Proceedings of the eleventh annual conference on
  Computational learning theory}, pp.\  101--103, 1998.

\bibitem[Saito(2018)]{saito2018curriculum}
Atsushi Saito.
\newblock Curriculum learning based on reward sparseness for deep reinforcement
  learning of task completion dialogue management.
\newblock In \emph{Proceedings of the 2018 EMNLP workshop SCAI: The 2nd
  international workshop on search-oriented conversational AI}, pp.\  46--51,
  2018.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Smith \& Hipp(1994)Smith and Hipp]{smith1994spoken}
Ronnie~W Smith and D~Richard Hipp.
\newblock \emph{Spoken natural language dialog systems: A practical approach}.
\newblock Oxford University Press on Demand, 1994.

\bibitem[Snell et~al.(2022{\natexlab{a}})Snell, Kostrikov, Su, Yang, and
  Levine]{snell2022offline}
Charlie Snell, Ilya Kostrikov, Yi~Su, Mengjiao Yang, and Sergey Levine.
\newblock Offline rl for natural language generation with implicit language q
  learning.
\newblock \emph{arXiv preprint arXiv:2206.11871}, 2022{\natexlab{a}}.

\bibitem[Snell et~al.(2022{\natexlab{b}})Snell, Yang, Fu, Su, and
  Levine]{snell2022context}
Charlie Snell, Sherry Yang, Justin Fu, Yi~Su, and Sergey Levine.
\newblock Context-aware language modeling for goal-oriented dialogue systems.
\newblock \emph{arXiv preprint arXiv:2204.10198}, 2022{\natexlab{b}}.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{rlintro2018}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Takanobu et~al.(2019)Takanobu, Zhu, and Huang]{takanobu2019guided}
Ryuichi Takanobu, Hanlin Zhu, and Minlie Huang.
\newblock Guided dialog policy learning: Reward estimation for multi-domain
  task-oriented dialog.
\newblock \emph{arXiv preprint arXiv:1908.10719}, 2019.

\bibitem[Tur \& De~Mori(2011)Tur and De~Mori]{tur2011spoken}
Gokhan Tur and Renato De~Mori.
\newblock \emph{Spoken language understanding: Systems for extracting semantic
  information from speech}.
\newblock John Wiley \& Sons, 2011.

\bibitem[Verma et~al.(2022)Verma, Fu, Yang, and Levine]{verma2022chai}
Siddharth Verma, Justin Fu, Mengjiao Yang, and Sergey Levine.
\newblock Chai: A chatbot ai for task-oriented dialogue with offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2204.08426}, 2022.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Peng, and Wong]{wang2020learning}
Huimin Wang, Baolin Peng, and Kam-Fai Wong.
\newblock Learning efficient dialogue policy from demonstrations through
  shaping.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  6355--6365, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Novikov, Zolna, Merel,
  Springenberg, Reed, Shahriari, Siegel, Gulcehre, Heess,
  et~al.]{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh~S Merel, Jost~Tobias
  Springenberg, Scott~E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre,
  Nicolas Heess, et~al.
\newblock Critic regularized regression.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7768--7778, 2020{\natexlab{b}}.

\bibitem[Weisz et~al.(2018)Weisz, Budzianowski, Su, and
  Ga{\v{s}}i{\'c}]{weisz2018sample}
Gell{\'e}rt Weisz, Pawe{\l} Budzianowski, Pei-Hao Su, and Milica
  Ga{\v{s}}i{\'c}.
\newblock Sample efficient deep reinforcement learning for dialogue systems
  with large action spaces.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 26\penalty0 (11):\penalty0 2083--2097, 2018.

\bibitem[Wen et~al.(2015)Wen, Gasic, Mrksic, Su, Vandyke, and
  Young]{wen2015semantically}
Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and
  Steve Young.
\newblock Semantically conditioned lstm-based natural language generation for
  spoken dialogue systems.
\newblock \emph{arXiv preprint arXiv:1508.01745}, 2015.

\bibitem[Wen et~al.(2016)Wen, Vandyke, Mrksic, Gasic, Rojas-Barahona, Su,
  Ultes, and Young]{wen2016network}
Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina~M
  Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young.
\newblock A network-based end-to-end trainable task-oriented dialogue system.
\newblock \emph{arXiv preprint arXiv:1604.04562}, 2016.

\bibitem[Williams \& Young(2007)Williams and Young]{williams2007partially}
Jason~D Williams and Steve Young.
\newblock Partially observable markov decision processes for spoken dialog
  systems.
\newblock \emph{Computer Speech \& Language}, 21\penalty0 (2):\penalty0
  393--422, 2007.

\bibitem[Williams(1992)]{reinforce1992}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{huggingface2019}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Wu et~al.(2019{\natexlab{a}})Wu, Madotto, Hosseini-Asl, Xiong, Socher,
  and Fung]{wu2019transferable}
Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard
  Socher, and Pascale Fung.
\newblock Transferable multi-domain state generator for task-oriented dialogue
  systems.
\newblock In \emph{ACL}, pp.\  808--819, 2019{\natexlab{a}}.

\bibitem[Wu et~al.(2019{\natexlab{b}})Wu, Zhang, Li, and Yu]{wu2019alternating}
Qingyang Wu, Yichi Zhang, Yu~Li, and Zhou Yu.
\newblock Alternating recurrent dialog model with large-scale pre-trained
  language models.
\newblock \emph{arXiv preprint arXiv:1910.03756}, 2019{\natexlab{b}}.

\bibitem[Wu et~al.(2019{\natexlab{c}})Wu, Li, Liu, Gao, and Yang]{wu2019switch}
Yuexin Wu, Xiujun Li, Jingjing Liu, Jianfeng Gao, and Yiming Yang.
\newblock Switch-based active deep dyna-q: Efficient adaptive planning for
  task-completion dialogue policy learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  7289--7296, 2019{\natexlab{c}}.

\bibitem[Xia et~al.(2008)Xia, Liu, Wang, Zhang, and Li]{listmle2008}
Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li.
\newblock Listwise approach to learning to rank: theory and algorithm.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  1192--1199, 2008.

\bibitem[Yang et~al.(2022{\natexlab{a}})Yang, Feng, Zhang, and
  Zhou]{sdmgan2022}
Shentao Yang, Yihao Feng, Shujian Zhang, and Mingyuan Zhou.
\newblock Regularizing a model-based policy stationary distribution to
  stabilize offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  24980--25006. PMLR, 2022{\natexlab{a}}.

\bibitem[Yang et~al.(2022{\natexlab{b}})Yang, Wang, Zheng, Feng, and
  Zhou]{jointmatching2022}
Shentao Yang, Zhendong Wang, Huangjie Zheng, Yihao Feng, and Mingyuan Zhou.
\newblock A regularized implicit policy for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2202.09673}, 2022{\natexlab{b}}.

\bibitem[Yang et~al.(2022{\natexlab{c}})Yang, Zhang, Feng, and Zhou]{wmbrl2022}
Shentao Yang, Shujian Zhang, Yihao Feng, and Mingyuan Zhou.
\newblock A unified framework for alternating offline model training and policy
  learning.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
  (eds.), \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{c}}.

\bibitem[Yang et~al.(2021)Yang, Li, and Quan]{ubar2021}
Yunyi Yang, Yunhao Li, and Xiaojun Quan.
\newblock Ubar: Towards fully end-to-end task-oriented dialog system with
  gpt-2.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  14230--14238, 2021.

\bibitem[Yin et~al.(2019)Yin, Yue, and Zhou]{arsm2019}
Mingzhang Yin, Yuguang Yue, and Mingyuan Zhou.
\newblock Arsm: Augment-reinforce-swap-merge estimator for gradient
  backpropagation through categorical variables.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7095--7104. PMLR, 2019.

\bibitem[Young et~al.(2013)Young, Ga{\v{s}}i{\'c}, Thomson, and
  Williams]{young2013pomdp}
Steve Young, Milica Ga{\v{s}}i{\'c}, Blaise Thomson, and Jason~D Williams.
\newblock Pomdp-based statistical spoken dialog systems: A review.
\newblock \emph{Proceedings of the IEEE}, 101\penalty0 (5):\penalty0
  1160--1179, 2013.

\bibitem[Zhang et~al.(2019)Zhang, Hashimoto, Wu, Wan, Yu, Socher, and
  Xiong]{zhang2019find}
Jian-Guo Zhang, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wan, Philip~S Yu, Richard
  Socher, and Caiming Xiong.
\newblock Find or classify? dual strategy for slot-value predictions on
  multi-domain dialog state tracking.
\newblock \emph{arXiv preprint arXiv:1910.03544}, 2019.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Gong, and
  Choi]{zhang2021knowing}
Shujian Zhang, Chengyue Gong, and Eunsol Choi.
\newblock Knowing more about questions can help: Improving calibration in
  question answering.
\newblock \emph{arXiv preprint arXiv:2106.01494}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Gong, and
  Choi]{zhang2021learning}
Shujian Zhang, Chengyue Gong, and Eunsol Choi.
\newblock Learning with different amounts of annotation: From zero to many
  labels.
\newblock \emph{arXiv preprint arXiv:2109.04408}, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Gong, and
  Liu]{zhang2022passage}
Shujian Zhang, Chengyue Gong, and Xingchao Liu.
\newblock Passage-mask: A learnable regularization strategy for
  retriever-reader models.
\newblock \emph{arXiv preprint arXiv:2211.00915}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Gong, Liu, He, Chen, and
  Zhou]{zhang2022allsh}
Shujian Zhang, Chengyue Gong, Xingchao Liu, Pengcheng He, Weizhu Chen, and
  Mingyuan Zhou.
\newblock Allsh: Active learning guided by local sensitivity and hardness.
\newblock \emph{arXiv preprint arXiv:2205.04980}, 2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2022{\natexlab{c}})Zhang, Roller, Goyal, Artetxe, Chen,
  Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022{\natexlab{c}}.

\bibitem[Zhang et~al.(2020)Zhang, Ou, and Yu]{damd2020}
Yichi Zhang, Zhijian Ou, and Zhou Yu.
\newblock Task-oriented dialog systems that consider multiple appropriate
  responses under the same context.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  9604--9611, 2020.

\bibitem[Zhao et~al.(2019)Zhao, Xie, and Eskenazi]{zhao2019rethinking}
Tiancheng Zhao, Kaige Xie, and Maxine Eskenazi.
\newblock Rethinking action spaces for reinforcement learning in end-to-end
  dialog agents with latent variable models.
\newblock \emph{arXiv preprint arXiv:1902.08858}, 2019.

\bibitem[Zhu et~al.(2020)Zhu, Zhang, Fang, Li, Takanobu, Li, Peng, Gao, Zhu,
  and Huang]{zhu2020convlab}
Qi~Zhu, Zheng Zhang, Yan Fang, Xiang Li, Ryuichi Takanobu, Jinchao Li, Baolin
  Peng, Jianfeng Gao, Xiaoyan Zhu, and Minlie Huang.
\newblock Convlab-2: An open-source toolkit for building, evaluating, and
  diagnosing dialogue systems.
\newblock \emph{arXiv preprint arXiv:2002.04793}, 2020.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei,
  Christiano, and Irving]{ziegler2019fine}
Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario
  Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
