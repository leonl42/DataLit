%%%% ijcai22.tex

\typeout{IJCAI--22 Instructions for Authors}

% These are the instructions for authors for IJCAI-22.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai22}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow} %multirow for format of table 
\usepackage{amsmath} 
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage[misc,geometry]{ifsym}
\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

% PDF Info Is REQUIRED.
% Please **do not** include Title and Author information

\title{FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis}


\author{
Rongjie Huang$^1$\footnote{Work done during internship at Tencent AI Lab}\footnote{Equal contribution}\and
Max W. Y. Lam$^2$\footnotemark[2]\and
Jun Wang$^2$\and
Dan Su$^2$\and
Dong Yu$^3$\and
Yi Ren$^1$\and
Zhou Zhao$^1$\footnote{Corresponding author}\\
\affiliations
$^1$Zhejiang University \quad
$^2$Tencent AI Lab, China \quad
$^3$Tencent AI Lab, USA\\
\emails
\{rongjiehuang, rayeren, zhouzhao\}@zju.edu.cn,
\{maxwylam, joinerwang, dansu, dyu\}@tencent.com
}

\begin{document}

\maketitle
% while simultaneously achieving high quality, fast and diverse speech synthesis is still an open problem.
\begin{abstract}
%   Deep generative models have made massive progress in speech synthesis. 
Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hindered their applications to speech synthesis. This paper proposes FastDiff, a fast conditional diffusion model for high-quality speech synthesis. FastDiff employs a stack of time-aware location-variable convolutions of diverse receptive field patterns to efficiently model long-term time dependencies with adaptive conditions. A noise schedule predictor is also adopted to reduce the sampling steps without sacrificing the generation quality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer, FastDiff-TTS, which generates high-fidelity speech waveforms without any intermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff demonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech samples. Also, FastDiff enables a sampling speed of 58x faster than real-time on a V100 GPU, making diffusion models practically applicable to speech synthesis deployment for the first time. We further show that FastDiff generalized well to the mel-spectrogram inversion of unseen speakers, and FastDiff-TTS outperformed other competing methods in end-to-end text-to-speech synthesis.\footnote{Audio samples are available at \url{https://FastDiff.github.io/}.}

\end{abstract}

  % Specifically, 1) to capture the long dependencies of audio and promote quality, we adopt local variable convolutions with diverse receptive field patterns. 2) To promote computational efficiency, we include a noise schedule prediction network for reducing iterative refinement steps. 3) to tackle aperiodic (e.g., noise in waveforms) and high-frequency components issue, we utilize an additional postnet to refine the details of generated sample, which plays competitive games with a discriminator through adversarial diffusion process. 
  

\input{Sections/1_intro.tex}
\input{Sections/2_background.tex}
\input{Sections/3_Iter.tex}
\input{Sections/4_IterTTS.tex}
\input{Sections/6_related.tex}
\input{Sections/5_exp.tex}
\input{Sections/7_con.tex}

\clearpage
%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai22}


\appendix
\clearpage
\input{Sections/8_appendix.tex}


\end{document}

