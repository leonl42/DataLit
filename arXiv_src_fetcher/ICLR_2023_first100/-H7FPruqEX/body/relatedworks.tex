\section{Related Works}
\label{sec:relatedworks}

% \changnan{todo}
% \citep{op_reinforce} casts policy-based methods as minimizing a divergence measure between two special policies and regards the policy evaluation and the policy improvement as a projection operator and an improvement operator.
% It points out that an improvement operator compatible with the projection operator may be preferred.
% \changnan{\citep{op_reinforce} regards the policy evaluation and the policy improvement as a projection operator and an improvement operator.
% It points out that an improvement operator compatible with the projection operator may be preferred.}

% \haosen{gradient changing method in multi-task(1.Adapting auxiliary losses using gradient similarity 2.Gradient Surgery for Multi-Task Learning 3.Multiple-gradient descent algorithm (mgda) for multiobjective optimization,etc. ) Decoupling the Policy and Value Function. conflict between actor and critic related work.they are orthogonal to casa(1.Decoupling Value and Policy for Generalization in Reinforcement Learning ICML 2021 2.Phasic Policy Gradient. ICML 2021)compared to Bridging the Gap Between Value and Policy Based Reinforcement Learning, which is the nearest work to our method. our method works as a more elaborate one from different views.some representation work in reinforcement learning such as(1.A Geometric Perspective on Optimal Representations for Reinforcement Learning)we don't aim to learn a better or invariant representation via augmentation but just the original objective.some work in policy based rl and value based rl, mentioned R2D2, PPO, Lazer.}


% Model-free 
% In reinforcement learning,
% has shown great promise in solving complex sequential decision making problems, such as digital game playing, robotics and self-driving cars. T
% the agent's reward maximization objective is often optimized by
Both value-based or policy-based approaches comply with the principle of GPI,
% Value-based methods update the value functions for policy evaluation followed by a simple practice of policy improvement in the greedy form, 
% whereas policy-based methods improve the policy progressively with intermediate value functions. 
% Thus, for both value-based and policy-based strands, 
but two GPI steps are coarsely related to each other such that jointly optimizing both functions might potentially bring conflicts. 
Despite of such crucial issue in GPI with function approximation, most decent model-free algorithms adopt a standard policy improvement/evaluation regime without considering conflict diminishing properties. 
The issue of reducing conflicts among multiple models trained simultaneously was considered in earlier machine learning literature,
% , though the attention of such issue drawn on reinforcement learning problems comes much later. 
such as for robust parameter estimation for multiple estimators under incomplete data~\citep{robins1995semiparametric,lunceford2004stratification,kang2007demystifying} and multitask learning with  gradient similarity measure~\citep{ChenNHLKCA20,YuK0LHF20,JavaloyV22}. 
% gradient changing method in multi-task(1.Adapting auxiliary losses using gradient similarity 2.Gradient Surgery for Multi-Task Learning 3.Multiple-gradient descent algorithm (mgda) for multiobjective optimization,etc. ) 


When the idea is introduced to reinforcement learning, earliest attempts  tackle  conservative and safe policy iteration problems~\citep{KakadeL02,HazanK11,PirottaRPC13}. Recently, more works have emerged to study GPI in a fine-grained manner. In \citep{op_reinforce}, 
a new Bellman operator is introduced which implements GPI with a policy improvement operator and a projection operator, where the projection attempts to find the best approximation of policy among  realizable policies. 
In \citep{RaileanuF21}, the policy and value updates are decoupled by approximating two networks with representation regularization. 
In \citep{CobbeHKS21}, GPI  is separated into a policy improvement and a feature distillation step. On contrast to the aforementioned works, we tackle the conflicts in GPI at the gradient-level, with theoretical analysis. 
Our work is related to \citep{pcl}, which utilizes both the unbiasedness and stability of on-policy training and the data efficiency of off-policy training to form a soft consistency error. Our work bridges the gap between the two GPI steps from an alternative angle of establishing a closer relationship between policy and value functions in their forms, without the focus on off-policy correction.     
% the policy evaluation and the policy improvement as a projection operator and an improvement operator.
% It points out that an improvement operator compatible with the projection operator may be preferred.
{\colorred Due to the difficulty of controlling the gap between GPI steps, we instead consider $\chi$. 
The condition $\chi = 1$ is close to compatible value function \citep{sutton1999policy, kakade2001natural}, shown in Section \ref{sec:motivation} and Appendix \ref{app:comp_v}.}

{\colorred 
\section{Limitation}
\label{sec:limit}
It's noticeable that CASA is only applied on discrete action space for now. 
We further find CASA applicable to any function approximation that is able to estimate advantage functions of all actions.
We provide additional discussion on continuous action space in Appendix \ref{app:cts_space}.

Since $\pi$ shares all parameters of value functions, it brings $\chi=1$ but sacrifices the \textit{freedom} of $\pi$ to be parameterized by other parameters. 
We conjecture that CASA is one endpoint of a trade-off curve between $\chi$ and the \textit{freedom} of $\pi$, where the other endpoint is that $\pi$ shares no parameter with value functions. 
}
