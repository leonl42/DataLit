\section{Preliminary}
\label{sec:bg}

Consider an infinite-horizon MDP, defined by a tuple $(\mathcal{S}, \mathcal{A}, p, r, \gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0, 1]$ is the state transition probability function, $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ is the reward function, and $\gamma$ is the discounted factor.
The policy is a mapping  $\pi: \mathcal{S} \times \mathcal{A} \to [0, 1]$ which assigns a distribution over the action space given a state. 


The objective of reinforcement learning is to maximize the \emph{return}, or cumulative discounted rewards,  {\colorred 
\begin{equation}
\mbox{maximize} \ \mathcal{J} = \mathbb{E}_{traj \sim \pi} 
% \mathbb{E}_{s \sim d^\pi} 
\left[ \sum_t \gamma^t r(s_t, a_t)\right],
\end{equation}
where $traj = \{s_0, a_0, r_0, \dots\}$ is a trajectory sampled by $\pi$ with policy-environment interaction.}

Value-based methods maximize $\mathcal{J}$ by estimating various type of value functions: the state value function is defined as {\colorred $V^{\pi}(s) = \mathbb{E}_{\pi}\left[ \sum_t \gamma^t r_{t} | s_0=s \right]$}, the state-action value function is defined as {\colorred $Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ \sum_t \gamma^t r_{t} | s_0=s, a_0=a \right]$}; the advantage function is defined as $A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)$.
% \changnan{bellman equation deleted.}
% The connection between $V^\pi$ and $Q^\pi$ is given by the Bellman equation, 
% $$
% \begin{aligned}
%     \mathcal{T}Q^{\pi} (s, a) &= \mathbb{E}_{s'\sim p(\cdot | s, a)} [r + \gamma V^{\pi}(s')], \\
%     V^{\pi} (s) &= \mathbb{E}_{a \sim \pi(\cdot | s)} [Q^{\pi} (s, a)].
% \end{aligned}
% $$
% where $\mathcal{T}$ is the Bellman Operator.
The objective of maximizing the value functions in value-based methods 
% $\mathcal{J}$ by estimating the state-action value function $Q^\pi$, which 
can be improved through GPI until converging to the optimal policy.
For the approximated state-value function $Q_\theta$ that estimates $Q^\pi$, the policy evaluation is conducted by:{\colorred 
\begin{equation}
\mbox{minimize}\ \mathbb{E}_{\pi}
% \mathbb{E}_{s \sim d^\pi} 
 % \mathop{\mathbb{E}}_{a \sim \pi(\cdot | s)}
%  \mathbb{E}_{a \sim \pi(\cdot | s)} 
 [(Q^\pi(s, a) - Q_\theta (s, a))^2],
 \end{equation}}
where $Q^\pi$ is estimated by various methods, e.g., $\lambda$-return~\citep{Sutton88lambda} and ReTrace~\citep{retrace}.
% \haosen{here are some problems, some value-based method use $G(s,a)$ to estimate $Q^*(s,a)$ such as DQN. they use $\epsilon$-greedy only for exploration.} \haiyan{Added $G(s,a)$, and only greedy in $\epsilon$-greedy is related to policy improvement so don't need to mention $\epsilon$-greedy} 
% The policy evaluation can be achieved by applying gradient ascent on the direction of the gradient $\theta \leftarrow \theta + \eta \mathbb{E}_{\pi} [(
% G- Q_{\theta}) \nabla_{\theta} Q_{\theta}]$, where $\eta$ is the learning rate.
% There are various designs to help approximating $Q^\pi$. 
% A refined structure design is provided by dueling-DQN \citep{dueling_q}. 
% It estimates the state-action value function by the summation of the advantage function and the state value function.
% \citep{retrace} uses ReTrace to estimate $G$, 
% \begin{equation}
% \label{eq:retrace}
%     G=Q^{\Tilde{\pi}} (s_t, a_t) 
% = \textbf{E}_{\mu} [ Q_\theta^\pi(s_t, a_t) + \sum_{k \geq 0} \gamma^k 
% c_{[t+1:t+k]} \delta^{Q}_{t+k} Q ],
% \end{equation}
% where $\delta^{Q}_t Q \overset{def}{=} r_t + \gamma Q_\theta^\pi(s_{t+1}, a_{t+1}) - Q_\theta^\pi(s_t, a_t)$. 
The policy improvement is usually achieved by greedily selecting actions with the highest state-action values.
% selecting a series of actions whose corresponding state-action values are higher. 

Policy-based methods maximize $\mathcal{J}$ by optimizing some parameterized policy $\pi_\theta$ according to the policy gradient theorem \citep{sutton}, 
\begin{equation}
\nabla_\theta \mathcal{J} = % \mathop{\mathbb{E}}_{s\sim d^\pi} % \mathbb{E}_{s\sim d^\pi} 
\mathbb{E}_{\pi} %\mathbb{E}_{a \sim \pi(\cdot | s)}
[\Psi(s, a) \nabla_\theta \log \pi_\theta(a|s)].
\end{equation}
{\colorred The vanilla policy gradient uses $\Psi = \sum_{t=0}^\infty \gamma^t r_{t}$.}
Actor-critic algorithms approximate $\Psi(s,a)$ in the form of baseline, e.g., 
IMPALA~\citep{impala} adopts $\Psi(s, a) = r + \gamma V^{\Tilde{\pi}} (s') - V_{\theta} (s)$ and uses V-Trace to estimate $V^{\Tilde{\pi}}$.

% Let $\rho_t \overset{def}{=} \min\{\frac{\pi_t}{\mu_t}, \Bar{\rho} \}$.
% V-Trace estimates $V(s_t)$ by
% \begin{equation}
% \label{eq:vtrace}
%     V^{\Tilde{\pi}} (s_t) 
%         = \textbf{E}_{\mu} [ 
%         V(s_t) + \sum_{k \geq 0} \gamma^k 
%      c_{[t:t+k-1]} \rho_{t+k}  \delta^{V}_{t+k} V ],
% \end{equation}
% where $\delta^{V}_t V \overset{def}{=} r_t + \gamma V(s_{t+1}) - V(s_t)$. 
