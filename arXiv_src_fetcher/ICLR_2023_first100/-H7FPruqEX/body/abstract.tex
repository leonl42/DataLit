\begin{abstract}

% \changnan{check now, have to submit a version today}
% We study the problem of model-free reinforcement learning, which is often solved following the principle of Generalized Policy Iteration (GPI). While GPI is typically an interplay between policy evaluation and policy improvement, most conventional model-free methods assume the independence of the granularity and other details of the GPI steps, \haiyan{e.g., in actor-critic algorithm,  policy and value 
% % are predicted by independent output heads while they 
% get updated by different losses where the only shared term advantage is non-differentiable},
% \haosen{e.g. jointly optimize these two objectives with a shared network and a relative weight to each.} 
We study the problem of model-free reinforcement learning, which is often solved following the principle of Generalized Policy Iteration (GPI). 
While GPI is typically an interplay between policy evaluation and policy improvement, most conventional model-free methods with function approximation assume the independence of 
% the granularity and other details of the 
GPI steps, despite of the inherent connections between them. 
% \haosen{fact that these two optimization processes have no guarantee on no conflict between them due to their essentially different optimizing speeds and objectives, which will be reflected as a strong conflict of single-step gradients for gradient-based methods.} 
In this paper, we present a method that attempts to {eliminate} the inconsistency between policy evaluation step and policy improvement step, leading to a conflict averse GPI solution with gradient-based functional approximation.  
% which aims to alleviate the gradient conflict between the two GPI steps. 
% \changnan{how to measure this lower FA error?}
% To this end, we formulate a novel learning paradigm where taking the policy evaluation step is equivalent to some compensation of performing policy improvement, and thus effectively alleviates the gradient conflict between the two GPI steps. 
% \changnan{BAD description.}
% To this end, we formulate a novel learning paradigm where taking the policy improvement step is equivalent to a policy evaluation step and a self-bootstrapped policy improvement step.\changnan{not good}
% which incentivates more exploitation.}
% We also show that the form of our proposed solution is equivalent to performing entropy-regularized policy improvement and therefore prevents the policy from being trapped into suboptimal solutions. 
% \changnan{
% Furthermore, we  demonstrate that the policy evaluation step in turn is equivalent to performing entropy-regularized policy improvement, thus preventing the policy from being trapped into suboptimal solutions through incentivizing more exploration. \changnan{not good}
%}
Our method is capital to balancing exploitation and exploration between policy-based and value-based methods and is applicable to existing policy-based and value-based methods. 
%}
We conduct extensive experiments to study theoretical properties of our method and demonstrate the effectiveness of our method on Atari 200M benchmark.
% wherein our method could outperform the several state-of-the-art (SOTA) model-free RL baselines.
% For empirical evaluation, we conduct extensive experiments to evaluate our method on the large-scale benchmark Arcade Learning Environment (ALE). Empirical results show that our method could outperform several strong baselines in both human normalized score as well as saber score .
\end{abstract}
