\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Badia et~al.(2020)Badia, Piot, Kapturowski, Sprechmann, Vitvitskyi,
  Guo, and Blundell]{agent57}
Adri{\`a}~Puigdom{\`e}nech Badia, Bilal Piot, Steven Kapturowski, Pablo
  Sprechmann, Alex Vitvitskyi, Daniel Guo, and Charles Blundell.
\newblock Agent57: Outperforming the atari human benchmark.
\newblock \emph{arXiv preprint arXiv:2003.13350}, 2020.

\bibitem[Chen \& He(2020)Chen and He]{simsiam}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock \emph{arXiv preprint arXiv:2011.10566}, 2020.

\bibitem[Chen et~al.(2020)Chen, Ngiam, Huang, Luong, Kretzschmar, Chai, and
  Anguelov]{ChenNHLKCA20}
Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning
  Chai, and Dragomir Anguelov.
\newblock Just pick a sign: Optimizing deep multitask models with gradient sign
  dropout.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Cobbe et~al.(2021)Cobbe, Hilton, Klimov, and Schulman]{CobbeHKS21}
Karl Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman.
\newblock Phasic policy gradient.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2020--2027, 2021.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{em}
A.~P. Dempster, N.~M. Laird, and D.~B. Rubin.
\newblock Maximum likelihood from incomplete data via the em algorithm.
\newblock \emph{JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
  39\penalty0 (1):\penalty0 1--38, 1977.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{impala}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock \emph{arXiv preprint arXiv:1802.01561}, 2018.

\bibitem[Ghosh et~al.(2020)Ghosh, C~Machado, and Le~Roux]{op_reinforce}
Dibya Ghosh, Marlos C~Machado, and Nicolas Le~Roux.
\newblock An operator view of policy gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3397--3406, 2020.

\bibitem[Hazan \& Kale(2011)Hazan and Kale]{HazanK11}
Elad Hazan and Satyen Kale.
\newblock Better algorithms for benign bandits.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 1287--1311, 2011.

\bibitem[Hessel et~al.(2017)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1710.02298}, 2017.

\bibitem[Javaloy \& Valera(2022)Javaloy and Valera]{JavaloyV22}
Adri{\'{a}}n Javaloy and Isabel Valera.
\newblock Rotograd: Gradient homogenization in multitask learning.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}, 2022.

\bibitem[Jiang \& Li(2016)Jiang and Li]{dr}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  652--661. PMLR, 2016.

\bibitem[Kakade(2001)]{kakade2001natural}
Sham~M Kakade.
\newblock A natural policy gradient.
\newblock \emph{Advances in neural information processing systems}, 14, 2001.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{KakadeL02}
Sham~M. Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{Machine Learning, Proceedings of the Nineteenth
  International Conference {(ICML} 2002), University of New South Wales,
  Sydney, Australia, July 8-12, 2002}, pp.\  267--274, 2002.

\bibitem[Kang \& Schafer(2007)Kang and Schafer]{kang2007demystifying}
Joseph~DY Kang and Joseph~L Schafer.
\newblock Demystifying double robustness: A comparison of alternative
  strategies for estimating a population mean from incomplete data.
\newblock \emph{Statistical science}, 22\penalty0 (4):\penalty0 523--539, 2007.

\bibitem[Kapturowski et~al.(2018)Kapturowski, Ostrovski, Quan, Munos, and
  Dabney]{r2d2}
Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock In \emph{International conference on learning representations}, 2018.

\bibitem[Lunceford \& Davidian(2004)Lunceford and
  Davidian]{lunceford2004stratification}
Jared~K Lunceford and Marie Davidian.
\newblock Stratification and weighting via the propensity score in estimation
  of causal treatment effects: a comparative study.
\newblock \emph{Statistics in medicine}, 23\penalty0 (19):\penalty0 2937--2960,
  2004.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{dqn}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{retrace}
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett
  (eds.), \emph{Advances in Neural Information Processing Systems 29}, pp.\
  1054--1062. Curran Associates, Inc., 2016.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and Schuurmans]{pcl}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2775--2785, 2017.

\bibitem[Pedersen(2019)]{ftw}
Carsten~Lund Pedersen.
\newblock Re: Human-level performance in 3d multiplayer games with
  population-based reinforcement learning.
\newblock \emph{Science}, 2019.

\bibitem[Pirotta et~al.(2013)Pirotta, Restelli, Pecorino, and
  Calandriello]{PirottaRPC13}
Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello.
\newblock Safe policy iteration.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning, {ICML} 2013, Atlanta, GA, USA, 16-21 June 2013}, volume~28 of
  \emph{{JMLR} Workshop and Conference Proceedings}, pp.\  307--315, 2013.

\bibitem[Raileanu \& Fergus(2021)Raileanu and Fergus]{RaileanuF21}
Roberta Raileanu and Rob Fergus.
\newblock Decoupling value and policy for generalization in reinforcement
  learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  8787--8798, 2021.

\bibitem[Robins \& Rotnitzky(1995)Robins and
  Rotnitzky]{robins1995semiparametric}
James~M Robins and Andrea Rotnitzky.
\newblock Semiparametric efficiency in multivariate regression models with
  missing data.
\newblock \emph{Journal of the American Statistical Association}, 90\penalty0
  (429):\penalty0 122--129, 1995.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and Silver]{priority_q}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1511.05952}, 2015.

\bibitem[Schmidhuber(1997)]{lstm}
Sepp Hochreiter;~JÃ¼rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation.}, 1997.

\bibitem[Schmitt et~al.(2020)Schmitt, Hessel, and Simonyan]{laser}
Simon Schmitt, Matteo Hessel, and Karen Simonyan.
\newblock Off-policy actor-critic with shared experience replay.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8545--8554. PMLR, 2020.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sutton(1988)]{Sutton88lambda}
Richard~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Mach. Learn.}, 3:\penalty0 9--44, 1988.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Richard~S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Toromanoff et~al.(2019)Toromanoff, Wirbel, and Moutarde]{saber}
Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde.
\newblock Is deep reinforcement learning really superhuman on atari? leveling
  the playing field.
\newblock \emph{arXiv preprint arXiv:1908.04683}, 2019.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{alpha_star}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2016)Wang, Schaul, Hessel, Hasselt, Lanctot, and
  Freitas]{dueling_q}
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando
  Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1995--2003, 2016.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and Finn]{YuK0LHF20}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\end{thebibliography}
