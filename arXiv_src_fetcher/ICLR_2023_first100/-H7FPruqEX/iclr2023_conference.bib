
@article{sql,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1702.08165},
  year={2017}
}

@article{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{Sutton88lambda,
  author    = {Richard S. Sutton},
  title     = {Learning to Predict by the Methods of Temporal Differences},
  journal   = {Mach. Learn.},
  volume    = {3},
  pages     = {9--44},
  year      = {1988},
}

@article{impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  journal={arXiv preprint arXiv:1802.01561},
  year={2018}
}

@inproceedings{PirottaRPC13,
  author    = {Matteo Pirotta and
               Marcello Restelli and
               Alessio Pecorino and
               Daniele Calandriello},
  title     = {Safe Policy Iteration},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning,
               {ICML} 2013, Atlanta, GA, USA, 16-21 June 2013},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {28},
  pages     = {307--315},
  year      = {2013},
}

@inproceedings{KakadeL02,
  author    = {Sham M. Kakade and
               John Langford},
  title     = {Approximately Optimal Approximate Reinforcement Learning},
  booktitle = {Machine Learning, Proceedings of the Nineteenth International Conference
               {(ICML} 2002), University of New South Wales, Sydney, Australia, July
               8-12, 2002},
  pages     = {267--274},
  year      = {2002},
}

@article{kang2007demystifying,
  title={Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data},
  author={Kang, Joseph DY and Schafer, Joseph L},
  journal={Statistical science},
  volume={22},
  number={4},
  pages={523--539},
  year={2007},
}


@inproceedings{JavaloyV22,
  author    = {Adri{\'{a}}n Javaloy and
               Isabel Valera},
  title     = {RotoGrad: Gradient Homogenization in Multitask Learning},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  year      = {2022},
}

@inproceedings{ChenNHLKCA20,
  author    = {Zhao Chen and
               Jiquan Ngiam and
               Yanping Huang and
               Thang Luong and
               Henrik Kretzschmar and
               Yuning Chai and
               Dragomir Anguelov},
  title     = {Just Pick a Sign: Optimizing Deep Multitask Models with Gradient Sign
               Dropout},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
}

@article{HazanK11,
  author    = {Elad Hazan and
               Satyen Kale},
  title     = {Better Algorithms for Benign Bandits},
  journal   = {J. Mach. Learn. Res.},
  volume    = {12},
  pages     = {1287--1311},
  year      = {2011},
}
@inproceedings{YuK0LHF20,
  author    = {Tianhe Yu and
               Saurabh Kumar and
               Abhishek Gupta and
               Sergey Levine and
               Karol Hausman and
               Chelsea Finn},
  title     = {Gradient Surgery for Multi-Task Learning},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
}

@article{lunceford2004stratification,
  title={Stratification and weighting via the propensity score in estimation of causal treatment effects: a comparative study},
  author={Lunceford, Jared K and Davidian, Marie},
  journal={Statistics in medicine},
  volume={23},
  number={19},
  pages={2937--2960},
  year={2004},
}

@article{robins1995semiparametric,
  title={Semiparametric efficiency in multivariate regression models with missing data},
  author={Robins, James M and Rotnitzky, Andrea},
  journal={Journal of the American Statistical Association},
  volume={90},
  number={429},
  pages={122--129},
  year={1995},
}

@inproceedings{r2d2,
  title={Recurrent experience replay in distributed reinforcement learning},
  author={Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  booktitle={International conference on learning representations},
  year={2018}
}
@incollection{retrace,
title = {Safe and Efficient Off-Policy Reinforcement Learning},
author = {Munos, Remi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {1054--1062},
year = {2016},
publisher = {Curran Associates, Inc.},
}

@inproceedings{dr,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}

@article{agent57,
  title={Agent57: Outperforming the atari human benchmark},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Blundell, Charles},
  journal={arXiv preprint arXiv:2003.13350},
  year={2020}
}

@inproceedings{
apex,
title={Distributed Prioritized Experience Replay},
author={Dan Horgan and John Quan and David Budden and Gabriel Barth-Maron and Matteo Hessel and Hado van Hasselt and David Silver},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=H1Dy---0Z},
}


@article{td3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Van Hoof, Herke and Meger, David},
  journal={arXiv preprint arXiv:1802.09477},
  year={2018}
}

@article{sac,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{softandapp,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}


@article{gym,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{trpo,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}

@inproceedings{wxry,
  title={Mastering Complex Control in MOBA Games with Deep Reinforcement Learning.},
  author={Ye, Deheng and Liu, Zhao and Sun, Mingfei and Shi, Bei and Zhao, Peilin and Wu, Hao and Yu, Hongsheng and Yang, Shaojie and Wu, Xipeng and Guo, Qingwei and others},
  booktitle={AAAI},
  pages={6672--6679},
  year={2020}
}

@article{ftw,
  title={RE: Human-level Performance in 3D Multiplayer Games with Population-based Reinforcement Learning},
  author={Pedersen, Carsten Lund},
  journal={Science},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  journal={arXiv preprint arXiv:1710.02298},
  year={2017}
}

@article{dqn,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{dueling_q,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International conference on machine learning},
  pages={1995--2003},
  year={2016}
}

@article{priority_q,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@Article{ale,
  author = {{Bellemare}, M.~G. and {Naddaf}, Y. and {Veness}, J. and {Bowling}, M.},
  title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
  journal = {Journal of Artificial Intelligence Research},
  year = "2013",
  month = "jun",
  volume = "47",
  pages = "253--279",
}

@Article{ale2,
  author = {Marlos C. Machado and Marc G. Bellemare and Erik Talvitie and Joel Veness and Matthew J. Hausknecht and Michael Bowling},
  title = {Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents},
  journal = {Journal of Artificial Intelligence Research},
  volume = {61},
  pages = {523--562},
  year = {2018}
}

@book{sutton,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{alpha_star,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{eq_pg_q,
  title={Equivalence between policy gradients and soft q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017}
}

@inproceedings{pcl,
  title={Bridging the gap between value and policy based reinforcement learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2775--2785},
  year={2017}
}

@article{simsiam,
  title={Exploring Simple Siamese Representation Learning},
  author={Chen, Xinlei and He, Kaiming},
  journal={arXiv preprint arXiv:2011.10566},
  year={2020}
}

@article{em,
    author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
    title = {Maximum likelihood from incomplete data via the EM algorithm},
    journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
    year = {1977},
    volume = {39},
    number = {1},
    pages = {1--38}
}

@article{unreal,
  title={Reinforcement learning with unsupervised auxiliary tasks},
  author={Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1611.05397},
  year={2016}
}

@article{coex,
  title={Contingency-aware exploration in reinforcement learning},
  author={Choi, Jongwook and Guo, Yijie and Moczudilski, Marcin and Oh, Junhyuk and Wu, Neal and Norouzi, Mohammad and Lee, Honglak},
  journal={arXiv preprint arXiv:1811.01483},
  year={2018}
}

@article{ngu,
  title={Never Give Up: Learning Directed Exploration Strategies},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Mart{\'\i}n and Pritzel, Alexander and Bolt, Andew and others},
  journal={arXiv preprint arXiv:2002.06038},
  year={2020}
}

@inproceedings{laser,
  title={Off-policy actor-critic with shared experience replay},
  author={Schmitt, Simon and Hessel, Matteo and Simonyan, Karen},
  booktitle={International Conference on Machine Learning},
  pages={8545--8554},
  year={2020},
  organization={PMLR}
}

@article{discor,
  title={Discor: Corrective feedback in reinforcement learning via distribution correction},
  author={Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2003.07305},
  year={2020}
}

@article{lstm,
title={Long short-term memory},
author={Sepp Hochreiter; Jürgen Schmidhuber},
journal={Neural Computation.},
year={1997}
}

@inproceedings{uvfa,
  author={Tom Schaul and Daniel Horgan and Karol Gregor and David Silver},
  title={Universal Value Function Approximators},
  year={2015},
  cdate={1420070400000},
  pages={1312-1320},
  url={http://proceedings.mlr.press/v37/schaul15.html},
  booktitle={ICML},
  crossref={conf/icml/2015}
}

@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@article{op_reinforce,
  title={An operator view of policy gradient methods},
  author={Ghosh, Dibya and C Machado, Marlos and Le Roux, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3397--3406},
  year={2020}
}

@inproceedings{RaileanuF21,
  author    = {Roberta Raileanu and
               Rob Fergus},
  title     = {Decoupling Value and Policy for Generalization in Reinforcement Learning},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {8787--8798},
  year      = {2021},
}


@inproceedings{CobbeHKS21,
  author    = {Karl Cobbe and
               Jacob Hilton and
               Oleg Klimov and
               John Schulman},
  title     = {Phasic Policy Gradient},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {2020--2027},
  year      = {2021},
}

@misc{a3c,
      title={Asynchronous Methods for Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1602.01783},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{saber,
  title={Is deep reinforcement learning really superhuman on atari? leveling the playing field},
  author={Toromanoff, Marin and Wirbel, Emilie and Moutarde, Fabien},
  journal={arXiv preprint arXiv:1908.04683},
  year={2019}
}

@article{diayn,
  title={Diversity is all you need: Learning skills without a reward function},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.06070},
  year={2018}
}

@article{williams1991function,
  title={Function optimization using connectionist reinforcement learning algorithms},
  author={Williams, Ronald J and Peng, Jing},
  journal={Connection Science},
  volume={3},
  number={3},
  pages={241--268},
  year={1991},
  publisher={Taylor \& Francis}
}

@article{rnd,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@inproceedings{understanding_entropy,
title	= {Understanding the impact of entropy on policy optimization},
author	= {Zafarali Ahmed and Nicolas Le Roux and Mohammad Norouzi and Dale Schuurmans},
year	= {2019},
URL	= {https://arxiv.org/abs/1811.11214}
}

@inproceedings{
vmpo,
title={V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control},
author={H. Francis Song and Abbas Abdolmaleki and Jost Tobias Springenberg and Aidan Clark and Hubert Soyer and Jack W. Rae and Seb Noury and Arun Ahuja and Siqi Liu and Dhruva Tirumala and Nicolas Heess and Dan Belov and Martin Riedmiller and Matthew M. Botvinick},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SylOlp4FvH}
}

@inproceedings{a2c,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@inproceedings{polytope,
  title={The value function polytope in reinforcement learning},
  author={Dadashi, Robert and Taiga, Adrien Ali and Le Roux, Nicolas and Schuurmans, Dale and Bellemare, Marc G},
  booktitle={International Conference on Machine Learning},
  pages={1486--1495},
  year={2019},
  organization={PMLR}
}

@article{igl2019generalization,
  title={Generalization in reinforcement learning with selective noise injection and information bottleneck},
  author={Igl, Maximilian and Ciosek, Kamil and Li, Yingzhen and Tschiatschek, Sebastian and Zhang, Cheng and Devlin, Sam and Hofmann, Katja},
  journal={arXiv preprint arXiv:1910.12911},
  year={2019}
}

@inproceedings{vieillard2020leverage,
  title={Leverage the average: an analysis of KL regularization in reinforcement learning},
  author={Vieillard, Nino and Kozuno, Tadashi and Scherrer, Bruno and Pietquin, Olivier and Munos, R{\'e}mi and Geist, Matthieu},
  booktitle={NeurIPS-34th Conference on Neural Information Processing Systems},
  year={2020}
}

@inproceedings{asadi2017alternative,
  title={An alternative softmax operator for reinforcement learning},
  author={Asadi, Kavosh and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={243--252},
  year={2017},
  organization={PMLR}
}

@inproceedings{revisitingsoft,
  title={Revisiting the softmax bellman operator: New benefits and new perspective},
  author={Song, Zhao and Parr, Ron and Carin, Lawrence},
  booktitle={International Conference on Machine Learning},
  pages={5916--5925},
  year={2019},
  organization={PMLR}
}

@inproceedings{adaptiveepsilon,
  title={Adaptive $\varepsilon$-greedy exploration in reinforcement learning based on value differences},
  author={Tokic, Michel},
  booktitle={Annual Conference on Artificial Intelligence},
  pages={203--210},
  year={2010},
  organization={Springer}
}

@article{adaptiveepsilon2,
  title={An Adaptive Implementation of $\varepsilon$-Greedy in Reinforcement Learning},
  author={dos Santos Mignon, Alexandre and da Rocha, Ricardo Luis de Azevedo},
  journal={Procedia Computer Science},
  volume={109},
  pages={1146--1151},
  year={2017},
  publisher={Elsevier}
}

@article{gae,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}


@article{policychurn,
  title={The Phenomenon of Policy Churn},
  author={Schaul, Tom and Barreto, Andr{\'e} and Quan, John and Ostrovski, Georg},
  journal={arXiv preprint arXiv:2206.00730},
  year={2022}
}