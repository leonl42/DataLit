\begin{thebibliography}{10}

\bibitem{Agarwal_Kakade_Lee_Mahajan_2019}
Alekh Agarwal, Sham~M. Kakade, Jason~D. Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: {O}ptimality,
  approximation, and distribution shift.
\newblock {\em Journal of Machine Learning Research}, 22(98):1--76, 2021.

\bibitem{yuan2023general}
Carlo Alfano, Rui Yuan, and Patrick Rebeschini.
\newblock A novel framework for policy mirror descent with general
  parameterization and linear convergence.
\newblock In {\em Advances in Neural Information Processing Systems}, 2023.

\bibitem{Bhandari_Russo_2021}
Jalaj Bhandari and Daniel Russo.
\newblock On the linear convergence of policy gradient methods for finite
  {MDP}s.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, volume 130, pages 2386--2394, 2021.

\bibitem{bhabdari2024or}
Jalj Bhandari and Daniel Russo.
\newblock Global optimality guarantees for policy gradient methods.
\newblock {\em Operations Research}, 2024.

\bibitem{Cen_Cheng_Chen_Wei_Chi_2022}
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock {\em Operations Research}, 70(4):2563--2578, 2022.

\bibitem{ref-AlphaTensor}
Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino
  Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco~J.
  R.~Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis
  Hassabis, and Pushmeet Kohli.
\newblock Discovering faster matrix multiplication algorithms with
  reinforcement learning.
\newblock {\em Nature}, 610:47--53, 2022.

\bibitem{Johnson_Pike-Burke_Rebeschini_2023}
Emmeran Johnson, Ciara Pike-Burke, and Patrick Rebeschini.
\newblock Optimal convergence rate for exact policy mirror descent in
  discounted markov decision processes.
\newblock In {\em Sixteenth European Workshop on Reinforcement Learning}, 2023.

\bibitem{ref-AlphaFold}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna
  Potapenko, Alex Bridgland, Clemens Meyer, Simon A.~A. Kohl, Andrew~J.
  Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub
  Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy,
  Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer,
  Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew~W. Senior, Koray
  Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock {\em Nature}, 596:583--589, 2021.

\bibitem{kakade2002npg}
Sham Kakade.
\newblock A natural policy gradient.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1531--1538, 2001.

\bibitem{kakade2002approximately}
Sham~M. Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  267--274, 2002.

\bibitem{Khodadadian_Jhunjhunwala_Varma_Maguluri_2021}
Sajad Khodadadian, Prakirt~Raj Jhunjhunwala, Sushil~Mahavir Varma, and
  Siva~Theja Maguluri.
\newblock On the linear convergence of natural policy gradient algorithm.
\newblock In {\em IEEE Conference on Decision and Control}, pages 3794--3799,
  2021.

\bibitem{Lan_2021}
Guanghui Lan.
\newblock Policy mirror descent for reinforcement learning: {L}inear
  convergence, new sampling complexity, and generalized problem classes.
\newblock {\em Mathematical Programming}, 198(1):1059--1106, 2021.

\bibitem{li2023exponential}
Gen Li, Yuting Wei, Yuejie Chi, and Yuxin Chen.
\newblock Softmax policy gradient methods can take exponential time to
  converge.
\newblock {\em Mathematical Programming}, 201:707--802, 2023.

\bibitem{Li_Zhao_Lan_2022}
Yan Li, Guanghui Lan, and Tuo Zhao.
\newblock Homotopic policy mirror descent: {P}olicy convergence, algorithmic
  regularization, and improved sample complexity.
\newblock {\em Mathematical Programming}, 2023.

\bibitem{ppgliu}
Jiacai Liu, Wenye Li, and Ke~Wei.
\newblock Projected policy gradient converges in a finite number of iterations.
\newblock {\em arXiv:2311.01104}, 2023.

\bibitem{mei2021normalized}
Jincheng Mei, Yue Gao, Bo~Dai, Csaba Szepesvári, and Dale Schuurmans.
\newblock Leveraging non-uniformity in first-order non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{Mei_Xiao_Dai_Li_Szepesvari_Schuurmans_2020}
Jincheng Mei, Chenjun Xiao, Bo~Dai, Lihong Li, Csaba Szepesvári, and Dale
  Schuurmans.
\newblock Escaping the gravitational pull of softmax.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{Mei_Xiao_Szepesvari_Schuurmans_2020}
Jincheng Mei, Chenjun Xiao, Csaba Szepesvári, and Dale Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In {\em International Conference on Machine Learning}, pages
  6820--6829, 2020.

\bibitem{robot3}
Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun,
  and Marco Hutter.
\newblock Learning robust perceptive locomotion for quadrupedal robots in the
  wild.
\newblock {\em Science Robotics}, 7(62), 2022.

\bibitem{Nachum2017softPI}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{PI-super}
Martin~L. Puterman and Shelby~L. Brumelle.
\newblock On the convergence of policy iteration in stationary dynamic
  programming.
\newblock {\em Mathematics of Operations Research}, 4(1):60--69, 1979.

\bibitem{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages
  1889--1897, 2015.

\bibitem{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv:1707.06347}, 2017.

\bibitem{shani2020trpo}
Lior Shani, Yonathan Efroni, and Shie Mannor.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized {MDP}s.
\newblock In {\em AAAI Conference on Artifical Intelligence}, 2020.

\bibitem{ref-AlphaGo}
David Silver, Aja Huang, Chris~J. Maddison, Arthur Guez, Laurent Sifre, George
  van~den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal
  Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray
  Kavukcuoglu, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em Nature}, 529:484--489, 2016.

\bibitem{suttonRL}
Richard~S. Sutton and Andrew~G. Barto.
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock MIT Press, Cambridge, 2018.

\bibitem{pg}
Richard~S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1057--1063, 1999.

\bibitem{Williams1992}
Ronald~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine Learning}, 8:229--256, 1992.

\bibitem{Xiao_2022}
Lin Xiao.
\newblock On the convergence rates of policy gradient methods.
\newblock {\em Journal of Machine Learning Research}, 23(282):1--36, 2022.

\bibitem{Yuan_Du_Gower_Lazaric_Xiao_2022}
Rui Yuan, Simon~S. Du, Robert~M. Gower, Alessandro Lazaric, and Lin Xiao.
\newblock Linear convergence of natural policy gradient methods with log-linear
  policies.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{Zhan_Cen_Huang_Chen_Lee_Chi_2021}
Wenhao Zhan, Shicong Cen, Baihe Huang, Yuxin Chen, Jason~D. Lee, and Yuejie
  Chi.
\newblock Policy mirror descent for regularized reinforcement learning: {A}
  generalized framework with linear convergence.
\newblock {\em SIAM Journal on Optimization}, 33(2):1061--1091, 2023.

\bibitem{Zhang_Koppel_Bedi_Szepesvari_Wang_2020}
Junyu Zhang, Alec Koppel, Amrit~Singh Bedi, Csaba Szepesvári, and Mengdi Wang.
\newblock {V}ariational policy gradient method for reinforcement learning with
  general utilities.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4572--4583, 2020.

\end{thebibliography}
