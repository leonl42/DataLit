\section{Conclusion}\label{sec:conclusion}
%\begin{itemize}
%\item sample complexity
%\item parameterization, function approximation 
%\item risk sensitive, average reward
%\end{itemize}
In this paper, we comprehensively study the convergence of a family of basic policy gradient methods  for RL, including PPG, softmax PG, softmax NPG, entropy softmax PG, entropy softmax NPG, and soft PI. New  convergence results have been obtained for them based on  elementary analysis techniques.
There are several lines of direction for future work. For instance, it is interesting to see whether the analysis techniques can be used to handle the stochastic case where the policy gradient is evaluated through samples or the case in which  policy parameterization involves function approximation. It is also likely to extend the analysis study to other scenarios in addition to the discounted MDP setting, such as the  average reward MPD setting or the risk sensitive RL problem. 