\section{General Analysis Framework}\label{sec:overview}

\subsection{Preliminaries}
\label{subsec:preliminaries}
% Recall V, Q, A. 指出来 期望A=0
% Define Bellman operator, V 满足bellman 方程， 给两个policy, 
% 给出pdl 【lemma】
% 定义最优的Bellman operator，指出两个都是contraction，定义提升
% 说一下PI，PI的提升
% lemma 2.1, 2.7, 2.8, 【2.8之前定义次优概率】
% lemma 2.4, 2.5

% notation
% A_s^*, A_s^pi, Delta, tilde{\rho}, 点一下假设tilde{\mu}>0
% L_k^{k+1}, L_k^*这节主要基于这两个量讨论不带熵的情况，相关的情况也能扩展到带熵，具体在6里面提及
% note L_k^*= ... 同时说明T^*
% shorthand notation 主要是_{s,a}这种

%Recall the definitions of state value $V^\pi(s)$, action  value $Q^\pi(s,a)$ and advantage function $A^\pi(s,a)$ in the introduction. 
Compared to the state value defined in \eqref{eq:state-value} which fixes the initial state, the action value is defined by fixing the initial state-action pair,
\begin{align*}
Q^{\pi}\left( s,a \right) &:=\mathbb{E} \left[ \sum_{t=0}^{\infty}{\gamma ^tr\left( s_t,a_t\right)}|s_0=s,a_0=a,\pi \right].
\numberthis\label{eq:action-value}
\end{align*}
In addition, the advantage function is defined as $A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)$. 
It can be verified that
\begin{align*}
    \mathbb{E}_{a\sim\pi(\cdot|s)} \left[ A^\pi(s,a) \right] = 0.
\end{align*}
For an arbitrary vector $V \in \mathbb{R}^{|\calS|}$, the Bellman operator induced by a policy $\pi$ is defined as
\begin{align*}
    \calT^\pi V(s) := \mathbb{E}_{a\sim \pi(\cdot|s)} \mathbb{E}_{s^\prime \sim P(\cdot|s,a)} \left[ r(s,a) + \gamma V(s^\prime) \right].
\end{align*}
It is easy to see that $V^\pi$ is the fixed point of $\mathcal{T}^\pi$, i.e. 
$
    \calT^\pi V^\pi = V^\pi.
$
Given two policies $\pi_1, \pi_2 \in \Pi$, one has
\begin{align*}
    \calT^{\pi_1}V^{\pi_2}(s) - V^{\pi_2}(s) = \sum_{a\in\calA} \pi_1(a|s) A^{\pi_2}(s,a).
\end{align*}
Moreover, the value difference between two policies can be given based on $\calT^{\pi_1}V^{\pi_2}(s) - V^{\pi_2}(s)$.
\begin{lemma}[Performance difference lemma \cite{kakade2002approximately}]
    Given two policies $\pi_1$ and $\pi_2$, there holds
    \begin{align*}
        V^{\pi_1}(\rho) - V^{\pi_2}(\rho) &= \frac{1}{1-\gamma} \mathbb{E}_{s\sim d^{\pi_1}_\rho} \left[ \calT^{\pi_1} V^{\pi_2}(s) - V^{\pi_2}(s) \right] \\
        &= \frac{1}{1-\gamma} \mathbb{E}_{s\sim d^{\pi_1}_\rho} \left[ \sum_{a\in \calA} \pi_1(a|s) A^{\pi_2}(s,a) \right].
    \end{align*}
    \label{lem:PDL}
\end{lemma}

The optimal Bellman operator is defined as
\begin{align*}
    \calT V(s) := \max_{\pi \in \Pi} \calT^{\pi} V(s) = \max_{a\in\calA} \left\{ \mathbb{E}_{s^\prime\sim P(\cdot|s,a)} \left[ r(s,a) + \gamma V(s^\prime) \right] \right\}.
\end{align*}
Let $V^*$ and $Q^*$ be the optimal state and action values, with the corresponding optimal policy denoted by $\pi^*$ (may be non-unique), and define $A^*(s,a) = Q^*(s,a) - V^*(s)$. It is well-known that $V^*$ satisfies the Bellman optimality equation: $\calT V^* = V^*$. It is also noted that both $\calT^\pi$ and $\calT$ are $\gamma$-contraction under the infinity norm:
\begin{align*}
    \left\| \calT^\pi V_1 - \calT^\pi V_2 \right\|_\infty \leq \gamma \left\| V_1 - V_2 \right\|_\infty\quad\mbox{and}\quad\left\| \calT V_1 - \calT V_2 \right\|_\infty \leq \gamma \left\| V_1 - V_2 \right\|_\infty.
\end{align*}

%For the policy sequence $\{ \pi^k \}_{k=0}^T$ generated by some algorithm, it is natural to define the performance improvement by algorithm in the $k$-th iteration on state $s$ as:
%\begin{align*}
 %   \calT^{k+1} V^k(s) - V^k(s) = \sum_{a\in\calA} \pi^{k+1}(a|s) A^k(s,a)
%\end{align*}
%where $\calT^{k+1}$ is short for $\calT^{\pi^{k+1}}$ and similar for $V^k$ and $A^k$. 
Given a state value $V^k$ (short for $V^{\pi^k}$) associated with policy $\pi^k$,  policy iteration  (PI) attempts to obtain a new  policy $\pi^{k+1}$ through the greedy improvement:
\begin{align*}
    \mathrm{supp}(\pi^{k+1}(\cdot|s))\subset \argmax_a Q^k(s,a),\numberthis\label{eq:PI-update}
\end{align*}
where $Q^k(s,a)$ is short for $Q^{\pi^k}(s,a)$.
It is easy to see that the improvement of $\pi^{k+1}$ over $\pi^k$ is given by
\begin{align*}
   \mathcal{T}^{k+1}V^k(s)-V^k(s)= \calT V^k(s) - V^k(s) = \max _{a\in\calA} A^k(s,a),
\end{align*}
where $\mathcal{T}^{k+1}$ and $A^k(s,a)$ are short for $\mathcal{T}^{\pi^{k+1}}$ and  $A^{\pi^k}(s,a)$, respectively.

%Similarly, the improvement by the optimal policy in the $k$-th iteration on state $s$ is
%\begin{align*}
%    \calT^* V^k(s) - V^k(s) = \sum_{a\in\calA} \pi^{*}(a|s) A^k(s,a)
%\end{align*}
%where $\calT^*$ is short for $\calT^{\pi^*}$ for some optimal policy $\pi^*$.

Next we present a few useful lemmas that will be used frequently later. The first two lemmas can be verified directly.
\begin{lemma}
    {Assume $r(s,a) \in [0,1]$.} For any policy $\pi$, one has
    \begin{align*}
        V^\pi(s) \in \left[ 0, \frac{1}{1-\gamma} \right], \; Q^\pi(s,a) \in \left[ 0, \frac{1}{1-\gamma} \right], \; A^\pi(s,a) \in \left[ -\frac{1}{1-\gamma}, \frac{1}{1-\gamma} \right].
    \end{align*}
\end{lemma}

\begin{lemma}\label{lem:QV-relation}
    For any policy $\pi$,
    \begin{align*}
        \left\| Q^* - Q^\pi \right\|_\infty \leq \gamma \left\| V^* - V^\pi \right\|_\infty, \quad \left\| A^* - A^\pi \right\|_\infty \leq \left\| V^* - V^\pi \right\|_\infty.
    \end{align*}
\end{lemma}
\noindent Before presenting the next lemma, further define $\calA^*_s$ as the optimal action set at state $s$,
\begin{align*}
    \calA^*_s := \argmax_{a\in\calA} \; A^*(s,a),
\end{align*}
and define $b^\pi_s$ as the probability on non-optimal actions
\begin{align*}
    b^\pi_s := \sum_{a\not\in\calA^*_s} \pi(a|s).
\end{align*}
The optimal advantage gap, denoted $\Delta$,  is defined as
\begin{align*}
    \Delta := \min_{s\in\tilde{S}, a\not\in\calA^*_s} \left| A^*(s,a) \right|
\end{align*}
where $\tilde{S} = \left\{ s\in\calS: \calA^*_s \neq \calA \right\}$. Intuitively, $b^\pi_s$ measures  the non-optimality of $\pi$, and $\Delta$ in some extent reflects the difficulty of the RL problem in the MDP setting. The following lemma shows that the state value error is of the same order as the non-optimal  probability measure, which can be proved directly by applying the performance difference lemma to $-(V^\pi(\rho)-V^*(\rho))$.
\begin{lemma}[\cite{Khodadadian_Jhunjhunwala_Varma_Maguluri_2021,ppgliu}]\label{lem:bsk-bounds}
    For any $\rho \in \Delta(\calS)$,
    \begin{align*}
        \Delta \cdot \mathbb{E}_{s\sim\rho} [b^\pi_s] \leq V^*(\rho) - V^\pi(\rho) \leq \frac{1}{(1-\gamma)^2} \cdot \mathbb{E}_{s\sim d^\pi_\rho} [b^\pi_s].
    \end{align*}
\end{lemma}
%The proof of the first equality can be found in REF and the second one in REF. In the following we introduce two useful results in the probability theory.

The last two lemmas are concerned with the properties of the  covariance of random variables. 
They can be easily verified  and we include the proofs in Appendix~\ref{sec:proofs-covariance} for completeness.
\begin{lemma}\label{lem:covariance-identity} For any random variable $X$ and two real-valued functions $f$, $g$, there holds
\begin{align*}
    \mathrm{Cov}(f(X), g(X)) = \frac{1}{2}\mathbb{E}_{X, Y} \left[ (f(X)-f(Y)) (g(X)-g(Y)) \right],
\end{align*}
where $Y$ is an i.i.d. copy of  $X$.

\end{lemma}

\begin{lemma}\label{lem:positive-covariance}
    For any random variable $X$ and two monotonically increasing functions $f$ and $g$, there holds
\begin{align*}
    \mathrm{Cov}(f(X), g(X)) \geq 0.
\end{align*}

\end{lemma}



In this paper, we consider minimizing $V^{\pi_\theta}(\mu)$ for fixed $\mu\in\Delta(\mathcal{S})$ with $\tilde{\mu}=\min_s\mu(s)>0$. The state value error $V^*(\rho)-V^k(\rho)$ will be evacuated for an arbitrary $\rho\in\Delta(\mathcal{S})$. Whenever it is necessary, we will assume $\tilde{\rho}=\min_s\rho(s)>0$. Otherwise, $V^*(\rho)-V^k(\rho)$ can be evaluated through $V^*(\mu)-V^k(\mu)$ since 
\begin{align*}
    V^*(\rho)-V^k(\rho)=& \sum_s \rho(s)\left(V^*(s)-V^k(s)\right)\\
    &=\sum_s \frac{\rho(s)}{\mu(s)}\mu(s)\left(V^*(s)-V^k(s)\right)\\
    &\leq \left\|\frac{\rho}{\mu}\right\|_\infty\left(V^*(\mu)-V^k(\mu)\right).
\end{align*}
For any policy $\pi$, define the visitation measure $d_\rho^\pi:\mathcal{S}\rightarrow\Delta(\mathcal{S})$ as 
\begin{align*}
d_{\rho}^{\pi}\left( s \right) :=\left( 1-\gamma \right) \mathbb{E}\left[ \sum_{t=0}^{\infty}{\gamma ^t\mathbf{1}\left\{ s_t=s \right\}}|s_0\sim \rho,\,\pi \right]. 
\end{align*}
The discuss in this section mainly centers around the following two quantities:
\begin{align*}
    \calL_k^{k+1} &= \frac{1}{1-\gamma} \mathbb{E}_{s\sim d_\rho^{\pi^{k,*}}} \left[ \calT^{k+1} V^k(s) - V^k(s) \right]
    = \frac{1}{1-\gamma} \mathbb{E}_{s\sim d_\rho^{\pi^{k,*}}} \left[ \sum_{a\in\calA} \pi^{k+1}(a|s) A^k(s,a) \right],\numberthis\label{eq:Lk-non}\\
    \calL_k^* &= \frac{1}{1-\gamma} \mathbb{E}_{s\sim d_\rho^{\pi^{k,*}}} \left[ \calT^* V^k(s) - V^k(s) \right]
    = \frac{1}{1-\gamma} \mathbb{E}_{s\sim d_\rho^{\pi^{k,*}}} \left[ \sum_{a\in\calA} \pi^*(a|s) A^k(s,a) \right]\numberthis\label{eq:Lstar-non},
\end{align*}
where $d_\rho^{\pi^{k,*}}$ is the visitation measure corresponding to certain optimal polity $\pi^{k,*}$ chosen at the $k$-th iteration, and $\mathcal{T}^*$ is the Bellman operator associated with $\pi^{k,*}$. Unless stated otherwise, we will assume the same optimal policy is used across all the $k$ most of the times, and in this case $d_\rho^{\pi^{k,*}}$ is simplified to $d_\rho^*$.
It follows from the performance difference lemma that 
\begin{align*}
    \calL_k^*=V^*(\rho)-V^k(\rho),
\end{align*}
and thus $\calL_k^*$ is irrelevant to the choice of the optimal policy. In addition, the following bounds hold for $\mathcal{L}^*_k$, whose proof can be found in Appendix~\ref{sec:proofs-covariance}.
\begin{lemma}\label{lem:LstarvsLmax}
For any optimal policy $\pi^*$, there holds 
\begin{align*}
\mathcal{L}_k^*\leq \frac{1}{1-\gamma}\mathbb{E}_{s\sim d_\rho^*}\left[\max_a A^k(s,a)\right]\leq \frac{1}{(1-\gamma)\tilde{\rho}}\mathcal{L}^*_k.
\end{align*}

\end{lemma}


In addition to $\mathcal{A}_s^*$, the notation $\calA^\pi_s$ will also be used, defined as 
\begin{align*}
    \calA^\pi_s := \argmax_{a\in\calA} \; A^\pi(s,a).
\end{align*}
Moreover,  we may use for example $\pi^k(a|s)$ and $\pi^k_{s,a}$, $\pi^k_s$ and $\pi^k(\cdot|s)$, $A^k(s,a)$ and $A^k_{s,a}$ exchangeably.  
%By definition, $\calL^{k+1}_k$ is the average improvement by some specific algorithm and $\calL^*_k$ is the average improvement by some optimal policy. Note that $\calL^*_k = V^*(\rho) - V^k(\rho)$ according to the performance difference lemma (Lemma REF). In the following part, we will show that different relations between these two qualities lead to various convergence results.

%%%%%%%%%%%%%%%%%%%%
\subsection{Sublinear Convergence Analysis}

In this subsection, we present two general conditions for the sublinear convergence of a RL algorithm, as well as a general condition to establish the sublinear linear convergence lower bound.
\begin{theorem}[Sublinear Convergence]\label{thm:sublinear-general}
Let $\{\pi^k\}_{k\geq 0}$ be a policy sequence which satisfies $\mathcal{T}^{k+1}V^k\geq V^k$ for all $k$ \textup{(}or equivalently, $\sum_a\pi^{k+1}(a|s)A^k(s,a)\geq 0,\,\,\forall\, s$\textup{)}. Suppose there exists a sequence of optimal policies $\{\pi^{k,*}\}_{k\geq 0}$ and a positive constant sequence $\{C_k\}_{k\geq 0}$ such that 
\begin{align*}
\mathcal{L}_k^{k+1}\geq C_k\left(\mathcal{L}_k^*\right)^2,
\end{align*}
where both $\mathcal{L}_k^{k+1}$ and $\mathcal{L}_k^*$ are defined with respect to the visitation measure $d_\rho^{\pi^{k,*}}$. Then 
\begin{align*}
V^*(\rho) - V^k(\rho)\leq \frac{1}{k}\frac{1}{C\,(1-\gamma)\,\vartheta},
\end{align*}
where $C = \inf_kC_k$ and $\vartheta=\inf_k\left\|\frac{d_\rho^{\pi^{k,*}}}{\rho}\right\|_\infty^{-1}\geq \tilde{\rho}$.
\end{theorem}
\begin{proof}
Note that $\mathcal{L}_k^*=V^*(\rho)-V^k(\rho)$ is independent of $d_\rho^{\pi^{k,*}}$. In addition,  
\begin{align*}
\mathcal{L}_{k}^*-\mathcal{L}_{k+1}^* = V^{k+1}(\rho) - V^k(\rho) &= \frac{1}{1-\gamma} \sum_s d^{k+1}_\rho(s) \sum_a \pi^{k+1}(a|s) A^k(s,a) \\
&= \frac{1}{1-\gamma} \sum_s \frac{d^{k+1}_\rho(s)}{d^{\pi^{k, *}}_\rho(s)} d^{\pi^{k, *}}_\rho(s) \sum_a \pi^{k+1}(a|s) A^k(s,a)  \\
&\geq \sum_s \frac{\rho(s)}{d^{\pi^{k, *}}_\rho(s)} d^{\pi^{k, *}}_\rho(s) \sum_a \pi^{k+1}(a|s) A^k(s,a)  \\
&\geq (1-\gamma)\left\|\frac{d_\rho^{\pi^{k,*}}}{\rho}\right\|_\infty^{-1}\mathcal{L}_k^{k+1}\\
&\geq (1-\gamma)\left\|\frac{d_\rho^{\pi^{k,*}}}{\rho}\right\|_\infty^{-1}C_k\,\left(\mathcal{L}_k^*\right)^2\\
&\geq C(1-\gamma)\,\vartheta\left(\mathcal{L}_k^*\right)^2,
\end{align*}
where the first inequality leverages that $\sum_a\pi^{k+1}(a|s)A^k(s,a) \geq 0$. Therefore, 
\begin{align*}
\frac{1}{\mathcal{L}_{k+1}^*}-\frac{1}{\mathcal{L}_k^*} = \frac{\mathcal{L}_{k}^*-\mathcal{L}_{k+1}^*}{\mathcal{L}_k^*\mathcal{L}_{k+1}^*}\geq \frac{\mathcal{L}_{k}^*-\mathcal{L}_{k+1}^*}{\left(\mathcal{L}_k^*\right)^2}\geq C\,(1-\gamma)\,\vartheta,
\end{align*}
where the first inequality leverages that $\calL_k^* \geq \calL_{k+1}^*$. It follows that 
\begin{align*}
\frac{1}{\mathcal{L}_k^*}\geq \sum_{j=0}^{k-1}\left(\frac{1}{\mathcal{L}_{j+1}^*}-\frac{1}{\mathcal{L}_j^*}\right)\geq C\,(1-\gamma)\,\vartheta\,k,
\end{align*}
which completes the proof.    
\end{proof}
\begin{remark}\label{remark:sublinear}
%    {\color{red}(remark 2.3 and 2.4 of jiacai)}
From the proof, it is not difficult to see if one can show that
\begin{align*}
    \mathcal{L}_k^*-\mathcal{L}_{k+1}^*\geq \tilde{C}\left(\mathcal{L}_k^*\right)^2,
\end{align*}
then the $O(1/k)$ sublinear convergence also follows directly.
Based on the ascent lemma for $L$-smooth functions and the gradient domination property for $V^{\pi_\theta}(\mu)$,  the sublinear convergence of softmax PG for $\eta=1/L=\frac{(1-\gamma)^2}{8}$ is established  in \textup{\cite{Mei_Xiao_Szepesvari_Schuurmans_2020}} by showing that \textup{(}here $\mathcal{L}_k^*$ is defined with respect to $\mu$, i.e., $\mathcal{L}_k^*=V^*(\mu)-V^k(\mu)$\textup{)}
\begin{align*}
 \mathcal{L}_k^*-\mathcal{L}_{k+1}^* \ge \frac{\left( 1-\gamma \right) ^3}{16}\left\| \nabla _{\theta}V^{k}\left( \mu \right) \right\| _{2}^{2}\ge \frac{\left( 1-\gamma \right) ^5\kappa^2}{16|\mathcal{S}|}\left\| \frac{d_{\mu}^{\pi^*}}{\mu} \right\| _{\infty}^{-2}\left( \mathcal{L}_k^* \right) ^2.
\end{align*}
\noindent
In \textup{\cite{ppgliu}}, the sublinear convergence of PPG for any constant step size has been established by showing 
\begin{align*}
    \mathcal{L}_k^{k+1}\geq \frac{1-\gamma}{1+\frac{2+5|\mathcal{A}|}{\eta_k\,\tilde{\mu}}}(\mathcal{L}_k^*)^2
\end{align*}
 based on the particular update of PPG.
%and thus the sublinear convergence for 
\end{remark}
%%%%%
\begin{theorem}[Sublinear Convergence by Controlling Error Terms]\label{thm:sublinear-global-error}
    Let $\{\pi^k\}_{k\geq 0}$ be a policy sequence which satisfies $\mathcal{T}^{k+1}V^k\geq V^k$ for all $k$ \textup{(}or equivalently, $\sum_a\pi^{k+1}(a|s)A^k(s,a)\geq 0,\,\,\forall\, s$\textup{)}.  Assume that 
    \begin{align*}
\mathcal{L}_k^{k+1}\geq C\mathcal{L}_k^*-\varepsilon_k\quad\mbox{where } C>0\mbox{ and }\sum_{k=0}^\infty\varepsilon_k<\infty.\numberthis\label{sublinear_condition_by_controlling_error_term}
\end{align*}
Then 
\begin{align*}
V^*(\rho)-V^k(\rho)\leq \frac{1}{k\,C}\left(\frac{1}{(1-\gamma)^2}+\sum_{t=0}^{k-1}\varepsilon_t\right).
\end{align*}
\end{theorem}
\begin{proof}
    Noticing that $d_{d_\rho^*}^{k+1}(s)\geq (1-\gamma)d_\rho^*(s)$, one has 
\begin{align*}
\mathcal{L}_k^{k+1} & =\frac{1}{1-\gamma}\sum_sd_\rho^*(s)\left(\mathcal{T}^{k+1}V^k(s)- V^k(s)\right)\\
&\leq\frac{1}{(1-\gamma)^2}\sum_sd_{d_\rho^*}^{k+1}(s)\left(\mathcal{T}^{k+1}V^k(s)- V^k(s)\right)\\
&=\frac{1}{(1-\gamma)}\left(V^{k+1}(d_\rho^*)-V^k(d_\rho^*)\right).
\end{align*}
It follows that 
\begin{align*}
\mathcal{L}_k^*&\leq\frac{1}{k}\sum_{t=0}^{k-1}\mathcal{L}_t^*\\
&\leq \frac{1}{k\, C}\sum_{t=0}^{k-1}\left(\mathcal{L}_t^{t+1}+\varepsilon_t\right)\\
&\leq \frac{1}{k\,C}\left(\frac{1}{1-\gamma}\left(V^k(d_\rho^*)-V^0(d_\rho^*)\right)+\sum_{t=0}^{k-1}\varepsilon_t\right)\\
&\leq \frac{1}{k\,C}\left(\frac{1}{(1-\gamma)^2}+\sum_{t=0}^{k-1}\varepsilon_t\right),
\end{align*}
which completes the proof.
\end{proof}
\begin{remark}
    Using the three point lemma, it is shown in \textup{\cite{Agarwal_Kakade_Lee_Mahajan_2019,Xiao_2022,Lan_2021}}  that \eqref{sublinear_condition_by_controlling_error_term} is met with $C=1$ and 
\begin{align}
\varepsilon _k=\frac{1}{\eta _k}D_k-\frac{1}{\eta _k}D_{k+1}
\label{xiao,lan's formulation}
\end{align}
for softmax NPG \footnote{Though the sublinear analysis of NPG in \textup{\cite{Agarwal_Kakade_Lee_Mahajan_2019}} does not use the three point lemma explicitly, it can  be cast as a special case of that for PMD in \textup{\cite{Xiao_2022}}.} and its extension  policy mirror descent \textup{(}PMD\textup{)}, where $D_k:=\mathbb{E} _{s\sim d_{\rho}^{*}}\left[ B_h\left( \pi _{s}^{*},\pi _{s}^{k} \right) \right]$ with $B_h$ being the Bregman distance associated with a function  $h$. In this case, one has \textup{(}with constant step size $\eta$\textup{)}
$
\sum_{k=0}^{\infty}{\varepsilon _k}\leq \frac{1}{\eta}D_0.
$
It is worth noting that if increasing step size is adopted, linear convergence can also be established from \eqref{sublinear_condition_by_controlling_error_term} and \eqref{xiao,lan's formulation}, see \textup{\cite{Xiao_2022,Lan_2021,Li_Zhao_Lan_2022}} for more details.
\end{remark}




%%%%%%%
\begin{theorem}[Sublinear Lower Bound]\label{thm:sublinear-upper}
    Let $\{\pi^k\}_{k\geq 0}$ be a policy sequence which satisfies $\mathcal{T}^{k+1}V^k\geq V^k$ for all $k$ \textup{(}or equivalently, $\sum_a\pi^{k+1}(a|s)A^k(a|s)\geq 0,\,\,\forall\, s$\textup{)}.
    Assume that
\begin{align*}
0\leq \mathcal{L}_k^{k+1}\leq C\,\left(\mathcal{L}_k^*\right)^2.
\end{align*}
Then for any fixed $\sigma\in(0,1)$, there exists a time $T(\sigma)$ such that 
\begin{align*}
\forall\,k\geq T(\sigma):\quad V^*(\rho)-V^k(\rho)\geq \frac{1}{k}\frac{1-\sigma}{2\,C}\left\|\frac{1}{d_\rho^*}\right\|_\infty^{-1}.
\end{align*}
\end{theorem}
\begin{proof}
First note that 
\begin{align*}\mathcal{L}_k^*-\mathcal{L}_{k+1}^* & = \frac{1}{1-\gamma}\sum_s d_\rho^{k+1}(s)\sum_a\pi^{k+1}(a|s)A^k(s,a)\\&=\frac{1}{1-\gamma}\sum_s\frac{d_\rho^{k+1}(s)}{d_\rho^*(s)}d_\rho^*(s)\sum_a\pi^{k+1}(a|s)A^k(s,a)\\&\leq \left\|\frac{1}{d_\rho^*}\right\|_\infty\mathcal{L}_k^{k+1}\\&\leq C\left\|\frac{1}{d_\rho^*}\right\|_\infty\left(\mathcal{L}_k^*\right)^2,\end{align*}
where the first inequality utilizes the condition $\sum_a\pi^{k+1}(a|s)A^k(s,a)\geq 0$. It follows that 
\begin{align*}
\frac{1}{\mathcal{L}_{k+1}^*}-\frac{1}{\mathcal{L}_k^*} = \frac{\mathcal{L}_{k}^*-\mathcal{L}_{k+1}^*}{\mathcal{L}_k^*\mathcal{L}_{k+1}^*}\leq \frac{C\left\|\frac{1}{d_\rho^*}\right\|_\infty\left(\mathcal{L}_k^*\right)^2}{\mathcal{L}_k^*\mathcal{L}_{k+1}^*}= C\left\|\frac{1}{d_\rho^*}\right\|_\infty\frac{\mathcal{L}_k^*}{\mathcal{L}_{k+1}^*}.
\end{align*}
Since $\mathcal{L}_k^{k+1}\geq 0$, the sequence $\{V^k(\rho)\}$ is non-deceasing, and thus $\lim_{k\rightarrow\infty} V^k(\rho)$ exists. If $\lim_{k\rightarrow\infty} V^k(\rho)<V^*(\rho)$, then the result holds automatically. Thus, it suffices to consider the case $\lim_{k\rightarrow\infty} V^k(\rho)=V^*(\rho)$, in which case $\mathcal{L}_k^*\rightarrow 0$. Thus, there exists $T_1(\sigma)$ such that $\mathcal{L}_k^*\leq \frac{\sigma}{C}\left\|\frac{1}{d_\rho^*}\right\|_\infty^{-1}$ for $k\geq T_1(\sigma)$ and  
\begin{align*}
\mathcal{L}_{k+1}^*\geq \mathcal{L}_k^*-C\left\|\frac{1}{d_\rho^*}\right\|_\infty\left(\mathcal{L}_k^*\right)^2\geq (1-\sigma)\mathcal{L}_k^*.
\end{align*}
Substituting this result into the above inequality gives 
\begin{align*}
\frac{1}{\mathcal{L}_{k+1}^*}-\frac{1}{\mathcal{L}_k^*}\leq C\left\|\frac{1}{d_\rho^*}\right\|_\infty(1-\sigma)^{-1}.
\end{align*}
Consequently, for $k\geq T_1(\sigma)$,
\begin{align*}
\frac{1}{\mathcal{L}_k^*}&=\sum_{t=T_1(\sigma)}^{k-1}\left(\frac{1}{\mathcal{L}^*_{t+1}}-\frac{1}{\mathcal{L}_t^*}\right)+\frac{1}{\mathcal{L}_{T_1(\sigma)}^*}\\
&\leq (k-T_1(\sigma))C\left\|\frac{1}{d_\rho^*}\right\|_\infty(1-\sigma)^{-1}+\frac{1}{\mathcal{L}_{T_1(\sigma)}^*}\\
&\leq k\,C\left\|\frac{1}{d_\rho^*}\right\|_\infty(1-\sigma)^{-1}+\frac{1}{\mathcal{L}_{T_1(\sigma)}^*}
\end{align*}
As $\frac{1}{\mathcal{L}_{T_1(\sigma)}^*}$ is fixed, there exists a $T(\sigma)\ge T_1(\sigma)$ such that $\frac{1}{\mathcal{L}_{T_1(\sigma)}^*}\leq k\,C\left\|\frac{1}{d_\rho^*}\right\|_\infty(1-\sigma)^{-1}$ for $k\geq T(\sigma)$. Therefore, in this range, 
\begin{align*}
\frac{1}{\mathcal{L}_k^*}\leq 2\,k\,C\left\|\frac{1}{d_\rho^*}\right\|_\infty(1-\sigma)^{-1},
\end{align*}
as claimed. 
\end{proof}
\begin{remark}
%    {\color{red}(remark 2.7 and 2.8 of jiacai)}
It is easily seen from the proof that if one can show that  
\begin{align*}
    \mathcal{L}_k^*-\mathcal{L}_{k+1}^*\le \tilde{C}\left(\mathcal{L}_k^*\right)^2,
\end{align*}
then the $O(1/k)$ sublinear lower bound can also be established. For softmax PG, using the smoothness and the gradient domination of $V^{\pi_\theta}(\mu)$,
it is shown in \textup{\cite{Mei_Xiao_Szepesvari_Schuurmans_2020}}   that \textup{(}here $\mathcal{L}_k^*$ is defined with respect to $\mu$, i.e., $\mathcal{L}_k^*=V^*(\mu)-V^k(\mu)$\textup{)}
$$
\mathcal{L}_k^*-\mathcal{L}_{k+1}^* \le 
\left( \frac{4\eta  ^{2}}{\left( 1-\gamma \right) ^3}+\eta  \right) \left\| \nabla _{\theta}V^{k}\left( \mu \right) \right\| _{2}^{2} \leq \left( \frac{4\eta  ^{2}}{\left( 1-\gamma \right) ^3}+\eta  \right) \frac{2}{(1-\gamma)^2 \Delta^2} \cdot \left(\calL_k^*\right)^2. 
$$
 {Together with the condition on the step size to guarantee the monotonicity of $V^k(\mu)$}, the local $
\mathcal{O} \left( \frac{1}{k}\left( 1-\gamma \right) ^5\Delta ^2 \right) $ lower bound is further established  in \textup{\cite{Mei_Xiao_Szepesvari_Schuurmans_2020}}.
\end{remark}
%%%%%%%%%%%%%%%%%%%%
\subsection{Global Linear Convergence Analysis}
In this subsection, we present the general conditions for the global linear convergence of a RL algorithm in terms of the weighted state value error as well as  in terms of infinite norm.

\begin{theorem}[Linear Convergence under Weighted State Value Error]\label{thm:linear-global}
Let $\{\pi^k\}_{k\geq 0}$ be a policy sequence which satisfies $\mathcal{T}^{k+1}V^k\geq V^k$ for all $k$ \textup{(}or equivalently, $\sum_a\pi^{k+1}(a|s)A^k(s,a)\geq 0,\,\,\forall\, s$\textup{)}.
    Assume that 
\begin{align*}
\mathcal{L}_k^{k+1} \geq C_k\,\mathcal{L}_k^*,
\end{align*}
where $\mathcal{L}_k^{k+1}$ and $\mathcal{L}_k^*$ are defined under certain optimal policy $\pi^{k,*}$.
One has
\begin{align*}
\mathcal{L}_{k+1}^* \leq \left(1-(1-\gamma)\left\|\frac{d_\rho^{\pi^{k,*}}}{\rho}\right\|_\infty^{-1}C_k\right)\mathcal{L}_k^*.
\end{align*}
\end{theorem}
\begin{proof}
First note that $\mathcal{L}_k^*-\mathcal{L}_{k+1}^*=V^{k+1}(\rho)-V^k(\rho)$. By Lemma~\ref{lem:PDL}, one has 
\begin{align*}
\mathcal{L}_k^*-\mathcal{L}_{k+1}^* & = \frac{1}{1-\gamma}\sum_s d_\rho^{k+1}(s)\sum_a\pi^{k+1}(a|s)A^k(s,a)\\
&=\frac{1}{1-\gamma}\sum_s\frac{d_\rho^{k+1}(s)}{d_\rho^{\pi^{k,*}}(s)}d_\rho^{\pi^{k,*}}(s)\sum_a\pi^{k+1}(a|s)A^k(s,a)\\
&\geq \sum_s\frac{\rho(s)}{d_\rho^{\pi^{k,*}}(s)}d_\rho^{\pi^{k,*}}(s)\sum_a\pi^{k+1}(a|s)A^k(s,a)\\
&\geq (1-\gamma)\left\|\frac{d_\rho^{\pi^{k,*}}}{\rho}\right\|_\infty^{-1}\mathcal{L}_k^{k+1}\\
&\geq (1-\gamma)\left\|\frac{d_\rho^{\pi^{k,*}}}{\rho}\right\|_\infty^{-1}C_k\mathcal{L}_k^*,
\end{align*}
which completes the proof after rearrangement. Note that the first inequality requires  $\sum_a\pi^{k+1}(a|s)A^k(s,a)\geq 0,\,\forall s$.
\end{proof}

%%%%%
\begin{theorem}[Linear Convergence under Infinite Norm: I]\label{thm:linear-infinity}
    Assume for some $C_k\in(0,1)$,
\begin{align*}\forall\,s:\quad\sum_a\pi^{k+1}_{s,a}A^k_{s,a}\geq C_k\,\max_aA^k_{s,a}.
\end{align*}
Then, 
\begin{align*}
\|V^*-V^{k+1}\|_\infty\leq \left(1-(1-\gamma)\,C_k\right)\|V^*-V^{k}\|_\infty.
\end{align*}
\end{theorem}
\begin{proof}
    First the assumption can be rewritten as 
\begin{align*}
\mathcal{T}^{k+1}V^k(s)-V^k(s)\geq C_k\left(\mathcal{T}V^k(s)-V^k(s)\right).
\end{align*}
It follows that $\forall s$,
\begin{align*}
 V^*(s)-V^{k+1}(s)&=V^*(s)-\mathcal{T}^{k+1}V^{k+1}(s)\\
&\leq V^*(s)-\mathcal{T}^{k+1}V^{k}(s)\\
&\leq V^*(s)-V^{k}(s)-C_k\left(\mathcal{T}V^{k}(s)-V^{k}(s)\right)\\
&=C_k\left(\mathcal{T}V^*(s)-\mathcal{T}V^{k}(s)\right)+(1-C_k)\left(V^*(s)-V^{k}(s)\right),
\end{align*}
where the second line follows from the fact $V^{k+1}\geq V^k$ since $\forall\,s$ 
\begin{align*}
    V^{k+1}(s)-V^k(s)=\sum_a\pi^{k+1}_{s,a}A^k_{s,a}\ge 0.
\end{align*}
Noticing that $V^*(s)-V^k(s)\geq 0,\,\forall s$, the application of the contraction property of Bellman optimality operator yields that
\begin{align*}
\|V^*-V^{k+1}\|_\infty\leq \left(1-(1-\gamma)\,C_k\right)\|V^*-V^{k}\|_\infty,
\end{align*}
which completes the proof.
\end{proof}

%%%%%%%%
A slightly different form of Theorem~\ref{thm:linear-infinity} is given below, which can be proved in a similar manner. The proof details are omitted.
\begin{theorem}[Linear Convergence under Infinite Norm: II]\label{thm:linear-infinity-02}
Let $\{\pi^k\}_{k\geq 0}$ be a policy sequence which satisfies $\mathcal{T}^{k+1}V^k\geq V^k$ for all $k$ (or equivalently, $\sum_a\pi^{k+1}(a|s)A^k(s,a)\geq 0,\,\,\forall\, s$. Assume for some $C_k\in (0,1)$, 
\begin{align*}
\forall\,s:\quad \sum_{a}\pi^{k+1}_{s,a}A^k_{s,a}\geq C_k\sum_{a}\pi^{k,*}_{s,a}A^k_{s,a},
\end{align*}
where $\pi^{k,*}$ is an optimal policy which may vary from iteration to iteration. Then,
\begin{align*}
\|V^*-V^{k+1}\|_\infty\leq \left(1-(1-\gamma)\,C_k\right)\|V^*-V^{k}\|_\infty.
\end{align*}
\end{theorem}
%%%%%%%%
\begin{theorem}[Linear Convergence under Infinite Norm by Controlling Error Terms]\label{thm:linear-infinity-error}
Let $\{\pi^k\}_{k\geq 0}$ be a policy sequence which satisfies $\mathcal{T}^{k+1}V^k\geq V^k$ for all $k$ \textup{(}or equivalently, $\sum_a\pi^{k+1}(a|s)A^k(s,a)\geq 0,\,\,\forall\, s$\textup{)}.
 Assume there exists a constant $c_0>0$ such that 
\begin{align*}
\forall\,s:\quad \sum_a\pi^{k+1}_{s,a}A^k_{s,a}\geq C\,\max_aA^k_{s,a}-\varepsilon_k\quad\mbox{and}\quad \sum_{t=0}^{k-1} \frac{\varepsilon_t}{(1-(1-\gamma)C)^{t+1}}\leq c_0.\numberthis\label{linear_condition by maxA and controlling error terms }
\end{align*}
Then 
\begin{align*}
\|V^*-V^k\|_\infty\leq (1-(1-\gamma)C)^k\left(\|V^*-V^0\|_\infty+c_0\right).
\end{align*}

\end{theorem}

\begin{proof}
    Repeating the proof of Theorem~\ref{thm:linear-infinity} yields
    \begin{align*}
\|V^*-V^{k+1}\|_\infty\leq \left(1-(1-\gamma)C\right)\|V^*-V^{k}\|_\infty+\varepsilon_k.
\end{align*}
Iterating this procedure gives
\begin{align*}
\|V^*-V^k\|_\infty&\leq \left(1-(1-\gamma)C\right)^{k}\|V^*-V^{0}\|_\infty+\sum_{t=0}^{k-1}\varepsilon_t\left(1-(1-\gamma)C\right)^{k-1-t}\\
&=\left(1-(1-\gamma)C\right)^{k}\left(\|V^*-V^{0}\|_\infty+\sum_{t=0}^{k-1}\frac{\varepsilon_t}{(1-(1-\gamma)C)^{t+1}}\right),
\end{align*}
which completes the proof.
\end{proof}
\begin{remark}
%    {\color{red}(remarks 2.12-2.15 of jiacai)}
It is shown in \textup{\cite{ppgliu}}  that {\eqref{linear_condition by maxA and controlling error terms }} holds with $C=1$ and $
\varepsilon _k=\frac{2}{\eta _k\tilde{\mu}}$ for PPG. Furthermore, if the step size $\eta_k$ obeys
$$
\eta_k \geq \frac{2}{\tilde{\mu} (1-\gamma) c_0 \gamma ^{2k+1}},
$$
then the second condition of~\eqref{linear_condition by maxA and controlling error terms } is satisfied, which gives the $\gamma$-rate convergence of PPG. In \textup{\cite{Johnson_Pike-Burke_Rebeschini_2023}}, it shows  that \eqref{linear_condition by maxA and controlling error terms } is satisfied for PMD with $C=1$ and 
$$
\varepsilon _k=\frac{1}{\eta _k}\underset{s\in \mathcal{S}}{\max}\underset{\tilde{\pi}_{s}^{k+1}\in \widetilde{\Pi}_{s}^{k+1}}{\min}B_h\left( \tilde{\pi}_{s}^{k+1},\pi _{s}^{k} \right),
$$
where $B_h$ is the Bregman distance associated with the function $h$ and $\tilde{\Pi}^{k+1}_s$ is defined as 
$$
\widetilde{\Pi}_{s}^{k+1}:=\Big\{ p\in \Delta \left( \mathcal{A} \right) :\sum_{a\in \mathcal{A} _{s}^{k}}{p\left( a \right)}=1 \Big\} .
$$
Considering the step size $$
\eta _k\ge \frac{1}{\gamma ^{2k+1}}\underset{s\in \mathcal{S}}{\max}\underset{\tilde{\pi}_{s}^{k+1}\in \widetilde{\Pi}_{s}^{k+1}}{\min}B_h\left( \tilde{\pi}_{s}^{k+1},\pi _{s}^{k} \right)$$
gives the $\gamma$-rate convergence of PMD  established in \textup{\cite{Johnson_Pike-Burke_Rebeschini_2023}}.

\end{remark}