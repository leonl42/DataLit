\section{Related Work}

\textbf{Motion Mimicking.} 
Motion mimicking is a technique used to produce realistic animations in physics-based characters by learning skills from motion captures~\citep{hong2019physics, peng2018deepmimic, peng2018sfv, lee2019scalable}. This approach has been applied to various downstream tasks in physics-based animation, including generating new motions by recombining primitive actions~\citep{peng2019mcp, luo2020carl}, achieving specific goals~\citep{bergamin2019drecon,park2019learning, peng2021amp}, and as a pre-training method for general-purpose motor skills~\citep{merel2018hierarchical, hasenclever2020comic, peng2022ase, won2022physics}. The scalability of these tasks can be limited by the motion mimicking process, which is a key part of the pipeline in approaches like ScaDiver~\citep{won2020scalable}. In this work, we demonstrate that the problem of scalability can be addressed using differentiable dynamics.

\textbf{Speeding Up Motion Mimicking}
Most motion mimicking works are based on a DRL framework~\citep{peng2018deepmimic, bergamin2019drecon}, whose optimization is expensive. Several recent works speed up the DRL process by hyper-parameter searching~\citep{yang2021efficient} and constraint relaxation~\citep{ma2021learning}. Another line of work learns world models to achieve end-to-end gradient optimization~\citep{won2022physics, fussell2021supertrack}. However, learning world models requires extra training and introduces the risk of error accumulation. We are the first to use the off-the-shelf DPS for motion mimicking, which achieves better sample efficiency than optimized DRL frameworks and requires no additional world models.


\textbf{Differentiable Physics.}
Differentiable physics gains traction recently with the emerging differentiable physics simulators (DPS)~\citep{hu2019chainqueen, hu2019difftaichi, huang2021plasticinelab, qiao2021differentiable}. It has brought success in many domains and provides a distinctly new approach to control tasks. Supertrack~\citep{fussell2021supertrack} proposes to learn a world model to approximate the differentiable physics and has achieved promising results. However, its policy horizon is limited during training due to the accumulation of errors in the world model.
Various applications stem from DPS like system identification~\citep{gradsim}, continuous controls~\citep{lin2022diffskill, xu2022accelerated} in the past few years and demonstrate promising results. Compared to black-box counterparts which learn the dynamics with neural networks~\citep{fussell2021supertrack, li2020visual}, differentiable simulations utilize physical models to provide more reliable gradients with better interpretability. On the other hand, the analytical gradient from DPS suffers from noise or can sometimes be wrong~\citep{zhong2022differentiable, suh2022does} in a contact-rich environment. Non-smoothness or discontinuity in the contact event requires specific techniques to compute the gradient. However, they often introduce noise into the gradient~\citep{zhong2022differentiable}. Thus, in this work, we empirically show how to handle the noise in the gradient in a contact-rich environment for motion mimicking.  

\textbf{Policy Optimization with Differentiable Physics Simulators.} The integration of differentiable simulators in policy optimization has been a significant advancement in reinforcement learning. With analytical gradient calculation, the sample efficiency of policy learning is significantly improved~\citep{Mozer1989AFB, pmlr-v139-mora21a, xu2022accelerated}. However, the implementation is challenged by the presence of noisy gradients and the risk of exploding or vanishing gradients~\citep{Degrave2017ADP}. The policy may also get stuck at sub-optimal points due to the local nature of the gradient. To address these issues, various approaches have been proposed. SHAC~\citep{xu2022accelerated} truncates trajectories into smaller segments to prevent exploding or vanishing gradients, but this requires careful design. On the other hand, PODS~\citep{pmlr-v139-mora21a} utilizes second-order gradients, leading to monotonic policy improvement and faster convergence compared to first-order methods. However, it relies on strong assumptions about the second-order derivatives of the differentiable simulator and is sensitive to their accuracy. An alternative approach is the Imitation Learning framework with DPS proposed by ILD~\citep{chen2022imitation}. Although it is simple and effective for robot control, it struggles with the exploration of highly dynamic motions. To tackle these challenges, we introduce a replay mechanism to enhance its performance.



