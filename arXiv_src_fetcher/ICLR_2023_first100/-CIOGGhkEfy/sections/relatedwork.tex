\section{Related Work}

\textbf{Backdoor attacks} \cite{badnet} first used a modified dataset to insert backdoors during training, producing models that make correct predictions on clean data, but have new functionality when a specific trigger feature is present. Improvements to this process have since been made to create attacks that assume stronger threat models. \cite{quantisationbackdoor} demonstrated that backdoors can remain dormant until deployment, where the backdoor is activated by weight quantisation, while \cite{bob} manipulated the order of data within a batch to shape gradients that simulate a backdoored dataset using clean data. \cite{invisiblebackdoor} first investigated triggers that are difficult for humans to identify. Attacks that insert backdoors without modifying a dataset were also demonstrated, for example by inserting malicious components directly into the model's architecture \citep{architecturebackdoor}, or by perturbing the model's weights after training \citep{weightattack}. Many of these techniques assume direct access to either the model itself or its training set. Methods that use preprocessing such as image scaling \citep{imagescaling0, imagescaling1} or image rotation \citep{rotatebackdoor} to indirectly insert backdoors have been shown to be an effective mechanism to insert backdoors into machine learning pipelines. However, some of these attacks require additional modification of the dataset after the preprocessing is performed.

In this paper, we present three new backdoors which can be implemented as part of common augmentation techniques. This provides a new mechanism for inserting backdoors into the training pipeline while also remaining covert by inserting the backdoor through the augmentations' random parameters rather than by direct modification of the images in a dataset.

\textbf{Augmentation} Image data augmentation has been shown to be effective at improving model generalisation. Simple data augmentation strategies such as flipping, translation \citep{resnet, translation2}, scaling, and rotation \citep{rotation} are commonly used to improve model accuracy in image classification tasks, practically teaching invariance through semantically-meaningful transformations~\citep{lyle2020benefits}. More complex augmentation methods based on generative deep learning \citep{dagan, cyclegan} are now common as they have demonstrated strong performance on tasks where class-invariant transforms are non-trivial and are hard to define for a human.

Rather than encoding a direct invariance, Cutout \citep{cutout} removes a random portion of each image, while mixing techniques \citep{cutmix, mixup} mix two random images into one image with a combined label. AugMix \citep{augmix} uses random compositions of simpler transforms to provide more possible augmentations, while AutoAugment \citep{autoaug} tunes compositions of transforms to maximise classifier performance. We provide an overview of different types of augmentations and how they relate to each other in \Cref{fig:taxonomy}.