\section{Related Work}
\label{sec:related work}
\textbf{Communication-efficient training.}
There has been various lines of research focusing on improving communication efficiency in large-scale training, such as using asynchrony \citep{niu2011hogwild,lian2015asynchronous,xie2020zeno++}, decentralization \citep{lian2017can,lu2021optimal}, gradient quantization \citep{alistarh2017qsgd,wen2017terngrad}, gradient sparsification \citep{wangni2017gradient,wang2018atomo}, local steps \citep{stich2018local,lin2018don}, etc. 
In this paper we study the aggressive 1-bit compression, which was first introduced in \citep{seide20141} to speed up speech model training, where an algorithm called 1-bit SGD is proposed. After that, \citet{wen2017terngrad} proposes adding 0 as an additional numerical level and \citet{liu2018signsgd} discusses the use of zero-th order oracle in 1-bit SGD. \citet{chen2019distributed,balles2018dissecting,xu2019signprox} study the correlation and combination between 1-bit SGD and other techniques. Convergence analysis on 1-bit SGD is given in \citep{bernstein2018signsgd,karimireddy2019error,safaryan2021stochastic}. 
\citet{bernstein2018signsgd2,sohn2019election,le2020distributed,lyu2021dp} investigate the robustness of 1-bit SGD.
Among all the variants of 1-bit communication, the design with error feedback mechanism has shown to work best both empirically \citep{seide20141} and theoretically \citep{karimireddy2019error}.
Other lines of research applies 1-bit communication to various scenarios such as federated learning \citep{jin2020stochastic,yue2021federated}, decentralized learning \citep{lu2020moniqua,koloskova2019decentralized}, meta learning \citep{fan2021sign}, etc. Perhaps the closest works to this paper are \citep{tang20211,li20211}, which propose using two-stage training to enable 1-bit Adam and 1-bit Lamb, respectively. Different from those two work, 0/1 Adam addresses non-linearity challenges in adaptive optimizers by considering both extreme quantization and local steps. Furthermore, we also study how to apply extreme communication compression on GPT-style models, which to the best our knowledge is still under-explored.  

\textbf{Adaptive learning rate optimizers.}
One of the most popular adaptive optimizers is Adam, which was first introduced in \citep{kingma2014adam}. It uses both first and second moment information of stochastic gradient to perform optimizer steps and has shown significant benefits on training deep learning models. \citet{reddi2019convergence} spots the issue of Adam convergence and provides a variant called AMSGrad while \citet{zaheer2018adaptive} argues the Adam only converges with large batch sizes.
Multiple lines of theoretical study on Adam are given in \citep{fang2019convergence,alacaoglu2020new,defossez2020simple}.
Additionally,
\citet{chen2018convergence,zhou2018convergence,lu2020mixml,danilova2020recent,zou2019sufficient} provide more general analysis on Adam-type optimizers.
Subsequently, other variants of Adam are proposed in \citep{luo2019adaptive,chen2019zo,huang2018nostalgic,wang2019sadam, zhou2018adashift, zhuang2021momentum,zhuang2020adabelief}. 
Unlike these methods, which focus on improving the convergence of generic optimizations for DNN models, our work studies how to maximize the communication efficiency of Adam in large-scale distributed training settings. 

% the communication efficiency of Adam in data-center model training.
% We emphasize there is a major distinction between these previous works and {\myalgo}, as they 
% investigates how to improve Adam statistically while {\myalgo} studies the communication efficiency of Adam in data-center model training.