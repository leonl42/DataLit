\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Radford et~al.(2019{\natexlab{a}})Radford, Wu, Child, Luan, Amodei,
  and Sutskever]{gpt-2}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019{\natexlab{a}}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{gpt-3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{CoRR}, abs/2005.14165, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari,
  Casper, Liu, Prabhumoye, Zerveas, Korthikanti, Zheng, Child, Aminabadi,
  Bernauer, Song, Shoeybi, He, Houston, Tiwary, and Catanzaro]{mt-nlg}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
  Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
  Vijay Korthikanti, Elton Zheng, Rewon Child, Reza~Yazdani Aminabadi, Julie
  Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh
  Tiwary, and Bryan Catanzaro.
\newblock Using deepspeed and megatron to train megatron-turing {NLG} 530b, {A}
  large-scale generative language model.
\newblock \emph{CoRR}, abs/2201.11990, 2022.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock \emph{Advances in Neural Information Processing Systems},
  30:\penalty0 1709--1720, 2017.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In \emph{Fifteenth Annual Conference of the International Speech
  Communication Association}. Citeseer, 2014.

\bibitem[Bernstein et~al.(2018{\natexlab{a}})Bernstein, Wang, Azizzadenesheli,
  and Anandkumar]{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  560--569. PMLR, 2018{\natexlab{a}}.

\bibitem[Stich(2018)]{stich2018local}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock \emph{arXiv preprint arXiv:1805.09767}, 2018.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Liu, Tang, Shang, Liu, Sun, and
  Jiao]{wang2019signadam++}
Dong Wang, Yicheng Liu, Wenwo Tang, Fanhua Shang, Hongying Liu, Qigong Sun, and
  Licheng Jiao.
\newblock signadam++: Learning confidences for deep neural networks.
\newblock In \emph{2019 International Conference on Data Mining Workshops
  (ICDMW)}, pages 186--195. IEEE, 2019{\natexlab{a}}.

\bibitem[Tang et~al.(2021)Tang, Gan, Awan, Rajbhandari, Li, Lian, Liu, Zhang,
  and He]{tang20211}
Hanlin Tang, Shaoduo Gan, Ammar~Ahmad Awan, Samyam Rajbhandari, Conglong Li,
  Xiangru Lian, Ji~Liu, Ce~Zhang, and Yuxiong He.
\newblock 1-bit adam: Communication efficient large-scale training with adam's
  convergence speed.
\newblock \emph{arXiv preprint arXiv:2102.02888}, 2021.

\bibitem[Niu et~al.(2011)Niu, Recht, R{\'e}, and Wright]{niu2011hogwild}
Feng Niu, Benjamin Recht, Christopher R{\'e}, and Stephen~J Wright.
\newblock Hogwild!: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1106.5730}, 2011.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  28:\penalty0 2737--2745, 2015.

\bibitem[Xie et~al.(2020)Xie, Koyejo, and Gupta]{xie2020zeno++}
Cong Xie, Sanmi Koyejo, and Indranil Gupta.
\newblock Zeno++: Robust fully asynchronous sgd.
\newblock In \emph{International Conference on Machine Learning}, pages
  10495--10503. PMLR, 2020.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1705.09056}, 2017.

\bibitem[Lu and De~Sa(2021)]{lu2021optimal}
Yucheng Lu and Christopher De~Sa.
\newblock Optimal complexity in decentralized training.
\newblock In \emph{International Conference on Machine Learning}, pages
  7111--7123. PMLR, 2021.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock \emph{arXiv preprint arXiv:1705.07878}, 2017.

\bibitem[Wangni et~al.(2017)Wangni, Wang, Liu, and Zhang]{wangni2017gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock \emph{arXiv preprint arXiv:1710.09854}, 2017.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Sievert, Charles, Liu, Wright,
  and Papailiopoulos]{wang2018atomo}
Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen Wright, and
  Dimitris Papailiopoulos.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock \emph{arXiv preprint arXiv:1806.04090}, 2018{\natexlab{a}}.

\bibitem[Lin et~al.(2018)Lin, Stich, Patel, and Jaggi]{lin2018don}
Tao Lin, Sebastian~U Stich, Kumar~Kshitij Patel, and Martin Jaggi.
\newblock Don't use large mini-batches, use local sgd.
\newblock \emph{arXiv preprint arXiv:1808.07217}, 2018.

\bibitem[Liu et~al.(2018)Liu, Chen, Chen, and Hong]{liu2018signsgd}
Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong.
\newblock signsgd via zeroth-order oracle.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Chen, Sun, Wu, and
  Hong]{chen2019distributed}
Xiangyi Chen, Tiancong Chen, Haoran Sun, Zhiwei~Steven Wu, and Mingyi Hong.
\newblock Distributed training with heterogeneous data: Bridging median-and
  mean-based algorithms.
\newblock \emph{arXiv preprint arXiv:1906.01736}, 2019{\natexlab{a}}.

\bibitem[Balles and Hennig(2018)]{balles2018dissecting}
Lukas Balles and Philipp Hennig.
\newblock Dissecting adam: The sign, magnitude and variance of stochastic
  gradients.
\newblock In \emph{International Conference on Machine Learning}, pages
  404--413. PMLR, 2018.

\bibitem[Xu and Kamilov(2019)]{xu2019signprox}
Xiaojian Xu and Ulugbek~S Kamilov.
\newblock Signprox: One-bit proximal algorithm for nonconvex stochastic
  optimization.
\newblock In \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 7800--7804. IEEE, 2019.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock In \emph{International Conference on Machine Learning}, pages
  3252--3261. PMLR, 2019.

\bibitem[Safaryan and Richt{\'a}rik(2021)]{safaryan2021stochastic}
Mher Safaryan and Peter Richt{\'a}rik.
\newblock Stochastic sign descent methods: New algorithms and better theory.
\newblock In \emph{International Conference on Machine Learning}, pages
  9224--9234. PMLR, 2021.

\bibitem[Bernstein et~al.(2018{\natexlab{b}})Bernstein, Zhao, Azizzadenesheli,
  and Anandkumar]{bernstein2018signsgd2}
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock signsgd with majority vote is communication efficient and fault
  tolerant.
\newblock \emph{arXiv preprint arXiv:1810.05291}, 2018{\natexlab{b}}.

\bibitem[Sohn et~al.(2019)Sohn, Han, Choi, and Moon]{sohn2019election}
Jy-yong Sohn, Dong-Jun Han, Beongjun Choi, and Jaekyun Moon.
\newblock Election coding for distributed learning: Protecting signsgd against
  byzantine attacks.
\newblock \emph{arXiv preprint arXiv:1910.06093}, 2019.

\bibitem[Le~Phong and Phuong(2020)]{le2020distributed}
Trieu Le~Phong and Tran~Thi Phuong.
\newblock Distributed signsgd with improved accuracy and network-fault
  tolerance.
\newblock \emph{IEEE Access}, 8:\penalty0 191839--191849, 2020.

\bibitem[Lyu(2021)]{lyu2021dp}
Lingjuan Lyu.
\newblock Dp-signsgd: When efficiency meets privacy and robustness.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3070--3074. IEEE, 2021.

\bibitem[Jin et~al.(2020)Jin, Huang, He, Dai, and Wu]{jin2020stochastic}
Richeng Jin, Yufan Huang, Xiaofan He, Huaiyu Dai, and Tianfu Wu.
\newblock Stochastic-sign sgd for federated learning with theoretical
  guarantees.
\newblock \emph{arXiv preprint arXiv:2002.10940}, 2020.

\bibitem[Yue et~al.(2021)Yue, Jin, Wong, and Dai]{yue2021federated}
Kai Yue, Richeng Jin, Chau-Wai Wong, and Huaiyu Dai.
\newblock Federated learning via plurality vote.
\newblock \emph{arXiv preprint arXiv:2110.02998}, 2021.

\bibitem[Lu and De~Sa(2020)]{lu2020moniqua}
Yucheng Lu and Christopher De~Sa.
\newblock Moniqua: Modulo quantized communication in decentralized sgd.
\newblock In \emph{International Conference on Machine Learning}, pages
  6415--6425. PMLR, 2020.

\bibitem[Koloskova et~al.(2019)Koloskova, Lin, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Tao Lin, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized deep learning with arbitrary communication compression.
\newblock \emph{arXiv preprint arXiv:1907.09356}, 2019.

\bibitem[Fan et~al.(2021)Fan, Ram, and Liu]{fan2021sign}
Chen Fan, Parikshit Ram, and Sijia Liu.
\newblock Sign-maml: Efficient model-agnostic meta-learning by signsgd.
\newblock \emph{arXiv preprint arXiv:2109.07497}, 2021.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Awan, Tang, Rajbhandari, and
  He]{li20211}
Conglong Li, Ammar~Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He.
\newblock 1-bit lamb: Communication efficient large-scale large-batch training
  with lamb's convergence speed.
\newblock \emph{arXiv preprint arXiv:2104.06069}, 2021{\natexlab{a}}.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Fang and Klabjan(2019)]{fang2019convergence}
Biyi Fang and Diego Klabjan.
\newblock Convergence analyses of online adam algorithm in convex setting and
  two-layer relu neural network.
\newblock \emph{arXiv preprint arXiv:1905.09356}, 2019.

\bibitem[Alacaoglu et~al.(2020)Alacaoglu, Malitsky, Mertikopoulos, and
  Cevher]{alacaoglu2020new}
Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, and Volkan Cevher.
\newblock A new regret analysis for adam-type algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  202--210. PMLR, 2020.

\bibitem[D{\'e}fossez et~al.(2020)D{\'e}fossez, Bottou, Bach, and
  Usunier]{defossez2020simple}
Alexandre D{\'e}fossez, L{\'e}on Bottou, Francis Bach, and Nicolas Usunier.
\newblock A simple convergence proof of adam and adagrad.
\newblock \emph{arXiv preprint arXiv:2003.02395}, 2020.

\bibitem[Chen et~al.(2018)Chen, Liu, Sun, and Hong]{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.02941}, 2018.

\bibitem[Zhou et~al.(2018{\natexlab{a}})Zhou, Chen, Cao, Tang, Yang, and
  Gu]{zhou2018convergence}
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018{\natexlab{a}}.

\bibitem[Lu et~al.(2020)Lu, Nash, and De~Sa]{lu2020mixml}
Yucheng Lu, Jack Nash, and Christopher De~Sa.
\newblock Mixml: A unified analysis of weakly consistent parallel learning.
\newblock \emph{arXiv preprint arXiv:2005.06706}, 2020.

\bibitem[Danilova et~al.(2020)Danilova, Dvurechensky, Gasnikov, Gorbunov,
  Guminov, Kamzolov, and Shibaev]{danilova2020recent}
Marina Danilova, Pavel Dvurechensky, Alexander Gasnikov, Eduard Gorbunov,
  Sergey Guminov, Dmitry Kamzolov, and Innokentiy Shibaev.
\newblock Recent theoretical advances in non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2012.06188}, 2020.

\bibitem[Zou et~al.(2019)Zou, Shen, Jie, Zhang, and Liu]{zou2019sufficient}
Fangyu Zou, Li~Shen, Zequn Jie, Weizhong Zhang, and Wei Liu.
\newblock A sufficient condition for convergences of adam and rmsprop.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 11127--11135, 2019.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{luo2019adaptive}
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock \emph{arXiv preprint arXiv:1902.09843}, 2019.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Liu, Xu, Li, Lin, Hong, and
  Cox]{chen2019zo}
Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, and David
  Cox.
\newblock Zo-adamm: Zeroth-order adaptive momentum method for black-box
  optimization.
\newblock \emph{arXiv preprint arXiv:1910.06513}, 2019{\natexlab{b}}.

\bibitem[Huang et~al.(2018)Huang, Wang, and Dong]{huang2018nostalgic}
Haiwen Huang, Chang Wang, and Bin Dong.
\newblock Nostalgic adam: Weighting more of the past gradients when designing
  the adaptive learning rate.
\newblock \emph{arXiv preprint arXiv:1805.07557}, 2018.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Lu, Tu, and Zhang]{wang2019sadam}
Guanghui Wang, Shiyin Lu, Weiwei Tu, and Lijun Zhang.
\newblock Sadam: A variant of adam for strongly convex functions.
\newblock \emph{arXiv preprint arXiv:1905.02957}, 2019{\natexlab{b}}.

\bibitem[Zhou et~al.(2018{\natexlab{b}})Zhou, Zhang, Lu, Wang, Zhang, and
  Yu]{zhou2018adashift}
Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong
  Yu.
\newblock Adashift: Decorrelation and convergence of adaptive learning rate
  methods.
\newblock \emph{arXiv preprint arXiv:1810.00143}, 2018{\natexlab{b}}.

\bibitem[Zhuang et~al.(2021)Zhuang, Ding, Tang, Dvornek, Tatikonda, and
  Duncan]{zhuang2021momentum}
Juntang Zhuang, Yifan Ding, Tommy Tang, Nicha Dvornek, Sekhar~C Tatikonda, and
  James Duncan.
\newblock Momentum centering and asynchronous update for adaptive gradient
  methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhuang et~al.(2020)Zhuang, Tang, Ding, Tatikonda, Dvornek,
  Papademetris, and Duncan]{zhuang2020adabelief}
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar~C Tatikonda, Nicha Dvornek,
  Xenophon Papademetris, and James Duncan.
\newblock Adabelief optimizer: Adapting stepsizes by the belief in observed
  gradients.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 18795--18806, 2020.

\bibitem[Basu et~al.(2020)Basu, Data, Karakus, and Diggavi]{basu2020qsparse}
Debraj Basu, Deepesh Data, Can Karakus, and Suhas~N Diggavi.
\newblock Qsparse-local-sgd: Distributed sgd with quantization, sparsification,
  and local computations.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 217--226, 2020.

\bibitem[Pytorch(2014)]{pytorchTorchvisionmodelsx2014}
Pytorch.
\newblock Torchvision 0.11.0 documentation --- pytorch.org.
\newblock \url{https://pytorch.org/vision/stable/models.html}, 2014.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Zhang, and He]{li2021curriculum}
Conglong Li, Minjia Zhang, and Yuxiong He.
\newblock Curriculum learning: A regularization method for efficient and stable
  billion-scale gpt model pre-training.
\newblock \emph{arXiv preprint arXiv:2108.06084}, 2021{\natexlab{b}}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018{\natexlab{b}}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Radford et~al.(2019{\natexlab{b}})Radford, Wu, Amodei, Amodei, Clark,
  Brundage, and Sutskever]{radford2019better}
Alec Radford, Jeffrey Wu, Dario Amodei, Daniela Amodei, Jack Clark, Miles
  Brundage, and Ilya Sutskever.
\newblock Better language models and their implications.
\newblock \emph{OpenAI blog}, 1:\penalty0 2, 2019{\natexlab{b}}.

\bibitem[Trinh and Le(2018)]{trinh2018simple}
Trieu~H Trinh and Quoc~V Le.
\newblock A simple method for commonsense reasoning.
\newblock \emph{arXiv preprint arXiv:1806.02847}, 2018.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Rashkin, Bisk, Farhadi,
  Roesner, and Choi]{zellers2019defending}
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,
  Franziska Roesner, and Yejin Choi.
\newblock Defending against neural fake news.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
