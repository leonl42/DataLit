@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014},
  organization={Citeseer}
}

@article{chen2018convergence,
  title={On the convergence of a class of adam-type algorithms for non-convex optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  journal={arXiv preprint arXiv:1808.02941},
  year={2018}
}

@article{niu2011hogwild,
  title={Hogwild!: A lock-free approach to parallelizing stochastic gradient descent},
  author={Niu, Feng and Recht, Benjamin and R{\'e}, Christopher and Wright, Stephen J},
  journal={arXiv preprint arXiv:1106.5730},
  year={2011}
}

@article{lian2015asynchronous,
  title={Asynchronous parallel stochastic gradient for nonconvex optimization},
  author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  pages={2737--2745},
  year={2015}
}

@inproceedings{lu2021optimal,
  title={Optimal complexity in decentralized training},
  author={Lu, Yucheng and De Sa, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={7111--7123},
  year={2021},
  organization={PMLR}
}

@article{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={1709--1720},
  year={2017}
}

@article{wangni2017gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  journal={arXiv preprint arXiv:1710.09854},
  year={2017}
}

@article{stich2018local,
  title={Local SGD converges fast and communicates little},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1805.09767},
  year={2018}
}

@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3252--3261},
  year={2019},
  organization={PMLR}
}

@inproceedings{liu2018signsgd,
  title={signSGD via zeroth-order oracle},
  author={Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{safaryan2021stochastic,
  title={Stochastic sign descent methods: New algorithms and better theory},
  author={Safaryan, Mher and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={9224--9234},
  year={2021},
  organization={PMLR}
}

@inproceedings{balles2018dissecting,
  title={Dissecting adam: The sign, magnitude and variance of stochastic gradients},
  author={Balles, Lukas and Hennig, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={404--413},
  year={2018},
  organization={PMLR}
}

@article{luo2019adaptive,
  title={Adaptive gradient methods with dynamic bound of learning rate},
  author={Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  journal={arXiv preprint arXiv:1902.09843},
  year={2019}
}

@article{zaheer2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{danilova2020recent,
  title={Recent theoretical advances in non-convex optimization},
  author={Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  journal={arXiv preprint arXiv:2012.06188},
  year={2020}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@article{tang20211,
  title={1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed},
  author={Tang, Hanlin and Gan, Shaoduo and Awan, Ammar Ahmad and Rajbhandari, Samyam and Li, Conglong and Lian, Xiangru and Liu, Ji and Zhang, Ce and He, Yuxiong},
  journal={arXiv preprint arXiv:2102.02888},
  year={2021}
}

@article{li20211,
  title={1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed},
  author={Li, Conglong and Awan, Ammar Ahmad and Tang, Hanlin and Rajbhandari, Samyam and He, Yuxiong},
  journal={arXiv preprint arXiv:2104.06069},
  year={2021}
}

@article{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1808.05671},
  year={2018}
}

@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}

@article{chen2018closing,
  title={Closing the generalization gap of adaptive gradient methods in training deep neural networks},
  author={Chen, Jinghui and Zhou, Dongruo and Tang, Yiqi and Yang, Ziyan and Cao, Yuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1806.06763},
  year={2018}
}

@inproceedings{lu2020moniqua,
  title={Moniqua: Modulo quantized communication in decentralized SGD},
  author={Lu, Yucheng and De Sa, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={6415--6425},
  year={2020},
  organization={PMLR}
}

@article{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  journal={arXiv preprint arXiv:1705.09056},
  year={2017}
}

@inproceedings{xie2020zeno++,
  title={Zeno++: Robust fully asynchronous SGD},
  author={Xie, Cong and Koyejo, Sanmi and Gupta, Indranil},
  booktitle={International Conference on Machine Learning},
  pages={10495--10503},
  year={2020},
  organization={PMLR}
}

@article{wen2017terngrad,
  title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  journal={arXiv preprint arXiv:1705.07878},
  year={2017}
}

@article{wang2018atomo,
  title={Atomo: Communication-efficient learning via atomic sparsification},
  author={Wang, Hongyi and Sievert, Scott and Charles, Zachary and Liu, Shengchao and Wright, Stephen and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:1806.04090},
  year={2018}
}

@article{lin2018don,
  title={Don't use large mini-batches, use local SGD},
  author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
  journal={arXiv preprint arXiv:1808.07217},
  year={2018}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{chen2019distributed,
  title={Distributed training with heterogeneous data: Bridging median-and mean-based algorithms},
  author={Chen, Xiangyi and Chen, Tiancong and Sun, Haoran and Wu, Zhiwei Steven and Hong, Mingyi},
  journal={arXiv preprint arXiv:1906.01736},
  year={2019}
}

@article{jin2020stochastic,
  title={Stochastic-sign SGD for federated learning with theoretical guarantees},
  author={Jin, Richeng and Huang, Yufan and He, Xiaofan and Dai, Huaiyu and Wu, Tianfu},
  journal={arXiv preprint arXiv:2002.10940},
  year={2020}
}

@article{yue2021federated,
  title={Federated Learning via Plurality Vote},
  author={Yue, Kai and Jin, Richeng and Wong, Chau-Wai and Dai, Huaiyu},
  journal={arXiv preprint arXiv:2110.02998},
  year={2021}
}

@article{koloskova2019decentralized,
  title={Decentralized deep learning with arbitrary communication compression},
  author={Koloskova, Anastasia and Lin, Tao and Stich, Sebastian U and Jaggi, Martin},
  journal={arXiv preprint arXiv:1907.09356},
  year={2019}
}

@article{bernstein2018signsgd2,
  title={signSGD with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@article{sohn2019election,
  title={Election coding for distributed learning: Protecting signsgd against byzantine attacks},
  author={Sohn, Jy-yong and Han, Dong-Jun and Choi, Beongjun and Moon, Jaekyun},
  journal={arXiv preprint arXiv:1910.06093},
  year={2019}
}

@article{le2020distributed,
  title={Distributed SignSGD With Improved Accuracy and Network-Fault Tolerance},
  author={Le Phong, Trieu and Phuong, Tran Thi},
  journal={IEEE Access},
  volume={8},
  pages={191839--191849},
  year={2020},
  publisher={IEEE}
}

@inproceedings{lyu2021dp,
  title={DP-SIGNSGD: When Efficiency Meets Privacy and Robustness},
  author={Lyu, Lingjuan},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3070--3074},
  year={2021},
  organization={IEEE}
}

@article{fan2021sign,
  title={Sign-MAML: Efficient Model-Agnostic Meta-Learning by SignSGD},
  author={Fan, Chen and Ram, Parikshit and Liu, Sijia},
  journal={arXiv preprint arXiv:2109.07497},
  year={2021}
}

@inproceedings{xu2019signprox,
  title={Signprox: One-bit proximal algorithm for nonconvex stochastic optimization},
  author={Xu, Xiaojian and Kamilov, Ulugbek S},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7800--7804},
  year={2019},
  organization={IEEE}
}

@inproceedings{wang2019signadam++,
  title={signADAM++: Learning Confidences for Deep Neural Networks},
  author={Wang, Dong and Liu, Yicheng and Tang, Wenwo and Shang, Fanhua and Liu, Hongying and Sun, Qigong and Jiao, Licheng},
  booktitle={2019 International Conference on Data Mining Workshops (ICDMW)},
  pages={186--195},
  year={2019},
  organization={IEEE}
}

@inproceedings{alacaoglu2020new,
  title={A new regret analysis for Adam-type algorithms},
  author={Alacaoglu, Ahmet and Malitsky, Yura and Mertikopoulos, Panayotis and Cevher, Volkan},
  booktitle={International Conference on Machine Learning},
  pages={202--210},
  year={2020},
  organization={PMLR}
}

@article{chen2019zo,
  title={Zo-adamm: Zeroth-order adaptive momentum method for black-box optimization},
  author={Chen, Xiangyi and Liu, Sijia and Xu, Kaidi and Li, Xingguo and Lin, Xue and Hong, Mingyi and Cox, David},
  journal={arXiv preprint arXiv:1910.06513},
  year={2019}
}

@article{huang2018nostalgic,
  title={Nostalgic adam: Weighting more of the past gradients when designing the adaptive learning rate},
  author={Huang, Haiwen and Wang, Chang and Dong, Bin},
  journal={arXiv preprint arXiv:1805.07557},
  year={2018}
}

@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of adam and rmsprop},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11127--11135},
  year={2019}
}

@article{lu2020mixml,
  title={Mixml: A unified analysis of weakly consistent parallel learning},
  author={Lu, Yucheng and Nash, Jack and De Sa, Christopher},
  journal={arXiv preprint arXiv:2005.06706},
  year={2020}
}

@article{fang2019convergence,
  title={Convergence analyses of online adam algorithm in convex setting and two-layer relu neural network},
  author={Fang, Biyi and Klabjan, Diego},
  journal={arXiv preprint arXiv:1905.09356},
  year={2019}
}

@article{wang2019sadam,
  title={Sadam: A variant of adam for strongly convex functions},
  author={Wang, Guanghui and Lu, Shiyin and Tu, Weiwei and Zhang, Lijun},
  journal={arXiv preprint arXiv:1905.02957},
  year={2019}
}

@article{defossez2020simple,
  title={A Simple Convergence Proof of Adam and Adagrad},
  author={D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal={arXiv preprint arXiv:2003.02395},
  year={2020}
}

@inproceedings{tang2019doublesqueeze,
  title={Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression},
  author={Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={6155--6165},
  year={2019},
  organization={PMLR}
}

@article{chen2021convergence,
  title={On the Convergence of Decentralized Adaptive Gradient Methods},
  author={Chen, Xiangyi and Karimi, Belhal and Zhao, Weijie and Li, Ping},
  journal={arXiv preprint arXiv:2109.03194},
  year={2021}
}

@inproceedings{yu2019linear,
  title={On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization},
  author={Yu, Hao and Jin, Rong and Yang, Sen},
  booktitle={International Conference on Machine Learning},
  pages={7184--7193},
  year={2019},
  organization={PMLR}
}

@article{ortiz2021trade,
  title={Trade-offs of Local SGD at Scale: An Empirical Study},
  author={Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Rabbat, Mike and Morcos, Ari and Ballas, Nicolas},
  journal={arXiv preprint arXiv:2110.08133},
  year={2021}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{basu2020qsparse,
  title={Qsparse-Local-SGD: Distributed SGD With Quantization, Sparsification, and Local Computations},
  author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas N},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={217--226},
  year={2020},
  publisher={IEEE}
}

@misc{pytorchTorchvisionmodelsx2014,
	author = {Pytorch},
	title = {Torchvision 0.11.0 documentation --- pytorch.org},
	howpublished = {\url{https://pytorch.org/vision/stable/models.html}},
	year = {2014},
	note = {},
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{radford2019better,
  title={Better language models and their implications},
  author={Radford, Alec and Wu, Jeffrey and Amodei, Dario and Amodei, Daniela and Clark, Jack and Brundage, Miles and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  pages={2},
  year={2019}
}

@article{gpt-2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{gpt-3,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  journal   = {CoRR},
  volume    = {abs/2005.14165},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.14165},
  archivePrefix = {arXiv},
  eprint    = {2005.14165},
  timestamp = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{trinh2018simple,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

@article{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{li2021curriculum,
  title={Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training},
  author={Li, Conglong and Zhang, Minjia and He, Yuxiong},
  journal={arXiv preprint arXiv:2108.06084},
  year={2021}
}

@article{zhou2018adashift,
  title={Adashift: Decorrelation and convergence of adaptive learning rate methods},
  author={Zhou, Zhiming and Zhang, Qingru and Lu, Guansong and Wang, Hongwei and Zhang, Weinan and Yu, Yong},
  journal={arXiv preprint arXiv:1810.00143},
  year={2018}
}

@article{zhuang2021momentum,
  title={Momentum Centering and Asynchronous Update for Adaptive Gradient Methods},
  author={Zhuang, Juntang and Ding, Yifan and Tang, Tommy and Dvornek, Nicha and Tatikonda, Sekhar C and Duncan, James},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zhuang2020adabelief,
  title={Adabelief optimizer: Adapting stepsizes by the belief in observed gradients},
  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar C and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18795--18806},
  year={2020}
}

@article{mt-nlg,
  author    = {Shaden Smith and
               Mostofa Patwary and
               Brandon Norick and
               Patrick LeGresley and
               Samyam Rajbhandari and
               Jared Casper and
               Zhun Liu and
               Shrimai Prabhumoye and
               George Zerveas and
               Vijay Korthikanti and
               Elton Zheng and
               Rewon Child and
               Reza Yazdani Aminabadi and
               Julie Bernauer and
               Xia Song and
               Mohammad Shoeybi and
               Yuxiong He and
               Michael Houston and
               Saurabh Tiwary and
               Bryan Catanzaro},
  title     = {Using DeepSpeed and Megatron to Train Megatron-Turing {NLG} 530B,
               {A} Large-Scale Generative Language Model},
  journal   = {CoRR},
  volume    = {abs/2201.11990},
  year      = {2022},
  Xurl       = {https://arxiv.org/abs/2201.11990},
  eprinttype = {arXiv},
  eprint    = {2201.11990},
  timestamp = {Wed, 02 Feb 2022 15:00:01 +0100},
  Xbiburl    = {https://dblp.org/rec/journals/corr/abs-2201-11990.bib},
  Xbibsource = {dblp computer science bibliography, https://dblp.org}
}