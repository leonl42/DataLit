\section{Related works on debiasing}
\label{sec:related}

Addressing the issue of biased data and how it affects neural networks generalization has been the subject of numerous works.
Back in 2011, Torralba~and~Efros~\cite{torralba2011unbiased} showed that many of the most commonly used datasets are affected by biases. In their work, they evaluate the cross-dataset generalization capabilities based on different criteria, showing how data collection could be improved. 
With a similar goal, Tommasi~\emph{et~al.}~\cite{tommasi2017deeper} propose different benchmarks for cross-dataset analysis, aimed at verifying how different debiasing methods affect the final performances.
Data collection should be carried out with great care, in order not to include unwanted biases. Leveraging data already publicly available could be another way of tackling the issue. Gupta~\emph{et~al.}~\cite{gupta2018robot} explore the possibility of reducing biases by exploiting different data sources, in the practical context of sensors-collected data. They propose a strategy to minimize the effects of imperfect execution and calibration errors, showing improvements in the generalization capability of the final model. 
Khosla~\emph{et al.}~\cite{Khosla2012UndoingTD} employ max-margin learning (SVM) to explicitly model dataset bias for different vision datasets.
Another issue related to the unwanted learning of spurious correlations in the data has been highlighted by recent works. For example, Song~\emph{et~al.} \cite{song2017machine} and Barbano~\emph{et~al.}~\cite{barbano2021bridging} show how traditional training approaches allows information not relevant to the learning task to be stored inside the network. The experiments carried out in these works show how accurately some side information can be recovered, resulting in a potential lack of privacy.
Beutel~\emph{et~at.}~\cite{beutel2019putting} provide insights on algorithmic fairness in a production setting, and propose a metric named \emph{conditional equality}. They also propose a method, absolute correlation regularization, for optimizing this metric during training.
Another possibility of addressing these issues on a data level is to employ generative models, such as GANs~\cite{goodfellow2014generative}, to clean-up the dataset with the aim of providing fairness~\cite{xu2018fairgan, sattigeri2018fairness}.
Mandras~\emph{et~al.}~\cite{madras2018learning} also employ a GAN to obtain fair representations.

All of the above mentioned approaches generally deal directly at the data level, and provide useful insights for designing more effective debiasing techniques. 
In the related literature, we can most often find debiasing approaches based on ensembling methods, adversarial setups or regularization terms which aim at obtaining an \emph{unbiased} model using \emph{biased} data. We distinguish three different classes of approaches, in order of complexity: those which need full explicit supervision on the bias features (e.g. using bias labels), those which do not need explicit bias labels but leverage some prior knowledge of the bias features, those which no dot need neither supervision nor prior-knowledge.

\subsection{Supervised approaches}
Among the relevant related works, the most common debiasing techniques are supervised, meaning that they require explicit bias knowledge in form of labels. The most common approach is to use an additional bias-capturing model, with the task of specifically capturing bias features. This bias-capturing model is then leveraged, either in an adversarial or collaborative fashion, to enforce the selection of unbiased features on the main model. 
We can find the typical supervised adversarial approach in the work by Alvi~\emph{et~al.}: BlindEye~\cite{alvi2018turning}. They employ an explicit bias classifier, which is trained on the same representation space as the target classifier, using a min-max optimization approach. In this way, the shared encoder is forced to extract unbiased representations.
Similarly, Kim~\emph{et~al.}~\cite{Kim_2019_CVPR} propose Learning Not to Learn (LNL), which leverages adversarial learning and gradient inversion to reach the same goal.
Adversarial approaches can be found in many other works, for example in the work by Wang~\emph{et al.}~\cite{wang2019iccv}, where they show that biases can be learned even when using balanced datasets, and they adopt an adversarial approach to remove unwanted features from intermediate representations of a neural network. 
Also, Xie~\emph{et al.}~\cite{Xie2017ControllableIT} propose an adversarial framework for learning invariant representations with respect to some attribute in the data, similarly to~\cite{alvi2018turning}.
Moving away from adversarial approaches, Wang~\emph{et al.}~\cite{wang2020fair} perform a thorough review of the related literature, and propose a technique based on an ensemble of classifiers trained on a shared feature space.
A similar approach is followed by Clark~\emph{et~al.} with LearnedMixin~\cite{ClarkYZ19}. They train a biased model with explicit supervision on the bias labels, and then they build a robust model forcing its prediction to be made on different features.
Another possibility is represented by the application of adjusted loss functions or regularization terms.
For example, Sagawa~\emph{et~al.} propose Group-DRO~\cite{sagawa2019distributionally}, which aims at improving the model performance on the \emph{worst-group} in the training set, defined based on prior knowledge of the bias distribution.
EnD~\cite{tartaglione2021end}, which will be presented in detail in Section~\ref{sec:supervised-end}, also belongs to this class of approaches. 

\subsection{Prior-guided approaches}
In many real-world cases, explicit bias labels are not available. However, it might still be possible to make some assumptions or have some prior knowledge about the nature of the bias.
Bahng~\emph{et~al.}~\cite{bahng2019rebias} propose ReBias, an ensembling-based technique. Similarly to the work presented earlier, they build a bias-capturing model (an ensemble in this case). The prior knowledge about the bias is used in designing the bias-capturing architecture (e.g. by using a smaller receptive fields for texture and color biases). The optimization process, consists in solving a min-max problem with the aim of promoting independence between the biased representations and the unbiased ones.
A similar assumption for building the bias-capturing model is made by Cadene~\emph{et~al.} with RUBi~\cite{cadene2019rubi}. In this work, logits re-weighting is used to promote independence of the predictions on the bias features.
Borrowing from domain generalization techniques, another kind of approach aiming at learning robust representation is proposed by  Wang~\emph{et~al.} with HEX~\cite{wang2018learning}. They propose a differentiable neural-network-based gray-level co-occurrence matrix~\cite{haralick1973textural, lam1996glcm}, to extract biased textural information, which are then employed for learning invariant representations.
A different context is presented by Hendricks~\emph{et~al.}~\cite{hendricks2018women}. They propose an Equalizer model and a loss formulation which explicitly takes into account gender bias in image captioning models. In this work, the prior is given by annotation masks indicating which features in an image are appropriate for determining gender.
Related to this approach, another possibility is to constrain the model prediction to match some prior annotation of the input, as done in the work of Ross~\emph{et~al.}~\cite{Ross2017RightFT}, where gradients re-weighting is used to encourage the model to focus on the right input regions. Similarly, Selvajaru~\emph{et~al.}~\cite{Selvaraju_2019_ICCV} propose HINT, which optimizes the alignment between account manual visual annotation and gradient-based importance masks, such as Grad-CAM~\cite{selvaraju2017grad}.

\subsection{Unsupervised approaches}
Increasing in complexity, we consider as unsupervised approaches those methods which do not \emph{i)} require explicit bias information \emph{ii)} use prior knowledge to design specific architectures. 
In this setting, building a bias-capturing model is a more difficult task, as it should rely on more general assumptions. 
For example, Nam~\emph{et~al.} propose technique named Learning from Failure (LfF). They exploit the training dynamics: a bias-capturing model is trained with a focus on \emph{easier} samples, using the Generalized Cross-Entropy~\cite{zhang2018gce} (GCE) loss, which are assumed to be aligned with the bias, while a debiased network is trained emphasizing samples which the bias-capturing model struggles to learn. These assumptions they make are especially relevant for our work, as U-EnD also leverages the training dynamics for building the bias-capturing model. %
Similar assumptions are also made by Luo~\emph{et~al.}~\cite{luo2022pseudo} where GCE is also used for dealing with biases in a medical setting using Chest X-Ray images.
Ji~\emph{et al.}~\cite{ji2019invariant} propose an unsupervised clustering methods which is able to learn representations invariant to some unknown or ``distractor'' classes in the data, by employing over-clustering. Altough not strictly for debiasing purposes, another clustering-based technique is proposed by Gansbeke~\emph{et~al.}~\cite{van2020scan}: they employ a two-step approach for unsupervised learning of representations, where they mine the dataset to obtain pseudo-labels based on neighbours clusters. This approach is also closely related to our work.

