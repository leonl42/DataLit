\section{Complete derivations for Section~\ref{sec:bias-theoretical-model}}

In this section we present the full derivations of the theoretical results.

\subsection{Derivation of \eqref{eq:MI_perfect}}
\label{appendix:MI_perfect}
Let \eqref{eq:entropy_assumption}, we can write the conditional entropy
\begin{align}
    H_{perf}&(B|Y) = -\sum_{i=1}^{N_T} P(y_i) \sum_{j=1}^{N_T} P(b_j | y_i) \log_{2} \left [P(b_j | y_i) \right ]\nonumber\\
    =& -\frac{1}{N_T} \sum_{i=1}^{N_T} \sum_{j=1}^{N_T} P(b_j | y_i) \log_{2} [P(b_j | y_i)]\nonumber\\
    =& -\frac{1}{N_T}  \sum_{i=1}^{N_T} \sum_{j=1}^{N_T} \left\{ [\rho \log_{2}(\rho)] \delta_{ij} +\right .\nonumber\\
    &\left . +\left[\frac{1-\rho}{N_T-1} \log_{2}\left(\frac{1-\rho}{N_T-1}\right)\right]\overline{\delta}_{ij} \right\}\nonumber\\
    =& -\frac{1}{N_T} \left\{ N_T\cdot [\rho \log_{2}(\rho)] +\right .\nonumber\\
    &\left . +(N_T^2 - N_T)\cdot\left[\frac{1-\rho}{N_T-1}\cdot \log_{2}\left(\frac{1-\rho}{N_T - 1}\right)\right]\right\}\nonumber\\
    =& - \rho \cdot \log_{2}(\rho) +\nonumber\\
    & - \left\{(1-\rho)\cdot [\log_{2}(1-\rho) - \log_{2}(N_T-1)]\right\}\nonumber\\
    =& - \rho \cdot \log_{2}(\rho) - \log_{2}(1-\rho) +\log_{2}(N_T-1) +\nonumber\\
    & + \rho \cdot \log_{2}(1-\rho) - \rho \cdot\log_{2}(N_T-1)\nonumber\\
    =& \rho \cdot \log_{2}\left[\frac{1-\rho}{(N_T-1) \cdot\rho}\right] + \log_{2}\left[\frac{N_T-1}{1-\rho}\right]\nonumber\\
    =& \log_{2}\left[\frac{1-\rho}{N_T-1}\right]^{\rho-1} + \rho \log_2 \rho\nonumber\\
    =& -\log_{2}\left[\frac{1-\rho}{N_T-1}\right]^{1-\rho} + \log_2 \rho\cdot \rho^{\rho-1}\nonumber\\
    =& -\log_{2}\rho\left[\frac{1-\rho}{\rho(N_T-1)}\right]^{1-\rho}
\end{align}

Once we have the conditional entropy, we can compute the mutual information

\begin{align}
    I_{perf}(B, Y) &= H(B) - H_{perf}(B|Y)\nonumber\\
    &= \log_2 N_T +\log_{2}\rho\left[\frac{1-\rho}{\rho(N_T-1)}\right]^{1-\rho}
\end{align}

from which, exploiting \eqref{eq:entropy_assumption}, we can obtained the normalized mutual information
\begin{align}
    \hat{I}_{perf}(B,Y) =& \frac{2\cdot I_{perf}(B, Y)}{H(B) + H(Y)}\nonumber\\
    =&\frac{2\cdot\left\{ \log_2 N_T +\log_{2}\rho\left[\frac{1-\rho}{\rho(N_T-1)}\right]^{1-\rho}\right\}}{2\cdot \log_2 N_T}\nonumber\\
    =& 1 + \log_{N_T}\left\{\rho\left[\frac{1-\rho}{\rho(N_T-1)}\right]^{1-\rho}\right\}\nonumber\\
    =& \log_{N_T}\left\{N_T\rho\left[\frac{1-\rho}{\rho(N_T-1)}\right]^{1-\rho}\right\}\nonumber
\end{align}

\subsection{Derivation of \eqref{eq:MIcool}}
\label{appendix:MI}
Let the marginal as in \eqref{eq:joint_bz}. Following the definition of mutual information, we can write
\begin{align}
    I&(B,Y) = \sum_{i,j} P(b_j,y_i) \log_2\frac{P(b_j,y_i)}{P(b_j)P(y_i)}\nonumber\\
    =&\frac{1}{N_T} \left\{ N_T \cdot\left[\rho (1 - \varepsilon) + \phi(1-\rho) \right] \cdot \right.\nonumber \\
    &\cdot \log_2 (N_T \cdot(\rho (1 - \varepsilon) + \phi(1-\rho))) +\nonumber \\
    &+ (N_T^2 - N_T) \cdot \left [\frac{(1-\phi)(1-\rho)}{N_T-1} + \frac{\rho\varepsilon}{N_T-2+\rho}\right ]\cdot\nonumber\\
    &\left .\cdot \log_2 \left[ N_T \cdot \left(\frac{(1-\phi)(1-\rho)}{N_T-1} + \frac{\rho\varepsilon}{N_T-2+\rho}\right)\right ] \right\}\nonumber\\
    =&\left[\rho (1 - \varepsilon) + \phi(1-\rho) \right] \cdot \nonumber\\
    &\cdot \left[\log_2 N_T + \log_2(\rho (1 - \varepsilon) + \phi(1-\rho)\right] + \nonumber\\
    &+ \left [(1-\phi)(1-\rho) + \frac{(N_T - 1)\rho\varepsilon}{N_T-2+\rho}\right ]\cdot\nonumber\\
    &\cdot \left[ \log_2 N_T + \log_2 \left(\frac{(1-\phi)(1-\rho)}{N_T-1} + \frac{\rho\varepsilon}{N_T-2+\rho}\right)\right ]\nonumber\\
    =&\log_2\left[\rho (1 - \varepsilon) + \phi(1-\rho)\right]^{\rho (1 - \varepsilon) + \phi(1-\rho)} +\nonumber\\
    &+ \log_2 \left(\frac{(1-\phi)(1-\rho)}{N_T-1} + \frac{\rho\varepsilon}{N_T-2+\rho}\right)^{(1-\phi)(1-\rho) + \frac{(N_T - 1)\rho\varepsilon}{N_T-2+\rho}}+\nonumber\\
    &+\log_2 N_T \left[\rho (1 - \varepsilon) + \phi(1-\rho) + (1-\phi)(1-\rho) + \frac{(N_T - 1)\rho\varepsilon}{N_T-2+\rho}\right ]\nonumber\\
    =&\log_2\left[\rho (1 - \varepsilon) + \phi(1-\rho)\right]^{\rho (1 - \varepsilon) + \phi(1-\rho)} + \nonumber\\
    &+ \log_2 \left(\frac{(1-\phi)(1-\rho)}{N_T-1} + \frac{\rho\varepsilon}{N_T-2+\rho}\right)^{(1-\phi)(1-\rho) + \frac{(N_T - 1)\rho\varepsilon}{N_T-2+\rho}}+\nonumber\\
    &+\log_2 N_T \left[1-\rho\varepsilon + \frac{(N_T - 1)\rho\varepsilon}{N_T-2+\rho}\right ]\label{eq:IBZA}\\
\end{align}
Similarly to how been done in Sec.\ref{appendix:MI_perfect}, we can obtain the normalized mutual information
\begin{align}
    \hat{I}&(B,Y) = \frac{2\cdot I(B, Y)}{H(B) + H(Y)} = \frac{I(B, Y)}{\log_2 N_T}\nonumber\\
    =& \log_{N_T}\left[\rho (1 - \varepsilon) + \phi(1-\rho)\right]^{\rho (1 - \varepsilon)+ \phi(1-\rho)}\nonumber\\
    &+ \log_{N_T} \left [\frac{(1-\phi)(1-\rho)}{N_T-1} + \frac{\rho\varepsilon}{N_T-2+\rho}\right ]^{(1-\phi)(1-\rho) + \frac{(N_T-1)\rho\varepsilon}{N_T-2+\rho}}\nonumber\\
    &+ 1 - \rho\varepsilon + \frac{(N_T-1)\rho\varepsilon}{N_T-2-\rho}\nonumber
\end{align}
which in this case results in a simple change of base for the logarithms in \eqref{eq:IBZA}.

\subsection{Derivation of biasness}
\label{appendix:v-derivation}
Using the theoretical model presented in Section~\ref{sec:theoretical-model}, we can compute $\phi$ by rewriting of Eq.~\ref{eq:joint_bz} in order to derive it. First, for easier readibility, we explicitly enumerate the cases given by the kronecker deltas of Eq.~\ref{eq:joint_bz}:
\begin{equation}
    P(b, y) = 
\begin{cases}
    \frac{1}{N_T} \left[\rho (1 - \varepsilon) + \phi(1-\rho) \right] \quad \text{if } b=y \\[2ex]
	\frac{1}{N_T}\left [\frac{(1-\phi)(1-\rho)}{N_T-1} + \frac{\rho\varepsilon}{N_T-2+\rho} \right]  \quad \text{if } b \neq y
\end{cases}
\end{equation}
From this, we can rewrite each case in order to obtain the bias tendency $\phi_{b,y}$ for each $b$ and $y$:
\begin{equation}
    \phi_{b,y} = 
\begin{cases}
     P(b, y)\frac{N_T}{1 - \rho} - \frac{\rho(1-\varepsilon)}{1 - \rho} \quad \text{if } b = y \\[2ex]
     1 - P(b,y)\frac{N_T^2-N_T}{1-\rho} + \varepsilon\frac{\rho(N_T-1)}{(1-\rho)(N_T-2-\rho)} \quad \text{if } b \neq y
\end{cases}
\end{equation}
Note that, by doing so, we are actually computing multiple $\phi$ values, as a function of $b$ and $y$, while in Eq.~\ref{eq:joint_bz} we assume the values to be constant (i.e. the bias tendency is the same independently from the value of $b$ and $y$ considered). 
In our experiments, to account for the assumptions we make ($\varepsilon = 0$) and for measurements errors due to the stochastic nature of the training, we clip the measured joint probability between $[\frac{\rho}{N_T}; \frac{1}{N_T}]$ for the diagonal values ($b=y$), and between $[0; \frac{1-\rho}{N_T(N_T-1)}]$ for the off-diagonal values ($b \neq y$). This ensures that the computed $\phi_{b,y}$ lies between the valid range $[0; 1]$.
Finally, we can compute the global $\phi$ by averaging all the $\phi_{b,y}$:
\begin{equation}
\phi = \frac{1}{N_B N_T}\sum_{b,y} \phi_{b,y}
\end{equation}
where $N_B$ is the number of bias classes.
 


