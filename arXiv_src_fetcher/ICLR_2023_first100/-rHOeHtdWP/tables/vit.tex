\begin{table*}[!h]
	\caption{
		Performance of the original Pyramid Vision Transformer (PVT) and its wider alternatives on the CIFAR10 dataset.
		The original model (PVTV2-B1) has 4 stages, each stage contains two attention layers.
		((1, 1), (2, 2), (5, 5), (8, 8)) describes the original model,
		for instance, the first block contains two layers with a single head each, represented as (1, 1).}
	\centering
	\begin{tabular}{c|ccc}
	\toprule
	\textbf{Name}
	& \textbf{Configuration}
	& \textbf{Accuracy}
	& \textbf{Parameters} \\
	\midrule
	Baseline
	& $((1, 1), (2, 2), (5, 5), (8, 8))$
	& $95.59 \pm 0.99$	
	& $13.5$M	 \\
	Wide
	& $((2), (4), (10), (16))$
	& $94.54 \pm 0.31$	
	& $7.7$M	 \\
	Wide-V2
	& $((4), (8), (20), (32))$
	& $94.94 \pm 0.20$	
	& $12.6$M	 \\
	\bottomrule
	\end{tabular}
	\label{tab:vit}
\end{table*}
