\section{Conclusion}


When fixing the size of the attention computation, in general wider Tranformers can sometimes outperform, or else match, their deeper counterparts on NLP based tasks.
Averaged across all tasks and attention mechanisms, Wide models achieved $+0.3\%$ accuracy compared to Deep models.
On most tasks the performance of Wide and Deep was similar, with Listops being the exception ($+1.5\%$).
On average going wide works well for almost all attention mechanisms.
Sinkhorn benefits the most from being wide, whereas Longformer loses performance.

Furthermore, wider networks are much smaller, only 71\% the size of our deepest models on average.
They also have additional advantages in interpretability and inference latency: $3.1 \times$ faster on CPU and $1.9 \times$ faster on GPU for IMDb byte level text classification.
On image based tasks however, deeper Tranformers are still superior due to pooling layers being able to capture more spatial information.

Whilst our study considers the effects of attention width across many attention mechanisms and various tasks, it remains to be seen whether these results hold for much larger models on much larger datasets, for example masked langauage modelling.
We therefore put forward wider and shallower models as a \emph{viable and desirable alternative} for small models on NLP tasks, and as an \emph{important area of research} for domains beyond this.

