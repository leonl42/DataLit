\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{sparse_tfm}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock \emph{arXiv preprint arXiv:2009.14794}, 2020.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Gilpin et~al.(2018)Gilpin, Bau, Yuan, Bajwa, Specter, and
  Kagal]{interpret1}
Leilani~H Gilpin, David Bau, Ben~Z Yuan, Ayesha Bajwa, Michael Specter, and
  Lalana Kagal.
\newblock Explaining explanations: An overview of interpretability of machine
  learning.
\newblock In \emph{2018 IEEE 5th International Conference on data science and
  advanced analytics (DSAA)}, pp.\  80--89. IEEE, 2018.

\bibitem[Guo et~al.(2019)Guo, Zheng, Tan, Chen, Chen, Zhao, and
  Huang]{guo2019nat}
Yong Guo, Yin Zheng, Mingkui Tan, Qi~Chen, Jian Chen, Peilin Zhao, and Junzhou
  Huang.
\newblock Nat: Neural architecture transformer for accurate and compact
  architectures.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hu et~al.(2021)Hu, Wang, Ma, Meng, Li, Xiao, Zhu, and Li]{rank_nas}
Chi Hu, Chenglong Wang, Xiangnan Ma, Xia Meng, Yinqiao Li, Tong Xiao, Jingbo
  Zhu, and Changliang Li.
\newblock Ranknas: Efficient neural architecture search by pairwise ranking.
\newblock \emph{arXiv preprint arXiv:2109.07383}, 2021.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{linear_tfm}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5156--5165. PMLR, 2020.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Krizhevsky et~al.(2014)Krizhevsky, Nair, and
  Hinton]{krizhevsky2014cifar}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock The cifar-10 dataset.
\newblock \emph{online: http://www. cs. toronto. edu/kriz/cifar. html},
  55\penalty0 (5), 2014.

\bibitem[Linardatos et~al.(2020)Linardatos, Papastefanopoulos, and
  Kotsiantis]{interpret2}
Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis.
\newblock Explainable ai: A review of machine learning interpretability
  methods.
\newblock \emph{Entropy}, 23\penalty0 (1):\penalty0 18, 2020.

\bibitem[Liu et~al.(2018)Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer]{local_tfm}
Peter~J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz
  Kaiser, and Noam Shazeer.
\newblock Generating wikipedia by summarizing long sequences.
\newblock \emph{arXiv preprint arXiv:1801.10198}, 2018.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  10012--10022, 2021.

\bibitem[Liu et~al.(2022)Liu, Li, Lu, Qin, Sun, Xu, and
  Zhong]{tfm_nas_efficient}
Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran
  Zhong.
\newblock Neural architecture search on efficient transformers and beyond.
\newblock \emph{arXiv preprint arXiv:2207.13955}, 2022.

\bibitem[Qin et~al.(2022)Qin, Sun, Deng, Li, Wei, Lv, Yan, Kong, and
  Zhong]{cosformer}
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie
  Yan, Lingpeng Kong, and Yiran Zhong.
\newblock cosformer: Rethinking softmax in attention.
\newblock \emph{arXiv preprint arXiv:2202.08791}, 2022.

\bibitem[Radev et~al.(2013)Radev, Muthukrishnan, Qazvinian, and Abu-Jbara]{acl}
Dragomir~R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara.
\newblock The acl anthology network corpus.
\newblock \emph{Language Resources and Evaluation}, 47\penalty0 (4):\penalty0
  919--944, 2013.

\bibitem[Rigotti et~al.(2021)Rigotti, Miksovic, Giurgiu, Gschwind, and
  Scotton]{tfm_interpret}
Mattia Rigotti, Christoph Miksovic, Ioana Giurgiu, Thomas Gschwind, and Paolo
  Scotton.
\newblock Attention-based interpretability with concept transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[So et~al.(2019)So, Le, and Liang]{so2019evolved}
David So, Quoc Le, and Chen Liang.
\newblock The evolved transformer.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5877--5886. PMLR, 2019.

\bibitem[Talpes et~al.(2020)Talpes, Sarma, Venkataramanan, Bannon, McGee,
  Floering, Jalote, Hsiong, Arora, Gorti, et~al.]{talpes2020compute}
Emil Talpes, Debjit~Das Sarma, Ganesh Venkataramanan, Peter Bannon, Bill McGee,
  Benjamin Floering, Ankit Jalote, Christopher Hsiong, Sahil Arora, Atchyuth
  Gorti, et~al.
\newblock Compute solution for tesla's full self-driving computer.
\newblock \emph{IEEE Micro}, 40\penalty0 (2):\penalty0 25--35, 2020.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Bahri, Yang, Metzler, and
  Juan]{sinkhorn}
Yi~Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan.
\newblock Sparse sinkhorn attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9438--9447. PMLR, 2020{\natexlab{a}}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Dehghani, Abnar, Shen, Bahri, Pham,
  Rao, Yang, Ruder, and Metzler]{lra}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}, 2020{\natexlab{b}}.

\bibitem[Tay et~al.(2021)Tay, Bahri, Metzler, Juan, Zhao, and
  Zheng]{synthesizer}
Yi~Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng.
\newblock Synthesizer: Rethinking self-attention for transformer models.
\newblock In \emph{International conference on machine learning}, pp.\
  10183--10192. PMLR, 2021.

\bibitem[Tsai et~al.(2020)Tsai, Ooi, Ferng, Chung, and Riesa]{tfm_nas_one_shot}
Henry Tsai, Jayden Ooi, Chun-Sung Ferng, Hyung~Won Chung, and Jason Riesa.
\newblock Finding fast transformers: One-shot neural architecture search by
  component composition.
\newblock \emph{arXiv preprint arXiv:2008.06808}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{tfm}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Wu, Liu, Cai, Zhu, Gan, and
  Han]{wang2020hat}
Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and
  Song Han.
\newblock Hat: Hardware-aware transformers for efficient natural language
  processing.
\newblock \emph{arXiv preprint arXiv:2005.14187}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Li, Khabsa, Fang, and
  Ma]{linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Xie, Li, Fan, Song, Liang, Lu, Luo, and
  Shao]{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  568--578, 2021.

\bibitem[Wang et~al.(2022)Wang, Xie, Li, Fan, Song, Liang, Lu, Luo, and
  Shao]{wang2022pvt}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pvt v2: Improved baselines with pyramid vision transformer.
\newblock \emph{Computational Visual Media}, 8\penalty0 (3):\penalty0 415--424,
  2022.

\bibitem[Wu et~al.(2019)Wu, Shen, and Van Den~Hengel]{cnn_wide_v_deep}
Zifeng Wu, Chunhua Shen, and Anton Van Den~Hengel.
\newblock Wider or deeper: Revisiting the resnet model for visual recognition.
\newblock \emph{Pattern Recognition}, 90:\penalty0 119--133, 2019.

\bibitem[Xue et~al.(2022{\natexlab{a}})Xue, Chen, Sun, Ren, Zheng, He, Jiang,
  and You]{deeper_vs_wider}
Fuzhao Xue, Jianghai Chen, Aixin Sun, Xiaozhe Ren, Zangwei Zheng, Xiaoxin He,
  Xin Jiang, and Yang You.
\newblock Deeper vs wider: A revisit of transformer configuration.
\newblock \emph{arXiv preprint arXiv:2205.10505}, 2022{\natexlab{a}}.

\bibitem[Xue et~al.(2022{\natexlab{b}})Xue, Shi, Wei, Lou, Liu, and
  You]{go_wide}
Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You.
\newblock Go wider instead of deeper.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  8779--8787, 2022{\natexlab{b}}.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{cnn_wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{bigbird}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang,
  et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17283--17297, 2020.

\end{thebibliography}
