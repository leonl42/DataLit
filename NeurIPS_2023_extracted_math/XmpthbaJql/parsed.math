('2.0', '\\small\n\\label{eq:attn}\n    \\begin{aligned}\n        &\\operatorname{Attn}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V})=\\operatorname{softmax}\\left(\\frac{ \\boldsymbol{Q} \\boldsymbol{K}^{\\mathrm{T}}}{\\sqrt{d}}\\right)\\boldsymbol{V} \\\\\n        &\\operatorname{FFN}(\\boldsymbol{x})={\\phi}(\\boldsymbol{x}\\boldsymbol{W}_1+\\boldsymbol{b}_1)\\boldsymbol{W}_2+\\boldsymbol{b}_2\\ \n    \\end{aligned}\\ ,')
('2.0', '\\boldsymbol{Q}')
('2.0', '\\boldsymbol{K}')
('2.0', '\\boldsymbol{V}')
('2.0', '\\boldsymbol{W}_1')
('2.0', '\\boldsymbol{W}_2')
('2.0', '\\boldsymbol{b}_1')
('2.0', '\\boldsymbol{b}_2')
('2.0', '\\phi')
('2.0', '\\boldsymbol{x}')
('2.0', '\\boldsymbol{Q}=\\boldsymbol{x}\\boldsymbol{W}_{q}')
('2.0', '\\boldsymbol{K}=\\boldsymbol{x}\\boldsymbol{W}_{k}')
('2.0', '\\boldsymbol{V}=\\boldsymbol{x}\\boldsymbol{W}_{v}')
('2.0', '\\boldsymbol{W}_q')
('2.0', '\\boldsymbol{W}_k')
('2.0', '\\boldsymbol{W}_v')
('2.1', '\\label{eq:prefix}\n\\begin{array}{l}\n\\operatorname{MHA}_{\\text{pre}} = \\operatorname{Attn}(\\boldsymbol{x}\\boldsymbol{W}_{q}, [\\boldsymbol{K}_{pre}; \\boldsymbol{x}\\boldsymbol{W}_{k}], [\\boldsymbol{V}_{pre}; \\boldsymbol{x}\\boldsymbol{W}_{v}]),\n\\end{array}')
('2.1', '\\boldsymbol{K}_{pre}')
('2.1', '\\boldsymbol{V}_{pre}')
('2.1', '(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V})')
('2.1', '(\\boldsymbol{Q},\\boldsymbol{K}_{pre},\\boldsymbol{V}_{pre})')
('2.1', '\\small\n\\label{eq:prefix_parallel}\n\\begin{array}{l}\n\\operatorname{MHA}_{\\text{pre}} = (1-\\lambda) \\underbrace{\\operatorname{Attn}\\left(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}\\right)}_{\\text {original attention }}+ \n\\lambda\\underbrace{\\operatorname{Attn}\\left(\\boldsymbol{Q}, \\boldsymbol{K}_{pre}, \\boldsymbol{V}_{pre}\\right)}_{\\text{prefix attention in parallel}},\n\\end{array}')
('2.1', '\\lambda')
('2.1', '\\lambda')
('2.1', '\\operatorname{Attn}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V})')
('2.1', '\\operatorname{Attn}(\\boldsymbol{Q}, \\boldsymbol{K}_{pre}, \\boldsymbol{V}_{pre})')
('2.1', '\\small\n\\label{eq:prompt}\n\\begin{array}{l}\n\\operatorname{MHA}_{\\text{pro}}  = \\operatorname{Attn}\\left([\\boldsymbol{x}; \\boldsymbol{x}_{pro} ] \\boldsymbol{W}_{q}, [\\boldsymbol{x}; \\boldsymbol{x}_{pro} ] \\boldsymbol{W}_{k}, [\\boldsymbol{x}; \\boldsymbol{x}_{pro} ] \\boldsymbol{W}_{v} \\right), \n\\end{array}')
('2.1', '\\boldsymbol{x}_{pro}')
('2.1', '\\boldsymbol{x}')
('2.1', '\\small\n\\label{eq:prompt_parallel}\n\\begin{array}{l}\n\\operatorname{MHA}_{\\text{pro}}\n=[ (1-\\lambda) \\underbrace{ \\operatorname{Attn}\\left(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}\\right) }_{\\text{original attention}}+ \n\\lambda \\underbrace{\\operatorname{Attn}\\left(\\boldsymbol{Q}, \\boldsymbol{K}_{pro}, \\boldsymbol{V}_{pro}\\right)}_{\\text{prompt attention in parallel}};\\boldsymbol{D}],\n\\end{array}')
('2.1', '\\boldsymbol{K}_{pro}=\\boldsymbol{x}_{pro}\\boldsymbol{W}_k')
('2.1', '\\boldsymbol{V}_{pro}=\\boldsymbol{x}_{pro}\\boldsymbol{W}_v')
('2.1', '\\boldsymbol{D}')
('2.1', '\\small\\boldsymbol{D}=(1-\\beta)\\operatorname{Attn}\\left(\\boldsymbol{Q}_{pro}, \\boldsymbol{K}_{pro}, \\boldsymbol{V}_{pro}\\right)+ \n\\beta\\operatorname{Attn}\\left(\\boldsymbol{Q}_{pro}, \\boldsymbol{K}, \\boldsymbol{V} \\right)')
('2.1', '\\lambda')
('2.1', '\\beta')
('2.1', '\\label{eq:adapter_parallel}\n\\operatorname{FFN}_{\\text{adapter}} = \\underbrace{ \\operatorname{FFN}(\\boldsymbol{x}) }_{\\text {original module}} + \n\\underbrace{ {\\phi}( \\operatorname{FFN}(\\boldsymbol{x}) \\boldsymbol{W}_{down}) \\boldsymbol{W}_{up} }_{\\text {adapter module in parallel} },')
('2.1', '\\boldsymbol{W}_{down}')
('2.1', '\\boldsymbol{W}_{up}')
('2.2', '\\pm')
('3.0', '\\label{eq:uni}\n\\boldsymbol{x}^{\\prime} = {\\color[RGB]{0,0,0}\\text{OP}}(\\boldsymbol{x})  + {\\color[RGB]{0,0,0}\\texttt{Res-Tuner}}(\\boldsymbol{x}) ,')
('3.0', '{\\color[RGB]{0,0,0}\\text{OP}}')
('3.1', '\\boldsymbol{x}_0')
('3.1', 'l')
('3.1', '\\boldsymbol{x}_l')
('3.1', '\\begin{aligned}\n    &\\boldsymbol{x}^{\\text{bypass}}_0 = \\boldsymbol{x}_0, \\\\\n    &\\boldsymbol{x}^{\\text{bypass}}_l = \\lambda{\\color[RGB]{0,0,0}\\texttt{Res-Tuner}}(\\boldsymbol{x}_{l}) + (1-\\lambda){\\color[RGB]{0,0,0}\\texttt{Res-Tuner}}(\\boldsymbol{x}^{\\text{bypass}}_{l-1}), l\\geq1 ,\n\\end{aligned}\\')
('3.1', '\\lambda')
('3.1', 'l')
('3.1', '(l-1)')
('4.2', '^{\\dagger}')
('4.2', '\\dagger')
('5.0', '512^{2}')
('8', '\\label{eq:prefix_detaied_derivation}\n\\begin{array}{l}\n\\operatorname{MHA}_{\\text{pre}} = \\operatorname{Attn}(\\boldsymbol{x}\\boldsymbol{W}_{q}, [\\boldsymbol{K}_{pre}; \\boldsymbol{x}\\boldsymbol{W}_{k}], [\\boldsymbol{V}_{pre}; \\boldsymbol{x}\\boldsymbol{W}_{v}]) \\\\ \\\\\n% \\text { head }=\\operatorname{Attn}\\left(\\boldsymbol{x} \\boldsymbol{W}_{q}, \\operatorname{concat}\\left(\\boldsymbol{K}_{pre}, \\boldsymbol{x} \\boldsymbol{W}_{k}\\right), \\operatorname{concat}\\left(\\boldsymbol{V}_{pre}, \\boldsymbol{x} \\boldsymbol{W}_{v}\\right)\\right) \\\\\n=\\operatorname{softmax}\\left(\\boldsymbol{x} \\boldsymbol{W}_{q} [\\boldsymbol{K}_{pre}; \\boldsymbol{x} \\boldsymbol{W}_{k}]^{\\top}\\right)\\left[\\begin{array}{c}\n\\boldsymbol{V}_{pre} \\\\ \\\\\n\\boldsymbol{x} \\boldsymbol{W}_{v}\n\\end{array}\\right] \\\\  \\\\\n=(1-\\lambda(\\boldsymbol{x})) \\operatorname{softmax}\\left(\\boldsymbol{x} \\boldsymbol{W}_{q} \\boldsymbol{W}_{k}^{\\top} \\boldsymbol{x}^{\\top}\\right) \\boldsymbol{x} \\boldsymbol{W}_{v}+\\lambda(\\boldsymbol{x}) \\operatorname{softmax}\\left(x \\boldsymbol{W}_{q} \\boldsymbol{K}_{pre}^{\\top}\\right) \\boldsymbol{V}_{pre} \\\\ \\\\\n=(1-\\lambda(\\boldsymbol{x})) \\operatorname{Attn}\\left(\\boldsymbol{x} \\boldsymbol{W}_{q}, \\boldsymbol{x} \\boldsymbol{W}_{k}, \\boldsymbol{x} \\boldsymbol{W}_{v}\\right)+\\lambda(\\boldsymbol{x}) \\operatorname{Attn}\\left(\\boldsymbol{x} \\boldsymbol{W}_{q}, \\boldsymbol{K}_{pre}, \\boldsymbol{V}_{pre}\\right) \\\\ \\\\\n=(1-\\lambda({\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{K}_{pre}})) \\underbrace{\\operatorname{Attn}\\left(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}\\right)}_{\\text {standard attention }}+\\lambda(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{K}_{pre}) \\underbrace{\\operatorname{Attn}\\left(\\boldsymbol{Q}, \\boldsymbol{K}_{pre}, \\boldsymbol{V}_{pre}\\right)}_{\\text {independent of } \\boldsymbol{K}_{pre}, \\boldsymbol{V}_{pre}}\n\\end{array}')
('8', '\\boldsymbol{Q}')
('8', '\\boldsymbol{K}')
('8', '\\boldsymbol{K}_{pre}')
('8', '\\boldsymbol{V}_{pre}')
('8', '\\lambda(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{K}_{pre}) =\\frac{\\sum_{i} \\exp \\left( \\boldsymbol{Q} \\boldsymbol{K}_{pre}^{\\top} \\right)_{i}}{\\sum_{i} \\exp \\left(\\boldsymbol{QK^{\\top}}\\right)_{i}+\\sum_{j} \\exp \\left(\\boldsymbol{Q} \\boldsymbol{K}_{pre}^{\\top}\\right)_{j}},')
('8', '\\label{eq:prompt_detaied_derivation}\n\\begin{array}{l}\n\\operatorname{MHA}_{\\text{pro}}  = \\operatorname{Attn}\\left([\\boldsymbol{x}; \\boldsymbol{x}_{pro} ] \\boldsymbol{W}_{q}, [\\boldsymbol{x}; \\boldsymbol{x}_{pro} ] \\boldsymbol{W}_{k}, [\\boldsymbol{x}; \\boldsymbol{x}_{pro} ] \\boldsymbol{W}_{v} \\right) \\\\ \\\\\n% \\text { head }=\\operatorname{Attn}\\left(\\operatorname{concat}\\left(\\boldsymbol{x}, \\boldsymbol{x}_{pro} \\right) \\boldsymbol{W}_{q}, \\operatorname{concat}\\left(\\boldsymbol{x}, \\boldsymbol{x}_{pro} \\right) \\boldsymbol{W}_{k}, \\operatorname{concat}\\left(\\boldsymbol{x}, \\boldsymbol{x}_{pro} \\right) \\boldsymbol{W}_{v} \\right) \\vspace{3ex}\\\\\n=\\operatorname{concat}\\left( \\operatorname{softmax}\\left(\\boldsymbol{x} \\boldsymbol{W}_{q} [\\boldsymbol{x} \\boldsymbol{W}_{k}; \\boldsymbol {x}_{pro} \\boldsymbol{W}_{k} ]^{\\top}\\right)\\left[\\begin{array}{c} \\boldsymbol{x} \\boldsymbol{W}_{v} \\\\ \\boldsymbol{x}_{pro} \\boldsymbol{W}_{v} \\end{array}\\right], \\right. \\vspace{1.5ex}\\\\\n\\left. \\hspace{1.6cm}  \\operatorname{softmax}\\left(\\boldsymbol{x}_{pro} \\boldsymbol{W}_{q} [\\boldsymbol{x} \\boldsymbol{W}_{k}; \\boldsymbol{x}_{pro} \\boldsymbol{W}_{k} ]^{\\top}\\right)\\left[\\begin{array}{c} \\boldsymbol{x} \\boldsymbol{W}_{v} \\\\ \\boldsymbol{x}_{pro} \\boldsymbol{W}_{v} \\end{array}\\right] \\right)  \\vspace{3ex}\\\\\n=\\operatorname{concat}\\left(( 1-\\lambda({\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{K}_{pro}})) \\operatorname{Attn}\\left(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}\\right)+\\lambda(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{K}_{pro}) \\operatorname{Attn}\\left(\\boldsymbol{Q}, \\boldsymbol{K}_{pro}, \\boldsymbol{V}_{pro}\\right),\n\\right. \\vspace{2.5ex}\\\\\n\\left. \\hspace{1.6cm} (1-\\beta({\\boldsymbol{Q}_{pro}, \\boldsymbol{K}_{pro}, \\boldsymbol{K}})) \\operatorname{Attn}\\left(\\boldsymbol{Q}_{pro}, \\boldsymbol{K}_{pro}, \\boldsymbol{V}_{pro}\\right)\n\\right. \\vspace{2.5ex}\\\\\n\\left. \\hspace{1.6cm} +\\beta(\\boldsymbol{Q}_{pro}, \\boldsymbol{K}_{pro}, \\boldsymbol{K}) \\operatorname{Attn}\\left(\\boldsymbol{Q}_{pro}, \\boldsymbol{K}, \\boldsymbol{V} \\right)\n\\right) \\\\ \\\\\n\\end{array}')
('8', '\\boldsymbol{K}_{pro}')
('8', '\\boldsymbol{V}_{pro}')
('8', '\\lambda(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{K}_{pro}) =\\frac{\\sum_{i} \\exp \\left( \\boldsymbol{Q} \\boldsymbol{K}_{pro}^{\\top} \\right)_{i}}{\\sum_{i} \\exp \\left(\\boldsymbol{QK^{\\top}}\\right)_{i}+\\sum_{j} \\exp \\left(\\boldsymbol{Q} \\boldsymbol{K}_{pro}^{\\top}\\right)_{j}},')
('8', '\\beta(\\boldsymbol{Q}_{pro}, \\boldsymbol{K}_{pro}, \\boldsymbol{K}) =\\frac{\\sum_{i} \\exp \\left( \\boldsymbol{Q}_{pro} \\boldsymbol{K}^{\\top} \\right)_{i}}{\\sum_{i} \\exp \\left(\\boldsymbol{Q}_{pro} \\boldsymbol{K}_{pro}^{\\top}\\right)_{i}+\\sum_{j} \\exp \\left(\\boldsymbol{Q}_{pro} \\boldsymbol{K}^{\\top}\\right)_{j}},')
('10.0', '\\star')
('10.0', '^\\star')
('10.0', '^\\star')
('10.0', '^\\star')
('10.0', '^\\star')
('10.0', '^\\star')
('10.0', '^\\star')
('11.0', '\\dagger')
('11.0', '^{\\dagger}')
('11.0', '^{\\dagger}')
('11.0', '^{\\dagger}')
('11.0', '^{\\dagger}')
('11.1', '\\rightarrow')
('11.1', '\\rightarrow')
('11.1', '\\rightarrow')
('11.1', '\\rightarrow')
('11.1', '\\rightarrow')
