\times
\rightarrow
X\in\mathcal{R}^{M\times C}
M=11
Z\in\mathcal{R}^{N\times C}
N=3
N
L
X\in\mathcal{R}^{M\times C}
M=11
Z\in\mathcal{R}^{N\times C}
N=3
N
L
X\in\mathcal{R}^{M}
\displaystyle p(X)=\prod_{m=0}^{M-1}p\Big{(}X_{m}\Big{|}X_{<m}\Big{)}.
\displaystyle p(X)=\prod_{m=0}^{M-1}p\Big{(}X_{m}\Big{|}X_{<m}\Big{)}.
X_{m}
X
X
p(X_{m}|\ldots)
m
X\in\mathcal{R}^{M\times C}
C
Z_{1}\in\mathcal{R}^{N\times C}
\displaystyle Z_{1}\leftarrow\text{CrossAttend}(X,Z_{0})
\displaystyle Z_{1}\leftarrow\text{CrossAttend}(X,Z_{0})
Z_{1}
N<M
\displaystyle Z_{l+1}\leftarrow\text{SelfAttend}(Z_{l},Z_{l}).
\displaystyle Z_{l+1}\leftarrow\text{SelfAttend}(Z_{l},Z_{l}).
M
N
l\in[1,L]
\mathcal{O}(MN)+\mathcal{O}(LN^{2})
N=3
N=3
M
N
N
N=3
\mathcal{O}(LM^{2})
2^{17}
2^{17}-2
100\%
2^{6}
2^{8}
2^{16}
2^{8}
100\%
<1\%
\times
\times
\times
\times
\times
\times
R\to G\to B
\times
64\times 64\times 3
\times
64\times 64\times 3
\{1024,4096,8192,16384\}
\{1024,4096,8192,16384\}
\{23,24,25,28\}
\{62,61,60,56\}
O(n^{2})
1.0
\times
\times
\times
\{1024,4096,8192,16384\}
\{16,64,128,512,1024\}
\leq 128
X_{KV}\in\mathbb{R}^{M\times C}
X_{Q}\in\mathbb{R}^{N\times D}
C
D
\displaystyle Q=f_{Q}(X_{Q})\text{;}\;K=f_{K}(X_{KV})\text{;}\;V=f_{V}(X_{KV})
\displaystyle Q=f_{Q}(X_{Q})\text{;}\;K=f_{K}(X_{KV})\text{;}\;V=f_{V}(X_{KV})
\displaystyle X_{QK}^{\text{pre}}=QK^{T}
\displaystyle X_{QK}^{\text{pre}}=QK^{T}
\displaystyle X_{QK}=\text{softmax}(X_{QK}^{\text{pre}}/\sqrt{F})
\displaystyle X_{QK}=\text{softmax}(X_{QK}^{\text{pre}}/\sqrt{F})
\displaystyle\text{Attn}(X_{Q},X_{KV})=X_{QKV}=f_{O}({X_{QK}V}),
\displaystyle\text{Attn}(X_{Q},X_{KV})=X_{QKV}=f_{O}({X_{QK}V}),
X_{QK}^{\text{pre}}
X_{QK}
\in\mathbb{R}^{N\times M}
X_{QKV}
\in\mathbb{R}^{N\times D}
f_{\{Q,K,V\}}
F
f_{O}
D
X_{Q}
\displaystyle X_{QKV}=\text{Attn}(\text{layerNorm}(X_{Q}),\text{layerNorm}(X_{%
KV}))
\displaystyle X_{QKV}
\displaystyle=\text{Attn}(\text{layerNorm}(X_{Q}),\text{layerNorm}(X_{KV}))
\displaystyle X_{QKV}=X_{QKV}+X_{Q}
\displaystyle X_{QKV}
\displaystyle=X_{QKV}+X_{Q}
\displaystyle X_{QKV}=X_{QKV}+\text{MLP}(\text{layerNorm}(X_{QKV})),
\displaystyle X_{QKV}
\displaystyle=X_{QKV}+\text{MLP}(\text{layerNorm}(X_{QKV})),
\text{CrossAttend}:X_{KV}\times X_{Q}\rightarrow X_{Q}
\text{SelfAttend}:X_{KV}\times X_{Q}\rightarrow X_{Q}
X_{Q}\neq X_{KV}
N<M
X_{Q}=X_{KV}
X_{Q}
N
X_{Q}=X_{KV}[-N{:},:]
X_{QK}^{\text{pre}}
(m^{\prime},m)
\times
m^{\prime},m\in[0,M)
m>m^{\prime}
X_{QK}^{\text{pre}}
(n,m)
\times
n\in[0,N),m\in[0,M)
m>n+M-N-1
X_{QK}^{\text{pre}}
-\infty
\text{CrossAttend}_{\text{cm}}
\text{SelfAttend}_{\text{cm}}
Z_{0}
Z_{-1}
\displaystyle Z_{0}\leftarrow\text{CrossAttend}_{\text{cm}}(X,X[-N{:},:])
\displaystyle Z_{0}\leftarrow\text{CrossAttend}_{\text{cm}}(X,X[-N{:},:])
\displaystyle Z_{l+1}\leftarrow\text{SelfAttend}_{\text{cm}}(Z_{l},Z_{l}),
\displaystyle Z_{l+1}\leftarrow\text{SelfAttend}_{\text{cm}}(Z_{l},Z_{l}),
{Z_{L}}
O(N^{2})
N=4
M=8
N=4
M=8
[\mathrm{heads},\mathrm{input\_length},\mathrm{self\_attention\_length}]
b1=0.1
b2=0.999
eps=1\mathrm{e}{-8}
3\mathrm{e}{-4}
1.0
z\_loss*log(z)^{2}
z\_loss=1\mathrm{e}{-4}
0.1
\times
\times
3\mathrm{e}{-4}
p{=}0.15
1600+1024=2624
0.7
2\mathrm{e}{-4}
1\mathrm{e}{-4}
0.1
0.25
\{0,0.5,0.75\}
\{12,24,48\}
\{0,0.5,0.75\}
\{12,24,48\}
\sim
\{0,0.5,0.75\}
\{12,24,48\}
\{0,0.5,0.75\}
\{12,24,48\}
