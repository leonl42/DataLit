\mathcal{S}
\mathcal{A}
T
r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}
\tau_{B}
\tau_{A}
P(\tau_{A}\prec\tau_{B})=\frac{\exp(r(\tau_{B}))}{\exp(r(\tau_{A}))+\exp(r(%
\tau_{B}))},
r(\tau)=\sum_{(s,a)\in\tau}r(s,a)
\tau=(s_{0},a_{0},\ldots,s_{T},a_{T})
\mathcal{P}
\tau_{1},\ldots,\tau_{N}
(i,j)\in\mathcal{P}
\tau_{i}\prec\tau_{j}
r_{\theta}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}
\theta
\mathcal{L}(\theta)=\prod_{(i,j)\in\mathcal{P}}\frac{\exp(r_{\theta}(\tau_{j})%
)}{\exp(r_{\theta}(\tau_{i}))+\exp(r_{\theta}(\tau_{j}))}.
\hat{r}
(\tau_{i},\tau_{j})
\eta
z
u
\hat{r}
(\tau_{i},\tau_{j})
\eta
z
u
r
x_{t}^{i}
r(x)
z_{t}^{i}
r(x)
z_{t}^{i}
x_{t}^{i}
\hat{r}(x,z)
u
\eta
\hat{r}
r
\hat{r}([x,z])
r
r
r
r_{\theta}
\mathcal{P}
r
96.6\%
\beta
\gamma\in(0,1]
P(\tau_{A}\prec\tau_{B})=\frac{\exp(\beta\sum_{t=1}^{H}\gamma^{H-t}r(s_{t}^{B}%
,a_{t}^{B}))}{\exp(\beta\sum_{t=1}^{H}\gamma^{H-t}r(s_{t}^{A},a_{t}^{A}))+\exp%
(\beta\sum_{t=1}^{H}\gamma^{H-t}r(s_{t}^{B},a_{t}^{B}))}.
\beta=1
\gamma=0.99
\epsilon=0.1
\beta=\infty
\delta
F_{max}
\mathcal{P}
\tau_{1},\ldots,\tau_{N}
(i,j)\in\mathcal{P}
\tau_{i}\prec\tau_{j}
r_{\theta}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}
\theta
\mathcal{L}(\theta)=\prod_{(i,j)\in\mathcal{P}}\frac{\exp(r_{\theta}(\tau_{j})%
)}{\exp(r_{\theta}(\tau_{i}))+\exp(r_{\theta}(\tau_{j}))}.
r_{\theta}
r_{\theta}
d_{train}
\hat{r}\leftarrow
epoch\in(0,100)
traj_{i},traj_{j},label\in d_{train}
\hat{r}_{i}\leftarrow\hat{r}(traj_{i})
\hat{r}_{j}\leftarrow\hat{r}(traj_{j})
loss\leftarrow CrossEntropyLoss(\hat{r}_{i},\hat{r}_{j},label)
loss.backward()
loss_{val}\leftarrow
loss_{val}
\hat{\pi}\leftarrow
\hat{r}
\hat{r},\hat{\pi}
\epsilon
\epsilon
\epsilon=0
\epsilon=1
\epsilon
\epsilon\in\{0,0.2,0.4,0.6,0.8,1\}
x
y
x
y
\displaystyle S(x)=-100\sqrt{(position_{x}^{2}+position_{y}^{2})}
\displaystyle S(x)
\displaystyle=-100\sqrt{(position_{x}^{2}+position_{y}^{2})}
\displaystyle+-100\sqrt{(velocity_{x}^{2}+velocity_{y}^{2})}
\displaystyle+-100\sqrt{(velocity_{x}^{2}+velocity_{y}^{2})}
\displaystyle+-100|angle|
\displaystyle+-100|angle|
\displaystyle+10(contact_{left})+10(contact_{right})
\displaystyle+10(contact_{left})+10(contact_{right})
\displaystyle R(x)=S(x)-S_{prev}(x)
\displaystyle R(x)
\displaystyle=S(x)-S_{prev}(x)
\displaystyle R(x)=R(x)-0.3(action)
\displaystyle R(x)
\displaystyle=R(x)-0.3(action)
R(x)=-||position_{fingertip}-position_{target}||_{2}+-||action||_{2}^{2}
\displaystyle r_{distance}=-||pos_{target}-pos_{spoon}||_{2}
\displaystyle r_{distance}
\displaystyle=-||pos_{target}-pos_{spoon}||_{2}
\displaystyle r_{action}=-||action||_{2}
\displaystyle r_{action}
\displaystyle=-||action||_{2}
\displaystyle r_{food}=f(Foods\_in\_mouth,Foods\_on\_floor)
\displaystyle r_{food}
\displaystyle=f(Foods\_in\_mouth,Foods\_on\_floor)
\displaystyle preferences\_score=g(Foods\_hit\_human,Sum\_food\_mouth\_velocities,
\displaystyle preferences\_score
\displaystyle=g(Foods\_hit\_human,Sum\_food\_mouth\_velocities,
\displaystyle Spoon\_pos\_real,Prev\_spoon\_pos\_real,Robot\_force\_on\_human)
\displaystyle Spoon\_pos\_real,Prev\_spoon\_pos\_real,Robot\_force\_on\_human)
\displaystyle R(x)=W_{distance}*r_{distance}+W_{action}*r_{action}+W_{food}*r_%
{food}+preferences\_score
\displaystyle R(x)
\displaystyle=W_{distance}*r_{distance}+W_{action}*r_{action}+W_{food}*r_{food%
}+preferences\_score
\displaystyle r_{distance}=-||pos_{target}-pos_{spoon}||_{2}
\displaystyle r_{distance}
\displaystyle=-||pos_{target}-pos_{spoon}||_{2}
\displaystyle r_{action}=-||action||_{2}
\displaystyle r_{action}
\displaystyle=-||action||_{2}
\displaystyle r_{scratch}=f(Tool\_pos\_real,Target\_pos\_real,Prev\_tool\_pos%
\_real,
\displaystyle r_{scratch}
\displaystyle=f(Tool\_pos\_real,Target\_pos\_real,Prev\_tool\_pos\_real,
\displaystyle Tool\_force,Prev\_tool\_force)
\displaystyle Tool\_force,Prev\_tool\_force)
\displaystyle preferences\_score=g(Spoon\_pos\_real,Prev\_spoon\_pos\_real,
\displaystyle preferences\_score
\displaystyle=g(Spoon\_pos\_real,Prev\_spoon\_pos\_real,
\displaystyle Robot\_force\_on\_human,Tool\_force,Target\_pos\_real)
\displaystyle Robot\_force\_on\_human,Tool\_force,Target\_pos\_real)
\displaystyle R(x)=W_{distance}*r_{distance}+W_{action}*r_{action}+W_{scratch}%
*r_{food}+preferences\_score
\displaystyle R(x)
\displaystyle=W_{distance}*r_{distance}+W_{action}*r_{action}+W_{scratch}*r_{%
food}+preferences\_score
\frac{\partial R}{\partial(s,a)}
D_{KL}(p||q)
p
q
D_{KL}(q||p)
D_{KL}(p||q)+D_{KL}(q||p)
{324\choose 2}=52326
{40\choose 2}=780
{324\choose 2}=52326
{40\choose 2}=780
\delta
F_{max}
\lambda
\lambda={0,0.01,0.1,1,10}
