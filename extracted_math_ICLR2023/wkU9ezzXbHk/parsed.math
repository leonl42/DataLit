4-7
7,500
52,500
1,250
(95.08\%)
(64.82\%)
100\%
4-7
(x_{t},x_{a},x_{v},q,c)
(x_{t},x_{a},x_{v})
\hat{u}
\hat{u},\hat{q},\hat{c}
h
\hat{h}
4
5
(x_{t},x_{a},x_{v},q,c)
(x_{t},x_{a},x_{v})
\hat{u}
\hat{u},\hat{q},\hat{c}
h
\hat{h}
4
5
x_{t}
x_{a}
x_{v}
\hat{h}
(\big{\{}u_{p}^{t}\big{\}}_{t=1}^{t=T},\big{\{}u_{q}^{t}\big{\}}_{t=1}^{t=T})
S
u_{p}
u_{q}
(i,j)
S
S_{i,j}=\texttt{ReLU}(w_{p}u_{p}^{i})\cdot\texttt{ReLU}(w_{q}u_{q}^{j})
S
\hat{u}_{p}^{t}
\hat{u}_{q}^{t}
w_{p}
w_{q}
\hat{u}_{p}^{i}
u_{q}^{t}
u_{p}^{i}
\hat{u}_{q}^{j}
u_{p}^{t}
u_{q}^{j}
\displaystyle\alpha_{i,:}=\texttt{softmax}(S_{i,:}),
\displaystyle\alpha_{i,:}
\displaystyle=\texttt{softmax}(S_{i,:}),
\displaystyle\hat{u_{p}^{i}}=\sum_{j}\alpha_{i,j}\cdot u_{q}^{j}
\displaystyle\hat{u_{p}^{i}}
\displaystyle=\sum_{j}\alpha_{i,j}\cdot u_{q}^{j}
\displaystyle\alpha_{:,j}=\texttt{softmax}(S_{:,j})
\displaystyle\alpha_{:,j}
\displaystyle=\texttt{softmax}(S_{:,j})
\displaystyle\hat{u_{q}^{j}}=\sum_{i}\alpha_{i,j}\cdot u_{p}^{i}
\displaystyle\hat{u_{q}^{j}}
\displaystyle=\sum_{i}\alpha_{i,j}\cdot u_{p}^{i}
u_{pq}=[\hat{u}_{p},\hat{u}_{q}]
\alpha_{i,:}
u_{q}^{t}
u_{p}^{i}
u_{p}^{i}
\alpha_{:,j}
u_{q}^{t}
u_{pq}
\hat{u_{p}^{t}}
\hat{u_{q}^{t}}
u_{p}^{t}
u_{q}^{t}
u_{pq}
u_{p}
u_{q}
x_{t}
x_{a}
u_{ta}
x_{t}
x_{a}
x_{v}
\hat{x}_{t}
\hat{x}_{a}
\hat{x}_{v}
200
100
250
\hat{q}
\hat{c}
\displaystyle\hat{x}_{t}=\texttt{BiLSTM}(x_{t}),\quad\hat{x}_{a}=\texttt{%
BiLSTM}(x_{a}),
\displaystyle\hat{x}_{t}=\texttt{BiLSTM}
\displaystyle(x_{t}),\quad\hat{x}_{a}=\texttt{BiLSTM}(x_{a}),
\displaystyle\hat{x}_{v}=\texttt{BiLSTM}(x_{v}),
\displaystyle\hat{x}_{v}=\texttt{BiLSTM}(x_{v}),
\displaystyle\hat{q}=\texttt{BiLSTM}(q),\quad\hat{c}=\texttt{BiLSTM}(c)
\displaystyle\hat{q}=\texttt{BiLSTM}
\displaystyle(q),\quad\hat{c}=\texttt{BiLSTM}(c)
(\hat{x}_{t},\hat{x}_{a},\hat{x}_{v})
(u_{ta},u_{av},u_{vt})
\hat{u}
\displaystyle u_{ta}=\texttt{Co-attention}(\hat{x}_{t},\hat{x}_{a})
\displaystyle u_{ta}
\displaystyle=\texttt{Co-attention}(\hat{x}_{t},\hat{x}_{a})
\displaystyle u_{av}=\texttt{Co-attention}(\hat{x}_{a},\hat{x}_{v})
\displaystyle u_{av}
\displaystyle=\texttt{Co-attention}(\hat{x}_{a},\hat{x}_{v})
\displaystyle u_{vt}=\texttt{Co-attention}(\hat{x}_{v},\hat{x}_{t})
\displaystyle u_{vt}
\displaystyle=\texttt{Co-attention}(\hat{x}_{v},\hat{x}_{t})
\hat{u}=\texttt{BiLSTM}([u_{ta},u_{av},u_{vt},\hat{x}_{t},\hat{x}_{a},\hat{x}_%
{v}])
v_{uq}
v_{uc}
\displaystyle v_{uq}=\texttt{Co-attention}(\hat{u},\hat{q}),
\displaystyle v_{uq}
\displaystyle=\texttt{Co-attention}(\hat{u},\hat{q}),
\displaystyle v_{uc}=\texttt{Co-attention}(\hat{u},\hat{c}),
\displaystyle v_{uc}
\displaystyle=\texttt{Co-attention}(\hat{u},\hat{c}),
\displaystyle h=\texttt{BiLSTM}([v_{uq},v_{uc},\hat{q},\hat{c},\hat{u}]),
\displaystyle h
\displaystyle=\texttt{BiLSTM}([v_{uq},v_{uc},\hat{q},\hat{c},\hat{u}]),
\displaystyle\beta=\texttt{Softmax}(w_{r}h),
\displaystyle\beta
\displaystyle=\texttt{Softmax}(w_{r}h),
\displaystyle\hat{h}=\sum_{k}\beta_{k}\cdot h^{k}
\displaystyle\hat{h}
\displaystyle=\sum_{k}\beta_{k}\cdot h^{k}
v_{u,q}
v_{u,c}
\hat{q}
\hat{c}
\hat{u}
\hat{h}
h
w_{r}
4-7\%
4-7\%
4-7
66.9
