T
X\in
{\mathbb{R}}^{T\times C\times W\times H}
T
\cdots
\sim
\%
L\times 64
L
N
N
N\times L\times 2L
\mathcal{X}
F_{f}\in
{\mathbb{R}}^{L\times H\times W}
C_{r}\in
{\mathbb{R}}^{L\times L\times 2)}
F_{f}
\mathcal{M}\in
{\mathbb{R}}^{L\times 2L}
\mathcal{D}
F_{f}
\mathcal{X}
T
F_{f}
L\times W\times H
H
W
99.9\%
1.0\backslash 2
1.0\backslash 3
1.0\backslash 4
1.0\backslash 5
99.9\%
\mathcal{M}
F_{f}
C_{b}:\text{GCN}(C_{r})
C_{r}\in
{\mathbb{R}}^{L\times 64}
L
64
L\times L\times 2
C_{b}
F_{f}
C_{b}
F_{f}
L
1\times 2L
\mathcal{M}
\bar{\mathcal{M}}
\text{Softmax}(d_{i}^{y_{i}})=\frac{\exp^{d_{i}^{y_{i}}}}{\sum_{c=1}^{C}\exp^{%
d_{c}^{y_{c}}}},
d_{i}^{y_{i}}
C
\widehat{\mathbf{d}_{i}}=
\left\{\widehat{d_{i}^{y_{1}}},\widehat{d_{i}^{y_{2}}},\ldots,\widehat{d_{i}^{%
y_{c}}}\right\}
\text{Lnf}(\widehat{d_{i}^{y_{i}}})=\frac{d_{i}^{y_{i}}+|\mathcal{D}_{\text{%
min}}|}{\sum_{c=1}^{C}(d_{c}^{y_{c}}+|\mathcal{D}_{\text{min}}|)},
|\mathcal{D}_{\text{min}}|
\times
\displaystyle\tilde{x}=\lambda x_{i}+(1-\lambda)x_{j},\quad\text{ where }x_{i}%
,x_{j}\text{ are raw input vectors }
\displaystyle\tilde{x}=\lambda x_{i}+(1-\lambda)x_{j},\quad\text{ where }x_{i}%
,x_{j}\text{ are raw input vectors }
\displaystyle\tilde{y}=\lambda y_{i}+(1-\lambda)y_{j},\quad\text{ where }y_{i}%
,y_{j}\text{ are label distribution vectors }
\displaystyle\tilde{y}=\lambda y_{i}+(1-\lambda)y_{j},\quad\text{ where }y_{i}%
,y_{j}\text{ are label distribution vectors }
\left(x_{i},y_{i}\right)
\left(x_{j},y_{j}\right)
\lambda\in[0,1]
\displaystyle\tilde{x}=\lambda x_{i}\times\text{mask}+(1-\lambda)x_{j}\times%
\text{mask},\quad\text{ where }x_{i},x_{j}\text{ are raw input vectors }
\displaystyle\tilde{x}=\lambda x_{i}\times\text{mask}+(1-\lambda)x_{j}\times%
\text{mask},\quad\text{ where }x_{i},x_{j}\text{ are raw input vectors }
\displaystyle\tilde{y}=\text{Lnf}(\lambda y_{i}+(1-\lambda)y_{j}),\quad\text{ %
where }y_{i},y_{j}\text{ are label distribution vectors }
\displaystyle\tilde{y}=\text{Lnf}(\lambda y_{i}+(1-\lambda)y_{j}),\quad\text{ %
where }y_{i},y_{j}\text{ are label distribution vectors }
L_{1}
\mathcal{L}=\frac{1}{N}\sum_{i=1}^{N}\left\|\widehat{d_{i}}-d_{i}\right\|+%
\lambda\mathcal{L}_{kl}+\beta\sum_{i=1}^{N}\sum_{i=1}^{L}\mathcal{L}_{2}(%
\mathcal{M}_{ii},\bar{\mathcal{M}_{ii}}),
N
d_{i}
\widehat{d}
\lambda
\mathcal{L}_{kl}
\beta
L_{1}
\mathcal{L}=\frac{1}{N}\sum_{i=1}^{N}\left\|\widehat{d_{i}}-d_{i}\right\|+%
\lambda_{1}\mathcal{L}_{kl}+\lambda_{2}\mathcal{L}_{p}+\beta\sum_{i=1}^{N}\sum%
_{i=1}^{L}\mathcal{L}_{2}(\mathcal{M}_{ii},\bar{\mathcal{M}_{ii}}),
\mathcal{L}_{p}
\lambda_{1}
\lambda_{2}
\beta
\downarrow
\downarrow
\downarrow
\downarrow
\uparrow
\uparrow
\downarrow
\uparrow
\mathscr{L}_{1,2:end}
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\mathscr{L}_{1,2:end}
\rightarrow
\lambda
\beta
10^{\{-6,-5,\ldots,-2,-1\}}
10^{\{-3,-2,\ldots,1,2\}}
\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4}
k
0.0001,0.001,0.001,0.001
\lambda_{1},\lambda_{2}
\lambda_{3}
10^{\{-6,-5,\ldots,-2,-1\}}
\rho
10^{-3}
C
\lambda
200
0.2
2\times 10^{-3}
1\times 10^{-4}
2\times 10^{-2}
1\times 10^{-4}
1\times 10^{-3}
1\times 10^{-4}
1\times 10^{-3}
1\times 10^{-4}
2\times 10^{-3}
1\times 10^{-4}
2\times 10^{-3}
1\times 10^{-4}
2\times 10^{-3}
1\times 10^{-4}
1\times 10^{-2}
1\times 10^{-4}
2\times 10^{-2}
1\times 10^{-4}
1\times 10^{-3}
1\times 10^{-4}
1\times 10^{-3}
1\times 10^{-4}
1\times 10^{-2}
1\times 10^{-4}
\downarrow
\downarrow
\downarrow
\downarrow
\uparrow
\uparrow
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\downarrow
\downarrow
\downarrow
\downarrow
\uparrow
\uparrow
\pm
\pm
\pm
\pm
\pm
\pm
\mathcal{L}_{p}
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\pm
\mathcal{L}_{p}
\times
1\times 10^{-3}
28
224
28
224
1\times 10^{-2}
\pm
\pm
\pm
\pm
\pm
\pm
\downarrow
\downarrow
\downarrow
\downarrow
\uparrow
\uparrow
\pm
\pm
\pm
\pm
\pm
\pm
\lambda_{2}
\epsilon
\downarrow
\epsilon
5\times 10
