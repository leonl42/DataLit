('2', '(\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\mathcal{R}, \\Omega, \\mathcal{O})')
('2', '\\mathcal{S}')
('2', '\\mathcal{A}')
('2', '\\Omega')
('2', '\\mathcal{T}')
('2', "\\mathcal{T}(s, a, s') = P(s'|s, a)")
('2', 's')
('2', "s'")
('2', 'a')
('2', '\\mathcal{R}')
('2', '\\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}')
('2', 's\\in\\mathcal{S}')
('2', 'a\\in\\mathcal{A}')
('2', '\\mathcal{O}')
('2', "\\mathcal{O}(s', a, o) = P(o|s', a)")
('2', 'o')
('2', 'a')
('2', "s'")
('2', 's_t\\in \\mathcal{S}')
('2', 'a_t\\in \\mathcal{A}')
('2', 's_{t+1}\\in \\mathcal{S}')
('2', '\\mathcal{T}(s_t, a_t, s_{t+1})')
('2', 'r_t = \\mathcal{R}(s_t, a_t)')
('2', '\\mathbb{E}\\big[\\sum_t \\gamma^t r_t\\big]')
('2', '\\gamma \\in [0, 1)')
('2', 'o_t \\in \\Omega')
('2', '\\mathcal{O}(s_{t+1}, a_t, o_t) = P(o_t|s_{t+1}, a_t)')
('2', 'h_t = \\{(o_0, a_0), (o_1, a_1), ..., (o_{t-1}, a_{t-1})\\}')
('2.0', 'Q: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}')
('2.0', 's')
('2.0', 'a')
('2.0', 'r')
('2.0', "s'")
('2.0', '\\alpha')
('2.0', 'Q')
('2.0', "Q(s, a) := Q(s, a) + \\alpha (r + \\max_{a'\\in \\mathcal{A}}Q(s', a') - Q(s, a))")
('2.0', 'Q')
('2.0', 'Q')
('2.0', "\\label{MSBE}\n    L(\\theta) = \\mathbb{E}_{(s, a, r, s')\\sim\\mathcal{D}}\\big[\\big(r + \\max_{a'\\in \\mathcal{A}} Q(s',a';\\theta') - Q(s, a; \\theta)\\big)^2\\big]")
('2.0', "(s, a, r, s')")
('2.0', 'D')
('2.0', "r + \\max_{a'\\in \\mathcal{A}} Q(s',a';\\theta')")
('2.0', "\\theta'")
('2.0', '\\theta')
('2.0', 'Q')
('2.0', 'h_{t:t+k} = \\{o_t, o_{t+1}, ..., o_{t+k}\\}')
('2.0', 't')
('2.0', 't+k')
('2.1', 'ith')
('2.1', '(B, C, D)')
('2.1', 'B')
('2.1', 'C')
('2.1', 'D')
('3', 'E^0 = \\text{Embedding}(h_{t:t+k}) + Pos')
('3', 'Q^{L-1} = E^{L-1}W^Q_{L-1}, K^{L-1} = E^{L-1}W^K_{L-1}, V^{L-1} = E^{L-1}W^V_{L-1}')
('3', 'E^L = \\text{LayerNorm}^L_1(\\text{Combine}^L_1(\\text{MultiHeadAttention}^{L}(Q^{L-1}, K^{L-1}, V^{L-1}), E^{L-1}))')
('3', 'E^L \\gets \\text{LayerNorm}^L_2(\\text{Combine}^L_2(\\text{FFN}^{L}(E^{L}), E^{L}))')
('3', 'Output \\gets \\text{FFN}^{N}(E^{N})')
('3', '(h_{t:t+k}, a_{t:t+k}, r_{t:t+k}, h_{t+1:t+k+1})')
('3', 'D')
('3', "L_i(\\theta) = \\mathbb{E}_{(.) \\sim D}\\big[\\big(r_{t+i-1} + \\max_{a'\\in \\mathcal{A}}Q(h_{t+1:t+i+1}, a'; \\theta') - Q(h_{t:t+i}, a_{t+i-1}; \\theta)\\big)^2\\big]")
('3', '\\theta \\gets \\theta - \\alpha \\nabla_\\theta \\sum_{i=1}^{k} L_i(\\theta)')
('4', 'k')
('4', 'h_{t:t+k} = \\{o_{t}, o_{t+1}, ..., o_{t+k-1}\\}')
('4', 'N')
('4.0', 'k')
('4.0', 'h_{t:t+k}')
('4.0', 'E^0')
('4.1', 'E^{L-1}')
('4.1', 'Q')
('4.1', 'K')
('4.1', 'V')
('4.1', 'W^Q')
('4.1', 'W^K')
('4.1', 'W^V')
('4.1', 'E^N')
('5.0', '2\\times3')
('5.3', '75.2 \\pm 7.2')
('5.3', '89.8 \\pm 1.9')
('5.3', '98.3 \\pm 1.0')
('5.3', '80.3 \\pm 6.4')
('5.3', '90.8 \\pm 3.0')
('5.3', '67.5 \\pm 10')
('5.3', '78.3 \\pm 4.4')
('5.3', '88.9 \\pm 1.2')
('5.3', '100 \\pm 0')
('5.3', '65.3 \\pm 6.7')
('5.3', '88.5 \\pm 1.8')
('5.3', '69.8 \\pm 10')
('5.3', '75.2 \\pm 7.2')
('5.3', '89.8 \\pm 1.9')
('5.3', '98.3 \\pm 1.0')
('5.3', '83.1 \\pm 5.0')
('5.3', '85.5 \\pm 1.9')
('5.3', '92 \\pm 3.9')
('5.3', '85.5 \\pm 3.7')
('5.3', '70.8 \\pm 2.8')
('5.3', '99 \\pm 0.4')
('5.3', '75.2 \\pm 7.2')
('5.3', '89.8 \\pm 1.9')
('5.3', '98.3 \\pm 1.0')
('5.3', '56.27 \\pm 14')
('5.3', '9.0 \\pm 2.2')
('5.3', '92.8 \\pm 0.7')
('8.0', 'k')
('8.0', 'k')
('8.0', '\\epsilon')
('8.0', '\\epsilon')
('8.0', '(1 - \\epsilon)')
('8.0', '\\epsilon')
('8.0', '1.0')
('8.0', '0.1')
('8.0', '10\\%')
('8.0', '0.1')
('8.1', 'd_{model}')
('8.1', '64')
('8.1', '128')
('8.1', '_{model}')
('8.1', '^{-4}')
('8.2', '\\{o_i\\}')
('8.2', 'W^Q')
('8.2', 'W^K')
('8.2', 'W^V')
('8.2', 'Q')
('8.2', 'K')
('8.2', 'V')
('8.2', '\\begin{aligned}\n\\label{eq:attention}\n    H &= \\text{Embedding}(\\{o_i\\}) \\\\\n    Q = HW^Q, K &= HW^K, V = HW^V \\\\\n    \\text{Attention}(Q, K, V) &= \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \\\\\n\\end{aligned}')
('8.2', 'd_k')
('8.2', 'K')
('8.2', 'W^Q')
('8.2', 'W^K')
('8.2', 'W^V')
('8.2', '\\begin{aligned}\n    \\text{MultiHead}(Q, K, V) &= \\text{Concat}(head_1, ..., head_h)W^O \\\\\n    \\text{where head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\end{aligned}')
('8.2', 'W_i^Q')
('8.2', 'W_i^K')
('8.2', 'W_i^V')
('9.1', '2\\times3')
('12', '(i, j)')
('12', 'ith')
('12', 'jth')
